import os
import sys
import random
import logging
import tempfile
from datetime import datetime

import gradio as gr
import torch
import numpy as np
from PIL import Image

import wan
from wan.configs import MAX_AREA_CONFIGS, WAN_CONFIGS, SUPPORTED_SIZES
from wan.utils.utils import save_video
import imageio.v2 as imageio
from PIL import ImageDraw, ImageFont

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    handlers=[logging.StreamHandler(stream=sys.stdout)]
)

# å…¨å±€æ¨¡å‹å˜é‡
wan_i2v = None

# æ£€æŸ¥ç‚¹è·¯å¾„
CKPT_DIR = "checkpoints/lingbot-world-base-cam"

# ç¤ºä¾‹æ•°æ®
EXAMPLES = [
    {
        "name": "ç¤ºä¾‹ 00 - å¥‡å¹»ä¸›æ—é£è¡Œ",
        "image": "examples/00/image.jpg",
        "action_path": "examples/00",
        "prompt": "The video presents a soaring journey through a fantasy jungle. The wind whips past the rider's blue hands gripping the reins, causing the leather straps to vibrate. The ancient gothic castle approaches steadily, its stone details becoming clearer against the backdrop of floating islands and distant waterfalls."
    },
    {
        "name": "ç¤ºä¾‹ 01 - å·¨çŸ³é˜µå…¨æ™¯",
        "image": "examples/01/image.jpg",
        "action_path": "examples/01",
        "prompt": "A slow panoramic sweep around Stonehenge on a misty, overcast day, capturing the ancient standing stones in serene stillness, with soft ambient wind and distant bird calls enhancing the timeless atmosphere."
    },
    {
        "name": "ç¤ºä¾‹ 02 - åŸå¸‚æ¼«æ¸¸",
        "image": "examples/02/image.jpg",
        "action_path": "examples/02",
        "prompt": "The video presents a cinematic, first-person wandering experience through a hyper-realistic urban environment rendered in a video game engine. It begins with a static, sun-drenched alley framed by graffiti-laden industrial walls and overhead power lines, immediately establishing a gritty, lived-in atmosphere. As the camera pans right and tilts upward, it reveals a sprawling cityscape dominated by towering skyscrapers and industrial infrastructure, all bathed in warm, late-afternoon light that casts long shadows and produces dramatic lens flares. The perspective then transitions into a smooth forward tracking shot along a cracked sidewalk, passing weathered fences, palm trees, and distant pedestrians, creating a sense of immersion and exploration. Midway, the camera briefly follows a walking figure before refocusing on the broader streetscape, culminating in a stabilized view of a small blue van parked at an intersection surrounded by urban elements like parking garages and traffic lights. The entire sequence is characterized by its photorealistic detail, dynamic lighting, and deliberate pacing, evoking the feel of a quiet, sunlit afternoon in a futuristic metropolis."
    }
]


def load_model():
    """åŠ è½½æ¨¡å‹"""
    global wan_i2v
    if wan_i2v is not None:
        return "æ¨¡å‹å·²åŠ è½½"
    
    logging.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    cfg = WAN_CONFIGS["i2v-A14B"]
    
    wan_i2v = wan.WanI2V(
        config=cfg,
        checkpoint_dir=CKPT_DIR,
        device_id=0,
        rank=0,
        t5_fsdp=False,
        dit_fsdp=False,
        use_sp=False,
        t5_cpu=False,
        convert_model_dtype=False,
    )
    logging.info("æ¨¡å‹åŠ è½½å®Œæˆ")
    return "æ¨¡å‹åŠ è½½å®Œæˆ"


def _infer_wasd_from_poses(poses, frame_num):
    trans = poses[:, :3, 3]
    dirs = []
    for i in range(len(trans) - 1):
        world_delta = trans[i + 1] - trans[i]
        R = poses[i, :3, :3]
        local_delta = R.T.dot(world_delta)
        dx, dy, dz = local_delta
        if abs(dx) > abs(dz):
            dirs.append('D' if dx > 0 else 'A')
        else:
            dirs.append('W' if dz < 0 else 'S')
    if len(dirs) > 0:
        dirs.append(dirs[-1])
    if len(dirs) < int(frame_num):
        dirs.extend([dirs[-1] if dirs else ''] * (int(frame_num) - len(dirs)))
    return dirs


def _overlay_wasd_on_video(input_file, action_path, frame_num, fps):
    poses_path = os.path.join(action_path, "poses.npy")
    if not os.path.exists(poses_path):
        return input_file
    poses = np.load(poses_path)
    dirs = _infer_wasd_from_poses(poses, frame_num)

    overlay_file = input_file.replace('.mp4', '_wasd.mp4')
    reader = imageio.get_reader(input_file)
    writer = imageio.get_writer(overlay_file, fps=fps)
    try:
        font = ImageFont.load_default()
    except Exception:
        font = None
    for i, frame in enumerate(reader):
        img_pil = Image.fromarray(frame).convert("RGBA")
        draw = ImageDraw.Draw(img_pil)
        label = dirs[i] if i < len(dirs) else ''
        if label:
            text = f"{label}"
            bbox = draw.textbbox((0, 0), text, font=font)
            text_w = bbox[2] - bbox[0]
            text_h = bbox[3] - bbox[1]
            pad = 8
            rect_xy = (10, 10, 10 + text_w + pad, 10 + text_h + pad)
            draw.rectangle(rect_xy, fill=(0, 0, 0, 140))
            draw.text((14, 12), text, fill=(255, 235, 59, 255), font=font)
        writer.append_data(np.array(img_pil.convert("RGB")))
    writer.close()
    reader.close()
    return overlay_file


def generate_video(
    image,
    prompt,
    action_path,
    size,
    frame_num,
    sample_steps,
    sample_shift,
    guide_scale,
    seed,
    progress=gr.Progress()
):
    """ç”Ÿæˆè§†é¢‘"""
    global wan_i2v
    
    if wan_i2v is None:
        load_model()
    
    if image is None:
        return None, "è¯·ä¸Šä¼ å›¾ç‰‡"
    
    if not prompt:
        return None, "è¯·è¾“å…¥æç¤ºè¯"
    
    if not action_path or not os.path.exists(action_path):
        return None, "è¯·é€‰æ‹©æœ‰æ•ˆçš„ç›¸æœºè½¨è¿¹è·¯å¾„"
    
    # å¤„ç†ç§å­
    if seed < 0:
        seed = random.randint(0, sys.maxsize)
    
    logging.info(f"å¼€å§‹ç”Ÿæˆè§†é¢‘...")
    logging.info(f"æç¤ºè¯: {prompt}")
    logging.info(f"ç›¸æœºè½¨è¿¹: {action_path}")
    logging.info(f"å°ºå¯¸: {size}")
    logging.info(f"å¸§æ•°: {frame_num}")
    logging.info(f"ç§å­: {seed}")
    
    try:
        # è½¬æ¢å›¾ç‰‡
        if isinstance(image, str):
            img = Image.open(image).convert("RGB")
        else:
            img = Image.fromarray(image).convert("RGB")
        
        cfg = WAN_CONFIGS["i2v-A14B"]
        
        # ç”Ÿæˆè§†é¢‘
        video = wan_i2v.generate(
            prompt,
            img,
            action_path=action_path,
            max_area=MAX_AREA_CONFIGS[size],
            frame_num=int(frame_num),
            shift=float(sample_shift),
            sample_solver='unipc',
            sampling_steps=int(sample_steps),
            guide_scale=float(guide_scale),
            seed=int(seed),
            offload_model=True
        )
        
        # ä¿å­˜è§†é¢‘
        formatted_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(tempfile.gettempdir(), f"lingbot_world_{formatted_time}.mp4")
        
        save_video(
            tensor=video[None],
            save_file=output_file,
            fps=cfg.sample_fps,
            nrow=1,
            normalize=True,
            value_range=(-1, 1)
        )

        # åœ¨è§†é¢‘ä¸Šå åŠ  WASD é£æ ¼çš„æ–¹å‘æ ‡è®°ï¼ˆåŸºäº poses.npy æ¨æ–­ç§»åŠ¨æ–¹å‘ï¼‰
        try:
            final_output = _overlay_wasd_on_video(
                input_file=output_file,
                action_path=action_path,
                frame_num=int(frame_num),
                fps=cfg.sample_fps,
            )
        except Exception as e:
            logging.warning(f"å åŠ  WASD æ ‡è®°å¤±è´¥: {e}")
            final_output = output_file

        del video
        torch.cuda.empty_cache()

        logging.info(f"è§†é¢‘ç”Ÿæˆå®Œæˆ: {final_output}")
        return final_output, f"ç”ŸæˆæˆåŠŸï¼ç§å­: {seed}\nè¾“å‡ºæ–‡ä»¶: {final_output}"
    
    except Exception as e:
        logging.error(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
        return None, f"ç”Ÿæˆå¤±è´¥: {str(e)}"


def load_example(example_name):
    """åŠ è½½ç¤ºä¾‹"""
    for example in EXAMPLES:
        if example["name"] == example_name:
            return (
                example["image"],
                example["prompt"],
                example["action_path"]
            )
    return None, "", ""


def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(title="LingBot-World è§†é¢‘ç”Ÿæˆ", theme=gr.themes.Soft()) as demo:
        # é¡¶éƒ¨é¢‘é“ä¿¡æ¯
        gr.HTML("""
        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 10px; margin-bottom: 20px;">
            <h1 style="color: white; margin: 0;">ğŸ¬ LingBot-World è§†é¢‘ç”Ÿæˆ</h1>
            <p style="color: #f0f0f0; margin-top: 10px;">
                ğŸ“º æ¬¢è¿è®¿é—®æˆ‘çš„ YouTube é¢‘é“: 
                <a href="https://www.youtube.com/@rongyi-ai" target="_blank" style="color: #ffeb3b; font-weight: bold;">
                    AI æŠ€æœ¯åˆ†äº«é¢‘é“
                </a>
            </p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“¥ è¾“å…¥è®¾ç½®")
                
                # ç¤ºä¾‹é€‰æ‹©
                example_dropdown = gr.Dropdown(
                    choices=[e["name"] for e in EXAMPLES],
                    label="é€‰æ‹©ç¤ºä¾‹",
                    info="é€‰æ‹©é¢„è®¾ç¤ºä¾‹å¿«é€Ÿä½“éªŒ"
                )
                
                # å›¾ç‰‡ä¸Šä¼ 
                image_input = gr.Image(
                    label="è¾“å…¥å›¾ç‰‡",
                    type="filepath",
                    height=300
                )
                
                # æç¤ºè¯
                prompt_input = gr.Textbox(
                    label="æç¤ºè¯ (Prompt)",
                    placeholder="è¯·è¾“å…¥æè¿°è§†é¢‘å†…å®¹çš„æç¤ºè¯...",
                    lines=5
                )
                
                # ç›¸æœºè½¨è¿¹è·¯å¾„
                action_path_input = gr.Textbox(
                    label="ç›¸æœºè½¨è¿¹è·¯å¾„",
                    placeholder="ä¾‹å¦‚: examples/00",
                    info="åŒ…å« poses.npy å’Œ intrinsics.npy çš„ç›®å½•è·¯å¾„"
                )
                
            with gr.Column(scale=1):
                gr.Markdown("## âš™ï¸ ç”Ÿæˆå‚æ•°")
                
                # å°ºå¯¸é€‰æ‹©
                size_dropdown = gr.Dropdown(
                    choices=list(SUPPORTED_SIZES["i2v-A14B"]),
                    value="480*832",
                    label="è¾“å‡ºå°ºå¯¸",
                    info="è§†é¢‘åˆ†è¾¨ç‡ (å®½*é«˜)"
                )
                
                # å¸§æ•°
                frame_num_slider = gr.Slider(
                    minimum=17,
                    maximum=161,
                    step=4,
                    value=81,
                    label="å¸§æ•°",
                    info="ç”Ÿæˆçš„è§†é¢‘å¸§æ•°ï¼Œå¿…é¡»æ˜¯ 4n+1 æ ¼å¼"
                )
                
                # é‡‡æ ·æ­¥æ•°
                sample_steps_slider = gr.Slider(
                    minimum=10,
                    maximum=50,
                    step=1,
                    value=30,
                    label="é‡‡æ ·æ­¥æ•°",
                    info="æ›´å¤šæ­¥æ•°=æ›´é«˜è´¨é‡ï¼Œä½†æ›´æ…¢"
                )
                
                # Shift å‚æ•°
                shift_slider = gr.Slider(
                    minimum=1.0,
                    maximum=15.0,
                    step=0.5,
                    value=3.0,
                    label="Shift å‚æ•°",
                    info="é‡‡æ ·åç§»å› å­"
                )
                
                # å¼•å¯¼ç³»æ•°
                guide_scale_slider = gr.Slider(
                    minimum=1.0,
                    maximum=10.0,
                    step=0.5,
                    value=5.0,
                    label="å¼•å¯¼ç³»æ•° (CFG Scale)",
                    info="æ›´é«˜=æ›´éµå¾ªæç¤ºè¯"
                )
                
                # éšæœºç§å­
                seed_input = gr.Number(
                    value=42,
                    label="éšæœºç§å­",
                    info="è®¾ä¸º -1 ä½¿ç”¨éšæœºç§å­"
                )
                
                # ç”ŸæˆæŒ‰é’®
                generate_btn = gr.Button(
                    "ğŸš€ ç”Ÿæˆè§†é¢‘",
                    variant="primary",
                    size="lg"
                )
        
        gr.Markdown("## ğŸ“¤ è¾“å‡ºç»“æœ")
        
        with gr.Row():
            with gr.Column():
                video_output = gr.Video(
                    label="ç”Ÿæˆçš„è§†é¢‘",
                    height=400
                )
                status_output = gr.Textbox(
                    label="çŠ¶æ€ä¿¡æ¯",
                    interactive=False
                )
        
        # äº‹ä»¶ç»‘å®š
        example_dropdown.change(
            fn=load_example,
            inputs=[example_dropdown],
            outputs=[image_input, prompt_input, action_path_input]
        )
        
        generate_btn.click(
            fn=generate_video,
            inputs=[
                image_input,
                prompt_input,
                action_path_input,
                size_dropdown,
                frame_num_slider,
                sample_steps_slider,
                shift_slider,
                guide_scale_slider,
                seed_input
            ],
            outputs=[video_output, status_output]
        )
        
        # ä½¿ç”¨è¯´æ˜
        gr.Markdown("""
        ---
        ## ğŸ“– ä½¿ç”¨è¯´æ˜
        
        1. **é€‰æ‹©ç¤ºä¾‹**: ä»ä¸‹æ‹‰èœå•é€‰æ‹©é¢„è®¾ç¤ºä¾‹ï¼Œæˆ–æ‰‹åŠ¨ä¸Šä¼ å›¾ç‰‡å’Œè®¾ç½®å‚æ•°
        2. **ä¸Šä¼ å›¾ç‰‡**: ä¸Šä¼ ä½œä¸ºè§†é¢‘èµ·å§‹å¸§çš„å›¾ç‰‡
        3. **è¾“å…¥æç¤ºè¯**: æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„è§†é¢‘å†…å®¹ï¼ˆå»ºè®®ä½¿ç”¨è‹±æ–‡ï¼‰
        4. **è®¾ç½®ç›¸æœºè½¨è¿¹**: æŒ‡å®šåŒ…å«ç›¸æœºè¿åŠ¨æ•°æ®çš„ç›®å½•
        5. **è°ƒæ•´å‚æ•°**: æ ¹æ®éœ€è¦è°ƒæ•´åˆ†è¾¨ç‡ã€å¸§æ•°ç­‰å‚æ•°
        6. **ç‚¹å‡»ç”Ÿæˆ**: ç­‰å¾…è§†é¢‘ç”Ÿæˆå®Œæˆ
        
        âš ï¸ **æ³¨æ„**: é¦–æ¬¡ç”Ÿæˆæ—¶éœ€è¦åŠ è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚
        """)
    
    return demo


if __name__ == "__main__":
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
