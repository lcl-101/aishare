import os
import multiprocessing
import requests
import io
import gradio as gr
from PIL import Image
import fitz  # PyMUPDF
import decord
import numpy as np

# è®¾ç½®ç¯å¢ƒå˜é‡ - åœ¨å¯¼å…¥ä»»ä½•CUDAç›¸å…³åº“ä¹‹å‰
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["VLLM_USE_MULTIPROCESS"] = "0"
os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "spawn"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["VLLM_USE_V1"] = "0"
os.environ["MAX_JOBS"] = "1"
os.environ["VLLM_ALLOW_LONG_MAX_MODEL_LEN"] = "1"

# åœ¨æœ€å¼€å§‹å°±è®¾ç½®å¤šè¿›ç¨‹æ–¹æ³•
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    pass

from transformers import AutoProcessor
from vllm import LLM, SamplingParams

# å…¨å±€å˜é‡
model_path = "checkpoints/Kimi-VL-A3B-Thinking-2506"
llm = None
processor = None
sampling_params = None

def initialize_model():
    """åˆå§‹åŒ–æ¨¡å‹"""
    global llm, processor, sampling_params
    if llm is None:
        print("Initializing model...")
        llm = LLM(
            model_path,
            trust_remote_code=True,
            max_num_seqs=1,
            max_model_len=131072,
            limit_mm_per_prompt={"image": 16},
            tensor_parallel_size=1,
            gpu_memory_utilization=0.6,
            dtype="bfloat16",
            disable_log_stats=True,
            enforce_eager=True,
            enable_prefix_caching=False,
        )
        processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        sampling_params = SamplingParams(max_tokens=32768, temperature=0.8)
        print("Model initialized successfully!")

def extract_thinking_and_summary(text: str, bot: str = "â—thinkâ–·", eot: str = "â—/thinkâ–·"):
    """æå–æ€è€ƒè¿‡ç¨‹å’Œæ€»ç»“"""
    if bot in text and eot not in text:
        return "", ""
    if eot in text:
        thinking = text[text.index(bot) + len(bot):text.index(eot)].strip()
        summary = text[text.index(eot) + len(eot):].strip()
        return thinking, summary
    return "", text

def download_and_cache_file(url, filename):
    """ä¸‹è½½æ–‡ä»¶åˆ°examplesç›®å½•"""
    examples_dir = "examples"
    os.makedirs(examples_dir, exist_ok=True)
    
    file_path = os.path.join(examples_dir, filename)
    
    if os.path.exists(file_path):
        print(f"File {filename} already exists in examples directory")
        return file_path
    
    print(f"Downloading {filename} from {url}...")
    response = requests.get(url, stream=True)
    response.raise_for_status()
    
    with open(file_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    
    print(f"Downloaded {filename} to examples directory")
    return file_path

def process_single_image(image, question):
    """å¤„ç†å•å¼ å›¾åƒ"""
    if image is None:
        return "è¯·ä¸Šä¼ ä¸€å¼ å›¾ç‰‡", ""
    
    messages = [
        {"role": "user", "content": [
            {"type": "image", "image": ""},
            {"type": "text", "text": question}
        ]}
    ]
    
    text = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
    outputs = llm.generate([{"prompt": text, "multi_modal_data": {"image": image}}], sampling_params)
    generated_text = outputs[0].outputs[0].text
    
    thinking, summary = extract_thinking_and_summary(generated_text)
    
    result = f"**æ€è€ƒè¿‡ç¨‹:**\n{thinking}\n\n**æ€»ç»“:**\n{summary}" if thinking else summary
    return result, generated_text

def process_pdf(pdf_file, question):
    """å¤„ç†PDFæ–‡ä»¶"""
    if pdf_file is None:
        return "è¯·ä¸Šä¼ ä¸€ä¸ªPDFæ–‡ä»¶", ""
    
    # è½¬æ¢PDFä¸ºå›¾åƒ
    doc = fitz.open(pdf_file.name)
    all_images = []
    
    max_pages = min(len(doc), 8)  # é™åˆ¶æœ€å¤š8é¡µ
    
    for page_num in range(max_pages):
        page = doc.load_page(page_num)
        pix = page.get_pixmap()
        image = Image.open(io.BytesIO(pix.tobytes("png")))
        all_images.append(image)
    
    doc.close()
    
    messages = [
        {"role": "user", "content": [
            *[{"type": "image", "image": ""} for _ in all_images],
            {"type": "text", "text": question}
        ]}
    ]
    
    text = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
    outputs = llm.generate([{"prompt": text, "multi_modal_data": {"image": all_images}}], sampling_params)
    generated_text = outputs[0].outputs[0].text
    
    thinking, summary = extract_thinking_and_summary(generated_text)
    
    result = f"**å¤„ç†äº†{len(all_images)}é¡µPDF**\n\n**æ€è€ƒè¿‡ç¨‹:**\n{thinking}\n\n**æ€»ç»“:**\n{summary}" if thinking else f"**å¤„ç†äº†{len(all_images)}é¡µPDF**\n\n{summary}"
    return result, generated_text

def resize_image_and_convert_to_pil(image, max_size=448):
    """è°ƒæ•´å›¾åƒå°ºå¯¸"""
    with Image.fromarray(image) as img:
        width, height = img.size
        max_side = max(width, height)
        scale_ratio = max_size / max_side
        new_width, new_height = int(width * scale_ratio), int(height * scale_ratio)
        return img.resize((new_width, new_height))

def fmt_timestamp(timestamp_sec):
    """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
    hours = int(timestamp_sec // 3600)
    minutes = int((timestamp_sec % 3600) // 60)
    seconds = int(timestamp_sec % 60)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

def process_video(video_file, question):
    """å¤„ç†è§†é¢‘æ–‡ä»¶"""
    if video_file is None:
        return "è¯·ä¸Šä¼ ä¸€ä¸ªè§†é¢‘æ–‡ä»¶", ""
    
    try:
        # æå–è§†é¢‘å¸§
        vr = decord.VideoReader(video_file.name)
        fps = vr.get_avg_fps()
        video_duration = int(len(vr) / fps)
        sample_fps = 0.5  # æ¯2ç§’é‡‡æ ·ä¸€å¸§
        sample_frames = min(int(video_duration * sample_fps) + 1, 12)  # æœ€å¤š12å¸§
        
        frame_inds = np.linspace(0, len(vr) - 1, sample_frames).round().astype(int)
        frames = vr.get_batch(frame_inds).asnumpy()
        timestamps = (frame_inds / fps).astype(np.int32)
        
        # è½¬æ¢å¸§ä¸ºPILå›¾åƒ
        images = [resize_image_and_convert_to_pil(frame) for frame in frames]
        
        # æ„å»ºæ¶ˆæ¯
        contents = []
        for timestamp in timestamps:
            contents.append({"type": "text", "text": fmt_timestamp(timestamp)})
            contents.append({"type": "image", "image": ""})
        contents.append({"type": "text", "text": question})
        
        messages = [{"role": "user", "content": contents}]
        
        text = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
        outputs = llm.generate([{"prompt": text, "multi_modal_data": {"image": images}}], sampling_params)
        generated_text = outputs[0].outputs[0].text
        
        thinking, summary = extract_thinking_and_summary(generated_text)
        
        result = f"**å¤„ç†äº†{len(images)}å¸§è§†é¢‘**\n\n**æ€è€ƒè¿‡ç¨‹:**\n{thinking}\n\n**æ€»ç»“:**\n{summary}" if thinking else f"**å¤„ç†äº†{len(images)}å¸§è§†é¢‘**\n\n{summary}"
        return result, generated_text
        
    except Exception as e:
        return f"å¤„ç†è§†é¢‘æ—¶å‡ºé”™: {str(e)}", ""

def process_gui_agent(image, task):
    """å¤„ç†GUI Agentä»»åŠ¡"""
    if image is None:
        return "è¯·ä¸Šä¼ ä¸€å¼ å±å¹•æˆªå›¾", ""
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªGUIæ™ºèƒ½åŠ©æ‰‹ã€‚ä½ ä¼šæ”¶åˆ°ä¸€ä¸ªä»»åŠ¡å’Œè®¡ç®—æœºå±å¹•çš„æˆªå›¾ã€‚ä½ éœ€è¦æ‰§è¡Œæ“ä½œå¹¶ç”Ÿæˆpyautoguiä»£ç æ¥å®Œæˆä»»åŠ¡ã€‚è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›å›å¤ï¼š

## æ“ä½œæ­¥éª¤:
æä¾›æ¸…æ™°ã€ç®€æ´ã€å¯æ‰§è¡Œçš„æŒ‡ä»¤ã€‚

## ä»£ç :
ç”Ÿæˆç›¸åº”çš„Pythonä»£ç ç‰‡æ®µï¼Œä½¿ç”¨pyautoguiåº“ï¼Œé€šè¿‡æ ‡å‡†åŒ–å±å¹•åæ ‡ï¼ˆ0åˆ°1ä¹‹é—´çš„å€¼ï¼‰ç‚¹å‡»è¯†åˆ«çš„UIå…ƒç´ ã€‚è„šæœ¬åº”è¯¥é€šè¿‡å°†æ ‡å‡†åŒ–åæ ‡è½¬æ¢ä¸ºå®é™…åƒç´ ä½ç½®æ¥åŠ¨æ€é€‚åº”å½“å‰å±å¹•åˆ†è¾¨ç‡ã€‚"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": [
            {"type": "image", "image": ""},
            {"type": "text", "text": f"## Task Instruction:\n{task}"}
        ]}
    ]
    
    # ä½¿ç”¨è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ç²¾ç¡®çš„ç»“æœ
    gui_sampling_params = SamplingParams(max_tokens=8192, temperature=0.2)
    
    text = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
    outputs = llm.generate([{"prompt": text, "multi_modal_data": {"image": image}}], gui_sampling_params)
    generated_text = outputs[0].outputs[0].text
    
    thinking, summary = extract_thinking_and_summary(generated_text)
    
    result = f"**æ€è€ƒè¿‡ç¨‹:**\n{thinking}\n\n**æ“ä½œæŒ‡ä»¤:**\n{summary}" if thinking else summary
    return result, generated_text

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    # å¯åŠ¨æ—¶å°±åˆå§‹åŒ–æ¨¡å‹
    print("ğŸš€ æ­£åœ¨å¯åŠ¨å¹¶åˆå§‹åŒ–æ¨¡å‹...")
    initialize_model()
    print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ŒWebUI å‡†å¤‡å°±ç»ªï¼")
    
    with gr.Blocks(title="Kimi-VL Demo WebUI", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸŒŸ Kimi-VL A3B Thinking WebUI")
        gr.Markdown("åŸºäº Kimi-VL-A3B-Thinking-2506 æ¨¡å‹çš„å¤šæ¨¡æ€AIåŠ©æ‰‹")
        
        with gr.Tabs():
            # Tab 1: å•å›¾åƒåˆ†æ
            with gr.TabItem("ğŸ–¼ï¸ å›¾åƒåˆ†æ"):
                with gr.Row():
                    with gr.Column():
                        image_input = gr.Image(type="pil", label="ä¸Šä¼ å›¾ç‰‡")
                        question_input = gr.Textbox(
                            label="é—®é¢˜",
                            placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜...",
                            value="è¿™æ˜¯ä»€ä¹ˆç§ç±»çš„çŒ«ï¼Ÿè¯·ç”¨ä¸€ä¸ªè¯å›ç­”ã€‚"
                        )
                        image_btn = gr.Button("ğŸ” åˆ†æå›¾åƒ", variant="primary")
                        
                        # ç¤ºä¾‹
                        gr.Examples(
                            examples=[
                                ["examples/demo1_cat.jpeg", "è¿™æ˜¯ä»€ä¹ˆç§ç±»çš„çŒ«ï¼Ÿè¯·ç”¨ä¸€ä¸ªè¯å›ç­”ã€‚"],
                                ["examples/demo2_child.jpg", "å­©å­çš„é‹å­å’Œè£™å­æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿè¯·ä»¥JSONæ ¼å¼å›ç­”ã€‚"],
                                ["examples/demo3_mathvista.jpg", "åœ¨Tinyç±»åˆ«ä¸­ï¼Œå“ªä¸ªæ¨¡å‹çš„è¯­ä¹‰æ ‡ç­¾å‡†ç¡®ç‡(%)æœ€é«˜ï¼Ÿ"],
                                ["examples/demo4_math_puzzle.jpg", "æ•°å­—1,3,4,5å’Œ7ä¸­æœ‰å››ä¸ªè¢«å¡«å…¥æ–¹æ¡†ä¸­ï¼Œä½¿è®¡ç®—æ­£ç¡®ã€‚å“ªä¸ªæ•°å­—æ²¡æœ‰è¢«ä½¿ç”¨ï¼Ÿ"]
                            ],
                            inputs=[image_input, question_input]
                        )
                    
                    with gr.Column():
                        image_output = gr.Textbox(label="åˆ†æç»“æœ", lines=15, max_lines=30, show_copy_button=True)
                        image_raw = gr.Textbox(label="åŸå§‹è¾“å‡º", lines=10, max_lines=20, visible=False)
            
            # Tab 2: PDFåˆ†æ
            with gr.TabItem("ğŸ“„ PDFåˆ†æ"):
                with gr.Row():
                    with gr.Column():
                        pdf_input = gr.File(label="ä¸Šä¼ PDFæ–‡ä»¶", file_types=[".pdf"])
                        pdf_question = gr.Textbox(
                            label="é—®é¢˜",
                            placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜...",
                            value="è°æ˜¯è¿™ä¸ªåŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯·åˆ†æå…¶æ€§èƒ½è¡¨ç°ï¼Ÿ"
                        )
                        pdf_btn = gr.Button("ğŸ“– åˆ†æPDF", variant="primary")
                        
                        # ç¤ºä¾‹
                        gr.Examples(
                            examples=[
                                ["examples/demo6_sample_paper.pdf", "è°æ˜¯è¿™ä¸ªåŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯·åˆ†æå…¶æ€§èƒ½è¡¨ç°ï¼Ÿ"],
                            ],
                            inputs=[pdf_input, pdf_question]
                        )
                    
                    with gr.Column():
                        pdf_output = gr.Textbox(label="åˆ†æç»“æœ", lines=15, max_lines=30, show_copy_button=True)
                        pdf_raw = gr.Textbox(label="åŸå§‹è¾“å‡º", lines=10, max_lines=20, visible=False)
            
            # Tab 3: è§†é¢‘åˆ†æ
            with gr.TabItem("ğŸ¥ è§†é¢‘åˆ†æ"):
                with gr.Row():
                    with gr.Column():
                        video_input = gr.File(label="ä¸Šä¼ è§†é¢‘æ–‡ä»¶", file_types=[".mp4", ".avi", ".mov"])
                        video_preview = gr.Video(label="è§†é¢‘é¢„è§ˆ", visible=False)
                        video_question = gr.Textbox(
                            label="é—®é¢˜",
                            placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜...",
                            value="è¯·å°†è¿™ä¸ªè§†é¢‘åˆ†å‰²ä¸ºåœºæ™¯ï¼Œä¸ºæ¯ä¸ªåœºæ™¯æä¾›å¼€å§‹æ—¶é—´ã€ç»“æŸæ—¶é—´å’Œè¯¦ç»†æè¿°ã€‚"
                        )
                        video_btn = gr.Button("ğŸ¬ åˆ†æè§†é¢‘", variant="primary")
                        
                        # ç¤ºä¾‹
                        gr.Examples(
                            examples=[
                                ["examples/demo7_video.mp4", "è¯·å°†è¿™ä¸ªè§†é¢‘åˆ†å‰²ä¸ºåœºæ™¯ï¼Œä¸ºæ¯ä¸ªåœºæ™¯æä¾›å¼€å§‹æ—¶é—´ã€ç»“æŸæ—¶é—´å’Œè¯¦ç»†æè¿°ã€‚"],
                            ],
                            inputs=[video_input, video_question]
                        )
                    
                    with gr.Column():
                        video_output = gr.Textbox(label="åˆ†æç»“æœ", lines=15, max_lines=30, show_copy_button=True)
                        video_raw = gr.Textbox(label="åŸå§‹è¾“å‡º", lines=10, max_lines=20, visible=False)
            
            # Tab 4: GUI Agent
            with gr.TabItem("ğŸ–±ï¸ GUIåŠ©æ‰‹"):
                with gr.Row():
                    with gr.Column():
                        gui_image = gr.Image(type="pil", label="ä¸Šä¼ å±å¹•æˆªå›¾")
                        gui_task = gr.Textbox(
                            label="ä»»åŠ¡æè¿°",
                            placeholder="è¯·æè¿°ä½ æƒ³è¦æ‰§è¡Œçš„GUIæ“ä½œ...",
                            value="ä»”ç»†æ£€æŸ¥å±å¹•æˆªå›¾ï¼Œç„¶åç‚¹å‡»è®ºæ–‡æäº¤è€…çš„ä¸ªäººèµ„æ–™ã€‚"
                        )
                        gui_btn = gr.Button("ğŸ¤– ç”Ÿæˆæ“ä½œ", variant="primary")
                        
                        # ç¤ºä¾‹
                        gr.Examples(
                            examples=[
                                ["examples/demo5_screenshot.png", "ä»”ç»†æ£€æŸ¥å±å¹•æˆªå›¾ï¼Œç„¶åç‚¹å‡»è®ºæ–‡æäº¤è€…çš„ä¸ªäººèµ„æ–™ã€‚"],
                            ],
                            inputs=[gui_image, gui_task]
                        )
                    
                    with gr.Column():
                        gui_output = gr.Textbox(label="æ“ä½œæŒ‡ä»¤", lines=15, max_lines=30, show_copy_button=True)
                        gui_raw = gr.Textbox(label="åŸå§‹è¾“å‡º", lines=10, max_lines=20, visible=False)
            
            # Tab 5: è®¾ç½®
            with gr.TabItem("âš™ï¸ è®¾ç½®"):
                with gr.Column():
                    gr.Markdown("### æ¨¡å‹çŠ¶æ€")
                    model_status = gr.Textbox(label="çŠ¶æ€", value="âœ… æ¨¡å‹å·²åˆå§‹åŒ–", interactive=False)
                    init_btn = gr.Button("ï¿½ é‡æ–°åˆå§‹åŒ–æ¨¡å‹")
                    
                    gr.Markdown("### é«˜çº§è®¾ç½®")
                    with gr.Row():
                        show_raw = gr.Checkbox(label="æ˜¾ç¤ºåŸå§‹è¾“å‡º", value=False)
                        temperature = gr.Slider(0.1, 1.0, value=0.8, label="Temperature")
                    
                    gr.Markdown("### ç¤ºä¾‹æ–‡ä»¶")
                    gr.Markdown("ä»¥ä¸‹ç¤ºä¾‹æ–‡ä»¶å·²ä¸‹è½½åˆ° `examples/` ç›®å½•:")
                    gr.Markdown("""
                    - `demo1_cat.jpeg` - çŒ«å’ªå›¾ç‰‡ (Demo 1)
                    - `demo2_child.jpg` - å„¿ç«¥å›¾ç‰‡ (Demo 2)
                    - `demo3_mathvista.jpg` - æ•°å­¦è§†è§‰é¢˜ç›® (Demo 3)
                    - `demo4_math_puzzle.jpg` - æ•°å­¦è°œé¢˜ (Demo 4)
                    - `demo5_screenshot.png` - å±å¹•æˆªå›¾ (Demo 5)
                    - `demo6_sample_paper.pdf` - ç¤ºä¾‹PDFè®ºæ–‡ (Demo 6)
                    - `demo7_video.mp4` - ç¤ºä¾‹è§†é¢‘ (Demo 7)
                    - æ›´å¤šé¢å¤–èµ„æºæ–‡ä»¶...
                    """)
        
        # äº‹ä»¶ç»‘å®š
        image_btn.click(
            process_single_image,
            inputs=[image_input, question_input],
            outputs=[image_output, image_raw]
        )
        
        pdf_btn.click(
            process_pdf,
            inputs=[pdf_input, pdf_question],
            outputs=[pdf_output, pdf_raw]
        )
        
        def update_video_preview(video_file):
            """æ›´æ–°è§†é¢‘é¢„è§ˆ"""
            if video_file is not None:
                return gr.update(value=video_file.name, visible=True)
            else:
                return gr.update(value=None, visible=False)
        
        video_input.change(
            update_video_preview,
            inputs=[video_input],
            outputs=[video_preview]
        )
        
        video_btn.click(
            process_video,
            inputs=[video_input, video_question],
            outputs=[video_output, video_raw]
        )
        
        gui_btn.click(
            process_gui_agent,
            inputs=[gui_image, gui_task],
            outputs=[gui_output, gui_raw]
        )
        
        def init_model_status():
            print("ğŸ”„ é‡æ–°åˆå§‹åŒ–æ¨¡å‹...")
            initialize_model()
            print("âœ… æ¨¡å‹é‡æ–°åˆå§‹åŒ–å®Œæˆï¼")
            return "âœ… æ¨¡å‹å·²é‡æ–°åˆå§‹åŒ–"
        
        init_btn.click(
            init_model_status,
            outputs=[model_status]
        )
        
        def toggle_raw_visibility(show):
            return {
                image_raw: gr.update(visible=show),
                pdf_raw: gr.update(visible=show),
                video_raw: gr.update(visible=show),
                gui_raw: gr.update(visible=show)
            }
        
        show_raw.change(
            toggle_raw_visibility,
            inputs=[show_raw],
            outputs=[image_raw, pdf_raw, video_raw, gui_raw]
        )
    
    return demo

if __name__ == "__main__":
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        show_error=True
    )
