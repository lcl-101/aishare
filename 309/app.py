#!/usr/bin/env python
"""
VibeVoice ASR Gradio æ¼”ç¤ºç¨‹åº
åŸºäº demo/vibevoice_asr_gradio_demo.py åˆ›å»ºçš„ç‹¬ç«‹ç¨‹åº
"""

import os
import sys
import torch
import numpy as np
import soundfile as sf
from pathlib import Path
import argparse
import time
import json
import gradio as gr
from typing import List, Dict, Tuple, Optional, Generator
import tempfile
import base64
import io
import traceback
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# Import TextIteratorStreamer for streaming generation
from transformers import TextIteratorStreamer, StoppingCriteria, StoppingCriteriaList

try:
    from liger_kernel.transformers import apply_liger_kernel_to_qwen2
    # Only apply RoPE, RMSNorm, SwiGLU patches (these affect the underlying Qwen2 layers)
    apply_liger_kernel_to_qwen2(
        rope=True,
        rms_norm=True,
        swiglu=True,
        cross_entropy=False,
    )
    print("âœ… Liger Kernel å·²åº”ç”¨åˆ° Qwen2 ç»„ä»¶ (RoPE, RMSNorm, SwiGLU)")
except Exception as e:
    print(f"âš ï¸ åº”ç”¨ Liger Kernel å¤±è´¥: {e}, å¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…: pip install liger-kernel")
    
# Try to import pydub for MP3 conversion
try:
    from pydub import AudioSegment
    HAS_PYDUB = True
except ImportError:
    HAS_PYDUB = False
    print("âš ï¸ è­¦å‘Š: pydub ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ WAV æ ¼å¼")

from vibevoice.modular.modeling_vibevoice_asr import VibeVoiceASRForConditionalGeneration
from vibevoice.processor.vibevoice_asr_processor import VibeVoiceASRProcessor
from vibevoice.processor.audio_utils import load_audio_use_ffmpeg, COMMON_AUDIO_EXTS


# é»˜è®¤æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°é¢„ä¸‹è½½çš„æ¨¡å‹ï¼‰
DEFAULT_MODEL_PATH = "checkpoints/VibeVoice-ASR"


class VibeVoiceASRInference:
    """VibeVoice ASR æ¨¡å‹æ¨ç†å°è£…ç±»ã€‚"""
    
    def __init__(self, model_path: str, device: str = "cuda", dtype: torch.dtype = torch.bfloat16, attn_implementation: str = "flash_attention_2"):
        """
        åˆå§‹åŒ– ASR æ¨ç†ç®¡é“ã€‚
        
        Args:
            model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆHuggingFace æ ¼å¼ç›®å½•æˆ–æ¨¡å‹åç§°ï¼‰
            device: è¿è¡Œæ¨ç†çš„è®¾å¤‡
            dtype: æ¨¡å‹æƒé‡æ•°æ®ç±»å‹
            attn_implementation: æ³¨æ„åŠ›å®ç°æ–¹å¼ ('flash_attention_2', 'sdpa', 'eager')
        """
        print(f"æ­£åœ¨ä» {model_path} åŠ è½½ VibeVoice ASR æ¨¡å‹")
        
        # Load processor
        self.processor = VibeVoiceASRProcessor.from_pretrained(model_path)
        
        # Load model
        print(f"ä½¿ç”¨æ³¨æ„åŠ›å®ç°: {attn_implementation}")
        self.model = VibeVoiceASRForConditionalGeneration.from_pretrained(
            model_path,
            dtype=dtype,
            device_map=device if device == "auto" else None,
            attn_implementation=attn_implementation,
            trust_remote_code=True
        )
        
        if device != "auto":
            self.model = self.model.to(device)
        
        self.device = device if device != "auto" else next(self.model.parameters()).device
        self.model.eval()
        
        # Print model info
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¿è¡Œäº {self.device}")
        print(f"ğŸ“Š æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e9:.2f}B)")
    
    def transcribe(
        self, 
        audio_path: str = None,
        audio_array: np.ndarray = None,
        sample_rate: int = None,
        max_new_tokens: int = 512,
        temperature: float = 0.0,
        top_p: float = 1.0,
        do_sample: bool = False,
        num_beams: int = 1,
        repetition_penalty: float = 1.0,
        context_info: str = None,
        streamer: Optional[TextIteratorStreamer] = None,
    ) -> dict:
        """
        å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬ã€‚
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            audio_array: éŸ³é¢‘æ•°ç»„ï¼ˆå¦‚æœä¸ä»æ–‡ä»¶åŠ è½½ï¼‰
            sample_rate: éŸ³é¢‘æ•°ç»„çš„é‡‡æ ·ç‡
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: é‡‡æ ·æ¸©åº¦ï¼ˆ0 è¡¨ç¤ºè´ªå©ªè§£ç ï¼‰
            top_p: æ ¸é‡‡æ ·çš„ Top-p å€¼ï¼ˆ1.0 è¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            num_beams: æŸæœç´¢çš„æŸæ•°ï¼ˆ1 è¡¨ç¤ºè´ªå©ªè§£ç ï¼‰
            repetition_penalty: é‡å¤æƒ©ç½šï¼ˆ1.0 è¡¨ç¤ºæ— æƒ©ç½šï¼‰
            context_info: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚çƒ­è¯ã€è¯´è¯äººåç§°ã€ä¸»é¢˜ç­‰ï¼‰
            streamer: å¯é€‰çš„ TextIteratorStreamer ç”¨äºæµå¼è¾“å‡º
            
        Returns:
            åŒ…å«è½¬å½•ç»“æœçš„å­—å…¸
        """
        # Process audio
        inputs = self.processor(
            audio=audio_path,
            sampling_rate=sample_rate,
            return_tensors="pt",
            add_generation_prompt=True,
            context_info=context_info
        )
        
        # Move to device
        inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                  for k, v in inputs.items()}
        
        # Generate
        generation_config = {
            "max_new_tokens": max_new_tokens,
            "temperature": temperature if temperature > 0 else None,
            "top_p": top_p if do_sample else None,
            "do_sample": do_sample,
            "num_beams": num_beams,
            "repetition_penalty": repetition_penalty,
            "pad_token_id": self.processor.pad_id,
            "eos_token_id": self.processor.tokenizer.eos_token_id,
        }
        
        # Add streamer if provided
        if streamer is not None:
            generation_config["streamer"] = streamer
        
        # Add stopping criteria for stop button support
        generation_config["stopping_criteria"] = StoppingCriteriaList([StopOnFlag()])
        
        # Remove None values
        generation_config = {k: v for k, v in generation_config.items() if v is not None}
        
        start_time = time.time()
        
        # Calculate input token statistics before generation
        input_ids = inputs['input_ids'][0]  # Shape: [seq_len]
        total_input_tokens = input_ids.shape[0]
        
        # Count padding tokens (tokens equal to pad_id)
        pad_id = self.processor.pad_id
        padding_mask = (input_ids == pad_id)
        num_padding_tokens = padding_mask.sum().item()
        
        # Count speech tokens (tokens between speech_start_id and speech_end_id)
        speech_start_id = self.processor.speech_start_id
        speech_end_id = self.processor.speech_end_id
        
        # Find speech regions
        input_ids_list = input_ids.tolist()
        num_speech_tokens = 0
        in_speech = False
        for token_id in input_ids_list:
            if token_id == speech_start_id:
                in_speech = True
                num_speech_tokens += 1  # Count speech_start token
            elif token_id == speech_end_id:
                in_speech = False
                num_speech_tokens += 1  # Count speech_end token
            elif in_speech:
                num_speech_tokens += 1
        
        # Text tokens = total - speech - padding
        num_text_tokens = total_input_tokens - num_speech_tokens - num_padding_tokens
        
        with torch.no_grad():
            output_ids = self.model.generate(
                **inputs,
                **generation_config
            )
        
        generation_time = time.time() - start_time
        
        # Decode output
        generated_ids = output_ids[0, inputs['input_ids'].shape[1]:]
        generated_text = self.processor.decode(generated_ids, skip_special_tokens=True)
        
        # Parse structured output
        try:
            transcription_segments = self.processor.post_process_transcription(generated_text)
        except Exception as e:
            print(f"è­¦å‘Š: è§£æç»“æ„åŒ–è¾“å‡ºå¤±è´¥: {e}")
            transcription_segments = []
        
        return {
            "raw_text": generated_text,
            "segments": transcription_segments,
            "generation_time": generation_time,
            "input_tokens": {
                "total": total_input_tokens,
                "speech": num_speech_tokens,
                "text": num_text_tokens,
                "padding": num_padding_tokens,
            },
        }


def clip_and_encode_audio(
    audio_data: np.ndarray,
    sr: int,
    start_time: float,
    end_time: float,
    segment_idx: int,
    use_mp3: bool = True,
    target_sr: int = 16000,
    mp3_bitrate: str = "32k"
) -> Tuple[int, Optional[str], Optional[str]]:
    """
    è£å‰ªéŸ³é¢‘ç‰‡æ®µå¹¶ç¼–ç ä¸º base64ã€‚
    
    Args:
        audio_data: å®Œæ•´éŸ³é¢‘æ•°ç»„
        sr: é‡‡æ ·ç‡
        start_time: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
        end_time: ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
        segment_idx: ç‰‡æ®µç´¢å¼•
        use_mp3: æ˜¯å¦ä½¿ç”¨ MP3 æ ¼å¼ï¼ˆæ›´å°ï¼‰
        target_sr: ç›®æ ‡é‡‡æ ·ç‡
        mp3_bitrate: MP3 æ¯”ç‰¹ç‡
        
    Returns:
        å…ƒç»„ (segment_idx, base64_string, error_message)
    """
    try:
        # Convert time to sample indices
        start_sample = int(start_time * sr)
        end_sample = int(end_time * sr)
        
        # Ensure indices are within bounds
        start_sample = max(0, start_sample)
        end_sample = min(len(audio_data), end_sample)
        
        if start_sample >= end_sample:
            return segment_idx, None, f"æ— æ•ˆçš„æ—¶é—´èŒƒå›´: [{start_time:.2f}s - {end_time:.2f}s]"
        
        # Extract segment
        segment_data = audio_data[start_sample:end_sample]
        
        # Downsample if needed
        if sr != target_sr and target_sr < sr:
            duration = len(segment_data) / sr
            new_length = int(duration * target_sr)
            indices = np.linspace(0, len(segment_data) - 1, new_length)
            segment_data = np.interp(indices, np.arange(len(segment_data)), segment_data)
            sr = target_sr
        
        # Convert float32 audio to int16 for encoding
        segment_data_int16 = (segment_data * 32768.0).astype(np.int16)
        
        # Convert to MP3 if pydub is available and use_mp3 is True
        if use_mp3 and HAS_PYDUB:
            try:
                wav_buffer = io.BytesIO()
                sf.write(wav_buffer, segment_data_int16, sr, format='WAV', subtype='PCM_16')
                wav_buffer.seek(0)
                
                audio_segment = AudioSegment.from_wav(wav_buffer)
                if audio_segment.channels > 1:
                    audio_segment = audio_segment.set_channels(1)
                mp3_buffer = io.BytesIO()
                audio_segment.export(mp3_buffer, format='mp3', bitrate=mp3_bitrate)
                mp3_buffer.seek(0)
                
                audio_bytes = mp3_buffer.read()
                audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
                audio_src = f"data:audio/mp3;base64,{audio_base64}"
                
                return segment_idx, audio_src, None
            except Exception as e:
                print(f"ç‰‡æ®µ {segment_idx} MP3 è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨ WAV: {e}")
        
        # Fall back to WAV format
        wav_buffer = io.BytesIO()
        sf.write(wav_buffer, segment_data_int16, sr, format='WAV', subtype='PCM_16')
        wav_buffer.seek(0)
        
        audio_bytes = wav_buffer.read()
        audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
        audio_src = f"data:audio/wav;base64,{audio_base64}"
        
        return segment_idx, audio_src, None
        
    except Exception as e:
        error_msg = f"è£å‰ªç‰‡æ®µ {segment_idx} æ—¶å‡ºé”™: {str(e)}"
        print(error_msg)
        return segment_idx, None, error_msg


def extract_audio_segments(audio_path: str, segments: List[Dict]) -> List[Tuple[str, str, Optional[str]]]:
    """
    ä»éŸ³é¢‘æ–‡ä»¶ä¸­é«˜æ•ˆæå–å¤šä¸ªç‰‡æ®µï¼ˆä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼‰ã€‚
    
    Args:
        audio_path: åŸå§‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        segments: åŒ…å« start_time, end_time ç­‰ä¿¡æ¯çš„ç‰‡æ®µå­—å…¸åˆ—è¡¨
    
    Returns:
        å…ƒç»„åˆ—è¡¨ (segment_label, audio_base64_src, error_msg)
    """
    try:
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        audio_data, sr = load_audio_use_ffmpeg(audio_path, resample=False)
        print(f"âœ… éŸ³é¢‘åŠ è½½å®Œæˆ: {len(audio_data)} é‡‡æ ·ç‚¹, {sr} Hz")
        
        tasks = []
        use_mp3 = HAS_PYDUB
        
        for i, seg in enumerate(segments):
            start_time = seg.get('start_time')
            end_time = seg.get('end_time')
            
            if (not isinstance(start_time, (int, float)) or 
                not isinstance(end_time, (int, float)) or 
                start_time >= end_time):
                tasks.append((i, None, None, None, None, None))
                continue
            
            tasks.append((audio_data, sr, start_time, end_time, i, use_mp3))
        
        results = []
        total_segments = len(tasks)
        completed_count = 0
        
        max_workers = os.cpu_count() or 4
        print(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†...")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {}
            for task in tasks:
                if task[0] is None:
                    continue
                future = executor.submit(clip_and_encode_audio, *task)
                futures[future] = task[4]
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                    completed_count += 1
                    if completed_count % 100 == 0 or completed_count == len(futures):
                        print(f"è¿›åº¦: {completed_count}/{len(futures)} ä¸ªç‰‡æ®µå·²å¤„ç† ({completed_count*100//len(futures)}%)")
                except Exception as e:
                    idx = futures[future]
                    results.append((idx, None, f"å¤„ç†é”™è¯¯: {str(e)}"))
                    completed_count += 1
                    print(f"ç‰‡æ®µ {idx} å¤„ç†é”™è¯¯: {e}")
        
        print(f"âœ… å®Œæˆå¤„ç†æ‰€æœ‰ {len(futures)} ä¸ªæœ‰æ•ˆç‰‡æ®µ")
        
        results.sort(key=lambda x: x[0])
        
        audio_segments = []
        for i, (idx, audio_src, error_msg) in enumerate(results):
            seg = segments[idx] if idx < len(segments) else {}
            start_time = seg.get('start_time', 'N/A')
            end_time = seg.get('end_time', 'N/A')
            speaker_id = seg.get('speaker_id', 'N/A')
            
            segment_label = f"ç‰‡æ®µ {idx+1}: [{start_time:.2f}s - {end_time:.2f}s] è¯´è¯äºº {speaker_id}"
            audio_segments.append((segment_label, audio_src, error_msg))
        
        return audio_segments
        
    except Exception as e:
        print(f"åŠ è½½éŸ³é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []


# Global variable to store the ASR model
asr_model = None

# Global stop flag for generation
stop_generation_flag = False


class StopOnFlag(StoppingCriteria):
    """è‡ªå®šä¹‰åœæ­¢æ¡ä»¶ï¼Œæ£€æŸ¥å…¨å±€æ ‡å¿—ã€‚"""
    def __call__(self, input_ids, scores, **kwargs):
        global stop_generation_flag
        return stop_generation_flag


def parse_time_to_seconds(val: Optional[str]) -> Optional[float]:
    """å°†ç§’æ•°æˆ– hh:mm:ss æ ¼å¼è§£æä¸ºæµ®ç‚¹ç§’æ•°ã€‚"""
    if val is None:
        return None
    val = val.strip()
    if not val:
        return None
    try:
        return float(val)
    except ValueError:
        pass
    if ":" in val:
        parts = val.split(":")
        if not all(p.strip().replace(".", "", 1).isdigit() for p in parts):
            return None
        parts = [float(p) for p in parts]
        if len(parts) == 3:
            h, m, s = parts
        elif len(parts) == 2:
            h = 0
            m, s = parts
        else:
            return None
        return h * 3600 + m * 60 + s
    return None


def slice_audio_to_temp(
    audio_data: np.ndarray,
    sample_rate: int,
    start_sec: Optional[float],
    end_sec: Optional[float]
) -> Tuple[Optional[str], Optional[str]]:
    """å°† audio_data è£å‰ªåˆ° [start_sec, end_sec) å¹¶å†™å…¥ä¸´æ—¶ WAV æ–‡ä»¶ã€‚"""
    n_samples = len(audio_data)
    full_duration = n_samples / float(sample_rate)
    start = 0.0 if start_sec is None else max(0.0, start_sec)
    end = full_duration if end_sec is None else min(full_duration, end_sec)
    if end <= start:
        return None, f"æ— æ•ˆçš„æ—¶é—´èŒƒå›´: start={start:.2f}s, end={end:.2f}s"
    start_idx = int(start * sample_rate)
    end_idx = int(end * sample_rate)
    segment = audio_data[start_idx:end_idx]
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    temp_file.close()
    segment_int16 = (segment * 32768.0).astype(np.int16)
    sf.write(temp_file.name, segment_int16, sample_rate, subtype='PCM_16')
    return temp_file.name, None


def initialize_model(model_path: str, device: str = "cuda", attn_implementation: str = "flash_attention_2"):
    """åˆå§‹åŒ– ASR æ¨¡å‹ã€‚"""
    global asr_model
    try:
        dtype = torch.bfloat16 if device != "cpu" else torch.float32
        asr_model = VibeVoiceASRInference(
            model_path=model_path,
            device=device,
            dtype=dtype,
            attn_implementation=attn_implementation
        )
        return f"âœ… æ¨¡å‹ä» {model_path} åŠ è½½æˆåŠŸ"
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}"


def transcribe_audio(
    audio_input,
    audio_path_input: str,
    start_time_input: str,
    end_time_input: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    do_sample: bool,
    repetition_penalty: float = 1.0,
    context_info: str = ""
) -> Generator[Tuple[str, str], None, None]:
    """
    è½¬å½•éŸ³é¢‘å¹¶è¿”å›å¸¦æœ‰éŸ³é¢‘ç‰‡æ®µçš„ç»“æœï¼ˆæµå¼ç‰ˆæœ¬ï¼‰ã€‚
    
    Args:
        audio_input: éŸ³é¢‘æ–‡ä»¶è·¯å¾„æˆ–å…ƒç»„ (sample_rate, audio_data)
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        temperature: é‡‡æ ·æ¸©åº¦ï¼ˆ0 è¡¨ç¤ºè´ªå©ªè§£ç ï¼‰
        top_p: æ ¸é‡‡æ ·çš„ Top-p å€¼
        do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
        context_info: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    
    Yields:
        å…ƒç»„ (raw_text, audio_segments_html)
    """
    if asr_model is None:
        yield "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼", ""
        return
    
    if not audio_path_input and audio_input is None:
        yield "âŒ è¯·æä¾›éŸ³é¢‘è¾“å…¥ï¼", ""
        return
    
    try:
        print("[ä¿¡æ¯] æ”¶åˆ°è½¬å½•è¯·æ±‚")
        start_sec = parse_time_to_seconds(start_time_input)
        end_sec = parse_time_to_seconds(end_time_input)
        print(f"[ä¿¡æ¯] è§£æçš„æ—¶é—´èŒƒå›´: start={start_sec}, end={end_sec}")
        if (start_time_input and start_sec is None) or (end_time_input and end_sec is None):
            yield "âŒ æ— æ•ˆçš„æ—¶é—´æ ¼å¼ã€‚è¯·ä½¿ç”¨ç§’æ•°æˆ– hh:mm:ss æ ¼å¼ã€‚", ""
            return

        audio_path = None
        audio_array = None
        sample_rate = None

        if audio_path_input:
            candidate = Path(audio_path_input.strip())
            if not candidate.exists():
                yield f"âŒ æŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨: {candidate}", ""
                return
            audio_path = str(candidate)
            print(f"[ä¿¡æ¯] ä½¿ç”¨æŒ‡å®šçš„éŸ³é¢‘è·¯å¾„: {audio_path}")
        elif isinstance(audio_input, str):
            audio_path = audio_input
            print(f"[ä¿¡æ¯] ä½¿ç”¨ä¸Šä¼ çš„éŸ³é¢‘è·¯å¾„: {audio_path}")
        elif isinstance(audio_input, tuple):
            sample_rate, audio_array = audio_input
            print(f"[ä¿¡æ¯] æ”¶åˆ°éº¦å…‹é£éŸ³é¢‘ï¼Œé‡‡æ ·ç‡={sample_rate}")
        elif audio_path is None:
            yield "âŒ æ— æ•ˆçš„éŸ³é¢‘è¾“å…¥æ ¼å¼ï¼", ""
            return

        # If slicing is requested, load and slice audio
        if start_sec is not None or end_sec is not None:
            print("[ä¿¡æ¯] æŒ‰è¯·æ±‚çš„æ—¶é—´èŒƒå›´è£å‰ªéŸ³é¢‘")
            if audio_array is None or sample_rate is None:
                try:
                    audio_array, sample_rate = load_audio_use_ffmpeg(audio_path, resample=False)
                    print("[ä¿¡æ¯] é€šè¿‡ ffmpeg åŠ è½½éŸ³é¢‘ç”¨äºè£å‰ª")
                except Exception as exc:
                    yield f"âŒ åŠ è½½éŸ³é¢‘è¿›è¡Œè£å‰ªå¤±è´¥: {exc}", ""
                    return
            sliced_path, err = slice_audio_to_temp(audio_array, sample_rate, start_sec, end_sec)
            if err:
                yield f"âŒ {err}", ""
                return
            audio_path = sliced_path
            print(f"[ä¿¡æ¯] è£å‰ªåçš„éŸ³é¢‘å·²å†™å…¥ä¸´æ—¶æ–‡ä»¶: {audio_path}")
        elif audio_array is not None and sample_rate is not None:
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            audio_path = temp_file.name
            temp_file.close()
            audio_data_int16 = (audio_array * 32768.0).astype(np.int16)
            sf.write(audio_path, audio_data_int16, sample_rate, subtype='PCM_16')
            print(f"[ä¿¡æ¯] éº¦å…‹é£éŸ³é¢‘å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {audio_path}")
        
        # Create streamer for real-time output
        streamer = TextIteratorStreamer(
            asr_model.processor.tokenizer, 
            skip_prompt=True, 
            skip_special_tokens=True
        )
        
        result_container = {"result": None, "error": None}
        
        def run_transcription():
            try:
                result_container["result"] = asr_model.transcribe(
                    audio_path=audio_path,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    repetition_penalty=repetition_penalty,
                    context_info=context_info if context_info and context_info.strip() else None,
                    streamer=streamer
                )
            except Exception as e:
                result_container["error"] = str(e)
                traceback.print_exc()
        
        print("[ä¿¡æ¯] å¼€å§‹æ¨¡å‹è½¬å½•ï¼ˆæµå¼æ¨¡å¼ï¼‰")
        start_time = time.time()
        transcription_thread = threading.Thread(target=run_transcription)
        transcription_thread.start()
        
        # Yield streaming output
        generated_text = ""
        token_count = 0
        for new_text in streamer:
            generated_text += new_text
            token_count += 1
            elapsed = time.time() - start_time
            formatted_text = generated_text.replace('},', '},\n')
            streaming_output = f"--- ğŸ”´ å®æ—¶æµå¼è¾“å‡º (tokens: {token_count}, æ—¶é—´: {elapsed:.1f}s) ---\n{formatted_text}"
            yield streaming_output, "<div style='padding: 20px; text-align: center; color: #6c757d;'>â³ æ­£åœ¨ç”Ÿæˆè½¬å½•ç»“æœ... å®Œæˆåå°†æ˜¾ç¤ºéŸ³é¢‘ç‰‡æ®µã€‚</div>"
        
        transcription_thread.join()
        
        if result_container["error"]:
            yield f"âŒ è½¬å½•è¿‡ç¨‹ä¸­å‡ºé”™: {result_container['error']}", ""
            return
        
        result = result_container["result"]
        generation_time = time.time() - start_time
        
        input_tokens = result.get('input_tokens', {})
        speech_tokens = input_tokens.get('speech', 0)
        text_tokens = input_tokens.get('text', 0)
        padding_tokens = input_tokens.get('padding', 0)
        total_input = input_tokens.get('total', 0)
        
        raw_output = f"--- âœ… åŸå§‹è¾“å‡º ---\n"
        raw_output += f"ğŸ“¥ è¾“å…¥: {total_input} tokens (ğŸ¤ è¯­éŸ³: {speech_tokens}, ğŸ“ æ–‡æœ¬: {text_tokens}, â¬œ å¡«å……: {padding_tokens})\n"
        raw_output += f"ğŸ“¤ è¾“å‡º: {token_count} tokens | â±ï¸ æ—¶é—´: {generation_time:.2f}s\n"
        raw_output += f"---\n"
        formatted_raw_text = result['raw_text'].replace('},', '},\n')
        raw_output += formatted_raw_text
        
        print(f"[è°ƒè¯•] åŸå§‹æ¨¡å‹è¾“å‡º:")
        print(f"[è°ƒè¯•] {result['raw_text']}")
        print(f"[è°ƒè¯•] æ‰¾åˆ° {len(result['segments'])} ä¸ªç‰‡æ®µ")
        
        audio_segments_html = ""
        segments = result['segments']
        
        if segments:
            num_segments = len(segments)
            print(f"[ä¿¡æ¯] åˆ›å»ºæ¯ä¸ªç‰‡æ®µçš„éŸ³é¢‘å‰ªè¾‘ ({num_segments} ä¸ªç‰‡æ®µ, 16kHz mono MP3 @ 32kbps)")
            
            audio_segments = extract_audio_segments(audio_path, segments)
            print("[ä¿¡æ¯] å®Œæˆåˆ›å»ºéŸ³é¢‘å‰ªè¾‘")
            
            total_duration = sum(
                (seg.get('end_time', 0) - seg.get('start_time', 0)) 
                for seg in segments 
                if isinstance(seg.get('start_time'), (int, float)) and isinstance(seg.get('end_time'), (int, float))
            )
            approx_size_kb = total_duration * 4
            
            theme_css = """
            <style>
            :root {
                --segment-bg: #f8f9fa;
                --segment-border: #e1e5e9;
                --segment-text: #495057;
                --segment-meta: #6c757d;
                --content-bg: white;
                --content-border: #007bff;
                --warning-bg: #fff3cd;
                --warning-border: #ffc107;
                --warning-text: #856404;
            }
            
            @media (prefers-color-scheme: dark) {
                :root {
                    --segment-bg: #2d3748;
                    --segment-border: #4a5568;
                    --segment-text: #e2e8f0;
                    --segment-meta: #a0aec0;
                    --content-bg: #1a202c;
                    --content-border: #4299e1;
                    --warning-bg: #744210;
                    --warning-border: #d69e2e;
                    --warning-text: #faf089;
                }
            }
            
            .audio-segments-container {
                max-height: 600px;
                overflow-y: auto;
                padding: 10px;
            }
            
            .audio-segment {
                margin-bottom: 15px;
                padding: 15px;
                border: 2px solid var(--segment-border);
                border-radius: 8px;
                background-color: var(--segment-bg);
                transition: all 0.3s ease;
            }
            
            .audio-segment:hover {
                box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            }
            
            .segment-header {
                margin-bottom: 10px;
            }
            
            .segment-title {
                margin: 0;
                color: var(--segment-text);
                font-size: 16px;
                font-weight: 600;
            }
            
            .segment-meta {
                margin-top: 5px;
                font-size: 14px;
                color: var(--segment-meta);
            }
            
            .segment-content {
                margin-bottom: 10px;
                padding: 12px;
                background-color: var(--content-bg);
                border-radius: 6px;
                border-left: 4px solid var(--content-border);
                color: var(--segment-text);
                line-height: 1.5;
            }
            
            .segment-audio {
                width: 100%;
                margin-top: 10px;
                border-radius: 4px;
            }
            
            .segment-warning {
                margin-top: 10px;
                padding: 10px;
                background-color: var(--warning-bg);
                border-radius: 4px;
                border-left: 4px solid var(--warning-border);
                color: var(--warning-text);
                font-size: 13px;
            }
            
            .segments-title {
                color: var(--segment-text);
                margin-bottom: 10px;
            }
            
            .segments-description {
                color: var(--segment-meta);
                margin-bottom: 20px;
            }
            
            .size-badge {
                display: inline-block;
                background: linear-gradient(135deg, #6c757d, #495057);
                color: white;
                padding: 4px 10px;
                border-radius: 12px;
                font-size: 12px;
                margin-left: 10px;
            }
            </style>
            """
            
            audio_segments_html = theme_css
            audio_segments_html += f"<div class='audio-segments-container'>"
            
            format_info = "MP3 32kbps 16kHz mono" if HAS_PYDUB else "WAV 16kHz"
            audio_segments_html += f"<h3 class='segments-title'>ğŸ”Š éŸ³é¢‘ç‰‡æ®µ ({num_segments} ä¸ªç‰‡æ®µ)"
            audio_segments_html += f"<span class='size-badge'>ğŸ“¦ ~{approx_size_kb:.0f}KB ({format_info})</span></h3>"
            audio_segments_html += "<p class='segments-description'>ğŸµ ç‚¹å‡»æ’­æ”¾æŒ‰é’®å¯ç›´æ¥æ”¶å¬æ¯ä¸ªç‰‡æ®µï¼</p>"
            
            for i, (label, audio_src, error_msg) in enumerate(audio_segments):
                seg = segments[i] if i < len(segments) else {}
                start_time = seg.get('start_time', 'N/A')
                end_time = seg.get('end_time', 'N/A')
                speaker_id = seg.get('speaker_id', 'N/A')
                content = seg.get('text', '')
                
                start_str = f"{start_time:.2f}" if isinstance(start_time, (int, float)) else str(start_time)
                end_str = f"{end_time:.2f}" if isinstance(end_time, (int, float)) else str(end_time)
                
                audio_segments_html += f"""
                <div class='audio-segment'>
                    <div class='segment-header'>
                        <h4 class='segment-title'>ç‰‡æ®µ {i+1}</h4>
                        <div class='segment-meta'>
                            <strong>æ—¶é—´:</strong> [{start_str}s - {end_str}s] | 
                            <strong>è¯´è¯äºº:</strong> {speaker_id}
                        </div>
                    </div>
                    
                    <div class='segment-content'>
                        {content}
                    </div>
                """
                
                if audio_src:
                    audio_type = 'audio/mp3' if 'audio/mp3' in audio_src else 'audio/wav'
                    audio_segments_html += f"""
                    <audio controls class='segment-audio' preload='none'>
                        <source src='{audio_src}' type='{audio_type}'>
                        æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒéŸ³é¢‘æ’­æ”¾ã€‚
                    </audio>
                    """
                elif error_msg:
                    audio_segments_html += f"""
                    <div class='segment-warning'>
                        <small>âŒ {error_msg}</small>
                    </div>
                    """
                else:
                    audio_segments_html += """
                    <div class='segment-warning'>
                        <small>æ­¤ç‰‡æ®µæ— æ³•æ’­æ”¾éŸ³é¢‘</small>
                    </div>
                    """
                
                audio_segments_html += "</div>"
            
            audio_segments_html += "</div>"
        else:
            audio_segments_html = """
            <style>
            :root {
                --no-segments-text: #6c757d;
            }
            
            @media (prefers-color-scheme: dark) {
                :root {
                    --no-segments-text: #a0aec0;
                }
            }
            
            .no-segments-container {
                padding: 20px;
                text-align: center;
                color: var(--no-segments-text);
                line-height: 1.6;
            }
            </style>
            <div class='no-segments-container'>
                <p>âŒ æ²¡æœ‰å¯ç”¨çš„éŸ³é¢‘ç‰‡æ®µã€‚</p>
                <p>è¿™å¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹è¾“å‡ºä¸­ä¸åŒ…å«æœ‰æ•ˆçš„æ—¶é—´æˆ³ã€‚</p>
            </div>
            """
        
        yield raw_output, audio_segments_html
        
    except Exception as e:
        print(f"è½¬å½•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print(traceback.format_exc())
        yield f"âŒ è½¬å½•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", ""


def create_gradio_interface(model_path: str, default_max_tokens: int = 8192, attn_implementation: str = "flash_attention_2"):
    """åˆ›å»ºå¹¶å¯åŠ¨ Gradio ç•Œé¢ã€‚
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆHuggingFace æ ¼å¼ç›®å½•æˆ–æ¨¡å‹åç§°ï¼‰
        default_max_tokens: max_new_tokens æ»‘å—çš„é»˜è®¤å€¼
        attn_implementation: æ³¨æ„åŠ›å®ç°æ–¹å¼
    """
    
    # Initialize model at startup
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_status = initialize_model(model_path, device, attn_implementation)
    print(model_status)
    
    if model_status.startswith("âŒ"):
        print("\n" + "="*80)
        print("ğŸ’¥ è‡´å‘½é”™è¯¯: æ¨¡å‹åŠ è½½å¤±è´¥ï¼")
        print("="*80)
        print("æ— æ³•å¯åŠ¨æ¼”ç¤ºï¼Œæ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹ã€‚è¯·æ£€æŸ¥:")
        print("  1. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("  2. æ¨¡å‹æ–‡ä»¶æ˜¯å¦æŸå")
        print("  3. æ˜¯å¦æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜")
        print("  4. CUDA æ˜¯å¦æ­£ç¡®å®‰è£…ï¼ˆå¦‚æœä½¿ç”¨ GPUï¼‰")
        print("="*80)
        sys.exit(1)
    
    # Custom CSS for Stop button styling
    custom_css = """
    #stop-btn {
        background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%) !important;
        border: none !important;
        color: white !important;
    }
    #stop-btn:hover {
        background: linear-gradient(135deg, #dc2626 0%, #b91c1c 100%) !important;
    }
    .youtube-banner {
        background: linear-gradient(135deg, #ff0000 0%, #cc0000 100%);
        color: white;
        padding: 15px 20px;
        border-radius: 10px;
        margin-bottom: 20px;
        text-align: center;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .youtube-banner a {
        color: white;
        text-decoration: none;
        font-weight: bold;
    }
    .youtube-banner a:hover {
        text-decoration: underline;
    }
    """
    
    with gr.Blocks(title="VibeVoice ASR è¯­éŸ³è¯†åˆ«æ¼”ç¤º") as demo:
        # YouTube é¢‘é“ä¿¡æ¯
        gr.HTML("""
        <div class="youtube-banner">
            <span style="font-size: 24px;">ğŸ“º</span>
            <span style="font-size: 18px; margin-left: 10px;">æ¬¢è¿è®¿é—®æˆ‘çš„ YouTube é¢‘é“ï¼š</span>
            <a href="https://www.youtube.com/@rongyikanshijie-ai" target="_blank" style="font-size: 20px; margin-left: 10px;">
                ğŸ¬ AI æŠ€æœ¯åˆ†äº«é¢‘é“
            </a>
            <span style="font-size: 14px; margin-left: 15px; opacity: 0.9;">| è®¢é˜…è·å–æ›´å¤š AI æŠ€æœ¯æ•™ç¨‹</span>
        </div>
        """)
        
        gr.Markdown("# ğŸ™ï¸ VibeVoice ASR è¯­éŸ³è¯†åˆ«æ¼”ç¤º")
        gr.Markdown("ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–é€šè¿‡éº¦å…‹é£å½•éŸ³ï¼Œå³å¯è·å¾—å¸¦æœ‰è¯´è¯äººåˆ†ç¦»çš„è¯­éŸ³è½¬æ–‡å­—ç»“æœã€‚")
        gr.Markdown(f"**å·²åŠ è½½æ¨¡å‹:** `{model_path}`")
        
        with gr.Row():
            with gr.Column(scale=1):
                # Generation parameters
                gr.Markdown("## âš™ï¸ ç”Ÿæˆå‚æ•°")
                max_tokens_slider = gr.Slider(
                    minimum=4096,
                    maximum=65536,
                    value=default_max_tokens,
                    step=4096,
                    label="æœ€å¤§ç”Ÿæˆ Token æ•°"
                )
                
                # Sampling parameters
                gr.Markdown("### ğŸ² é‡‡æ ·è®¾ç½®")
                do_sample_checkbox = gr.Checkbox(
                    value=False,
                    label="å¯ç”¨é‡‡æ ·",
                    info="å¯ç”¨éšæœºé‡‡æ ·è€Œéç¡®å®šæ€§è§£ç "
                )
                
                with gr.Column(visible=False) as sampling_params:
                    temperature_slider = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=0.0,
                        step=0.1,
                        label="æ¸©åº¦ (Temperature)",
                        info="0 = è´ªå©ªè§£ç ï¼Œè¶Šé«˜è¶Šéšæœº"
                    )
                    top_p_slider = gr.Slider(
                        minimum=0.0,
                        maximum=1.0,
                        value=1.0,
                        step=0.05,
                        label="Top-p (æ ¸é‡‡æ ·)",
                        info="1.0 = ä¸è¿‡æ»¤"
                    )
                
                repetition_penalty_slider = gr.Slider(
                    minimum=1.0,
                    maximum=1.2,
                    value=1.0,
                    step=0.01,
                    label="é‡å¤æƒ©ç½š",
                    info="1.0 = æ— æƒ©ç½šï¼Œè¶Šé«˜é‡å¤è¶Šå°‘ï¼ˆé€‚ç”¨äºè´ªå©ªè§£ç å’Œé‡‡æ ·ï¼‰"
                )
                
                # Context information section
                gr.Markdown("## ğŸ“‹ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰")
                context_info_input = gr.Textbox(
                    label="ä¸Šä¸‹æ–‡ä¿¡æ¯",
                    placeholder="Enter hotwords, speaker names, topics, or other context to help transcription...\nExample:\nJohn Smith\nMachine Learning\nOpenAI",
                    lines=4,
                    max_lines=8,
                    interactive=True,
                    info="æä¾›çƒ­è¯ã€ä¸“ä¸šæœ¯è¯­æˆ–è¯´è¯äººåç§°ç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥æé«˜å‡†ç¡®æ€§"
                )
            
            with gr.Column(scale=2):
                # Audio input section
                gr.Markdown("## ğŸµ éŸ³é¢‘è¾“å…¥")
                audio_input = gr.Audio(
                    label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–é€šè¿‡éº¦å…‹é£å½•éŸ³",
                    sources=["upload", "microphone"],
                    type="filepath",
                    interactive=True,
                    buttons=["download"]
                )
                
                with gr.Accordion("ğŸ“‚ é«˜çº§é€‰é¡¹ï¼šè¿œç¨‹è·¯å¾„ & æ—¶é—´è£å‰ª", open=False):
                    audio_path_input = gr.Textbox(
                        label="éŸ³é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                        placeholder="è¾“å…¥è¿œç¨‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„",
                        lines=1
                    )
                    with gr.Row():
                        start_time_input = gr.Textbox(
                            label="å¼€å§‹æ—¶é—´",
                            placeholder="ä¾‹å¦‚: 0 æˆ– 00:00:00",
                            lines=1,
                            info="ç•™ç©ºåˆ™ä»å¼€å¤´å¼€å§‹"
                        )
                        end_time_input = gr.Textbox(
                            label="ç»“æŸæ—¶é—´",
                            placeholder="ä¾‹å¦‚: 30.5 æˆ– 00:00:30.5",
                            lines=1,
                            info="ç•™ç©ºåˆ™ä½¿ç”¨å®Œæ•´é•¿åº¦"
                        )
                
                with gr.Row():
                    transcribe_button = gr.Button("ğŸ¯ å¼€å§‹è½¬å½•", variant="primary", size="lg", scale=3)
                    stop_button = gr.Button("â¹ï¸ åœæ­¢", variant="secondary", size="lg", scale=1, elem_id="stop-btn")
                
                # Results section
                gr.Markdown("## ğŸ“ è½¬å½•ç»“æœ")
                
                with gr.Tabs():
                    with gr.TabItem("åŸå§‹è¾“å‡º"):
                        raw_output = gr.Textbox(
                            label="åŸå§‹è½¬å½•è¾“å‡º",
                            lines=8,
                            max_lines=20,
                            interactive=False
                        )
                    
                    with gr.TabItem("éŸ³é¢‘ç‰‡æ®µ"):
                        audio_segments_output = gr.HTML(
                            label="æ’­æ”¾å„ä¸ªç‰‡æ®µä»¥éªŒè¯å‡†ç¡®æ€§"
                        )
        
        # Event handlers
        do_sample_checkbox.change(
            fn=lambda x: gr.update(visible=x),
            inputs=[do_sample_checkbox],
            outputs=[sampling_params]
        )
        
        def reset_stop_flag():
            """é‡ç½®åœæ­¢æ ‡å¿—ã€‚"""
            global stop_generation_flag
            stop_generation_flag = False
        
        def set_stop_flag():
            """è®¾ç½®åœæ­¢æ ‡å¿—ä»¥ä¸­æ–­ç”Ÿæˆã€‚"""
            global stop_generation_flag
            stop_generation_flag = True
            return "â¹ï¸ å·²è¯·æ±‚åœæ­¢..."
        
        transcribe_button.click(
            fn=reset_stop_flag,
            inputs=[],
            outputs=[],
            queue=False
        ).then(
            fn=transcribe_audio,
            inputs=[
                audio_input,
                audio_path_input,
                start_time_input,
                end_time_input,
                max_tokens_slider,
                temperature_slider,
                top_p_slider,
                do_sample_checkbox,
                repetition_penalty_slider,
                context_info_input
            ],
            outputs=[raw_output, audio_segments_output]
        )
        
        stop_button.click(
            fn=set_stop_flag,
            inputs=[],
            outputs=[raw_output],
            queue=False
        )
        
        # Add examples
        gr.Markdown("## ğŸ“‹ ä½¿ç”¨è¯´æ˜")
        gr.Markdown(f"""
        1. **ä¸Šä¼ éŸ³é¢‘**: ä½¿ç”¨éŸ³é¢‘ç»„ä»¶ä¸Šä¼ æ–‡ä»¶æˆ–é€šè¿‡éº¦å…‹é£å½•éŸ³
           - **æ”¯æŒçš„æ ¼å¼**: {', '.join(sorted(set([ext.lower() for ext in COMMON_AUDIO_EXTS])))}
           - å¯é€‰ï¼šè®¾ç½®**å¼€å§‹/ç»“æŸæ—¶é—´**ï¼ˆç§’æ•°æˆ– hh:mm:ss æ ¼å¼ï¼‰ä»¥åœ¨è½¬å½•å‰è£å‰ªéŸ³é¢‘
        2. **ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰**: æä¾›ä¸Šä¸‹æ–‡ä»¥æé«˜è½¬å½•å‡†ç¡®æ€§
           - æ·»åŠ çƒ­è¯ã€ä¸“æœ‰åè¯ã€è¯´è¯äººå§“åæˆ–æŠ€æœ¯æœ¯è¯­
           - æ¯è¡Œä¸€é¡¹æˆ–é€—å·åˆ†éš”
           - ç¤ºä¾‹: "John Smith", "OpenAI", "machine learning"
        3. **è°ƒæ•´å‚æ•°**: æ ¹æ®éœ€è¦é…ç½®ç”Ÿæˆå‚æ•°
        4. **å¼€å§‹è½¬å½•**: ç‚¹å‡»"å¼€å§‹è½¬å½•"æŒ‰é’®è·å–ç»“æœ
        5. **æŸ¥çœ‹ç»“æœ**: 
           - **åŸå§‹è¾“å‡º**: æŸ¥çœ‹æ¨¡å‹çš„åŸå§‹è¾“å‡º
           - **éŸ³é¢‘ç‰‡æ®µ**: ç›´æ¥æ’­æ”¾å„ä¸ªç‰‡æ®µä»¥éªŒè¯å‡†ç¡®æ€§
        
        **éŸ³é¢‘ç‰‡æ®µ**: æ¯ä¸ªç‰‡æ®µæ˜¾ç¤ºæ—¶é—´èŒƒå›´ã€è¯´è¯äºº IDã€è½¬å½•å†…å®¹ï¼Œä»¥åŠå¯ç›´æ¥æ’­æ”¾éªŒè¯çš„åµŒå…¥å¼éŸ³é¢‘æ’­æ”¾å™¨ã€‚
        """)
    
    return demo, custom_css


def main():
    parser = argparse.ArgumentParser(description="VibeVoice ASR Gradio æ¼”ç¤º")
    parser.add_argument(
        "--model_path", 
        type=str, 
        default=DEFAULT_MODEL_PATH,
        help="æ¨¡å‹è·¯å¾„ï¼ˆHuggingFace æ ¼å¼ç›®å½•æˆ–æ¨¡å‹åç§°ï¼‰"
    )
    parser.add_argument(
        "--attn_implementation",
        type=str,
        default="flash_attention_2",
        help="æ³¨æ„åŠ›å®ç°æ–¹å¼ï¼ˆé»˜è®¤: flash_attention_2ï¼‰"
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=32768,
        help="é»˜è®¤æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆé»˜è®¤: 32768ï¼‰"
    )
    parser.add_argument(
        "--host",
        type=str,
        default="0.0.0.0",
        help="æœåŠ¡å™¨ç»‘å®šçš„ä¸»æœºåœ°å€"
    )
    parser.add_argument(
        "--port",
        type=int,
        default=7860,
        help="æœåŠ¡å™¨ç»‘å®šçš„ç«¯å£"
    )
    parser.add_argument(
        "--share",
        action="store_true",
        help="åˆ›å»ºå…¬å…±é“¾æ¥"
    )
    
    args = parser.parse_args()
    
    # Create and launch interface
    demo, custom_css = create_gradio_interface(
        model_path=args.model_path,
        default_max_tokens=args.max_new_tokens,
        attn_implementation=args.attn_implementation
    )
    
    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨ VibeVoice ASR æ¼”ç¤º...")
    print(f"ğŸ“ æœåŠ¡å™¨åœ°å€: http://{args.host}:{args.port}")
    
    launch_kwargs = {
        "server_name": args.host,
        "server_port": args.port,
        "share": args.share,
        "show_error": True,
        "theme": gr.themes.Soft(),
        "css": custom_css,
    }
    
    demo.queue(default_concurrency_limit=3)
    demo.launch(**launch_kwargs)


if __name__ == "__main__":
    main()
