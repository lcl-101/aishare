"""
FunctionGemma å·¥å…·è°ƒç”¨æ¼”ç¤º - Gradio Web åº”ç”¨
åŸºäº Google FunctionGemma æ¨¡å‹çš„å‡½æ•°è°ƒç”¨åŠŸèƒ½
"""

import os
import re
import asyncio
import subprocess
import gradio as gr
from transformers import AutoProcessor, AutoModelForCausalLM
import torch
from openai import OpenAI
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig

# ================== é…ç½® ==================
MODEL_PATH = "checkpoints/functiongemma-270m-it"

# LLM API é…ç½®
OPENAI_API_KEY = "sk-4e3e00a0b4522d6d4c119ce2ddeb1722"
API_URL = "https://api.xxx.com/v1"
MODEL_NAME = "sykjtestuqwen2-5-72b-instruct"
AI_MAX_TOKENS = 32768
AI_TEMPERATURE = 0.2
AI_TIMEOUT = 120  # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥æ”¯æŒé•¿å†…å®¹æ€»ç»“

# ================== åŠ è½½æ¨¡å‹ ==================
print("æ­£åœ¨åŠ è½½ FunctionGemma æ¨¡å‹...")
processor = AutoProcessor.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)
print(f"æ¨¡å‹å·²åŠ è½½åˆ°: {model.device}")

# ================== åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ ==================
client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=API_URL,
    timeout=AI_TIMEOUT
)

# ================== å·¥å…·å®šä¹‰ ==================

# å·¥å…·1: è·å–æ˜¾å¡é…ç½®
gpu_info_schema = {
    "type": "function",
    "function": {
        "name": "get_gpu_info",
        "description": "Gets the current GPU configuration and status using nvidia-smi command. Returns detailed information about NVIDIA GPUs including memory usage, temperature, and utilization.",
        "parameters": {
            "type": "object",
            "properties": {},
            "required": [],
        },
    }
}

# å·¥å…·2: ç½‘é¡µæŠ“å–
web_crawler_schema = {
    "type": "function",
    "function": {
        "name": "crawl_webpage",
        "description": "Crawls and extracts content from a given URL. Returns the webpage content in markdown format.",
        "parameters": {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "description": "The URL of the webpage to crawl, e.g. https://example.com",
                },
            },
            "required": ["url"],
        },
    }
}

# å·¥å…·3: è·å–å½“å‰æ—¥æœŸæ—¶é—´
date_time_schema = {
    "type": "function",
    "function": {
        "name": "get_current_datetime",
        "description": "Gets the current date, time, and day of week. Use this when user asks about today's date, current time, what day it is, or any time-related questions.",
        "parameters": {
            "type": "object",
            "properties": {},
            "required": [],
        },
    }
}

ALL_TOOLS = [gpu_info_schema, web_crawler_schema, date_time_schema]

# ================== å·¥å…·å®ç° ==================

def execute_get_gpu_info():
    """æ‰§è¡Œ nvidia-smi å‘½ä»¤è·å–æ˜¾å¡ä¿¡æ¯"""
    try:
        result = subprocess.run(
            ["nvidia-smi"],
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            return result.stdout
        else:
            return f"é”™è¯¯: {result.stderr}"
    except FileNotFoundError:
        return "é”™è¯¯: æœªæ‰¾åˆ° nvidia-smi å‘½ä»¤ï¼Œå¯èƒ½æœªå®‰è£… NVIDIA é©±åŠ¨"
    except subprocess.TimeoutExpired:
        return "é”™è¯¯: å‘½ä»¤æ‰§è¡Œè¶…æ—¶"
    except Exception as e:
        return f"é”™è¯¯: {str(e)}"

def execute_get_datetime():
    """è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´"""
    from datetime import datetime
    import locale
    
    now = datetime.now()
    
    # æ˜ŸæœŸå‡ çš„ä¸­æ–‡æ˜ å°„
    weekdays = ['æ˜ŸæœŸä¸€', 'æ˜ŸæœŸäºŒ', 'æ˜ŸæœŸä¸‰', 'æ˜ŸæœŸå››', 'æ˜ŸæœŸäº”', 'æ˜ŸæœŸå…­', 'æ˜ŸæœŸæ—¥']
    weekday_cn = weekdays[now.weekday()]
    
    result = f"""å½“å‰æ—¥æœŸæ—¶é—´ä¿¡æ¯ï¼š
- æ—¥æœŸ: {now.strftime('%Yå¹´%mæœˆ%dæ—¥')}
- æ—¶é—´: {now.strftime('%H:%M:%S')}
- æ˜ŸæœŸ: {weekday_cn}
- ISOæ ¼å¼: {now.isoformat()}
- æ—¶é—´æˆ³: {int(now.timestamp())}"""
    
    return result

async def execute_crawl_webpage(url: str):
    """ä½¿ç”¨ crawl4ai æŠ“å–ç½‘é¡µå†…å®¹"""
    try:
        browser_config = BrowserConfig()
        run_config = CrawlerRunConfig()
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=run_config)
            return result.markdown if result.markdown else "æœªèƒ½æå–åˆ°ç½‘é¡µå†…å®¹"
    except Exception as e:
        return f"æŠ“å–é”™è¯¯: {str(e)}"

def summarize_with_llm(content: str, max_length: int = 5000):
    """ä½¿ç”¨ LLM æ€»ç»“å†…å®¹"""
    # æˆªæ–­è¿‡é•¿çš„å†…å®¹
    if len(content) > max_length:
        content = content[:max_length] + "\n\n[å†…å®¹å·²æˆªæ–­...]"
    
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹åˆ†æå¸ˆã€‚è¯·ç”¨ä¸­æ–‡å¯¹ä»¥ä¸‹ç½‘é¡µå†…å®¹è¿›è¡Œç®€æ´ã€æœ‰æ¡ç†çš„æ€»ç»“ï¼Œçªå‡ºå…³é”®ä¿¡æ¯å’Œè¦ç‚¹ã€‚å›å¤è¯·æ§åˆ¶åœ¨ 500 å­—ä»¥å†…ã€‚"},
                {"role": "user", "content": f"è¯·æ€»ç»“ä»¥ä¸‹ç½‘é¡µå†…å®¹ï¼š\n\n{content}"}
            ],
            max_tokens=1024,
            temperature=AI_TEMPERATURE
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"LLM æ€»ç»“é”™è¯¯: {str(e)}"

# ================== è§£æå‡½æ•°è°ƒç”¨ ==================

def parse_function_call(output: str):
    """è§£æ FunctionGemma çš„å‡½æ•°è°ƒç”¨è¾“å‡º"""
    # åŒ¹é…æ ¼å¼: <start_function_call>call:function_name{params}<end_function_call>
    pattern = r'<start_function_call>call:(\w+)\{([^}]*)\}<end_function_call>'
    match = re.search(pattern, output)
    
    if match:
        func_name = match.group(1)
        params_str = match.group(2)
        
        # è§£æå‚æ•°
        params = {}
        if params_str:
            # è§£ææ ¼å¼: key:<escape>value<escape>
            param_pattern = r'(\w+):<escape>([^<]*)<escape>'
            param_matches = re.findall(param_pattern, params_str)
            for key, value in param_matches:
                params[key] = value
        
        return func_name, params
    
    # å°è¯•åŒ¹é…æ— å‚æ•°çš„è°ƒç”¨
    pattern_no_params = r'<start_function_call>call:(\w+)\{\}<end_function_call>'
    match = re.search(pattern_no_params, output)
    if match:
        return match.group(1), {}
    
    return None, None

# ================== FunctionGemma æ¨ç† ==================

def generate_function_call(user_query: str, tools: list):
    """ä½¿ç”¨ FunctionGemma ç”Ÿæˆå‡½æ•°è°ƒç”¨"""
    message = [
        {
            "role": "developer",
            "content": "You are a model that can do function calling with the following functions"
        },
        {
            "role": "user",
            "content": user_query
        }
    ]
    
    inputs = processor.apply_chat_template(
        message,
        tools=tools,
        add_generation_prompt=True,
        return_dict=True,
        return_tensors="pt"
    )
    
    # å°†è¾“å…¥ç§»åˆ°è®¾å¤‡ä¸Š
    input_ids = inputs["input_ids"].to(model.device)
    attention_mask = inputs.get("attention_mask")
    if attention_mask is not None:
        attention_mask = attention_mask.to(model.device)
    
    # æ¸…é™¤ CUDA ç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    with torch.no_grad():
        out = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=128,
            do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç é¿å… CUDA é‡‡æ ·é—®é¢˜
        )
    
    # è§£ç è¾“å‡ºï¼Œä¿ç•™ç‰¹æ®Štokenä»¥ä¾¿è§£æå‡½æ•°è°ƒç”¨
    generated_ids = out[0][len(input_ids[0]):]
    output = processor.decode(generated_ids, skip_special_tokens=False)
    
    # æ¸…ç†è¾“å‡ºä¸­çš„ pad token
    output = output.replace("<pad>", "").strip()
    
    return output

# ================== ä¸»å¤„ç†å‡½æ•° ==================

def process_gpu_query(user_query: str):
    """å¤„ç†æ˜¾å¡æŸ¥è¯¢è¯·æ±‚"""
    if not user_query.strip():
        return "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", "", ""
    
    # æ­¥éª¤1: ä½¿ç”¨ FunctionGemma ç”Ÿæˆå‡½æ•°è°ƒç”¨
    raw_output = generate_function_call(user_query, [gpu_info_schema])
    
    # æ­¥éª¤2: è§£æå‡½æ•°è°ƒç”¨
    func_name, params = parse_function_call(raw_output)
    
    if func_name == "get_gpu_info":
        # æ­¥éª¤3: æ‰§è¡Œå·¥å…·
        tool_result = execute_get_gpu_info()
        
        tool_call_info = f"""ğŸ“ **å·¥å…·è°ƒç”¨ä¿¡æ¯**
- è¯†åˆ«åˆ°çš„å‡½æ•°: `{func_name}`
- å‚æ•°: `{params if params else 'æ— å‚æ•°'}`
- åŸå§‹è¾“å‡º: `{raw_output.strip()}`"""
        
        return tool_call_info, raw_output.strip(), tool_result
    else:
        return f"æœªèƒ½è¯†åˆ«å‡½æ•°è°ƒç”¨\nåŸå§‹è¾“å‡º: {raw_output}", raw_output.strip(), "æœªæ‰§è¡Œå·¥å…·"

async def process_web_query_async(user_query: str, url: str):
    """å¤„ç†ç½‘é¡µæŠ“å–è¯·æ±‚ï¼ˆå¼‚æ­¥ï¼‰"""
    if not user_query.strip():
        return "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", "", "", ""
    
    if not url.strip():
        return "è¯·è¾“å…¥è¦æŠ“å–çš„ç½‘å€", "", "", ""
    
    # æ­¥éª¤1: ä½¿ç”¨ FunctionGemma ç”Ÿæˆå‡½æ•°è°ƒç”¨
    query_with_url = f"{user_query} The URL is: {url}"
    raw_output = generate_function_call(query_with_url, [web_crawler_schema])
    
    # æ­¥éª¤2: è§£æå‡½æ•°è°ƒç”¨
    func_name, params = parse_function_call(raw_output)
    
    if func_name == "crawl_webpage":
        # ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„ URLï¼ˆå¦‚æœæ¨¡å‹æ²¡æœ‰æ­£ç¡®è§£æï¼‰
        target_url = params.get("url", url)
        if not target_url or target_url == url:
            target_url = url
        
        # æ­¥éª¤3: æ‰§è¡Œç½‘é¡µæŠ“å–
        crawl_result = await execute_crawl_webpage(target_url)
        
        # æ­¥éª¤4: ä½¿ç”¨ LLM æ€»ç»“
        summary = summarize_with_llm(crawl_result)
        
        tool_call_info = f"""ğŸ“ **å·¥å…·è°ƒç”¨ä¿¡æ¯**
- è¯†åˆ«åˆ°çš„å‡½æ•°: `{func_name}`
- ç›®æ ‡ç½‘å€: `{target_url}`
- åŸå§‹è¾“å‡º: `{raw_output.strip()}`"""
        
        # æˆªæ–­æ˜¾ç¤ºçš„åŸå§‹å†…å®¹
        display_content = crawl_result[:3000] + "..." if len(crawl_result) > 3000 else crawl_result
        
        return tool_call_info, raw_output.strip(), display_content, summary
    else:
        return f"æœªèƒ½è¯†åˆ«å‡½æ•°è°ƒç”¨\nåŸå§‹è¾“å‡º: {raw_output}", raw_output.strip(), "æœªæ‰§è¡ŒæŠ“å–", "æ— æ³•ç”Ÿæˆæ€»ç»“"

def process_web_query(user_query: str, url: str):
    """å¤„ç†ç½‘é¡µæŠ“å–è¯·æ±‚ï¼ˆåŒæ­¥åŒ…è£…ï¼‰"""
    return asyncio.run(process_web_query_async(user_query, url))

# ================== æ™ºèƒ½èŠå¤©å¤„ç† ==================

def extract_url_from_text(text: str):
    """ä»æ–‡æœ¬ä¸­æå– URL"""
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    match = re.search(url_pattern, text)
    return match.group(0) if match else None

def should_use_tools(user_message: str):
    """
    é¢„åˆ¤æ–­ç”¨æˆ·æ¶ˆæ¯æ˜¯å¦å¯èƒ½éœ€è¦è°ƒç”¨å·¥å…·
    ä½¿ç”¨å…³é”®è¯åŒ¹é…æ¥é¿å…ä¸å¿…è¦çš„ FunctionGemma è°ƒç”¨
    è¿”å›: (éœ€è¦å·¥å…·, æ¨æµ‹çš„å·¥å…·ç±»å‹)
    """
    msg_lower = user_message.lower()
    
    # GPU ç›¸å…³å…³é”®è¯
    gpu_keywords = ['gpu', 'cuda', 'nvidia', 'æ˜¾å¡', 'æ˜¾å­˜', 'æ˜¾ç¤ºå¡', 'graphics', 'å›¾å½¢å¡', 
                    'nvidia-smi', 'ç®—åŠ›', 'vram', 'æ˜¾å¡é…ç½®', 'æ˜¾å¡ä¿¡æ¯', 'æ˜¾å¡çŠ¶æ€']
    
    # ç½‘é¡µæŠ“å–ç›¸å…³å…³é”®è¯
    web_keywords = ['http://', 'https://', 'www.', 'ç½‘é¡µ', 'æŠ“å–', 'crawl', 'fetch', 
                    'ç½‘ç«™', 'url', 'é“¾æ¥', 'webpage', 'website']
    
    # æ—¥æœŸæ—¶é—´ç›¸å…³å…³é”®è¯
    datetime_keywords = ['æ—¥æœŸ', 'æ—¶é—´', 'ä»Šå¤©', 'ç°åœ¨', 'å‡ å·', 'å‡ ç‚¹', 'æ˜ŸæœŸ', 'å‘¨å‡ ', 
                         'date', 'time', 'today', 'now', 'what day', 'current time', 'å‡ æ—¥']
    
    for kw in gpu_keywords:
        if kw in msg_lower:
            return True, "gpu"
    
    for kw in web_keywords:
        if kw in msg_lower:
            return True, "web"
    
    for kw in datetime_keywords:
        if kw in msg_lower:
            return True, "datetime"
    
    return False, None

def chat_with_tools(user_message: str, history: list):
    """
    æ™ºèƒ½èŠå¤©ï¼šè‡ªåŠ¨è¯†åˆ«æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
    å…ˆç”¨å…³é”®è¯é¢„åˆ¤æ–­ï¼Œå†ä½¿ç”¨ FunctionGemma å†³å®šè°ƒç”¨å“ªä¸ªå·¥å…·
    history ä½¿ç”¨ Gradio 6.x messages æ ¼å¼: [{"role": "user/assistant", "content": "..."}, ...]
    """
    if not user_message.strip():
        return history, ""
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    history = list(history) + [{"role": "user", "content": user_message}]
    
    # é¢„åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
    needs_tools, tool_hint = should_use_tools(user_message)
    
    tool_info = ""
    func_name = None
    params = {}
    raw_output = ""
    
    if needs_tools:
        # ä½¿ç”¨ FunctionGemma åˆ¤æ–­è°ƒç”¨å“ªä¸ªå·¥å…·
        raw_output = generate_function_call(user_message, ALL_TOOLS)
        func_name, params = parse_function_call(raw_output)
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœ FunctionGemma æ²¡æœ‰æ­£ç¡®è¯†åˆ«ï¼Œæ ¹æ®å…³é”®è¯æç¤ºç›´æ¥è°ƒç”¨
        if func_name is None and tool_hint:
            if tool_hint == "gpu":
                func_name = "get_gpu_info"
            elif tool_hint == "web":
                func_name = "crawl_webpage"
            elif tool_hint == "datetime":
                func_name = "get_current_datetime"
            raw_output = f"[å…³é”®è¯åŒ¹é…å¤‡ç”¨] -> {func_name}"
    
    if func_name == "get_gpu_info":
        # è°ƒç”¨ GPU ä¿¡æ¯å·¥å…·
        tool_info = f"ğŸ”§ **FunctionGemma å†³å®šè°ƒç”¨å·¥å…·**: `get_gpu_info`\n\nåŸå§‹è¾“å‡º: `{raw_output}`"
        gpu_result = execute_get_gpu_info()
        
        # ä½¿ç”¨ LLM ç”Ÿæˆå‹å¥½çš„å›å¤
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åŠ©æ‰‹ã€‚ç”¨æˆ·è¯¢é—®äº† GPU ä¿¡æ¯ï¼Œä½ å·²ç»é€šè¿‡è°ƒç”¨ nvidia-smi å‘½ä»¤è·å–äº†æ˜¾å¡ä¿¡æ¯ã€‚è¯·æ ¹æ®è·å–åˆ°çš„ä¿¡æ¯ï¼Œç”¨å‹å¥½ã€ä¸“ä¸šçš„ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"},
                    {"role": "user", "content": f"ç”¨æˆ·é—®é¢˜: {user_message}\n\nnvidia-smi è¾“å‡ºç»“æœ:\n{gpu_result}\n\nè¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·ã€‚"}
                ],
                max_tokens=1024,
                temperature=AI_TEMPERATURE
            )
            assistant_reply = response.choices[0].message.content
        except Exception as e:
            assistant_reply = f"è·å–åˆ°çš„ GPU ä¿¡æ¯:\n```\n{gpu_result}\n```\n\n(LLM å›å¤ç”Ÿæˆå¤±è´¥: {str(e)})"
        
        # æ·»åŠ å·¥å…·è°ƒç”¨æ ‡è®°
        full_reply = f"ğŸ”§ *[å·²è°ƒç”¨å·¥å…·: get_gpu_info]*\n\n{assistant_reply}"
        history.append({"role": "assistant", "content": full_reply})
        
    elif func_name == "crawl_webpage":
        # è°ƒç”¨ç½‘é¡µæŠ“å–å·¥å…·
        url = params.get("url") or extract_url_from_text(user_message)
        
        if not url:
            history.append({"role": "assistant", "content": "âŒ æŠ±æ­‰ï¼Œæˆ‘éœ€è¦ä¸€ä¸ªæœ‰æ•ˆçš„ URL æ‰èƒ½æŠ“å–ç½‘é¡µå†…å®¹ã€‚è¯·åœ¨æ¶ˆæ¯ä¸­åŒ…å«å®Œæ•´çš„ç½‘å€ï¼ˆå¦‚ https://example.comï¼‰"})
            return history, ""
        
        tool_info = f"ğŸ”§ **FunctionGemma å†³å®šè°ƒç”¨å·¥å…·**: `crawl_webpage`\n\nç›®æ ‡ URL: `{url}`\n\nåŸå§‹è¾“å‡º: `{raw_output}`"
        
        # å¼‚æ­¥æŠ“å–ç½‘é¡µ
        try:
            crawl_result = asyncio.run(execute_crawl_webpage(url))
            
            # ä½¿ç”¨ LLM æ€»ç»“å†…å®¹
            summary = summarize_with_llm(crawl_result)
            
            full_reply = f"ğŸ”§ *[å·²è°ƒç”¨å·¥å…·: crawl_webpage]*\n\nğŸ“„ **ç½‘é¡µå†…å®¹æ€»ç»“** ({url}):\n\n{summary}"
            history.append({"role": "assistant", "content": full_reply})
            
        except Exception as e:
            history.append({"role": "assistant", "content": f"âŒ æŠ“å–ç½‘é¡µæ—¶å‡ºé”™: {str(e)}"})
            return history, ""
    
    elif func_name == "get_current_datetime":
        # è°ƒç”¨æ—¥æœŸæ—¶é—´å·¥å…·
        tool_info = f"ğŸ”§ **FunctionGemma å†³å®šè°ƒç”¨å·¥å…·**: `get_current_datetime`\n\nåŸå§‹è¾“å‡º: `{raw_output}`"
        datetime_result = execute_get_datetime()
        
        # ä½¿ç”¨ LLM ç”Ÿæˆå‹å¥½çš„å›å¤
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚ç”¨æˆ·è¯¢é—®äº†æ—¥æœŸæ—¶é—´ä¿¡æ¯ï¼Œä½ å·²ç»è·å–äº†å½“å‰çš„æ—¥æœŸå’Œæ—¶é—´ã€‚è¯·æ ¹æ®è·å–åˆ°çš„ä¿¡æ¯ï¼Œç”¨å‹å¥½çš„ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"},
                    {"role": "user", "content": f"ç”¨æˆ·é—®é¢˜: {user_message}\n\nè·å–åˆ°çš„æ—¥æœŸæ—¶é—´ä¿¡æ¯:\n{datetime_result}\n\nè¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·ã€‚"}
                ],
                max_tokens=512,
                temperature=AI_TEMPERATURE
            )
            assistant_reply = response.choices[0].message.content
        except Exception as e:
            assistant_reply = f"è·å–åˆ°çš„æ—¥æœŸæ—¶é—´ä¿¡æ¯:\n{datetime_result}\n\n(LLM å›å¤ç”Ÿæˆå¤±è´¥: {str(e)})"
        
        # æ·»åŠ å·¥å…·è°ƒç”¨æ ‡è®°
        full_reply = f"ğŸ”§ *[å·²è°ƒç”¨å·¥å…·: get_current_datetime]*\n\n{assistant_reply}"
        history.append({"role": "assistant", "content": full_reply})
    
    else:
        # æ²¡æœ‰éœ€è¦è°ƒç”¨çš„å·¥å…·ï¼Œç›´æ¥ä½¿ç”¨ LLM å›å¤
        if needs_tools and tool_hint:
            tool_info = f"â„¹ï¸ **å…³é”®è¯æ£€æµ‹åˆ°å¯èƒ½éœ€è¦å·¥å…·ï¼Œä½†æœªæˆåŠŸè°ƒç”¨**\n\nåŸå§‹è¾“å‡º: `{raw_output}`\n\nå°†ç›´æ¥ä½¿ç”¨ LLM å›ç­”..."
        else:
            tool_info = "â„¹ï¸ **æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨å…³é”®è¯**\n\nç›´æ¥ä½¿ç”¨ LLM å›ç­”..."
        
        try:
            # æ„å»ºå†å²æ¶ˆæ¯
            messages = [
                {"role": "system", "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. æŸ¥è¯¢ GPU/æ˜¾å¡ä¿¡æ¯ - ç”¨æˆ·å¯ä»¥é—®"æˆ‘çš„æ˜¾å¡é…ç½®æ˜¯ä»€ä¹ˆï¼Ÿ"æˆ–"GPU ä½¿ç”¨æƒ…å†µæ€æ ·ï¼Ÿ"
2. æŠ“å–å’Œæ€»ç»“ç½‘é¡µå†…å®¹ - ç”¨æˆ·å¯ä»¥è¯´"å¸®æˆ‘æ€»ç»“ https://example.com çš„å†…å®¹"
3. è·å–å½“å‰æ—¥æœŸæ—¶é—´ - ç”¨æˆ·å¯ä»¥é—®"ä»Šå¤©å‡ å·ï¼Ÿ"æˆ–"ç°åœ¨å‡ ç‚¹ï¼Ÿ"

å¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸è¿™äº›åŠŸèƒ½ç›¸å…³ï¼Œå¯ä»¥å¼•å¯¼ä»–ä»¬ä½¿ç”¨è¿™äº›åŠŸèƒ½ã€‚"""}
            ]
            
            # æ·»åŠ å†å²å¯¹è¯ (messages æ ¼å¼)
            for h in history[:-1]:
                role = h.get("role", "user")
                content = h.get("content", "") or ""
                if content and isinstance(content, str):
                    # ç§»é™¤å·¥å…·è°ƒç”¨æ ‡è®°ç”¨äºä¸Šä¸‹æ–‡
                    clean_content = re.sub(r'ğŸ”§ \*\[å·²è°ƒç”¨å·¥å…·: \w+\]\*\n\n', '', content)
                    messages.append({"role": role, "content": clean_content})
            
            messages.append({"role": "user", "content": user_message})
            
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages,
                max_tokens=2048,
                temperature=AI_TEMPERATURE
            )
            history.append({"role": "assistant", "content": response.choices[0].message.content})
        except Exception as e:
            history.append({"role": "assistant", "content": f"âŒ LLM å›å¤ç”Ÿæˆå¤±è´¥: {str(e)}"})
    
    return history, tool_info

# ================== Gradio ç•Œé¢ ==================

# è‡ªå®šä¹‰ CSS
custom_css = """
.youtube-banner {
    background: linear-gradient(135deg, #ff0000 0%, #cc0000 100%);
    color: white;
    padding: 15px 20px;
    border-radius: 10px;
    text-align: center;
    margin-bottom: 20px;
}
.youtube-banner a {
    color: white;
    text-decoration: none;
    font-weight: bold;
}
.youtube-banner a:hover {
    text-decoration: underline;
}
.tool-output {
    background-color: #f0f0f0;
    padding: 10px;
    border-radius: 5px;
    font-family: monospace;
}
"""

with gr.Blocks() as demo:
    
    # YouTube é¢‘é“ä¿¡æ¯æ¨ªå¹…
    gr.HTML("""
    <div class="youtube-banner">
        <h2>ğŸ¬ æ¬¢è¿è®¿é—®æˆ‘çš„ YouTube é¢‘é“</h2>
        <p>é¢‘é“åç§°ï¼š<strong>AI æŠ€æœ¯åˆ†äº«é¢‘é“</strong></p>
        <p><a href="https://www.youtube.com/@rongyikanshijie-ai" target="_blank">
            ğŸ‘‰ ç‚¹å‡»è®¢é˜…ï¼šhttps://www.youtube.com/@rongyikanshijie-ai
        </a></p>
    </div>
    """)
    
    gr.Markdown("""
    # ğŸ¤– FunctionGemma å·¥å…·è°ƒç”¨æ¼”ç¤º
    
    æœ¬åº”ç”¨æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Google çš„ FunctionGemma æ¨¡å‹è¿›è¡Œå‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰ã€‚
    æ¨¡å‹ä¼šæ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¾“å…¥ï¼Œè‡ªåŠ¨è¯†åˆ«éœ€è¦è°ƒç”¨çš„å·¥å…·ï¼Œå¹¶æ‰§è¡Œç›¸åº”æ“ä½œã€‚
    
    ---
    """)
    
    with gr.Tabs():
        # ============ Tab 1: æ˜¾å¡ä¿¡æ¯æŸ¥è¯¢ ============
        with gr.TabItem("ğŸ–¥ï¸ æ˜¾å¡é…ç½®æŸ¥è¯¢"):
            gr.Markdown("""
            ### åŠŸèƒ½è¯´æ˜
            è¾“å…¥å…³äºæ˜¾å¡/GPUçš„é—®é¢˜ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨è°ƒç”¨ `nvidia-smi` å‘½ä»¤è·å–æ˜¾å¡é…ç½®ä¿¡æ¯ã€‚
            """)
            
            with gr.Row():
                with gr.Column(scale=1):
                    gpu_input = gr.Textbox(
                        label="è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="What is my GPU configuration?",
                        lines=2,
                        value="What is my GPU configuration?"
                    )
                    gpu_examples = gr.Examples(
                        examples=[
                            ["What is my GPU configuration?"],
                            ["Show me the current GPU status"],
                            ["Tell me about the graphics card on this machine"],
                            ["What NVIDIA GPU do I have?"],
                        ],
                        inputs=gpu_input,
                        label="ç¤ºä¾‹æç¤ºè¯"
                    )
                    gpu_btn = gr.Button("ğŸ” æŸ¥è¯¢æ˜¾å¡ä¿¡æ¯", variant="primary")
                
                with gr.Column(scale=2):
                    gpu_tool_info = gr.Markdown(label="å·¥å…·è°ƒç”¨ä¿¡æ¯")
                    gpu_raw_output = gr.Textbox(
                        label="FunctionGemma åŸå§‹è¾“å‡º",
                        lines=2,
                        interactive=False
                    )
                    gpu_result = gr.Textbox(
                        label="å·¥å…·æ‰§è¡Œç»“æœ (nvidia-smi è¾“å‡º)",
                        lines=15,
                        interactive=False
                    )
            
            gpu_btn.click(
                fn=process_gpu_query,
                inputs=[gpu_input],
                outputs=[gpu_tool_info, gpu_raw_output, gpu_result]
            )
        
        # ============ Tab 2: ç½‘é¡µæŠ“å–ä¸æ€»ç»“ ============
        with gr.TabItem("ğŸŒ ç½‘é¡µæŠ“å–ä¸æ€»ç»“"):
            gr.Markdown("""
            ### åŠŸèƒ½è¯´æ˜
            è¾“å…¥é—®é¢˜å’Œç›®æ ‡ç½‘å€ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨è°ƒç”¨ç½‘é¡µæŠ“å–å·¥å…·è·å–å†…å®¹ï¼Œç„¶åä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½æ€»ç»“ã€‚
            """)
            
            with gr.Row():
                with gr.Column(scale=1):
                    web_input = gr.Textbox(
                        label="è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="Please crawl this webpage and summarize the content",
                        lines=2,
                        value="Please crawl this webpage and summarize the content"
                    )
                    url_input = gr.Textbox(
                        label="ç›®æ ‡ç½‘å€",
                        placeholder="https://example.com",
                        lines=1,
                        value="https://example.com"
                    )
                    web_examples = gr.Examples(
                        examples=[
                            ["Please crawl this webpage and summarize the content", "https://example.com"],
                            ["Get the content from this URL", "https://news.ycombinator.com"],
                            ["Fetch and analyze this webpage", "https://github.com"],
                        ],
                        inputs=[web_input, url_input],
                        label="ç¤ºä¾‹æç¤ºè¯"
                    )
                    web_btn = gr.Button("ğŸ” æŠ“å–å¹¶æ€»ç»“", variant="primary")
                
                with gr.Column(scale=2):
                    web_tool_info = gr.Markdown(label="å·¥å…·è°ƒç”¨ä¿¡æ¯")
                    web_raw_output = gr.Textbox(
                        label="FunctionGemma åŸå§‹è¾“å‡º",
                        lines=2,
                        interactive=False
                    )
                    web_content = gr.Textbox(
                        label="æŠ“å–çš„ç½‘é¡µå†…å®¹ï¼ˆéƒ¨åˆ†ï¼‰",
                        lines=8,
                        interactive=False
                    )
                    web_summary = gr.Markdown(label="ğŸ“ LLM æ™ºèƒ½æ€»ç»“")
            
            web_btn.click(
                fn=process_web_query,
                inputs=[web_input, url_input],
                outputs=[web_tool_info, web_raw_output, web_content, web_summary]
            )
        
        # ============ Tab 3: æ™ºèƒ½èŠå¤© ============
        with gr.TabItem("ğŸ’¬ æ™ºèƒ½èŠå¤©"):
            gr.Markdown("""
            ### åŠŸèƒ½è¯´æ˜
            åœ¨è¿™é‡Œä¸ AI è‡ªç„¶å¯¹è¯ã€‚å½“ä½ è¯¢é—® GPU ä¿¡æ¯ã€æ—¥æœŸæ—¶é—´æˆ–éœ€è¦æŠ“å–ç½‘é¡µå†…å®¹æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è°ƒç”¨ç›¸åº”å·¥å…·ï¼
            
            **è¯•è¯•è¿™æ ·é—®ï¼š**
            - "æˆ‘çš„æ˜¾å¡é…ç½®æ˜¯ä»€ä¹ˆï¼Ÿ"
            - "å¸®æˆ‘æ€»ç»“ä¸€ä¸‹ https://example.com çš„å†…å®¹"
            - "ä»Šå¤©æ˜¯å‡ å·ï¼Ÿç°åœ¨å‡ ç‚¹äº†ï¼Ÿ"
            """)
            
            with gr.Row():
                with gr.Column(scale=2):
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯è®°å½•",
                        height=450
                    )
                    
                    with gr.Row():
                        chat_input = gr.Textbox(
                            label="è¾“å…¥æ¶ˆæ¯",
                            placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼šæˆ‘çš„æ˜¾å¡æ˜¯ä»€ä¹ˆå‹å·ï¼Ÿæˆ–è€…ï¼šå¸®æˆ‘æ€»ç»“ https://example.com",
                            lines=2,
                            scale=4
                        )
                        chat_btn = gr.Button("å‘é€ ğŸ’¬", variant="primary", scale=1)
                    
                    with gr.Row():
                        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯")
                        
                    chat_examples = gr.Examples(
                        examples=[
                            ["ä½ å¥½ï¼Œä½ æ˜¯è°å•Šï¼Ÿ"],
                            ["æˆ‘çš„æ˜¾å¡é…ç½®æ˜¯ä»€ä¹ˆï¼Ÿ"],
                            ["å¸®æˆ‘æŠ“å–å¹¶æ€»ç»“ https://cj.sina.com.cn/articles/view/2290787940/888aa66401901gt2w çš„å†…å®¹"],
                            ["ä»Šå¤©æ˜¯å‡ å·ï¼Ÿ"],
                        ],
                        inputs=chat_input,
                        label="ç¤ºä¾‹å¯¹è¯"
                    )
                
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ”§ å·¥å…·è°ƒç”¨æ—¥å¿—")
                    tool_log = gr.Markdown(
                        value="*ç­‰å¾…ç”¨æˆ·è¾“å…¥...*",
                        label="FunctionGemma å·¥å…·è°ƒç”¨æ—¥å¿—"
                    )
                    
                    gr.Markdown("""
                    ---
                    ### å¯ç”¨å·¥å…·
                    
                    | å·¥å…· | åŠŸèƒ½ |
                    |------|------|
                    | `get_gpu_info` | è·å– GPU é…ç½®ä¿¡æ¯ |
                    | `crawl_webpage` | æŠ“å–ç½‘é¡µå†…å®¹ |
                    | `get_current_datetime` | è·å–å½“å‰æ—¥æœŸæ—¶é—´ |
                    
                    ---
                    **å·¥ä½œæµç¨‹:**
                    1. ç”¨æˆ·è¾“å…¥é—®é¢˜
                    2. FunctionGemma åˆ†ææ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
                    3. å¦‚éœ€è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·è·å–ç»“æœ
                    4. LLM æ ¹æ®å·¥å…·ç»“æœç”Ÿæˆå›å¤
                    """)
            
            # ç»‘å®šäº‹ä»¶
            chat_btn.click(
                fn=chat_with_tools,
                inputs=[chat_input, chatbot],
                outputs=[chatbot, tool_log]
            ).then(
                fn=lambda: "",
                outputs=[chat_input]
            )
            
            chat_input.submit(
                fn=chat_with_tools,
                inputs=[chat_input, chatbot],
                outputs=[chatbot, tool_log]
            ).then(
                fn=lambda: "",
                outputs=[chat_input]
            )
            
            clear_btn.click(
                fn=lambda: ([], "*ç­‰å¾…ç”¨æˆ·è¾“å…¥...*"),
                outputs=[chatbot, tool_log]
            )
    
    gr.Markdown("""
    ---
    ### ğŸ“Œ æŠ€æœ¯è¯´æ˜
    
    - **FunctionGemma**: Google å¼€å‘çš„ä¸“é—¨ç”¨äºå‡½æ•°è°ƒç”¨çš„è½»é‡çº§æ¨¡å‹
    - **å·¥å…·å®šä¹‰**: ä½¿ç”¨ JSON Schema æ ¼å¼å®šä¹‰å·¥å…·æ¥å£
    - **ç½‘é¡µæŠ“å–**: åŸºäº crawl4ai åº“å®ç°
    - **å†…å®¹æ€»ç»“**: ä½¿ç”¨ Qwen 2.5 72B å¤§è¯­è¨€æ¨¡å‹
    
    ---
    <p style="text-align: center; color: gray;">
        Made with â¤ï¸ | FunctionGemma å·¥å…·è°ƒç”¨æ¼”ç¤º
    </p>
    """)

# ================== å¯åŠ¨åº”ç”¨ ==================
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        css=custom_css
    )
