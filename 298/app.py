#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Qwen Image Fun - Gradio Web UI
åŸºäº examples/qwenimage_fun/predict_t2i_control.py å’Œ examples/qwenimage_fun/predict_i2i_inpaint.py
"""

import os
import sys
import gc
import torch
import gradio as gr
from PIL import Image
import numpy as np

from omegaconf import OmegaConf
from diffusers import FlowMatchEulerDiscreteScheduler

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path
current_file_path = os.path.abspath(__file__)
project_root = os.path.dirname(current_file_path)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from videox_fun.dist import set_multi_gpus_devices
from videox_fun.models import (AutoencoderKLQwenImage,
                               Qwen2_5_VLForConditionalGeneration,
                               Qwen2Tokenizer, QwenImageControlTransformer2DModel)
from videox_fun.pipeline import QwenImageControlPipeline
from videox_fun.utils.fm_solvers import FlowDPMSolverMultistepScheduler
from videox_fun.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler
from videox_fun.utils.fp8_optimization import (convert_model_weight_to_float8,
                                               convert_weight_dtype_wrapper)
from videox_fun.utils.lora_utils import merge_lora, unmerge_lora
from videox_fun.utils.utils import get_image_latent

# ====================== å…¨å±€é…ç½® ======================
# GPU å†…å­˜æ¨¡å¼
GPU_MEMORY_MODE = "model_cpu_offload_and_qfloat8"

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_PATH = "config/qwenimage/qwenimage_control.yaml"

# æ¨¡å‹è·¯å¾„ (ä½¿ç”¨ checkpoints æ–‡ä»¶å¤¹)
MODEL_NAME = "checkpoints/Qwen-Image-2512"
TRANSFORMER_PATH = "checkpoints/Qwen-Image-2512-Fun-Controlnet-Union/Qwen-Image-2512-Fun-Controlnet-Union.safetensors"

# æ•°æ®ç±»å‹
WEIGHT_DTYPE = torch.bfloat16

# é¢„è®¾åˆ†è¾¨ç‡é€‰é¡¹
RESOLUTION_OPTIONS = [
    "1728x992 (16:9 æ¨ªç‰ˆ)",
    "992x1728 (9:16 ç«–ç‰ˆ)",
    "1536x1024 (3:2 æ¨ªç‰ˆ)",
    "1024x1536 (2:3 ç«–ç‰ˆ)",
    "1024x1024 (1:1 æ­£æ–¹å½¢)",
    "1280x960 (4:3 æ¨ªç‰ˆ)",
    "960x1280 (3:4 ç«–ç‰ˆ)",
    "2048x1024 (2:1 è¶…å®½)",
    "1024x2048 (1:2 è¶…é«˜)",
]

# é‡‡æ ·å™¨é€‰é¡¹
SAMPLER_OPTIONS = ["Flow", "Flow_Unipc", "Flow_DPM++"]

# YouTube é¢‘é“ä¿¡æ¯
YOUTUBE_CHANNEL_NAME = "AI æŠ€æœ¯åˆ†äº«é¢‘é“"
YOUTUBE_CHANNEL_URL = "https://www.youtube.com/@rongyikanshijie-ai"

# ====================== å…¨å±€å˜é‡ ======================
pipeline = None
device = None

# ====================== ç¤ºä¾‹æç¤ºè¯ ======================
EXAMPLE_PROMPT = "ç”»é¢ä¸­å¤®æ˜¯ä¸€ä½å¹´è½»å¥³å­©ï¼Œå¥¹æ‹¥æœ‰ä¸€å¤´ä»¤äººå°è±¡æ·±åˆ»çš„äº®ç´«è‰²é•¿å‘ï¼Œå‘ä¸åœ¨æµ·é£ä¸­è½»ç›ˆé£˜æ‰¬ï¼Œè¥é€ å‡ºåŠ¨æ„Ÿè€Œå”¯ç¾çš„æ•ˆæœã€‚å¥¹çš„é•¿å‘ä¸¤ä¾§å„æ‰ç€é»‘è‰²è´è¶ç»“å‘é¥°ï¼Œå¢æ·»äº†å‡ åˆ†å¯çˆ±ä¸ä¿çš®æ„Ÿã€‚å¥³å­©èº«ç©¿ä¸€è¢­çº¯ç™½è‰²æ— è¢–è¿è¡£è£™ï¼Œè£™æ‘†è½»ç›ˆé£˜é€¸ï¼Œä¸å¥¹æ¸…æ–°çš„æ°”è´¨å®Œç¾å¥‘åˆã€‚å¥¹çš„å¦†å®¹ç²¾è‡´è‡ªç„¶ï¼Œæ·¡ç²‰è‰²çš„å”‡å¦†å’Œæ¸©æŸ”çš„çœ¼ç¥æµéœ²å‡ºæ¬é™ä¼˜é›…çš„æ°”è´¨ã€‚å¥¹å•æ‰‹å‰è…°ï¼Œå§¿æ€è‡ªä¿¡ä»å®¹ï¼Œç›®å…‰ç›´è§†é•œå¤´ï¼Œå±•ç°å‡ºæ—¢ç”œç¾åˆä¸å¤±ä¸ªæ€§çš„é­…åŠ›ã€‚èƒŒæ™¯æ˜¯ä¸€ç‰‡å¼€é˜”çš„æµ·æ™¯ï¼Œæ¹›è“çš„æµ·æ°´åœ¨é˜³å…‰ç…§å°„ä¸‹æ³¢å…‰ç²¼ç²¼ï¼Œé—ªçƒç€é’»çŸ³èˆ¬çš„å…‰èŠ’ã€‚å¤©ç©ºå‘ˆç°å‡ºæ¸…æ¾ˆçš„è”šè“è‰²ï¼Œç‚¹ç¼€ç€å‡ æœµæ´ç™½çš„äº‘æœµï¼Œè¥é€ å‡ºæ™´æœ—æ˜åªšçš„å¤æ—¥æ°›å›´ã€‚ç”»é¢å‰æ™¯å³ä¸‹è§’å¯è§ç²‰ç´«è‰²çš„å°èŠ±ä¸›å’Œç»¿è‰²æ¤ç‰©ï¼Œä¸ºæ•´ä½“æ„å›¾å¢æ·»äº†è‡ªç„¶ç”Ÿæœºå’Œè‰²å½©å±‚æ¬¡ã€‚æ•´å¼ ç…§ç‰‡è‰²è°ƒæ˜äº®æ¸…æ–°ï¼Œç´«è‰²å¤´å‘ä¸ç™½è‰²è£™è£…ã€è“è‰²æµ·å¤©å½¢æˆé²œæ˜è€Œå’Œè°çš„è‰²å½©å¯¹æ¯”ã€‚"

EXAMPLE_PROMPT_4 = "ä¸€åªå¨é£å‡›å‡›çš„èŠ±è±¹æ­£é¢ç‰¹å†™ï¼Œé”åˆ©çš„çœ¼ç›ç›´è§†å‰æ–¹ï¼Œè„¸ä¸Šå¸ƒæ»¡ç‹¬ç‰¹çš„æ–‘ç‚¹èŠ±çº¹ã€‚èƒŒæ™¯æ˜¯æ¨¡ç³Šçš„ä¸›æ—ç»¿å¶ï¼Œé˜³å…‰é€è¿‡æ ‘å¶æ´’ä¸‹æ–‘é©³çš„å…‰å½±ã€‚"
EXAMPLE_NEGATIVE_PROMPT_4 = "æ¨¡ç³Š, å˜å½¢, ä½è´¨é‡"


def load_pipeline():
    """åŠ è½½æ¨¡å‹å’Œç®¡é“"""
    global pipeline, device
    
    print("=" * 50)
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    print("=" * 50)
    
    # è®¾ç½®è®¾å¤‡
    device = set_multi_gpus_devices(1, 1)
    
    # åŠ è½½é…ç½®
    config = OmegaConf.load(CONFIG_PATH)
    
    # åŠ è½½ Transformer
    print(f"æ­£åœ¨åŠ è½½ Transformer: {MODEL_NAME}")
    transformer = QwenImageControlTransformer2DModel.from_pretrained(
        MODEL_NAME, 
        subfolder="transformer",
        low_cpu_mem_usage=True,
        torch_dtype=WEIGHT_DTYPE,
        transformer_additional_kwargs=OmegaConf.to_container(config['transformer_additional_kwargs']),
    ).to(WEIGHT_DTYPE)
    
    # åŠ è½½ ControlNet æƒé‡
    if TRANSFORMER_PATH is not None and os.path.exists(TRANSFORMER_PATH):
        print(f"æ­£åœ¨åŠ è½½ ControlNet æƒé‡: {TRANSFORMER_PATH}")
        if TRANSFORMER_PATH.endswith("safetensors"):
            from safetensors.torch import load_file
            state_dict = load_file(TRANSFORMER_PATH)
        else:
            state_dict = torch.load(TRANSFORMER_PATH, map_location="cpu")
        state_dict = state_dict["state_dict"] if "state_dict" in state_dict else state_dict
        m, u = transformer.load_state_dict(state_dict, strict=False)
        print(f"ç¼ºå¤±é”®: {len(m)}, æ„å¤–é”®: {len(u)}")
    
    # åŠ è½½ VAE
    print(f"æ­£åœ¨åŠ è½½ VAE: {MODEL_NAME}")
    vae = AutoencoderKLQwenImage.from_pretrained(
        MODEL_NAME, 
        subfolder="vae"
    ).to(WEIGHT_DTYPE)
    
    # åŠ è½½ Tokenizer å’Œ Text Encoder
    print(f"æ­£åœ¨åŠ è½½ Tokenizer å’Œ Text Encoder: {MODEL_NAME}")
    tokenizer = Qwen2Tokenizer.from_pretrained(
        MODEL_NAME, subfolder="tokenizer"
    )
    text_encoder = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        MODEL_NAME, subfolder="text_encoder", torch_dtype=WEIGHT_DTYPE
    )
    
    # åŠ è½½ Scheduler (é»˜è®¤ä½¿ç”¨ Flow)
    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
        MODEL_NAME, 
        subfolder="scheduler"
    )
    
    # åˆ›å»º Pipeline
    pipeline = QwenImageControlPipeline(
        vae=vae,
        tokenizer=tokenizer,
        text_encoder=text_encoder,
        transformer=transformer,
        scheduler=scheduler,
    )
    
    # åº”ç”¨ GPU å†…å­˜ä¼˜åŒ–
    if GPU_MEMORY_MODE == "sequential_cpu_offload":
        pipeline.enable_sequential_cpu_offload(device=device)
    elif GPU_MEMORY_MODE == "model_cpu_offload_and_qfloat8":
        convert_model_weight_to_float8(transformer, exclude_module_name=["img_in", "txt_in", "timestep"], device=device)
        convert_weight_dtype_wrapper(transformer, WEIGHT_DTYPE)
        pipeline.enable_model_cpu_offload(device=device)
    elif GPU_MEMORY_MODE == "model_cpu_offload":
        pipeline.enable_model_cpu_offload(device=device)
    elif GPU_MEMORY_MODE == "model_full_load_and_qfloat8":
        convert_model_weight_to_float8(transformer, exclude_module_name=["img_in", "txt_in", "timestep"], device=device)
        convert_weight_dtype_wrapper(transformer, WEIGHT_DTYPE)
        pipeline.to(device=device)
    else:
        pipeline.to(device=device)
    
    print("=" * 50)
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("=" * 50)
    
    return pipeline


def parse_resolution(resolution_str):
    """è§£æåˆ†è¾¨ç‡å­—ç¬¦ä¸²"""
    # æ ¼å¼: "1728x992 (16:9 æ¨ªç‰ˆ)"
    res_part = resolution_str.split(" ")[0]
    width, height = map(int, res_part.split("x"))
    return height, width  # è¿”å› (height, width) æ ¼å¼


def get_scheduler(sampler_name):
    """è·å–è°ƒåº¦å™¨"""
    scheduler_dict = {
        "Flow": FlowMatchEulerDiscreteScheduler,
        "Flow_Unipc": FlowUniPCMultistepScheduler,
        "Flow_DPM++": FlowDPMSolverMultistepScheduler,
    }
    Chosen_Scheduler = scheduler_dict.get(sampler_name, FlowMatchEulerDiscreteScheduler)
    return Chosen_Scheduler.from_pretrained(MODEL_NAME, subfolder="scheduler")


def generate_t2i_control(
    prompt,
    negative_prompt,
    control_image,
    resolution,
    sampler_name,
    guidance_scale,
    num_inference_steps,
    control_context_scale,
    seed,
    progress=gr.Progress(track_tqdm=True)
):
    """æ–‡ç”Ÿå›¾ + Control æ¨¡å¼"""
    global pipeline, device
    
    if pipeline is None:
        return None, "é”™è¯¯ï¼šæ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç¨å€™..."
    
    if control_image is None:
        return None, "é”™è¯¯ï¼šè¯·ä¸Šä¼ æ§åˆ¶å›¾åƒ"
    
    if not prompt.strip():
        return None, "é”™è¯¯ï¼šè¯·è¾“å…¥æç¤ºè¯"
    
    try:
        # è§£æåˆ†è¾¨ç‡
        height, width = parse_resolution(resolution)
        sample_size = [height, width]
        
        # æ›´æ–°è°ƒåº¦å™¨
        pipeline.scheduler = get_scheduler(sampler_name)
        
        # è®¾ç½®éšæœºç§å­
        if seed == -1:
            seed = torch.randint(0, 2**32 - 1, (1,)).item()
        generator = torch.Generator(device=device).manual_seed(int(seed))
        
        # å‡†å¤‡è¾“å…¥å›¾åƒ
        with torch.no_grad():
            # æ—  inpaint å›¾åƒ
            inpaint_image_input = torch.zeros([1, 3, sample_size[0], sample_size[1]])
            # å…¨ç™½ mask
            mask_image_input = torch.ones([1, 1, sample_size[0], sample_size[1]]) * 255
            # æ§åˆ¶å›¾åƒ
            control_image_input = get_image_latent(control_image, sample_size=sample_size)[:, :, 0]
            
            # ç”Ÿæˆå›¾åƒ
            sample = pipeline(
                prompt, 
                negative_prompt=negative_prompt,
                height=sample_size[0],
                width=sample_size[1],
                generator=generator,
                true_cfg_scale=guidance_scale,
                num_inference_steps=num_inference_steps,
                image=inpaint_image_input,
                mask_image=mask_image_input,
                control_image=control_image_input,
                control_context_scale=control_context_scale
            ).images
        
        # æ¸…ç† GPU å†…å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        result_image = sample[0]
        return result_image, f"ç”ŸæˆæˆåŠŸï¼ä½¿ç”¨çš„ç§å­: {seed}"
    
    except Exception as e:
        torch.cuda.empty_cache()
        gc.collect()
        return None, f"ç”Ÿæˆå¤±è´¥: {str(e)}"


def generate_i2i_inpaint(
    prompt,
    negative_prompt,
    inpaint_image,
    mask_image,
    control_image,
    resolution,
    sampler_name,
    guidance_scale,
    num_inference_steps,
    control_context_scale,
    seed,
    progress=gr.Progress(track_tqdm=True)
):
    """å›¾ç”Ÿå›¾ + Inpaint æ¨¡å¼"""
    global pipeline, device
    
    if pipeline is None:
        return None, "é”™è¯¯ï¼šæ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç¨å€™..."
    
    if inpaint_image is None:
        return None, "é”™è¯¯ï¼šè¯·ä¸Šä¼ å¾…ä¿®å¤å›¾åƒ"
    
    if mask_image is None:
        return None, "é”™è¯¯ï¼šè¯·ä¸Šä¼ é®ç½©å›¾åƒ"
    
    if not prompt.strip():
        return None, "é”™è¯¯ï¼šè¯·è¾“å…¥æç¤ºè¯"
    
    try:
        # è§£æåˆ†è¾¨ç‡
        height, width = parse_resolution(resolution)
        sample_size = [height, width]
        
        # æ›´æ–°è°ƒåº¦å™¨
        pipeline.scheduler = get_scheduler(sampler_name)
        
        # è®¾ç½®éšæœºç§å­
        if seed == -1:
            seed = torch.randint(0, 2**32 - 1, (1,)).item()
        generator = torch.Generator(device=device).manual_seed(int(seed))
        
        # å‡†å¤‡è¾“å…¥å›¾åƒ
        with torch.no_grad():
            # Inpaint å›¾åƒ
            inpaint_image_input = get_image_latent(inpaint_image, sample_size=sample_size)[:, :, 0]
            # Mask å›¾åƒ
            mask_image_input = get_image_latent(mask_image, sample_size=sample_size)[:, :1, 0]
            # æ§åˆ¶å›¾åƒ (å¯é€‰)
            if control_image is not None:
                control_image_input = get_image_latent(control_image, sample_size=sample_size)[:, :, 0]
            else:
                control_image_input = torch.zeros([1, 3, sample_size[0], sample_size[1]])
            
            # ç”Ÿæˆå›¾åƒ
            sample = pipeline(
                prompt, 
                negative_prompt=negative_prompt,
                height=sample_size[0],
                width=sample_size[1],
                generator=generator,
                true_cfg_scale=guidance_scale,
                num_inference_steps=num_inference_steps,
                image=inpaint_image_input,
                mask_image=mask_image_input,
                control_image=control_image_input,
                control_context_scale=control_context_scale
            ).images
        
        # æ¸…ç† GPU å†…å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        result_image = sample[0]
        return result_image, f"ç”ŸæˆæˆåŠŸï¼ä½¿ç”¨çš„ç§å­: {seed}"
    
    except Exception as e:
        torch.cuda.empty_cache()
        gc.collect()
        return None, f"ç”Ÿæˆå¤±è´¥: {str(e)}"


def create_ui():
    """åˆ›å»º Gradio UI"""
    
    # å‡†å¤‡ç¤ºä¾‹å›¾åƒ
    example_control_images = []
    example_inpaint_images = []
    
    # ä» asset æ–‡ä»¶å¤¹è·å–ç¤ºä¾‹
    asset_dir = "asset"
    if os.path.exists(asset_dir):
        # æ§åˆ¶å›¾åƒç¤ºä¾‹
        if os.path.exists(os.path.join(asset_dir, "pose.jpg")):
            example_control_images.append(os.path.join(asset_dir, "pose.jpg"))
        
        # Inpaint ç¤ºä¾‹
        if os.path.exists(os.path.join(asset_dir, "8.png")):
            example_inpaint_images.append(os.path.join(asset_dir, "8.png"))
    
    # ä» checkpoints æ–‡ä»¶å¤¹è·å–æ›´å¤šç¤ºä¾‹
    controlnet_asset_dir = "checkpoints/Qwen-Image-2512-Fun-Controlnet-Union/asset"
    if os.path.exists(controlnet_asset_dir):
        for img_name in ["pose.jpg", "pose2.jpg", "canny.jpg", "depth.jpg", "hed.jpg", "scribble.jpg"]:
            img_path = os.path.join(controlnet_asset_dir, img_name)
            if os.path.exists(img_path):
                example_control_images.append(img_path)
        
        # Inpaint ç¤ºä¾‹
        for img_name in ["inpaint.jpg"]:
            img_path = os.path.join(controlnet_asset_dir, img_name)
            if os.path.exists(img_path):
                example_inpaint_images.append(img_path)
        
        # Mask ç¤ºä¾‹
        mask_path = os.path.join(controlnet_asset_dir, "mask.jpg")
    
    # ä» asset æ–‡ä»¶å¤¹è·å– mask
    mask_example = os.path.join(asset_dir, "mask.png") if os.path.exists(os.path.join(asset_dir, "mask.png")) else None
    
    # CSS æ ·å¼
    css = """
    .youtube-banner {
        background: linear-gradient(135deg, #ff0000, #cc0000);
        color: white;
        padding: 15px 20px;
        border-radius: 10px;
        margin-bottom: 20px;
        text-align: center;
        font-size: 16px;
    }
    .youtube-banner a {
        color: white;
        text-decoration: underline;
        font-weight: bold;
    }
    .title-text {
        text-align: center;
        margin-bottom: 10px;
    }
    """
    
    with gr.Blocks(css=css, title="Qwen Image Fun - AI å›¾åƒç”Ÿæˆ") as demo:
        # YouTube é¢‘é“ä¿¡æ¯
        gr.HTML(f"""
        <div class="youtube-banner">
            ğŸ“º æ¬¢è¿è®¿é—®æˆ‘çš„ YouTube é¢‘é“: <a href="{YOUTUBE_CHANNEL_URL}" target="_blank">{YOUTUBE_CHANNEL_NAME}</a> 
            | æ›´å¤š AI æ•™ç¨‹å’ŒæŠ€æœ¯åˆ†äº«ï¼Œæ¬¢è¿è®¢é˜…ï¼
        </div>
        """)
        
        # æ ‡é¢˜
        gr.Markdown("""
        # ğŸ¨ Qwen Image Fun - AI å›¾åƒç”Ÿæˆ
        åŸºäº Qwen-Image-2512 æ¨¡å‹ï¼Œæ”¯æŒæ–‡ç”Ÿå›¾æ§åˆ¶ (T2I Control) å’Œå›¾åƒä¿®å¤ (I2I Inpaint) ä¸¤ç§æ¨¡å¼ã€‚
        """)
        
        with gr.Tabs():
            # ==================== æ–‡ç”Ÿå›¾ + Control æ ‡ç­¾é¡µ ====================
            with gr.TabItem("ğŸ“ æ–‡ç”Ÿå›¾ + Control"):
                gr.Markdown("### ä½¿ç”¨æ§åˆ¶å›¾åƒï¼ˆå§¿åŠ¿ã€è¾¹ç¼˜ã€æ·±åº¦ç­‰ï¼‰ç”Ÿæˆå›¾åƒ")
                
                with gr.Row():
                    with gr.Column(scale=1):
                        t2i_prompt = gr.Textbox(
                            label="æç¤ºè¯",
                            placeholder="è¯·è¾“å…¥æè¿°å›¾åƒå†…å®¹çš„æç¤ºè¯...",
                            lines=5,
                            value=""
                        )
                        t2i_negative_prompt = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯",
                            placeholder="è¾“å…¥ä¸å¸Œæœ›å‡ºç°çš„å†…å®¹...",
                            lines=2,
                            value=""
                        )
                        t2i_control_image = gr.Image(
                            label="æ§åˆ¶å›¾åƒï¼ˆå§¿åŠ¿å›¾ã€è¾¹ç¼˜å›¾ã€æ·±åº¦å›¾ç­‰ï¼‰",
                            type="filepath",
                            height=300
                        )
                        
                        with gr.Row():
                            t2i_resolution = gr.Dropdown(
                                label="è¾“å‡ºåˆ†è¾¨ç‡",
                                choices=RESOLUTION_OPTIONS,
                                value=RESOLUTION_OPTIONS[0]
                            )
                            t2i_sampler = gr.Dropdown(
                                label="é‡‡æ ·å™¨",
                                choices=SAMPLER_OPTIONS,
                                value=SAMPLER_OPTIONS[0]
                            )
                        
                        with gr.Row():
                            t2i_guidance_scale = gr.Slider(
                                label="å¼•å¯¼ç³»æ•° (CFG Scale)",
                                minimum=1.0,
                                maximum=10.0,
                                value=4.0,
                                step=0.5
                            )
                            t2i_steps = gr.Slider(
                                label="æ¨ç†æ­¥æ•°",
                                minimum=10,
                                maximum=100,
                                value=50,
                                step=5
                            )
                        
                        with gr.Row():
                            t2i_control_scale = gr.Slider(
                                label="æ§åˆ¶å¼ºåº¦",
                                minimum=0.0,
                                maximum=1.0,
                                value=0.80,
                                step=0.05
                            )
                            t2i_seed = gr.Number(
                                label="éšæœºç§å­ (-1 ä¸ºéšæœº)",
                                value=-1,
                                precision=0
                            )
                        
                        t2i_generate_btn = gr.Button("ğŸ¨ ç”Ÿæˆå›¾åƒ", variant="primary", size="lg")
                    
                    with gr.Column(scale=1):
                        t2i_output_image = gr.Image(
                            label="ç”Ÿæˆç»“æœ",
                            type="pil",
                            height=500
                        )
                        t2i_status = gr.Textbox(
                            label="çŠ¶æ€ä¿¡æ¯",
                            interactive=False
                        )
                
                # ç¤ºä¾‹
                if example_control_images:
                    gr.Markdown("### ğŸ“Œ ç¤ºä¾‹")
                    t2i_examples = gr.Examples(
                        examples=[
                            [
                                (EXAMPLE_PROMPT_4 if i == 3 else EXAMPLE_PROMPT),
                                (EXAMPLE_NEGATIVE_PROMPT_4 if i == 3 else " "),
                                img,
                                RESOLUTION_OPTIONS[0],
                                "Flow",
                                4.0,
                                50,
                                0.80,
                                43,
                            ]
                            for i, img in enumerate(example_control_images[:4])
                        ],
                        inputs=[t2i_prompt, t2i_negative_prompt, t2i_control_image, t2i_resolution, 
                               t2i_sampler, t2i_guidance_scale, t2i_steps, t2i_control_scale, t2i_seed],
                        label="ç‚¹å‡»åŠ è½½ç¤ºä¾‹"
                    )
                
                # ç»‘å®šç”Ÿæˆäº‹ä»¶
                t2i_generate_btn.click(
                    fn=generate_t2i_control,
                    inputs=[t2i_prompt, t2i_negative_prompt, t2i_control_image, t2i_resolution,
                           t2i_sampler, t2i_guidance_scale, t2i_steps, t2i_control_scale, t2i_seed],
                    outputs=[t2i_output_image, t2i_status]
                )
            
            # ==================== å›¾ç”Ÿå›¾ + Inpaint æ ‡ç­¾é¡µ ====================
            with gr.TabItem("ğŸ–Œï¸ å›¾ç”Ÿå›¾ + Inpaint"):
                gr.Markdown("### ä½¿ç”¨åŸå›¾å’Œé®ç½©è¿›è¡Œå›¾åƒä¿®å¤æˆ–é‡ç»˜")
                
                with gr.Row():
                    with gr.Column(scale=1):
                        i2i_prompt = gr.Textbox(
                            label="æç¤ºè¯",
                            placeholder="è¯·è¾“å…¥æè¿°å›¾åƒå†…å®¹çš„æç¤ºè¯...",
                            lines=5,
                            value=""
                        )
                        i2i_negative_prompt = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯",
                            placeholder="è¾“å…¥ä¸å¸Œæœ›å‡ºç°çš„å†…å®¹...",
                            lines=2,
                            value=""
                        )
                        
                        with gr.Row():
                            i2i_inpaint_image = gr.Image(
                                label="å¾…ä¿®å¤å›¾åƒ",
                                type="filepath",
                                height=200
                            )
                            i2i_mask_image = gr.Image(
                                label="é®ç½©å›¾åƒï¼ˆç™½è‰²åŒºåŸŸå°†è¢«é‡ç»˜ï¼‰",
                                type="filepath",
                                height=200
                            )
                        
                        i2i_control_image = gr.Image(
                            label="æ§åˆ¶å›¾åƒï¼ˆå¯é€‰ï¼Œç”¨äºå¼•å¯¼ç”Ÿæˆï¼‰",
                            type="filepath",
                            height=200
                        )
                        
                        with gr.Row():
                            i2i_resolution = gr.Dropdown(
                                label="è¾“å‡ºåˆ†è¾¨ç‡",
                                choices=RESOLUTION_OPTIONS,
                                value=RESOLUTION_OPTIONS[0]
                            )
                            i2i_sampler = gr.Dropdown(
                                label="é‡‡æ ·å™¨",
                                choices=SAMPLER_OPTIONS,
                                value=SAMPLER_OPTIONS[0]
                            )
                        
                        with gr.Row():
                            i2i_guidance_scale = gr.Slider(
                                label="å¼•å¯¼ç³»æ•° (CFG Scale)",
                                minimum=1.0,
                                maximum=10.0,
                                value=4.0,
                                step=0.5
                            )
                            i2i_steps = gr.Slider(
                                label="æ¨ç†æ­¥æ•°",
                                minimum=10,
                                maximum=100,
                                value=50,
                                step=5
                            )
                        
                        with gr.Row():
                            i2i_control_scale = gr.Slider(
                                label="æ§åˆ¶å¼ºåº¦",
                                minimum=0.0,
                                maximum=1.0,
                                value=0.80,
                                step=0.05
                            )
                            i2i_seed = gr.Number(
                                label="éšæœºç§å­ (-1 ä¸ºéšæœº)",
                                value=-1,
                                precision=0
                            )
                        
                        i2i_generate_btn = gr.Button("ğŸ–Œï¸ å¼€å§‹ä¿®å¤", variant="primary", size="lg")
                    
                    with gr.Column(scale=1):
                        i2i_output_image = gr.Image(
                            label="ç”Ÿæˆç»“æœ",
                            type="pil",
                            height=500
                        )
                        i2i_status = gr.Textbox(
                            label="çŠ¶æ€ä¿¡æ¯",
                            interactive=False
                        )
                
                # ç¤ºä¾‹
                inpaint_example_path = "asset/8.png"
                mask_example_path = "asset/mask.png"
                pose_example_path = "asset/pose.jpg"
                
                if os.path.exists(inpaint_example_path) and os.path.exists(mask_example_path):
                    gr.Markdown("### ğŸ“Œ ç¤ºä¾‹")
                    i2i_examples = gr.Examples(
                        examples=[
                            [EXAMPLE_PROMPT, " ", inpaint_example_path, mask_example_path, 
                             pose_example_path if os.path.exists(pose_example_path) else None,
                             RESOLUTION_OPTIONS[0], "Flow", 4.0, 50, 0.80, 43]
                        ],
                        inputs=[i2i_prompt, i2i_negative_prompt, i2i_inpaint_image, i2i_mask_image,
                               i2i_control_image, i2i_resolution, i2i_sampler, i2i_guidance_scale,
                               i2i_steps, i2i_control_scale, i2i_seed],
                        label="ç‚¹å‡»åŠ è½½ç¤ºä¾‹"
                    )
                
                # ç»‘å®šç”Ÿæˆäº‹ä»¶
                i2i_generate_btn.click(
                    fn=generate_i2i_inpaint,
                    inputs=[i2i_prompt, i2i_negative_prompt, i2i_inpaint_image, i2i_mask_image,
                           i2i_control_image, i2i_resolution, i2i_sampler, i2i_guidance_scale,
                           i2i_steps, i2i_control_scale, i2i_seed],
                    outputs=[i2i_output_image, i2i_status]
                )
        
        # ä½¿ç”¨è¯´æ˜
        gr.Markdown("""
        ---
        ### ğŸ“– ä½¿ç”¨è¯´æ˜
        
        **æ–‡ç”Ÿå›¾ + Control æ¨¡å¼:**
        1. ä¸Šä¼ ä¸€å¼ æ§åˆ¶å›¾åƒï¼ˆå¦‚å§¿åŠ¿å›¾ã€è¾¹ç¼˜å›¾ã€æ·±åº¦å›¾ç­‰ï¼‰
        2. è¾“å…¥æè¿°ç›®æ ‡å›¾åƒçš„æç¤ºè¯
        3. è°ƒæ•´å‚æ•°åç‚¹å‡»"ç”Ÿæˆå›¾åƒ"
        
        **å›¾ç”Ÿå›¾ + Inpaint æ¨¡å¼:**
        1. ä¸Šä¼ å¾…ä¿®å¤çš„åŸå§‹å›¾åƒ
        2. ä¸Šä¼ é®ç½©å›¾åƒï¼ˆç™½è‰²åŒºåŸŸè¡¨ç¤ºéœ€è¦é‡ç»˜çš„éƒ¨åˆ†ï¼‰
        3. å¯é€‰ä¸Šä¼ æ§åˆ¶å›¾åƒæ¥å¼•å¯¼ç”Ÿæˆ
        4. è¾“å…¥æç¤ºè¯åç‚¹å‡»"å¼€å§‹ä¿®å¤"
        
        **å‚æ•°è¯´æ˜:**
        - **å¼•å¯¼ç³»æ•° (CFG Scale)**: å€¼è¶Šå¤§ï¼Œç”Ÿæˆç»“æœè¶Šæ¥è¿‘æç¤ºè¯æè¿°ï¼Œä½†å¯èƒ½é™ä½å›¾åƒè´¨é‡
        - **æ¨ç†æ­¥æ•°**: æ­¥æ•°è¶Šå¤šï¼Œå›¾åƒè´¨é‡è¶Šé«˜ï¼Œä½†ç”Ÿæˆæ—¶é—´è¶Šé•¿
        - **æ§åˆ¶å¼ºåº¦**: æ§åˆ¶å›¾åƒå¯¹ç”Ÿæˆç»“æœçš„å½±å“ç¨‹åº¦
        - **éšæœºç§å­**: ç›¸åŒç§å­ä¼šç”Ÿæˆç›¸åŒç»“æœï¼Œ-1 ä¸ºéšæœº
        """)
    
    return demo


if __name__ == "__main__":
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    print("=" * 60)
    print("Qwen Image Fun - Gradio Web UI")
    print("=" * 60)
    
    load_pipeline()
    
    # åˆ›å»ºå¹¶å¯åŠ¨ UI
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        inbrowser=True
    )
