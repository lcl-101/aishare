#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
from pathlib import Path

# é¦–å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¿…é¡»åœ¨å¯¼å…¥å…¶ä»–æ¨¡å—ä¹‹å‰
JOBS_DIR = Path(__file__).parent.absolute()
MODEL_BASE = str(JOBS_DIR / "checkpoints" / "Hunyuan-GameCraft-1.0" / "stdmodels")
os.environ["MODEL_BASE"] = MODEL_BASE
os.environ["PYTHONPATH"] = f"{JOBS_DIR}:{os.environ.get('PYTHONPATH', '')}"
print(f"ğŸ”§ é¢„è®¾ç½®MODEL_BASEç¯å¢ƒå˜é‡: {MODEL_BASE}")
print(f"ğŸ”§ é¢„è®¾ç½®PYTHONPATHç¯å¢ƒå˜é‡: {os.environ['PYTHONPATH']}")

# ç°åœ¨æ‰å¯¼å…¥å…¶ä»–æ¨¡å—
import gradio as gr
import threading
import time
import tempfile
from PIL import Image
import torch
import numpy as np
import torchvision.transforms as transforms
import random

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(JOBS_DIR))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from hymm_sp.config import parse_args
from hymm_sp.sample_inference import HunyuanVideoSampler
from hymm_sp.data_kits.data_tools import save_videos_grid


class CropResize:
    """
    Custom transform to resize and crop images to a target size while preserving aspect ratio.
    """
    def __init__(self, size=(704, 1216)):
        self.target_h, self.target_w = size  

    def __call__(self, img):
        w, h = img.size
        scale = max(self.target_w / w, self.target_h / h)
        new_size = (int(h * scale), int(w * scale))
        resize_transform = transforms.Resize(new_size, interpolation=transforms.InterpolationMode.BILINEAR)
        resized_img = resize_transform(img)
        crop_transform = transforms.CenterCrop((self.target_h, self.target_w))
        return crop_transform(resized_img)


class HunyuanGameCraftWebUI:
    def __init__(self):
        self.output_dir = "./results"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æ¨¡å‹è·¯å¾„é…ç½®
        self.checkpoint_dir = JOBS_DIR / "checkpoints" / "Hunyuan-GameCraft-1.0"
        self.model_base = str(self.checkpoint_dir / "stdmodels")
        
        # æ¨¡å‹é…ç½®é€‰é¡¹
        self.model_configs = {
            "æ ‡å‡†æ¨¡å‹": {
                "checkpoint_path": str(self.checkpoint_dir / "gamecraft_models" / "mp_rank_00_model_states.pt"),
                "infer_steps": 30,
                "cfg_scale": 2.0,
                "use_fp8": False,
                "description": "æ ‡å‡†è´¨é‡æ¨¡å‹ï¼Œæ¨ç†è¾ƒæ…¢ä½†è´¨é‡è¾ƒé«˜"
            },
            "åŠ é€Ÿæ¨¡å‹": {
                "checkpoint_path": str(self.checkpoint_dir / "gamecraft_models" / "mp_rank_00_model_states_distill.pt"),
                "infer_steps": 8,
                "cfg_scale": 1.0,
                "use_fp8": True,
                "description": "è’¸é¦åŠ é€Ÿæ¨¡å‹ï¼Œæ¨ç†å¿«é€Ÿä½†è´¨é‡ç•¥ä½"
            }
        }
        
        # é»˜è®¤ä½¿ç”¨æ ‡å‡†æ¨¡å‹
        self.current_model = "æ ‡å‡†æ¨¡å‹"
        self.checkpoint_path = self.model_configs[self.current_model]["checkpoint_path"]
        
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {self.model_base}")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        self.model_available = self.check_model_files()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹ä¸ºNoneï¼Œå»¶è¿ŸåŠ è½½
        self.hunyuan_video_sampler = None
        self.model_loaded = False
        
        # åŠ¨ä½œåˆ—è¡¨é€‰é¡¹
        self.action_options = {
            "w": "å‘å‰ç§»åŠ¨ (W)",
            "s": "å‘åç§»åŠ¨ (S)", 
            "a": "å‘å·¦ç§»åŠ¨ (A)",
            "d": "å‘å³ç§»åŠ¨ (D)",
            "wa": "å‘å‰+å‘å·¦ (W+A)",
            "wd": "å‘å‰+å‘å³ (W+D)",
            "sa": "å‘å+å‘å·¦ (S+A)",
            "sd": "å‘å+å‘å³ (S+D)"
        }
        
        # åå‘æ˜ å°„ï¼šä»æ˜¾ç¤ºæ–‡æœ¬åˆ°åŠ¨ä½œä»£ç 
        self.action_reverse_map = {v: k for k, v in self.action_options.items()}
        
        # é¢„è®¾çš„promptæ¨¡æ¿
        self.prompt_templates = {
            "ä¸­ä¸–çºªæ‘åº„": "A charming medieval village with cobblestone streets, thatched-roof houses, and vibrant flower gardens under a bright blue sky.",
            "è‡ªå®šä¹‰": ""
        }

    def check_model_files(self):
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        model_base_exists = os.path.exists(self.model_base)
        
        print(f"æ¨¡å‹æ£€æŸ¥:")
        print(f"  - åŸºç¡€æ¨¡å‹ç›®å½•: {'âœ…' if model_base_exists else 'âŒ'} {self.model_base}")
        
        # æ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
        all_models_available = True
        for model_name, config in self.model_configs.items():
            checkpoint_exists = os.path.exists(config["checkpoint_path"])
            print(f"  - {model_name}: {'âœ…' if checkpoint_exists else 'âŒ'} {config['checkpoint_path']}")
            if not checkpoint_exists:
                all_models_available = False
        
        return model_base_exists and all_models_available

    def switch_model(self, model_name):
        """åˆ‡æ¢æ¨¡å‹é…ç½®"""
        if model_name in self.model_configs:
            self.current_model = model_name
            config = self.model_configs[model_name]
            self.checkpoint_path = config["checkpoint_path"]
            
            # å¦‚æœå·²ç»åŠ è½½äº†æ¨¡å‹ï¼Œéœ€è¦é‡æ–°åŠ è½½
            if self.model_loaded:
                self.model_loaded = False
                self.hunyuan_video_sampler = None
                print(f"ğŸ”„ åˆ‡æ¢åˆ°{model_name}ï¼Œæ¨¡å‹å°†åœ¨ä¸‹æ¬¡ç”Ÿæˆæ—¶é‡æ–°åŠ è½½")
            
            # è¿”å›æ¨èçš„å‚æ•°è®¾ç½®
            return (
                config["infer_steps"],
                config["cfg_scale"],
                f"âœ… å·²åˆ‡æ¢åˆ°{model_name} - {config['description']}"
            )
        else:
            return None, None, f"âŒ æœªçŸ¥çš„æ¨¡å‹: {model_name}"

    def load_model(self, progress_callback=None):
        """åŠ è½½æ¨¡å‹"""
        if self.model_loaded:
            return True
            
        try:
            if progress_callback:
                progress_callback(0.1, "å‡†å¤‡åŠ è½½æ¨¡å‹...")
            
            # è®¾ç½®å•GPUåˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
            os.environ["RANK"] = "0"
            os.environ["WORLD_SIZE"] = "1"
            os.environ["LOCAL_RANK"] = "0"
            os.environ["MASTER_ADDR"] = "localhost"
            os.environ["MASTER_PORT"] = "29605"
            
            # åˆ›å»ºé»˜è®¤å‚æ•°
            args = self.create_default_args()
            print(f"âœ… å‚æ•°åˆ›å»ºæˆåŠŸï¼Œlatent_channels: {args.latent_channels}")
            
            if progress_callback:
                progress_callback(0.3, "åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ...")
            
            # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆå•GPUæ¨¡å¼ï¼‰
            from hymm_sp.modules.parallel_states import initialize_distributed
            print("ğŸ”„ åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ...")
            initialize_distributed(args.seed)
            print("âœ… åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
            
            if progress_callback:
                progress_callback(0.5, "åŠ è½½Hunyuanè§†é¢‘ç”Ÿæˆå™¨...")
            
            # åŠ è½½æ¨¡å‹
            self.hunyuan_video_sampler = HunyuanVideoSampler.from_pretrained(
                self.checkpoint_path, 
                args=args, 
                device=self.device
            )
            
            if progress_callback:
                progress_callback(0.9, "æ¨¡å‹åŠ è½½å®Œæˆ...")
            
            self.model_loaded = True
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def create_default_args(self):
        """åˆ›å»ºé»˜è®¤å‚æ•°"""
        # ä½¿ç”¨parse_argsåˆ›å»ºé»˜è®¤å‚æ•°ï¼Œä½†ä¸ä»å‘½ä»¤è¡Œè§£æ
        import argparse
        parser = argparse.ArgumentParser()
        from hymm_sp.config import add_extra_args, sanity_check_args
        parser = add_extra_args(parser)
        
        # è·å–å½“å‰æ¨¡å‹é…ç½®
        config = self.model_configs[self.current_model]
        
        # åˆ›å»ºé»˜è®¤å‚æ•°ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„æ¨¡å‹å‚æ•°
        base_args = [
            "--ckpt", self.checkpoint_path,
            "--seed", "250160",
            "--infer-steps", str(config["infer_steps"]),
            "--cfg-scale", str(config["cfg_scale"]),
            "--flow-shift-eval-video", "5.0",
            "--video-size", "704", "1216",
            "--sample-n-frames", "33",
            "--model", "HYVideo-T/2",
            "--vae", "884-16c-hy0801",
            "--text-encoder", "llava-llama-3-8b",
            "--text-encoder-2", "clipL",
            "--tokenizer", "llava-llama-3-8b",
            "--tokenizer-2", "clipL",
            "--rope-theta", "256",
            "--precision", "bf16",
            "--reproduce",
            "--use-deepcache", "1",  # å¯ç”¨DeepCacheåŠ é€Ÿ
            "--use-sage"  # å¯ç”¨SAGEæ³¨æ„åŠ›ä¼˜åŒ–
        ]
        
        # å¦‚æœæ˜¯åŠ é€Ÿæ¨¡å‹ï¼Œæ·»åŠ FP8ä¼˜åŒ–
        if config["use_fp8"]:
            base_args.append("--use-fp8")
        
        args = parser.parse_args(base_args)
        
        # æ‰§è¡Œå‚æ•°æ£€æŸ¥ï¼Œè¿™ä¼šè‡ªåŠ¨è®¾ç½®latent_channels
        args = sanity_check_args(args)
        
        return args

    def validate_inputs(self, image, prompt, action_list, action_speeds):
        """éªŒè¯è¾“å…¥å‚æ•°"""
        if image is None:
            return False, "è¯·ä¸Šä¼ ä¸€å¼ å‚è€ƒå›¾ç‰‡"
        
        if not prompt.strip():
            return False, "è¯·è¾“å…¥æè¿°æç¤ºè¯"
            
        if not action_list:
            return False, "è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ"
            
        if len(action_list) != len(action_speeds):
            return False, "åŠ¨ä½œåˆ—è¡¨å’Œé€Ÿåº¦åˆ—è¡¨é•¿åº¦å¿…é¡»ä¸€è‡´"
            
        for speed in action_speeds:
            if not (0 <= speed <= 3):
                return False, "åŠ¨ä½œé€Ÿåº¦å¿…é¡»åœ¨0-3ä¹‹é—´"
                
        return True, "éªŒè¯é€šè¿‡"

    def generate_video(self, image, prompt_template, custom_prompt, add_pos_prompt, add_neg_prompt,
                      action_list_display, action_speeds_text, video_width, video_height, 
                      cfg_scale, seed, infer_steps, flow_shift, progress=gr.Progress()):
        """ç”Ÿæˆè§†é¢‘çš„ä¸»å‡½æ•°"""
        
        try:
            # ç¡®å®šæœ€ç»ˆçš„prompt
            if prompt_template == "è‡ªå®šä¹‰":
                final_prompt = custom_prompt
            else:
                final_prompt = self.prompt_templates.get(prompt_template, custom_prompt)
            
            # è½¬æ¢åŠ¨ä½œåˆ—è¡¨
            action_list = [self.action_reverse_map.get(action, action) for action in action_list_display]
            
            # è§£æåŠ¨ä½œé€Ÿåº¦
            try:
                action_speeds = [float(x.strip()) for x in action_speeds_text.split()]
            except:
                return None, "âŒ åŠ¨ä½œé€Ÿåº¦æ ¼å¼é”™è¯¯", ""
            
            # éªŒè¯è¾“å…¥
            is_valid, msg = self.validate_inputs(image, final_prompt, action_list, action_speeds)
            if not is_valid:
                return None, f"âŒ {msg}", ""
            
            # åŠ è½½æ¨¡å‹
            progress(0.1, desc="æ£€æŸ¥æ¨¡å‹çŠ¶æ€...")
            if not self.model_loaded:
                if not self.load_model(lambda p, desc: progress(p * 0.4, desc=desc)):
                    return None, "âŒ æ¨¡å‹åŠ è½½å¤±è´¥", ""
            
            progress(0.5, desc="å‡†å¤‡è¾“å…¥æ•°æ®...")
            
            # ä¿å­˜ä¸´æ—¶å›¾ç‰‡
            temp_image_path = os.path.join(tempfile.gettempdir(), f"input_image_{int(time.time())}.png")
            image.save(temp_image_path)
            
            # å‡†å¤‡å›¾åƒå˜æ¢
            closest_size = (video_height, video_width)
            ref_image_transform = transforms.Compose([
                CropResize(closest_size),
                transforms.CenterCrop(closest_size),
                transforms.ToTensor(), 
                transforms.Normalize([0.5], [0.5])
            ])
            
            # å¤„ç†å‚è€ƒå›¾åƒ
            raw_ref_images = [image.convert('RGB')]
            ref_images_pixel_values = [ref_image_transform(ref_image) for ref_image in raw_ref_images]
            ref_images_pixel_values = torch.cat(ref_images_pixel_values).unsqueeze(0).unsqueeze(2).to(self.device)
            
            progress(0.6, desc="ç¼–ç å‚è€ƒå›¾åƒ...")
            
            # ç¼–ç å‚è€ƒå›¾åƒ
            with torch.autocast(device_type="cuda", dtype=torch.float16, enabled=True):
                self.hunyuan_video_sampler.pipeline.vae.enable_tiling()
                
                raw_last_latents = self.hunyuan_video_sampler.vae.encode(
                    ref_images_pixel_values
                ).latent_dist.sample().to(dtype=torch.float16)
                raw_last_latents.mul_(self.hunyuan_video_sampler.vae.config.scaling_factor)
                raw_ref_latents = raw_last_latents.clone()
                
                self.hunyuan_video_sampler.pipeline.vae.disable_tiling()
            
            # ç”Ÿæˆè§†é¢‘
            ref_images = raw_ref_images
            last_latents = raw_last_latents
            ref_latents = raw_ref_latents
            
            progress(0.7, desc="å¼€å§‹ç”Ÿæˆè§†é¢‘...")
            
            out_cat = None
            for idx, action_id in enumerate(action_list):
                is_image = (idx == 0)  # ç¬¬ä¸€ä¸ªåŠ¨ä½œä½¿ç”¨å›¾åƒ
                
                progress(0.7 + 0.2 * (idx / len(action_list)), desc=f"ç”ŸæˆåŠ¨ä½œ {idx+1}/{len(action_list)}: {action_id}")
                
                outputs = self.hunyuan_video_sampler.predict(
                    prompt=final_prompt,
                    action_id=action_id,
                    action_speed=action_speeds[idx],                    
                    is_image=is_image,
                    size=(video_height, video_width),
                    seed=seed,
                    last_latents=last_latents,
                    ref_latents=ref_latents,
                    video_length=33,  # å›ºå®š33å¸§
                    guidance_scale=cfg_scale,
                    num_images_per_prompt=1,
                    negative_prompt=add_neg_prompt,
                    infer_steps=infer_steps,
                    flow_shift=flow_shift,
                    ref_images=ref_images,
                    output_dir=self.output_dir,
                    return_latents=True,
                )
                
                # æ›´æ–°latents
                ref_latents = outputs["ref_latents"]
                last_latents = outputs["last_latents"]
                
                # æ‹¼æ¥è§†é¢‘
                sub_samples = outputs['samples'][0]
                if idx == 0:
                    out_cat = sub_samples
                else:
                    out_cat = torch.cat([out_cat, sub_samples], dim=2)
            
            progress(0.95, desc="ä¿å­˜è§†é¢‘...")
            
            # ä¿å­˜è§†é¢‘
            timestamp = int(time.time())
            save_path_mp4 = os.path.join(self.output_dir, f"generated_video_{timestamp}.mp4")
            save_videos_grid(out_cat, save_path_mp4, n_rows=1, fps=24)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_image_path):
                os.remove(temp_image_path)
            
            progress(1.0, desc="ç”Ÿæˆå®Œæˆ!")
            return save_path_mp4, "âœ… è§†é¢‘ç”ŸæˆæˆåŠŸ!", f"è§†é¢‘å·²ä¿å­˜åˆ°: {save_path_mp4}"
                
        except Exception as e:
            return None, f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}", str(e)

    def update_prompt(self, template_choice):
        """æ ¹æ®æ¨¡æ¿é€‰æ‹©æ›´æ–°prompt"""
        if template_choice == "è‡ªå®šä¹‰":
            return gr.update(visible=True, value="")
        else:
            return gr.update(visible=False, value=self.prompt_templates[template_choice])

    def load_sample(self, sample_name):
        """åŠ è½½ç¤ºä¾‹æ•°æ®"""
        samples = {
            "ä¸­ä¸–çºªæ‘åº„": {
                "image": "asset/village.png",
                "prompt": "A charming medieval village with cobblestone streets, thatched-roof houses, and vibrant flower gardens under a bright blue sky.",
                "add_pos_prompt": "Realistic, High-quality.",
                "add_neg_prompt": "overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion, blurring, text, subtitles, static, picture, black border.",
                "action_list": ["å‘å‰ç§»åŠ¨ (W)", "å‘åç§»åŠ¨ (S)", "å‘å³ç§»åŠ¨ (D)", "å‘å·¦ç§»åŠ¨ (A)"],
                "action_speeds": "0.2 0.2 0.2 0.2",
                "seed": 250160
            }
        }
        
        if sample_name in samples:
            sample = samples[sample_name]
            # åŠ è½½å›¾ç‰‡
            image_path = sample["image"]
            if os.path.exists(image_path):
                image = Image.open(image_path)
            else:
                image = None
                
            return (
                image,
                sample_name,  # ç›´æ¥ä½¿ç”¨sample_nameä½œä¸ºprompt_templateï¼Œè‡ªåŠ¨é€‰æ‹©å¯¹åº”æ¨¡æ¿
                sample["prompt"],  # custom_prompt
                sample["add_pos_prompt"],
                sample["add_neg_prompt"],
                sample["action_list"],
                sample["action_speeds"],
                sample["seed"]
            )
        else:
            return None, "è‡ªå®šä¹‰", "", "", "", [], "", 250160

    def create_interface(self):
        """åˆ›å»ºGradioç•Œé¢"""
        
        # æ£€æµ‹GPUä¿¡æ¯
        gpu_info = "CPUæ¨¡å¼"
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            gpu_info = f"{gpu_name} ({gpu_memory:.1f}GB)"
        
        with gr.Blocks(title="Hunyuan GameCraft è§†é¢‘ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as interface:
            gr.Markdown("# ğŸ® Hunyuan GameCraft è§†é¢‘ç”Ÿæˆå™¨")
            gr.Markdown("åŸºäºè…¾è®¯æ··å…ƒGameCraftæ¨¡å‹çš„äº¤äº’å¼è§†é¢‘ç”Ÿæˆå·¥å…·")
            
            # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
            status_info = f"ğŸ“Š ç³»ç»ŸçŠ¶æ€: {'âœ… æ¨¡å‹å·²å°±ç»ª' if self.model_available else 'âš ï¸ æ¨¡å‹æ–‡ä»¶ç¼ºå¤±'} | GPU: {gpu_info}"
            gr.Markdown(status_info)
            
            # æ·»åŠ ç¤ºä¾‹åŒºåŸŸ
            with gr.Row():
                gr.Markdown("## ğŸ¯ å¿«é€Ÿå¼€å§‹")
            with gr.Row():
                with gr.Column(scale=2):
                    sample_dropdown = gr.Dropdown(
                        choices=["ä¸­ä¸–çºªæ‘åº„"],
                        label="é€‰æ‹©å®˜æ–¹ç¤ºä¾‹",
                        value=None,
                        interactive=True
                    )
                with gr.Column(scale=1):
                    load_sample_btn = gr.Button("ğŸ“¥ åŠ è½½ç¤ºä¾‹", variant="secondary")
            
            # æ¨¡å‹é€‰æ‹©åŒºåŸŸ
            with gr.Row():
                gr.Markdown("## ğŸ¤– æ¨¡å‹é€‰æ‹©")
            with gr.Row():
                with gr.Column(scale=2):
                    model_dropdown = gr.Dropdown(
                        choices=list(self.model_configs.keys()),
                        value=self.current_model,
                        label="é€‰æ‹©æ¨¡å‹ç‰ˆæœ¬",
                        interactive=True
                    )
                    model_info = gr.Textbox(
                        value=f"å½“å‰: {self.model_configs[self.current_model]['description']}",
                        label="æ¨¡å‹ä¿¡æ¯",
                        interactive=False,
                        lines=1
                    )
                with gr.Column(scale=1):
                    switch_model_btn = gr.Button("ğŸ”„ åˆ‡æ¢æ¨¡å‹", variant="secondary")
            
            gr.Markdown("---")  # åˆ†éš”çº¿
            
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("## ğŸ“¸ è¾“å…¥è®¾ç½®")
                    
                    # å›¾ç‰‡ä¸Šä¼ 
                    image_input = gr.Image(
                        label="å‚è€ƒå›¾ç‰‡", 
                        type="pil",
                        height=300
                    )
                    
                    # Promptè®¾ç½®
                    with gr.Group():
                        gr.Markdown("### ğŸ“ æè¿°æç¤ºè¯")
                        prompt_template = gr.Dropdown(
                            choices=list(self.prompt_templates.keys()),
                            value="ä¸­ä¸–çºªæ‘åº„",
                            label="é€‰æ‹©æ¨¡æ¿"
                        )
                        custom_prompt = gr.Textbox(
                            label="è‡ªå®šä¹‰æè¿°",
                            placeholder="è¾“å…¥æ‚¨çš„è‡ªå®šä¹‰æè¿°...",
                            lines=3,
                            visible=False
                        )
                        
                        add_pos_prompt = gr.Textbox(
                            label="é™„åŠ æ­£é¢æç¤ºè¯",
                            value="Realistic, High-quality.",
                            lines=2
                        )
                        add_neg_prompt = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯",
                            value="overexposed, low quality, deformation, a poor composition, bad hands, bad teeth, bad eyes, bad limbs, distortion, blurring, text, subtitles, static, picture, black border.",
                            lines=3
                        )
                    
                    # åŠ¨ä½œè®¾ç½®
                    with gr.Group():
                        gr.Markdown("### ğŸ® åŠ¨ä½œæ§åˆ¶")
                        gr.Markdown("æ¯ä¸ªåŠ¨ä½œå¯¹åº”33å¸§ï¼ˆ25FPSï¼‰ï¼Œå¯ä»¥ç»„åˆå¤šä¸ªåŠ¨ä½œç”Ÿæˆé•¿è§†é¢‘")
                        
                        action_checkboxes = gr.CheckboxGroup(
                            choices=list(self.action_options.values()),
                            label="é€‰æ‹©åŠ¨ä½œåºåˆ—",
                            value=["å‘å‰ç§»åŠ¨ (W)", "å‘åç§»åŠ¨ (S)", "å‘å³ç§»åŠ¨ (D)", "å‘å·¦ç§»åŠ¨ (A)"]
                        )
                        
                        action_speeds = gr.Textbox(
                            label="åŠ¨ä½œé€Ÿåº¦ (0-3ä¹‹é—´ï¼Œç”¨ç©ºæ ¼åˆ†éš”)",
                            value="0.2 0.2 0.2 0.2",
                            placeholder="ä¾‹å¦‚: 0.2 0.5 1.0 0.8"
                        )
                
                with gr.Column(scale=1):
                    gr.Markdown("## âš™ï¸ ç”Ÿæˆå‚æ•°")
                    
                    with gr.Group():
                        gr.Markdown("### ğŸ¥ è§†é¢‘è®¾ç½®")
                        with gr.Row():
                            video_width = gr.Slider(
                                minimum=512, maximum=1920, step=64, value=1216,
                                label="è§†é¢‘å®½åº¦"
                            )
                            video_height = gr.Slider(
                                minimum=512, maximum=1080, step=64, value=704,
                                label="è§†é¢‘é«˜åº¦"
                            )
                    
                    with gr.Group():
                        gr.Markdown("### ğŸ›ï¸ ç”Ÿæˆæ§åˆ¶")
                        cfg_scale = gr.Slider(
                            minimum=1.0, maximum=10.0, step=0.1, value=2.0,
                            label="CFG Scale (æ§åˆ¶æç¤ºè¯éµå¾ªåº¦)"
                        )
                        seed = gr.Number(
                            label="éšæœºç§å­", 
                            value=250160,
                            precision=0
                        )
                        infer_steps = gr.Slider(
                            minimum=5, maximum=100, step=1, value=30,
                            label="æ¨ç†æ­¥æ•° (åŠ é€Ÿæ¨¡å‹æ¨è8æ­¥ï¼Œæ ‡å‡†æ¨¡å‹æ¨è30-50æ­¥)"
                        )
                        flow_shift = gr.Slider(
                            minimum=1.0, maximum=10.0, step=0.1, value=5.0,
                            label="Flow Shift"
                        )
                    
                    # ç”ŸæˆæŒ‰é’®
                    generate_btn = gr.Button(
                        "ğŸš€ å¼€å§‹ç”Ÿæˆè§†é¢‘", 
                        variant="primary",
                        size="lg"
                    )
            
            # è¾“å‡ºåŒºåŸŸ
            with gr.Row():
                with gr.Column():
                    gr.Markdown("## ğŸ“¹ ç”Ÿæˆç»“æœ")
                    status_output = gr.Textbox(
                        label="çŠ¶æ€",
                        value="ç­‰å¾…å¼€å§‹ç”Ÿæˆ...",
                        lines=2
                    )
                    video_output = gr.Video(
                        label="ç”Ÿæˆçš„è§†é¢‘",
                        height=400
                    )
            
            # æ—¥å¿—è¾“å‡º
            with gr.Accordion("ğŸ“‹ è¯¦ç»†æ—¥å¿—", open=False):
                log_output = gr.Textbox(
                    label="ç”Ÿæˆæ—¥å¿—",
                    lines=10,
                    max_lines=20
                )
            
            # äº‹ä»¶ç»‘å®š
            prompt_template.change(
                fn=self.update_prompt,
                inputs=[prompt_template],
                outputs=[custom_prompt]
            )
            
            # ç¤ºä¾‹åŠ è½½äº‹ä»¶
            load_sample_btn.click(
                fn=self.load_sample,
                inputs=[sample_dropdown],
                outputs=[
                    image_input, prompt_template, custom_prompt,
                    add_pos_prompt, add_neg_prompt, 
                    action_checkboxes, action_speeds, seed
                ]
            )
            
            # æ¨¡å‹åˆ‡æ¢äº‹ä»¶
            switch_model_btn.click(
                fn=self.switch_model,
                inputs=[model_dropdown],
                outputs=[infer_steps, cfg_scale, model_info]
            )
            
            generate_btn.click(
                fn=self.generate_video,
                inputs=[
                    image_input, prompt_template, custom_prompt, 
                    add_pos_prompt, add_neg_prompt,
                    action_checkboxes, action_speeds,
                    video_width, video_height, cfg_scale, 
                    seed, infer_steps, flow_shift
                ],
                outputs=[video_output, status_output, log_output]
            )
            
            # æ·»åŠ ä½¿ç”¨è¯´æ˜
            gr.Markdown("## ğŸ’¡ ä½¿ç”¨æç¤º")
            gr.Markdown("""
            1. **å¿«é€Ÿå¼€å§‹**: ç‚¹å‡»"ğŸ“¥ åŠ è½½ç¤ºä¾‹"æŒ‰é’®ï¼Œè‡ªåŠ¨åŠ è½½ä¸­ä¸–çºªæ‘åº„ç¤ºä¾‹ï¼ˆåŒ…æ‹¬å›¾ç‰‡ã€æè¿°å’Œå‚æ•°ï¼‰
            2. **æ¨¡å‹é€‰æ‹©**: 
               - **æ ‡å‡†æ¨¡å‹**: è´¨é‡æ›´é«˜ï¼Œæ¨ç†æ­¥æ•°30æ­¥ï¼Œé€Ÿåº¦è¾ƒæ…¢ï¼Œé€‚åˆæœ€ç»ˆä½œå“
               - **åŠ é€Ÿæ¨¡å‹**: æ¨ç†æ­¥æ•°ä»…8æ­¥ï¼Œé€Ÿåº¦å¿«4å€ï¼Œè´¨é‡ç•¥ä½ï¼Œé€‚åˆå¿«é€Ÿé¢„è§ˆ
            3. **è‡ªå®šä¹‰ä½¿ç”¨**: 
               - ä¸Šä¼ å›¾ç‰‡ï¼šé€‰æ‹©ä¸€å¼ é«˜è´¨é‡çš„å‚è€ƒå›¾ç‰‡ä½œä¸ºè§†é¢‘çš„èµ·å§‹å¸§
               - è®¾ç½®æè¿°ï¼šé€‰æ‹©"è‡ªå®šä¹‰"æ¨¡æ¿ï¼Œè¾“å…¥æ‚¨çš„æè¿°æ–‡å­—
            4. **é…ç½®åŠ¨ä½œ**: é€‰æ‹©ç§»åŠ¨æ–¹å‘å’Œå¯¹åº”çš„é€Ÿåº¦å€¼
            5. **è°ƒæ•´å‚æ•°**: æ ¹æ®éœ€è¦è°ƒæ•´è§†é¢‘å°ºå¯¸å’Œç”Ÿæˆå‚æ•°
            6. **å¼€å§‹ç”Ÿæˆ**: ç‚¹å‡»ç”ŸæˆæŒ‰é’®ï¼Œç­‰å¾…è§†é¢‘ç”Ÿæˆå®Œæˆ
            
            **åŠ¨ä½œè¯´æ˜**:
            - W: å‘å‰ç§»åŠ¨ | S: å‘åç§»åŠ¨ | A: å‘å·¦ç§»åŠ¨ | D: å‘å³ç§»åŠ¨
            - å¯ä»¥ç»„åˆä½¿ç”¨ï¼Œå¦‚WAè¡¨ç¤ºå‘å‰å‘å·¦ç§»åŠ¨
            - é€Ÿåº¦å€¼èŒƒå›´0-3ï¼Œæ•°å€¼è¶Šå¤§ç§»åŠ¨è·ç¦»è¶Šå¤§
            
            **æ€§èƒ½å»ºè®®**:
            - é¦–æ¬¡ä½¿ç”¨å»ºè®®é€‰æ‹©"åŠ é€Ÿæ¨¡å‹"è¿›è¡Œå¿«é€Ÿæµ‹è¯•
            - æ»¡æ„æ•ˆæœåå†ä½¿ç”¨"æ ‡å‡†æ¨¡å‹"ç”Ÿæˆæœ€ç»ˆè§†é¢‘
            - åŠ é€Ÿæ¨¡å‹æ¨ç†æ—¶é—´çº¦ä¸ºæ ‡å‡†æ¨¡å‹çš„1/4
            
            **ç¡¬ä»¶è¦æ±‚**:
            - å»ºè®®ä½¿ç”¨NVIDIA GPUï¼Œè‡³å°‘8GBæ˜¾å­˜
            - æ¨ç†æ—¶é—´ä¸åŠ¨ä½œåºåˆ—é•¿åº¦æˆæ­£æ¯”
            """)
        
        return interface

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ Hunyuan GameCraft WebUI...")
    
    webui = HunyuanGameCraftWebUI()
    interface = webui.create_interface()
    
    # å¯åŠ¨ç•Œé¢
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=False
    )

if __name__ == "__main__":
    main()
