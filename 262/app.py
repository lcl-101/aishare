# Copyright (c) 2025 Zhipu AI Inc (authors: CogAudio Group Members)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
GLM-TTS Web Application
ç‹¬ç«‹çš„ Gradio Web ç•Œé¢ï¼Œä½¿ç”¨æœ¬åœ° checkpoints/GLM-TTS ç›®å½•ä¸‹çš„æ¨¡å‹
"""

import gradio as gr
import torch
import numpy as np
import logging
import os
import gc
from functools import partial

from transformers import AutoTokenizer, LlamaForCausalLM, WhisperFeatureExtractor

from cosyvoice.cli.frontend import TTSFrontEnd, SpeechTokenizer, TextFrontEnd
from llm.glmtts import GLMTTS
from utils.audio import mel_spectrogram
from utils.whisper_models.configuration_whisper import WhisperVQConfig
from utils.whisper_models.modeling_whisper import WhisperVQEncoder
from utils import seed_util
from hyperpyyaml import load_hyperpyyaml
import glob
import safetensors
import pathlib

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ¨¡å‹æ ¹ç›®å½• - ä½¿ç”¨æœ¬åœ° checkpoints/GLM-TTS ç›®å½•
MODEL_ROOT = os.path.join(CURRENT_DIR, "checkpoints", "GLM-TTS")

# å„æ¨¡å‹è·¯å¾„
SPEECH_TOKENIZER_PATH = os.path.join(MODEL_ROOT, "speech_tokenizer")
LLM_PATH = os.path.join(MODEL_ROOT, "llm")
FLOW_CKPT_PATH = os.path.join(MODEL_ROOT, "flow", "flow.pt")
FLOW_CONFIG_PATH = os.path.join(MODEL_ROOT, "flow", "config.yaml")
VOCOS_CKPT_PATH = os.path.join(MODEL_ROOT, "vocos2d", "generator_jit.ckpt")
HIFT_CKPT_PATH = os.path.join(MODEL_ROOT, "hift", "hift.pt")
TOKENIZER_PATH = os.path.join(MODEL_ROOT, "vq32k-phoneme-tokenizer")
FRONTEND_DIR = os.path.join(CURRENT_DIR, "frontend")

# LLM åºåˆ—é•¿åº¦é™åˆ¶
MAX_LLM_SEQ_INP_LEN = 750


# --- Token2Wav ç±»ï¼ˆä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„ï¼‰---

class Token2Wav:
    """Token åˆ°æ³¢å½¢è½¬æ¢å™¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„"""
    def __init__(self, flow, sample_rate: int = 24000, device: str = "cuda"):
        self.device = device
        self.flow = flow
        self.input_frame_rate = flow.input_frame_rate

        if sample_rate == 32000:
            self.hop_size = 640
            self.sample_rate = 32000
            self.vocoder = load_vocos_jit(device)
        elif sample_rate == 24000:
            self.hop_size = 480
            self.sample_rate = 24000
            self.vocoder = load_hift(device)
        else:
            raise ValueError(f"Unsupported sample_rate: {sample_rate}")
    
    def token2wav_with_cache(self,
                             token_bt,
                             n_timesteps: int = 10,
                             prompt_token: torch.Tensor = torch.zeros(1, 0, dtype=torch.int32),
                             prompt_feat: torch.Tensor = torch.zeros(1, 0, 80),
                             embedding: torch.Tensor = torch.zeros(1, 192),
    ):
        if isinstance(token_bt, (list, np.ndarray)):
            token_bt = torch.tensor(token_bt, dtype=torch.long)[None]
        elif not isinstance(token_bt, torch.Tensor):
            raise ValueError(f"Unsupported token_bt type: {type(token_bt)}")

        assert prompt_token.shape[1] != 0 and prompt_feat.shape[1] != 0
        mel, _ = self.flow.inference_with_cache(
            token=token_bt.to(self.device),
            prompt_token=prompt_token.to(self.device),
            prompt_feat=prompt_feat.to(self.device),
            embedding=embedding.to(self.device),
            n_timesteps=n_timesteps,
        )
        
        wav = self.vocoder(mel)

        return wav, mel


# --- æ¨¡å‹åŠ è½½å·¥å…·å‡½æ•° ---

def load_quantize_encoder(model_path):
    """åŠ è½½é‡åŒ–ç¼–ç å™¨"""
    logging.info(f'Loading quantize encoder from {model_path}...')
    config = WhisperVQConfig.from_pretrained(model_path)
    config.quantize_encoder_only = True
    model = WhisperVQEncoder(config)
    state_dict = {}
    for path in glob.glob(os.path.join(model_path, "model*.safetensors")):
        with safetensors.safe_open(path, framework="pt", device="cpu") as f:
            for key in f.keys():
                if key.startswith("model.encoder."):
                    new_key = key[len("model.encoder."):]
                    if new_key.startswith("layer_norm"):
                        continue
                    if new_key.startswith("layers"):
                        layer_id = int(new_key.split(".")[1])
                        if layer_id >= config.quantize_position:
                            continue
                    state_dict[new_key] = f.get_tensor(key)
    model.load_state_dict(state_dict)
    model.eval()
    model.cuda()
    return model


def load_speech_tokenizer(model_path):
    """åŠ è½½è¯­éŸ³ tokenizer"""
    model = load_quantize_encoder(model_path)
    feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)
    return model, feature_extractor


def load_flow_model(flow_ckpt_path, config_path, device):
    """åŠ è½½ Flow æ¨¡å‹"""
    logging.info(f'Loading flow model from {flow_ckpt_path}...')
    with open(config_path, 'r') as f:
        scratch_configs = load_hyperpyyaml(f)
        flow = scratch_configs['flow']

    tmp = torch.load(flow_ckpt_path, map_location=device)
    if isinstance(tmp, dict):
        flow.load_state_dict(tmp["model"])
    else:
        flow.load_state_dict(tmp)

    flow.to(device)
    flow.eval()
    return flow


def load_vocos_jit(device="cuda"):
    """åŠ è½½ Vocos JIT vocoder (32kHz)"""
    from utils.vocos_util import Vocos2DInference
    logging.info(f"Loading Vocos JIT model from {VOCOS_CKPT_PATH}...")
    return Vocos2DInference(VOCOS_CKPT_PATH, device=device)


def load_hift(device="cuda"):
    """åŠ è½½ HiFT vocoder (24kHz)"""
    from utils.hift_util import HiFTInference
    logging.info(f"Loading HiFT model from {HIFT_CKPT_PATH}...")
    return HiFTInference(HIFT_CKPT_PATH, device=device)


def get_special_token_ids(tokenize_fn):
    """è·å–ç‰¹æ®Š token IDs"""
    _special_token_ids = {
        "ats": "<|audio_0|>",
        "ate": "<|audio_32767|>",
        "boa": "<|begin_of_audio|>",
        "eoa": "<|user|>",
        "pad": "<|endoftext|>",
    }

    special_token_ids = {}
    endoftext_id = tokenize_fn("<|endoftext|>")[0]
    
    for k, v in _special_token_ids.items():
        __ids = tokenize_fn(v)
        if len(__ids) != 1:
            raise AssertionError(f"Token '{k}' ({v}) encoded to multiple tokens: {__ids}")
        if __ids[0] < endoftext_id:
            raise AssertionError(f"Token '{k}' ({v}) ID {__ids[0]} is smaller than endoftext ID {endoftext_id}")
        special_token_ids[k] = __ids[0]

    return special_token_ids


def load_frontends(speech_tokenizer, sample_rate=24000, use_phoneme=False):
    """åŠ è½½å‰ç«¯å¤„ç†æ¨¡å—"""
    if sample_rate == 32000:
        feat_extractor = partial(
            mel_spectrogram, 
            sampling_rate=sample_rate, 
            hop_size=640, 
            n_fft=2560, 
            num_mels=80, 
            win_size=2560, 
            fmin=0, 
            fmax=8000, 
            center=False
        )
        logging.info("Configured for 32kHz frontend.")
    elif sample_rate == 24000:
        feat_extractor = partial(
            mel_spectrogram, 
            sampling_rate=sample_rate, 
            hop_size=480, 
            n_fft=1920, 
            num_mels=80, 
            win_size=1920, 
            fmin=0, 
            fmax=8000, 
            center=False
        )
        logging.info("Configured for 24kHz frontend.")
    else:
        raise ValueError(f"Unsupported sampling_rate: {sample_rate}")

    glm_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code=True)
    tokenize_fn = lambda text: glm_tokenizer.encode(text)

    frontend = TTSFrontEnd(
        tokenize_fn,
        speech_tokenizer,
        feat_extractor,
        os.path.join(FRONTEND_DIR, "campplus.onnx"),
        os.path.join(FRONTEND_DIR, "spk2info.pt"),
        DEVICE,
    )
    text_frontend = TextFrontEnd(use_phoneme)
    return frontend, text_frontend


# --- å…¨å±€æ¨¡å‹ç¼“å­˜ ---
MODEL_CACHE = {
    "loaded": False,
    "sample_rate": None,
    "components": None
}


def load_models(use_phoneme=False, sample_rate=24000):
    """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
    logging.info(f"Loading models with sample_rate={sample_rate}...")
    
    # åŠ è½½ Speech Tokenizer
    _model, _feature_extractor = load_speech_tokenizer(SPEECH_TOKENIZER_PATH)
    speech_tokenizer = SpeechTokenizer(_model, _feature_extractor)

    # åŠ è½½å‰ç«¯
    frontend, text_frontend = load_frontends(speech_tokenizer, sample_rate=sample_rate, use_phoneme=use_phoneme)

    # åŠ è½½ LLM
    logging.info(f"Loading LLM from {LLM_PATH}...")
    llm = GLMTTS(
        llama_cfg_path=os.path.join(LLM_PATH, "config.json"), 
        mode="PRETRAIN"
    )
    llm.llama = LlamaForCausalLM.from_pretrained(LLM_PATH, torch_dtype=torch.float32).to(DEVICE)
    llm.llama_embedding = llm.llama.model.embed_tokens

    special_token_ids = get_special_token_ids(frontend.tokenize_fn)
    llm.set_runtime_vars(special_token_ids=special_token_ids)

    # åŠ è½½ Flow æ¨¡å‹
    flow = load_flow_model(FLOW_CKPT_PATH, FLOW_CONFIG_PATH, DEVICE)

    # åˆ›å»º Token2Wav è½¬æ¢å™¨ï¼ˆä½¿ç”¨è‡ªå®šä¹‰ç±»ï¼‰
    token2wav = Token2Wav(flow, sample_rate=sample_rate, device=DEVICE)

    logging.info("All models loaded successfully.")
    return frontend, text_frontend, speech_tokenizer, llm, token2wav


def get_models(use_phoneme=True, sample_rate=24000):
    """
    æ‡’åŠ è½½æ¨¡å‹ï¼Œå¦‚æœé‡‡æ ·ç‡æ”¹å˜åˆ™é‡æ–°åŠ è½½
    """
    if MODEL_CACHE["loaded"] and MODEL_CACHE["sample_rate"] == sample_rate:
        return MODEL_CACHE["components"]
    
    # æ¸…ç†æ—§æ¨¡å‹
    if MODEL_CACHE["components"]:
        del MODEL_CACHE["components"]
        gc.collect()
        torch.cuda.empty_cache()

    # åŠ è½½æ–°æ¨¡å‹
    frontend, text_frontend, speech_tokenizer, llm, flow = load_models(
        use_phoneme=use_phoneme, 
        sample_rate=sample_rate
    )
    
    MODEL_CACHE["components"] = (frontend, text_frontend, speech_tokenizer, llm, flow)
    MODEL_CACHE["sample_rate"] = sample_rate
    MODEL_CACHE["loaded"] = True
    
    return MODEL_CACHE["components"]


# --- LLM å’Œ Flow å‰å‘æ¨ç† ---

def _assert_shape_and_get_len(token):
    assert token.ndim == 2 and token.shape[0] == 1
    token_len = torch.tensor([token.shape[1]], dtype=torch.int32).to(token.device)
    return token_len


def local_llm_forward(llm, prompt_text_token, tts_text_token, prompt_speech_token, 
                      beam_size=1, sampling=25, sample_method="ras"):
    """LLM å•æ¬¡å‰å‘æ¨ç†"""
    prompt_text_token_len = _assert_shape_and_get_len(prompt_text_token)
    tts_text_token_len = _assert_shape_and_get_len(tts_text_token)
    prompt_speech_token_len = _assert_shape_and_get_len(prompt_speech_token)

    tts_speech_token = llm.inference(
        text=tts_text_token,
        text_len=tts_text_token_len,
        prompt_text=prompt_text_token,
        prompt_text_len=prompt_text_token_len,
        prompt_speech_token=prompt_speech_token,
        prompt_speech_token_len=prompt_speech_token_len,
        beam_size=beam_size,
        sampling=sampling,
        sample_method=sample_method,
        spk=None,
    )
    return tts_speech_token[0].tolist()


def local_flow_forward(flow, token_list, prompt_speech_tokens, speech_feat, embedding):
    """Flow å•æ¬¡å‰å‘æ¨ç†"""
    wav, full_mel = flow.token2wav_with_cache(
        token_list,
        prompt_token=prompt_speech_tokens,
        prompt_feat=speech_feat,
        embedding=embedding,
    )
    return wav.detach().cpu(), full_mel


# --- ç¼“å­˜å¤„ç† ---

def get_cached_prompt(cache, synth_text_token, device=DEVICE):
    """ä»ç¼“å­˜æ„å»º prompt tokens"""
    cache_text = cache["cache_text"]
    cache_text_token = cache["cache_text_token"]
    cache_speech_token = cache["cache_speech_token"]

    def __len_cache_text_token():
        return sum(map(lambda x: x.shape[1], cache_text_token))

    def __len_cache_speech_token():
        return sum(map(len, cache_speech_token))

    text_len = __len_cache_text_token()
    ta_ratio = __len_cache_speech_token() / (text_len if text_len > 0 else 1.0)

    __len_synth_text_token = synth_text_token.shape[1]
    __len_synth_audi_token_estim = int(ta_ratio * __len_synth_text_token)

    # å¦‚æœç¼“å­˜å¤ªé•¿åˆ™è£å‰ª
    while (__len_cache_speech_token() + __len_synth_audi_token_estim > MAX_LLM_SEQ_INP_LEN):
        if len(cache_speech_token) <= 1:
            break
        cache_text.pop(1)
        cache_text_token.pop(1)
        cache_speech_token.pop(1)

    # æ„å»ºæ–‡æœ¬ prompt
    prompt_text_token_from_cache = []
    for a_token in cache_text_token:
        prompt_text_token_from_cache.extend(a_token.squeeze().tolist())

    prompt_text_token = torch.tensor([prompt_text_token_from_cache]).to(device)

    # æ„å»ºè¯­éŸ³ prompt
    speech_tokens = []
    for a_cache_speech_token in cache_speech_token:
        speech_tokens.extend(a_cache_speech_token)

    llm_speech_token = torch.tensor([speech_tokens], dtype=torch.int32).to(device)

    return prompt_text_token, llm_speech_token


# --- ä¸»ç”Ÿæˆé€»è¾‘ ---

def generate_long(frontend, text_frontend, llm, flow, text_info, cache, device,
                  embedding, seed=0, sample_method="ras", flow_prompt_token=None,
                  speech_feat=None, use_phoneme=False, skip_normalize=False):
    """é•¿æ–‡æœ¬ç”Ÿæˆ"""
    outputs = []
    full_mels = []
    output_token_list = []
    uttid = text_info[0]
    syn_text = text_info[1]
    
    text_tn_dict = {
        "uttid": uttid,
        "syn_text": syn_text,
        "syn_text_tn": [],
        "syn_text_phoneme": [],
    }
    
    # å¦‚æœ skip_normalize ä¸º Trueï¼Œè¯´æ˜æ–‡æœ¬å·²ç»é¢„å¤„ç†è¿‡ï¼ˆåŒ…æ‹¬ G2Pï¼‰ï¼Œä¸å†åˆ†å‰²å’Œ normalize
    if skip_normalize:
        short_text_list = [syn_text]
    else:
        short_text_list = text_frontend.split_by_len(syn_text)

    for _, tts_text in enumerate(short_text_list):
        seed_util.set_seed(seed)
        
        # å¦‚æœ skip_normalizeï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æ–‡æœ¬
        if skip_normalize:
            tts_text_tn = tts_text
        else:
            tts_text_tn = text_frontend.text_normalize(tts_text)
            text_tn_dict["syn_text_tn"].append(tts_text_tn)
            
            if use_phoneme:
                tts_text_tn = text_frontend.g2p_infer(tts_text_tn)
                text_tn_dict["syn_text_phoneme"].append(tts_text_tn)
        
        tts_text_token = frontend._extract_text_token(tts_text_tn)

        cache_text = cache["cache_text"]
        cache_text_token = cache["cache_text_token"]
        cache_speech_token = cache["cache_speech_token"]

        if cache["use_cache"] and len(cache_text_token) > 1:
            prompt_text_token, prompt_speech_token = get_cached_prompt(cache, tts_text_token, device)
        else:
            prompt_text_token = cache_text_token[0].to(device)
            prompt_speech_token = torch.tensor([cache_speech_token[0]], dtype=torch.int32).to(device)

        # LLM æ¨ç†
        token_list_res = local_llm_forward(
            llm=llm,
            prompt_text_token=prompt_text_token,
            tts_text_token=tts_text_token,
            prompt_speech_token=prompt_speech_token,
            sample_method=sample_method
        )

        output_token_list.extend(token_list_res)

        # Flow æ¨ç†
        output, full_mel = local_flow_forward(
            flow=flow,
            token_list=token_list_res,
            prompt_speech_tokens=flow_prompt_token,
            speech_feat=speech_feat,
            embedding=embedding
        )

        # æ›´æ–°ç¼“å­˜
        if cache is not None:
            cache_text.append(tts_text_tn)
            cache_text_token.append(tts_text_token)
            cache_speech_token.append(token_list_res)

        outputs.append(output)
        if full_mel is not None:
            full_mels.append(full_mel)

    tts_speech = torch.concat(outputs, dim=1)
    tts_mel = torch.concat(full_mels, dim=-1) if full_mels else None

    return tts_speech, tts_mel, output_token_list, text_tn_dict


# --- Gradio æ¨ç†å¤„ç†å‡½æ•° ---

def run_inference(prompt_text, prompt_audio_path, input_text, seed, sample_rate, use_cache=True, use_g2p=False):
    """Gradio ä¸»æ¨ç†å…¥å£"""
    if not input_text:
        raise gr.Error("è¯·æä¾›è¦åˆæˆçš„æ–‡æœ¬ã€‚")
    if not prompt_audio_path:
        raise gr.Error("è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘æ–‡ä»¶ã€‚")
    if not prompt_text:
        gr.Warning("å‚è€ƒæ–‡æœ¬ä¸ºç©ºï¼Œåˆæˆæ•ˆæœå¯èƒ½ä¸ç†æƒ³ã€‚")

    try:
        # 1. åŠ è½½æ¨¡å‹
        frontend, text_frontend, _, llm, flow = get_models(use_phoneme=True, sample_rate=sample_rate)
        
        logging.info(f"G2P enabled: {use_g2p}")

        # 2. æ–‡æœ¬é¢„å¤„ç†
        norm_prompt_text = text_frontend.text_normalize(prompt_text) + ' '
        norm_input_text = text_frontend.text_normalize(input_text)
        
        # å¦‚æœå¯ç”¨ G2Pï¼Œå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡ŒéŸ³ç´ è½¬æ¢ï¼ˆç”¨äºå¤šéŸ³å­—å¤„ç†ï¼‰
        if use_g2p:
            norm_input_text = text_frontend.g2p_infer(norm_input_text)
            logging.info(f"G2P processed text: {norm_input_text}")
        
        logging.info(f"Normalized Prompt: {norm_prompt_text}")
        logging.info(f"Normalized Input: {norm_input_text}")

        # 3. ç‰¹å¾æå–
        prompt_text_token = frontend._extract_text_token(norm_prompt_text)
        prompt_speech_token = frontend._extract_speech_token([prompt_audio_path])
        speech_feat = frontend._extract_speech_feat(prompt_audio_path, sample_rate=sample_rate)
        embedding = frontend._extract_spk_embedding(prompt_audio_path)

        # 4. å‡†å¤‡ç¼“å­˜
        cache_speech_token_list = [prompt_speech_token.squeeze().tolist()]
        flow_prompt_token = torch.tensor(cache_speech_token_list, dtype=torch.int32).to(DEVICE)
        
        cache = {
            'cache_text': [norm_prompt_text],
            'cache_text_token': [prompt_text_token],
            'cache_speech_token': cache_speech_token_list,
            'use_cache': use_cache
        }

        # 5. è¿è¡Œç”Ÿæˆ
        tts_speech, _, _, _ = generate_long(
            frontend=frontend,
            text_frontend=text_frontend,
            llm=llm,
            flow=flow,
            text_info=['', norm_input_text],
            cache=cache,
            embedding=embedding,
            flow_prompt_token=flow_prompt_token,
            speech_feat=speech_feat,
            sample_method="ras",
            seed=seed,
            device=DEVICE,
            use_phoneme=False,
            skip_normalize=True  # æ–‡æœ¬å·²ç»åœ¨ run_inference ä¸­é¢„å¤„ç†è¿‡
        )

        # 6. åå¤„ç†éŸ³é¢‘
        audio_data = tts_speech.squeeze().cpu().numpy()
        audio_data = np.clip(audio_data, -1.0, 1.0)
        audio_int16 = (audio_data * 32767.0).astype(np.int16)

        return (sample_rate, audio_int16)

    except Exception as e:
        logging.error(f"Inference failed: {e}")
        import traceback
        traceback.print_exc()
        raise gr.Error(f"åˆæˆå¤±è´¥: {str(e)}")


def clear_memory():
    """æ¸…ç†æ˜¾å­˜å¹¶é‡ç½®æ¨¡å‹ç¼“å­˜"""
    global MODEL_CACHE
    if MODEL_CACHE["components"]:
        del MODEL_CACHE["components"]
    MODEL_CACHE["components"] = None
    MODEL_CACHE["loaded"] = False
    MODEL_CACHE["sample_rate"] = None
    
    gc.collect()
    torch.cuda.empty_cache()
    return "æ˜¾å­˜å·²æ¸…ç†ï¼Œæ¨¡å‹å°†åœ¨ä¸‹æ¬¡æ¨ç†æ—¶é‡æ–°åŠ è½½ã€‚"


def load_examples():
    """ä» examples ç›®å½•åŠ è½½ç¤ºä¾‹æ•°æ®"""
    import json
    examples_dir = os.path.join(CURRENT_DIR, "examples")
    examples = []
    
    # å®šä¹‰ç¤ºä¾‹æ–‡ä»¶å’Œå¯¹åº”çš„æ ‡ç­¾ï¼Œæ¯ç§è¯­è¨€åªå–ä¸€ä¸ª
    example_files = [
        ("example_zh.jsonl", "ä¸­æ–‡ç¤ºä¾‹"),
        ("example_en.jsonl", "è‹±æ–‡ç¤ºä¾‹"),
    ]
    
    for filename, label in example_files:
        filepath = os.path.join(examples_dir, filename)
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= 1:  # æ¯ä¸ªæ–‡ä»¶åªå–1ä¸ªç¤ºä¾‹
                        break
                    try:
                        item = json.loads(line.strip())
                        prompt_audio_path = os.path.join(CURRENT_DIR, item["prompt_speech"])
                        if os.path.exists(prompt_audio_path):
                            examples.append([
                                item["prompt_text"],
                                prompt_audio_path,
                                item["syn_text"],
                            ])
                    except:
                        continue
    
    # æ·»åŠ è‡ªå®šä¹‰çš„å¤šéŸ³å­—ç¤ºä¾‹ï¼ˆä¹¡éŸ³æ— æ”¹é¬“æ¯›è¡° - è¡°è¯» cuÄ«ï¼‰
    custom_prompt_audio = os.path.join(CURRENT_DIR, "examples", "prompt", "jiayan_zh.wav")
    if os.path.exists(custom_prompt_audio):
        examples.insert(1, [
            "ä»–å½“æ—¶è¿˜è·Ÿçº¿ä¸‹å…¶ä»–çš„ç«™å§åµæ¶ï¼Œç„¶åï¼Œæ‰“æ¶è¿›å±€å­äº†ã€‚",
            custom_prompt_audio,
            "å°‘å°ç¦»å®¶è€å¤§å›ï¼Œä¹¡éŸ³æ— æ”¹é¬“æ¯›è¡°ã€‚å„¿ç«¥ç›¸è§ä¸ç›¸è¯†ï¼Œç¬‘é—®å®¢ä»ä½•å¤„æ¥ã€‚",
        ])
    
    return examples


# --- Gradio UI å¸ƒå±€ ---

def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # æ£€æŸ¥é»˜è®¤å‚è€ƒéŸ³é¢‘æ˜¯å¦å­˜åœ¨
    default_prompt_audio = os.path.join(CURRENT_DIR, "examples", "prompt", "jiayan_zh.wav")
    if not os.path.exists(default_prompt_audio):
        default_prompt_audio = None
    
    with gr.Blocks(title="GLM-TTS è¯­éŸ³åˆæˆ", theme=gr.themes.Soft()) as app:
        gr.Markdown("# ğŸµ GLM-TTS å¼€æºè¯­éŸ³åˆæˆæ¼”ç¤º")
        gr.Markdown("é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ç”Ÿæˆ - åŸºäº GLM-TTS æ¨¡å‹")
        gr.Markdown(f"**æ¨¡å‹è·¯å¾„**: `{MODEL_ROOT}`")

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### 1. é›¶æ ·æœ¬å‚è€ƒè®¾ç½®")
                
                prompt_audio = gr.Audio(
                    label="ä¸Šä¼ å‚è€ƒéŸ³é¢‘ (ç”¨äºå…‹éš†éŸ³è‰²)",
                    type="filepath",
                    value=default_prompt_audio
                )
                
                prompt_text = gr.Textbox(
                    label="å‚è€ƒæ–‡æœ¬",
                    placeholder="è¾“å…¥å‚è€ƒéŸ³é¢‘ä¸­è¯´è¯çš„å†…å®¹...",
                    lines=2,
                    info="å‡†ç¡®çš„å‚è€ƒæ–‡æœ¬å¯ä»¥æé«˜éŸ³è‰²ç›¸ä¼¼åº¦",
                    value="ä»–å½“æ—¶è¿˜è·Ÿçº¿ä¸‹å…¶ä»–çš„ç«™å§åµæ¶ï¼Œç„¶åï¼Œæ‰“æ¶è¿›å±€å­äº†ã€‚"
                )

                gr.Markdown("### 2. è¾“å…¥è®¾ç½®")
                input_text = gr.Textbox(
                    label="è¦åˆæˆçš„æ–‡æœ¬",
                    value="æˆ‘æœ€çˆ±åƒäººå‚æœï¼Œä½ å–œæ¬¢åƒå—ï¼Ÿ", 
                    lines=5,
                    placeholder="è¾“å…¥æƒ³è¦åˆæˆçš„æ–‡æœ¬å†…å®¹..."
                )
                
                with gr.Accordion("é«˜çº§è®¾ç½®", open=True):
                    sample_rate = gr.Radio(
                        choices=[24000, 32000], 
                        value=24000, 
                        label="é‡‡æ ·ç‡ (Hz)",
                        info="32000Hz éŸ³è´¨æ›´é«˜ï¼Œä½†éœ€è¦æ›´å¤šè®¡ç®—èµ„æº"
                    )
                    seed = gr.Number(label="éšæœºç§å­", value=42, precision=0)
                    use_cache = gr.Checkbox(
                        label="ä½¿ç”¨ KV Cache", 
                        value=True, 
                        info="é•¿æ–‡æœ¬ç”Ÿæˆæ—¶æ›´å¿«"
                    )
                    use_g2p = gr.Checkbox(
                        label="å¯ç”¨ G2P (å¤šéŸ³å­—å¤„ç†)", 
                        value=False, 
                        info="å¯ç”¨åå¯æ›´å‡†ç¡®å¤„ç†å¤šéŸ³å­—ï¼Œå¦‚'é•¿å¤§'vs'é•¿åº¦'"
                    )

                generate_btn = gr.Button("ğŸš€ å¼€å§‹åˆæˆ", variant="primary", size="lg")
                clear_btn = gr.Button("ğŸ§¹ æ¸…ç†æ˜¾å­˜", variant="secondary")

            with gr.Column(scale=1):
                gr.Markdown("### 3. è¾“å‡ºç»“æœ")
                output_audio = gr.Audio(label="åˆæˆç»“æœ")
                status_msg = gr.Textbox(label="ç³»ç»ŸçŠ¶æ€", interactive=False)

        # ç¤ºä¾‹é€‰æ‹©
        gr.Markdown("### ğŸ“‹ ç¤ºä¾‹é€‰æ‹©")
        gr.Markdown("ç‚¹å‡»ä¸‹æ–¹ç¤ºä¾‹å¯å¿«é€Ÿå¡«å……å‚è€ƒéŸ³é¢‘ã€å‚è€ƒæ–‡æœ¬å’Œåˆæˆæ–‡æœ¬")
        
        example_data = load_examples()
        if example_data:
            gr.Examples(
                examples=example_data,
                inputs=[prompt_text, prompt_audio, input_text],
                label="é€‰æ‹©ç¤ºä¾‹",
                examples_per_page=6,
            )

        # äº‹ä»¶ç»‘å®š
        generate_btn.click(
            fn=run_inference,
            inputs=[prompt_text, prompt_audio, input_text, seed, sample_rate, use_cache, use_g2p],
            outputs=[output_audio]
        )

        clear_btn.click(
            fn=clear_memory,
            inputs=None,
            outputs=[status_msg]
        )

        # ä½¿ç”¨è¯´æ˜
        gr.Markdown("""
        ---
        ### ä½¿ç”¨è¯´æ˜
        1. **ä¸Šä¼ å‚è€ƒéŸ³é¢‘**: ä¸Šä¼ ä¸€æ®µæ¸…æ™°çš„è¯­éŸ³ä½œä¸ºéŸ³è‰²å‚è€ƒï¼ˆå»ºè®® 3-10 ç§’ï¼‰
        2. **å¡«å†™å‚è€ƒæ–‡æœ¬**: è¾“å…¥å‚è€ƒéŸ³é¢‘ä¸­è¯´è¯çš„å…·ä½“å†…å®¹
        3. **è¾“å…¥åˆæˆæ–‡æœ¬**: è¾“å…¥æ‚¨æƒ³è¦åˆæˆçš„æ–‡æœ¬å†…å®¹
        4. **ç‚¹å‡»åˆæˆ**: ç‚¹å‡»"å¼€å§‹åˆæˆ"æŒ‰é’®ç”Ÿæˆè¯­éŸ³
        
        ### æ³¨æ„äº‹é¡¹
        - é¦–æ¬¡è¿è¡Œéœ€è¦åŠ è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ç­‰å¾…å‡ åˆ†é’Ÿ
        - æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æ–‡æœ¬åˆæˆ
        - å‚è€ƒéŸ³é¢‘è´¨é‡è¶Šé«˜ï¼Œåˆæˆæ•ˆæœè¶Šå¥½
        
        ---
        ### ğŸ”¤ å¤šéŸ³å­—å¤„ç†è¯´æ˜
        
        å¯ç”¨ **G2P (å¤šéŸ³å­—å¤„ç†)** é€‰é¡¹åï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°†å¤šéŸ³å­—è½¬æ¢ä¸ºéŸ³ç´ æ ‡è®°ï¼Œç¡®ä¿å‘éŸ³å‡†ç¡®ã€‚
        
        **è‡ªå®šä¹‰å¤šéŸ³å­—é…ç½®æ–‡ä»¶**: `configs/G2P_replace_dict.jsonl`
        
        **æ ¼å¼ç¤ºä¾‹**:
        ```
        {"è¡°": "<|SH|><|UAI1|>"}           # è¡° â†’ shuÄi (ä¸€å£°ï¼Œè¡°è€)
        {"ä¹¡éŸ³æ— æ”¹é¬“æ¯›è¡°": "ä¹¡éŸ³æ— æ”¹é¬“æ¯›<|C|><|UEI1|>"}  # å¤è¯—ä¸­"è¡°"è¯» cuÄ«
        {"é•¿å¤§": "<|ZH|><|ANG3|><|D|><|A4|>"}   # é•¿å¤§ â†’ zhÇng dÃ 
        {"é•¿åº¦": "<|CH|><|ANG2|><|D|><|U4|>"}   # é•¿åº¦ â†’ chÃ¡ng dÃ¹
        ```
        
        **éŸ³ç´ æ ¼å¼**: `<|å£°æ¯|><|éŸµæ¯+å£°è°ƒ|>`
        - å£°è°ƒ: 1=ä¸€å£°, 2=äºŒå£°, 3=ä¸‰å£°, 4=å››å£°, 5=è½»å£°
        - ç¤ºä¾‹: `<|CH|><|ANG2|>` = chÃ¡ng (äºŒå£°)
        """)

    return app


# --- ä¸»å…¥å£ ---

if __name__ == "__main__":
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(MODEL_ROOT):
        print(f"é”™è¯¯: æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {MODEL_ROOT}")
        print("è¯·ç¡®ä¿å·²ä¸‹è½½æ¨¡å‹å¹¶æ”¾ç½®åœ¨ checkpoints/GLM-TTS ç›®å½•ä¸‹")
        exit(1)
    
    # æ£€æŸ¥å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
    required_paths = [
        SPEECH_TOKENIZER_PATH,
        LLM_PATH,
        FLOW_CKPT_PATH,
        FLOW_CONFIG_PATH,
        TOKENIZER_PATH,
    ]
    
    for path in required_paths:
        if not os.path.exists(path):
            print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            exit(1)
    
    print(f"æ¨¡å‹ç›®å½•: {MODEL_ROOT}")
    print(f"è®¾å¤‡: {DEVICE}")
    print("æ­£åœ¨å¯åŠ¨ GLM-TTS Web æœåŠ¡...")
    
    app = create_ui()
    app.queue().launch(
        server_name="0.0.0.0", 
        server_port=7860, 
        share=False
    )
