'''
å¤„ç†æµç¨‹ï¼š
1. é€šè¿‡ AsyncWebCrawler ä¸‹è½½ tut4it é¦–é¡µï¼Œè°ƒç”¨ Qwen æ¨¡å‹è·å–æ€»é¡µæ•°ã€‚
2. ä¾æ¬¡æŠ“å–æ¯ä¸€é¡µè¯¾ç¨‹åˆ—è¡¨ï¼Œè°ƒç”¨ Qwen æ¨¡å‹æå–è¯¾ç¨‹æ ‡é¢˜å’Œé“¾æ¥ã€‚
3. å¯¹æ¯ä¸ªè¯¾ç¨‹è¯¦æƒ…é¡µï¼Œè°ƒç”¨ Qwen æ¨¡å‹æå–è¯¾ç¨‹ä¸­æ–‡æè¿°å’Œä¸‹è½½é“¾æ¥ã€‚
4. ç»“æœå¯ç”¨äºå±•ç¤ºã€ä¿å­˜æˆ–åç»­åˆ†æã€‚
'''

import asyncio
import requests
import time
from crawl4ai import AsyncWebCrawler

# Qwen æ¨¡å‹é…ç½®
QWEN_API_URL = "http://10.112.113.1:8001/v1/chat/completions"
QWEN_MODEL_NAME = "/mnt/nvme0/Qwen2.5-72B-Instruct/"

# ç”¨äºè·å–æ€»é¡µæ•°
def ask_qwen_page(markdown_text: str) -> str:
    headers = {"Content-Type": "application/json"}
    prompt = (
        "è¯·ä»ä»¥ä¸‹ç½‘é¡µå†…å®¹ä¸­æå–ç«™ç‚¹çš„é¡µé¢æ€»æ•°ï¼Œä»¥æ•°å­—çš„å½¢å¼è¿”å›ï¼Œæ¯”å¦‚ 1100ï¼Œåªè¦æ•°å­—å°±è¡Œï¼Œå…¶å®ƒçš„éƒ½ä¸éœ€è¦ï¼š\n\n"
        f"{markdown_text}"
    )

    payload = {
        "model": QWEN_MODEL_NAME,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç½‘é¡µç»“æ„ä¿¡æ¯æå–åŠ©æ‰‹"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 32768
    }

    response = requests.post(QWEN_API_URL, headers=headers, json=payload)
    return response.json()["choices"][0]["message"]["content"]

# ç”¨äºä»åˆ—è¡¨é¡µæå–è¯¾ç¨‹æ ‡é¢˜å’Œé“¾æ¥
def ask_qwen_list(markdown_text: str) -> str:
    headers = {"Content-Type": "application/json"}
    prompt = (
        "è¯·ä»ä»¥ä¸‹ç½‘é¡µå†…å®¹ä¸­æå–æ‰€æœ‰è¯¾ç¨‹æ ‡é¢˜å’Œé“¾æ¥ï¼Œ"
        "åªè¿”å›çº¯æ–‡æœ¬æ ‡é¢˜å’Œå¯¹åº”çš„é“¾æ¥ï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™ç¬¦å·ï¼Œå¦‚ 'é“¾æ¥:', '[', ']', '(', ')' ç­‰ï¼Œ"
        "æ ¼å¼ä¸ºæ¯è¡Œä¸€æ¡è®°å½•ï¼Œæ ‡é¢˜å’Œé“¾æ¥ç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ï¼š\n"
        "The Ultimate Guide to Unity, https://tut4it.com/some-course\n\n"
        f"{markdown_text}"
    )

    payload = {
        "model": QWEN_MODEL_NAME,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç½‘é¡µç»“æ„ä¿¡æ¯æå–åŠ©æ‰‹"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 32768
    }

    response = requests.post(QWEN_API_URL, headers=headers, json=payload)
    return response.json()["choices"][0]["message"]["content"]

# ä»è¯¾ç¨‹è¯¦æƒ…é¡µæå–ä¸­æ–‡æè¿° + ä¸‹è½½é“¾æ¥
def ask_qwen_course_detail(markdown_text: str) -> str:
    headers = {"Content-Type": "application/json"}
    prompt = (
        "è¯·ä½ ä½œä¸ºä¸­æ–‡è¯­è¨€åŠ©æ‰‹ï¼Œä»ä»¥ä¸‹ç½‘é¡µå†…å®¹ä¸­æå–è¯¾ç¨‹çš„ä¸­æ–‡ç®€è¦æè¿°ï¼ˆçº¦50-100å­—ï¼‰ï¼Œ"
        "æè¿°åº”åŒ…æ‹¬è¯¥è¯¾ç¨‹æ•™æˆçš„å†…å®¹ã€æŠ€æœ¯æ–¹å‘æˆ–é€‚åˆäººç¾¤ã€‚"
        "åŒæ—¶ï¼Œæå–è¯¥é¡µé¢ä¸­æ‰€æœ‰ä¸‹è½½é“¾æ¥ï¼ˆå¦‚ç™¾åº¦ç½‘ç›˜ã€Google Driveã€OneDriveã€MediaFire ç­‰ï¼‰ã€‚\n\n"
        "è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼ˆè¯·ä¸¥æ ¼ä½¿ç”¨ä¸­æ–‡ï¼‰ï¼š\n"
        "è¯¾ç¨‹æè¿°ï¼šxxxxxï¼ˆç”¨ä¸­æ–‡æ¦‚æ‹¬ï¼‰\n"
        "ä¸‹è½½é“¾æ¥ï¼š\n- é“¾æ¥1\n- é“¾æ¥2\n\n"
        "ä»¥ä¸‹æ˜¯ç½‘é¡µå†…å®¹ï¼š\n\n"
        f"{markdown_text}"
    )

    payload = {
        "model": QWEN_MODEL_NAME,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç½‘é¡µç»“æ„ä¿¡æ¯æå–åŠ©æ‰‹"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 32768
    }

    response = requests.post(QWEN_API_URL, headers=headers, json=payload)
    return response.json()["choices"][0]["message"]["content"]

# å°†æ¨¡å‹ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨
def parse_qwen_answer_to_list(answer: str) -> list[tuple[str, str]]:
    lines = answer.strip().split("\n")
    result = []
    for line in lines:
        if "," in line:
            title, url = line.split(",", 1)
            if url.strip().startswith("http"):
                result.append((title.strip(), url.strip()))
    return result

# å¼‚æ­¥è·å–æ€»é¡µæ•°
async def crawl_and_extract_pagenum(url: str) -> int:
    async with AsyncWebCrawler(headless=True) as crawler:
        result = await crawler.arun(url)
        markdown = result.markdown
        answer = ask_qwen_page(markdown)
        return int(answer.strip())

# å¼‚æ­¥æŠ“å–æ‰€æœ‰è¯¾ç¨‹å¹¶æå–è¯¦æƒ…
async def crawl_and_extract_all(page_num: int):
    async with AsyncWebCrawler(headless=True) as crawler:
        for i in range(1, page_num + 1):
            url = f"https://tut4it.com/page/{i}/"
            print(f"\nğŸ“„ æŠ“å–åˆ—è¡¨é¡µï¼š{url}")
            result = await crawler.arun(url)
            markdown = result.markdown
            list_answer = ask_qwen_list(markdown)
            courses = parse_qwen_answer_to_list(list_answer)

            print(f"ğŸ“¦ ç¬¬ {i} é¡µå‘ç°è¯¾ç¨‹æ•°ï¼š{len(courses)}")
            for title, course_url in courses:
                print(f"\nğŸ”— æ‰“å¼€è¯¾ç¨‹ï¼šã€Š{title}ã€‹\n{course_url}")
                try:
                    detail_result = await crawler.arun(course_url)
                    detail_markdown = detail_result.markdown
                    detail_answer = ask_qwen_course_detail(detail_markdown)
                    print(detail_answer)
                except Exception as e:
                    print(f"âš ï¸ æŠ“å–å¤±è´¥ï¼š{e}")

                time.sleep(60)  # æ¯é—¨è¯¾ç¨‹é—´éš”ï¼Œé¿å…è¿‡å¿«

            time.sleep(60)  # æ¯é¡µé—´éš”

# ä¸»å‡½æ•°
async def main():
    page_num = await crawl_and_extract_pagenum("https://tut4it.com")
    print(f"ğŸŒ æ€»é¡µæ•°ï¼š{page_num}")
    await crawl_and_extract_all(page_num)

if __name__ == "__main__":
    asyncio.run(main())
