#!/usr/bin/env python3
"""
HunyuanVideo-Avatar ç‹¬ç«‹æ¼”ç¤ºåº”ç”¨
è¿è¡Œæ–¹å¼: python app.py
"""

import os
import sys
import math
import uuid
import warnings
import datetime
import numpy as np
import torch
import imageio
import gradio as gr
from PIL import Image
from einops import rearrange
import torchvision.transforms as transforms
from torchvision.transforms import ToPILImage

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"
os.environ["PYTHONPATH"] = "./"
os.environ["MODEL_BASE"] = "./weights"  # è®¾ç½®æ¨¡å‹åŸºç¡€è·¯å¾„
os.environ["DISABLE_SP"] = "1"  # ç¦ç”¨åºåˆ—å¹¶è¡Œï¼Œå•GPUæ¨¡å¼
os.environ["RANK"] = "0"
os.environ["WORLD_SIZE"] = "1"
os.environ["LOCAL_RANK"] = "0"
os.environ["MASTER_ADDR"] = "127.0.0.1"
os.environ["MASTER_PORT"] = "29500"
warnings.filterwarnings("ignore")

# é…ç½®
MODEL_BASE = "./weights"
TEMP_DIR = "./temp"
os.makedirs(TEMP_DIR, exist_ok=True)

# ===================== æ¨¡å‹åŠ è½½ =====================

def load_models(checkpoint_path, use_fp8=False, cpu_offload=False):
    """åŠ è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
    import torch.distributed as dist
    from hymm_sp.config import parse_args
    from hymm_sp.sample_inference_audio import HunyuanVideoSampler
    from transformers import WhisperModel, AutoFeatureExtractor
    from hymm_sp.data_kits.face_align import AlignImage
    from hymm_sp.modules.parallel_states import initialize_sequence_parallel_state, nccl_info
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆå•GPUæ¨¡å¼ï¼‰
    if not dist.is_initialized():
        dist.init_process_group(
            backend="nccl",
            init_method="env://",
            world_size=1,
            rank=0
        )
    
    # åˆå§‹åŒ–åºåˆ—å¹¶è¡ŒçŠ¶æ€
    initialize_sequence_parallel_state(1)
    
    # æ„å»ºå‚æ•°
    sys.argv = [
        'app.py',
        '--input', 'assets/test.csv',
        '--ckpt', checkpoint_path,
        '--sample-n-frames', '129',
        '--seed', '128',
        '--image-size', '704',
        '--cfg-scale', '7.5',
        '--infer-steps', '50',
        '--use-deepcache', '1',
        '--flow-shift-eval-video', '5.0',
    ]
    
    if use_fp8:
        sys.argv.append('--use-fp8')
    if cpu_offload:
        sys.argv.append('--cpu-offload')
    
    args = parse_args()
    
    print("=" * 60)
    print("Loading HunyuanVideo-Avatar models...")
    print("=" * 60)
    
    # åŠ è½½ä¸»æ¨¡å‹
    hunyuan_sampler = HunyuanVideoSampler.from_pretrained(checkpoint_path, args=args)
    args = hunyuan_sampler.args
    device = torch.device("cuda")
    
    # åŠ è½½ Whisper éŸ³é¢‘ç‰¹å¾æå–å™¨
    print("Loading Whisper model...")
    feature_extractor = AutoFeatureExtractor.from_pretrained(f"{MODEL_BASE}/ckpts/whisper-tiny/")
    wav2vec = WhisperModel.from_pretrained(f"{MODEL_BASE}/ckpts/whisper-tiny/").to(device=device, dtype=torch.float32)
    wav2vec.requires_grad_(False)
    
    # åŠ è½½äººè„¸å¯¹é½æ¨¡å‹
    print("Loading face alignment model...")
    det_path = os.path.join(MODEL_BASE, 'ckpts/det_align/detface.pt')
    align_instance = AlignImage("cuda", det_path=det_path)
    
    print("=" * 60)
    print("All models loaded successfully!")
    print("=" * 60)
    
    return {
        'sampler': hunyuan_sampler,
        'args': args,
        'wav2vec': wav2vec,
        'feature_extractor': feature_extractor,
        'align_instance': align_instance,
        'device': device,
    }


# ===================== æ•°æ®é¢„å¤„ç† =====================

def preprocess_data(args, image_path, audio_path, prompt, feature_extractor):
    """é¢„å¤„ç†è¾“å…¥æ•°æ®"""
    from hymm_sp.data_kits.audio_dataset import get_audio_feature
    
    llava_transform = transforms.Compose([
        transforms.Resize((336, 336), interpolation=transforms.InterpolationMode.BILINEAR),
        transforms.ToTensor(),
        transforms.Normalize((0.48145466, 0.4578275, 0.4082107), (0.26862954, 0.26130258, 0.27577711)),
    ])
    
    # å¤„ç† prompt
    if prompt is None or prompt.strip() == "":
        prompt = "Authentic, Realistic, Natural, High-quality, Lens-Fixed."
    else:
        prompt = "Authentic, Realistic, Natural, High-quality, Lens-Fixed, " + prompt
    
    fps = 25
    img_size = args.image_size
    
    # å¤„ç†å‚è€ƒå›¾åƒ
    ref_image = Image.open(image_path).convert('RGB')
    w, h = ref_image.size
    scale = img_size / min(w, h)
    new_w = round(w * scale / 64) * 64
    new_h = round(h * scale / 64) * 64
    
    if img_size == 704:
        img_size_long = 1216
        if new_w * new_h > img_size * img_size_long:
            scale = math.sqrt(img_size * img_size_long / w / h)
            new_w = round(w * scale / 64) * 64
            new_h = round(h * scale / 64) * 64
    
    ref_image = ref_image.resize((new_w, new_h), Image.LANCZOS)
    ref_image = torch.from_numpy(np.array(ref_image))
    
    # å¤„ç†éŸ³é¢‘
    audio_input, audio_len = get_audio_feature(feature_extractor, audio_path)
    audio_prompts = audio_input[0]
    
    # è¿åŠ¨å‚æ•°
    motion_bucket_id_heads = torch.from_numpy(np.array([25] * 4))
    motion_bucket_id_exps = torch.from_numpy(np.array([30] * 4))
    fps = torch.from_numpy(np.array(fps))
    
    # å¤„ç†å‚è€ƒå›¾åƒç”¨äºVAEå’ŒLLaVA
    to_pil = ToPILImage()
    pixel_value_ref = rearrange(ref_image.clone().unsqueeze(0), "b h w c -> b c h w")
    pixel_value_ref_llava = [llava_transform(to_pil(image)) for image in pixel_value_ref]
    pixel_value_ref_llava = torch.stack(pixel_value_ref_llava, dim=0)
    
    batch = {
        "text_prompt": [prompt],
        "audio_path": [audio_path],
        "image_path": [image_path],
        "fps": fps.unsqueeze(0).to(dtype=torch.float16),
        "audio_prompts": audio_prompts.unsqueeze(0).to(dtype=torch.float16),
        "audio_len": [audio_len],
        "motion_bucket_id_exps": motion_bucket_id_exps.unsqueeze(0),
        "motion_bucket_id_heads": motion_bucket_id_heads.unsqueeze(0),
        "pixel_value_ref": pixel_value_ref.unsqueeze(0).to(dtype=torch.float16),
        "pixel_value_ref_llava": pixel_value_ref_llava.unsqueeze(0).to(dtype=torch.float16)
    }
    
    return batch, audio_len


# ===================== æ¨ç†å‡½æ•° =====================

def generate_video(audio_path, image, prompt, models, progress=gr.Progress()):
    """ç”Ÿæˆè§†é¢‘çš„ä¸»å‡½æ•°"""
    if image is None:
        raise gr.Error("è¯·ä¸Šä¼ å‚è€ƒå›¾åƒ")
    if audio_path is None:
        raise gr.Error("è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
    
    progress(0.1, desc="å‡†å¤‡æ•°æ®...")
    
    # ä¿å­˜å›¾åƒåˆ°ä¸´æ—¶æ–‡ä»¶
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    temp_image_path = os.path.join(TEMP_DIR, f"input_{timestamp}.png")
    
    # image æ˜¯ numpy array (H, W, C) RGBæ ¼å¼
    Image.fromarray(image).save(temp_image_path)
    
    try:
        progress(0.2, desc="é¢„å¤„ç†æ•°æ®...")
        
        # é¢„å¤„ç†
        batch, audio_len = preprocess_data(
            models['args'],
            temp_image_path,
            audio_path,
            prompt,
            models['feature_extractor']
        )
        
        progress(0.3, desc="ç”Ÿæˆè§†é¢‘ä¸­ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
        
        # æ¨ç†
        outputs = models['sampler'].predict(
            models['args'],
            batch,
            models['wav2vec'],
            models['feature_extractor'],
            models['align_instance']
        )
        
        if outputs is None:
            raise gr.Error("è§†é¢‘ç”Ÿæˆå¤±è´¥")
        
        progress(0.9, desc="ä¿å­˜è§†é¢‘...")
        
        # å¤„ç†è¾“å‡º
        samples = outputs["samples"]
        sample = samples[0].unsqueeze(0)
        sample = sample[:, :, :audio_len]
        
        video = sample[0].permute(1, 2, 3, 0).clamp(0, 1).cpu().numpy()
        video = (video * 255.).astype(np.uint8)
        
        # ä¿å­˜è§†é¢‘
        output_video_path = os.path.join(TEMP_DIR, f"output_{timestamp}.mp4")
        imageio.mimsave(output_video_path, video, fps=25)
        
        # æ·»åŠ éŸ³é¢‘
        output_with_audio = output_video_path.replace(".mp4", "_audio.mp4")
        os.system(f"ffmpeg -i '{output_video_path}' -i '{audio_path}' -shortest '{output_with_audio}' -y -loglevel quiet")
        
        if os.path.exists(output_with_audio):
            os.remove(output_video_path)
            output_video_path = output_with_audio
        
        progress(1.0, desc="å®Œæˆ!")
        
        return output_video_path
        
    finally:
        # æ¸…ç†ä¸´æ—¶å›¾åƒæ–‡ä»¶
        if os.path.exists(temp_image_path):
            os.remove(temp_image_path)


# ===================== Gradio ç•Œé¢ =====================

def create_demo(models):
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # ä» test.csv ä¸­é€‰å–çš„ç¤ºä¾‹
    examples = [
        # [audio, image, prompt]
        ["assets/audio/2.WAV", "assets/image/1.png", "A person sits cross-legged by a campfire in a forested area."],
        ["assets/audio/2.WAV", "assets/image/2.png", "A person with long blonde hair wearing a green jacket, standing in a forested area during twilight."],
        ["assets/audio/3.WAV", "assets/image/3.png", "A person playing guitar by a campfire in a forest."],
        ["assets/audio/3.WAV", "assets/image/4.png", "A person wearing a green jacket stands in a forested area, with sunlight filtering through the trees."],
        ["assets/audio/4.WAV", "assets/image/src1.png", "A person sits cross-legged by a campfire in a forest at dusk."],
        ["assets/audio/4.WAV", "assets/image/src2.png", "A person in a green jacket stands in a forest at dusk."],
    ]
    
    def run_generation(audio_path, image, prompt):
        return generate_video(audio_path, image, prompt, models)
    
    with gr.Blocks(title="HunyuanVideo-Avatar Demo") as demo:
        gr.Markdown("""
        <div style="text-align: center; max-width: 800px; margin: 0 auto;">
            <h1 style="font-size: 2.5rem; font-weight: 700; margin-bottom: 1rem;">
                ğŸ¬ Tencent HunyuanVideo-Avatar Demo
            </h1>
            <p style="color: #888;">ä¸Šä¼ ä¸€å¼ äººåƒå›¾ç‰‡å’Œä¸€æ®µéŸ³é¢‘ï¼Œç”Ÿæˆè¯´è¯è§†é¢‘</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                prompt = gr.Textbox(
                    label="Prompt (æç¤ºè¯)", 
                    value="a person is speaking.",
                    placeholder="æè¿°è§†é¢‘å†…å®¹ï¼Œä¾‹å¦‚: a man is speaking, a woman is talking..."
                )
                
                audio_input = gr.Audio(
                    sources=["upload"],
                    type="filepath",
                    label="ğŸµ ä¸Šä¼ éŸ³é¢‘ (Upload Audio)",
                )
                
                image_input = gr.Image(
                    label="ğŸ–¼ï¸ ä¸Šä¼ å‚è€ƒå›¾åƒ (Reference Image)",
                    type="numpy",
                    height=400
                )
                
                generate_btn = gr.Button("ğŸš€ ç”Ÿæˆè§†é¢‘ (Generate)", variant="primary", size="lg")
            
            with gr.Column(scale=1):
                output_video = gr.Video(label="ğŸ¬ ç”Ÿæˆçš„è§†é¢‘ (Generated Video)")
        
        # æ·»åŠ ç¤ºä¾‹
        gr.Markdown("### ğŸ“Œ ç¤ºä¾‹ (Examples) - ç‚¹å‡»ä½¿ç”¨")
        gr.Examples(
            examples=examples,
            inputs=[audio_input, image_input, prompt],
            label="",
        )
        
        # ä½¿ç”¨è¯´æ˜
        gr.Markdown("""
        ---
        ### ğŸ“– ä½¿ç”¨è¯´æ˜
        1. **ä¸Šä¼ éŸ³é¢‘**: æ”¯æŒ WAV æ ¼å¼çš„è¯­éŸ³æ–‡ä»¶
        2. **ä¸Šä¼ å›¾åƒ**: ä¸Šä¼ ä¸€å¼ æ¸…æ™°çš„äººåƒç…§ç‰‡ï¼ˆæ­£é¢ç…§æ•ˆæœæœ€ä½³ï¼‰
        3. **è®¾ç½®æç¤ºè¯**: æè¿°è§†é¢‘å†…å®¹ï¼ˆå¯é€‰ï¼‰
        4. **ç‚¹å‡»ç”Ÿæˆ**: ç­‰å¾…å‡ åˆ†é’Ÿå³å¯è·å¾—è¯´è¯è§†é¢‘
        
        âš ï¸ **æ³¨æ„**: é¦–æ¬¡ç”Ÿæˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
        """)
        
        generate_btn.click(
            fn=run_generation,
            inputs=[audio_input, image_input, prompt],
            outputs=[output_video],
        )
    
    return demo


# ===================== ä¸»ç¨‹åº =====================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="HunyuanVideo-Avatar Demo")
    parser.add_argument("--port", type=int, default=7860, help="æœåŠ¡ç«¯å£")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")
    parser.add_argument("--use-fp8", action="store_true", help="ä½¿ç”¨ FP8 é‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
    parser.add_argument("--cpu-offload", action="store_true", help="CPU offloadï¼ˆä½æ˜¾å­˜æ¨¡å¼ï¼‰")
    cmd_args = parser.parse_args()
    
    # ç¡®å®šæ¨¡å‹è·¯å¾„
    if cmd_args.use_fp8:
        checkpoint_path = f"{MODEL_BASE}/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states_fp8.pt"
    else:
        checkpoint_path = f"{MODEL_BASE}/ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt"
    
    print(f"Checkpoint: {checkpoint_path}")
    print(f"Use FP8: {cmd_args.use_fp8}")
    print(f"CPU Offload: {cmd_args.cpu_offload}")
    
    # åŠ è½½æ¨¡å‹
    models = load_models(checkpoint_path, cmd_args.use_fp8, cmd_args.cpu_offload)
    
    # åˆ›å»ºå¹¶å¯åŠ¨ Gradio
    demo = create_demo(models)
    demo.launch(
        server_name="0.0.0.0",
        server_port=cmd_args.port,
        share=cmd_args.share,
        allowed_paths=["/"]
    )
