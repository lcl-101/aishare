#!/usr/bin/env python3
"""
TurboDiffusion Gradio Web Application
åŸºäº Gradio çš„è§†é¢‘ç”Ÿæˆ Web åº”ç”¨

æ”¯æŒä¸¤ç§æ¨¡å¼:
- æ–‡ç”Ÿè§†é¢‘ (T2V): æ ¹æ®æ–‡å­—æç¤ºç”Ÿæˆè§†é¢‘
- å›¾ç”Ÿè§†é¢‘ (I2V): æ ¹æ®å›¾ç‰‡å’Œæ–‡å­—æç¤ºç”Ÿæˆè§†é¢‘
"""

import os
import sys
import math
import tempfile
import uuid
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# è®¾ç½® PYTHONPATH
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "turbodiffusion"))

import torch
import gradio as gr
import numpy as np
from PIL import Image
from einops import rearrange, repeat
from tqdm import tqdm
import torchvision.transforms.v2 as T

# ============================================================
# åœ¨å¯¼å…¥ SLA ä¹‹å‰ï¼Œå…ˆå‡†å¤‡ PyTorch æ›¿ä»£å®ç°
# ============================================================
def mean_pool_pytorch(x, BLK):
    """çº¯ PyTorch å®ç°çš„ mean_poolï¼Œæ›¿ä»£ Triton ç‰ˆæœ¬"""
    B, H, L, D = x.shape
    num_blocks = (L + BLK - 1) // BLK
    pad_size = num_blocks * BLK - L
    
    if pad_size > 0:
        x_padded = torch.nn.functional.pad(x, (0, 0, 0, pad_size))
    else:
        x_padded = x
    
    x_reshaped = x_padded.view(B, H, num_blocks, BLK, D)
    # å¯¹äºéƒ¨åˆ†å¡«å……çš„æœ€åä¸€ä¸ªå—ï¼Œéœ€è¦æ­£ç¡®è®¡ç®—å‡å€¼
    x_mean = x_reshaped.float().mean(dim=3)
    return x_mean.to(x.dtype)

# å¯¼å…¥ SLA æ¨¡å—å¹¶æ›¿æ¢å‡½æ•°
import SLA.utils as sla_utils
sla_utils.mean_pool = mean_pool_pytorch

from imaginaire.utils.io import save_image_or_video
from imaginaire.utils import log

from rcm.datasets.utils import VIDEO_RES_SIZE_INFO
from rcm.utils.umt5 import clear_umt5_memory, get_umt5_embedding
from rcm.tokenizers.wan2pt1 import Wan2pt1VAEInterface

from inference.modify_model import tensor_kwargs, create_model

torch._dynamo.config.suppress_errors = True

# ============================================================
# å…¨å±€æ¨¡å‹å˜é‡
# ============================================================
t2v_model = None
i2v_high_noise_model = None
i2v_low_noise_model = None
tokenizer = None
text_encoder_path = "checkpoints/models_t5_umt5-xxl-enc-bf16.pth"

# å½“å‰æ´»è·ƒçš„æ¨¡å‹æ¨¡å¼ (ç”¨äºç®¡ç† GPU å†…å­˜)
current_mode = None  # "t2v" æˆ– "i2v"

# ============================================================
# æ¨¡å‹åŠ è½½å‡½æ•°
# ============================================================
def load_models():
    """å¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ¨¡å‹åˆ° GPU"""
    global t2v_model, i2v_high_noise_model, i2v_low_noise_model, tokenizer
    
    log.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    # åˆ›å»ºå‚æ•°å¯¹è±¡
    class Args:
        model = "Wan2.1-1.3B"
        attention_type = "sagesla"  # æ£€æŸ¥ç‚¹éœ€è¦ SageSLA æ³¨æ„åŠ›
        sla_topk = 0.1
        quant_linear = False  # H20 æœ‰ 96GB æ˜¾å­˜ï¼Œä¸éœ€è¦é‡åŒ–
        default_norm = False
    
    args = Args()
    
    # åŠ è½½ T2V æ¨¡å‹ (ç›´æ¥æ”¾ GPU)
    log.info("åŠ è½½ T2V æ¨¡å‹...")
    t2v_model = create_model(
        dit_path="checkpoints/TurboWan2.1-T2V-1.3B-480P.pth",
        args=args
    ).cuda().eval()
    torch.cuda.synchronize()
    log.success("T2V æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½ I2V æ¨¡å‹ (ç›´æ¥æ”¾ GPU)
    log.info("åŠ è½½ I2V é«˜å™ªå£°æ¨¡å‹...")
    args.model = "Wan2.2-A14B"
    i2v_high_noise_model = create_model(
        dit_path="checkpoints/TurboWan2.2-I2V-A14B-high-720P.pth",
        args=args
    ).cuda().eval()
    torch.cuda.synchronize()
    log.success("I2V é«˜å™ªå£°æ¨¡å‹åŠ è½½å®Œæˆ")
    
    log.info("åŠ è½½ I2V ä½å™ªå£°æ¨¡å‹...")
    i2v_low_noise_model = create_model(
        dit_path="checkpoints/TurboWan2.2-I2V-A14B-low-720P.pth",
        args=args
    ).cuda().eval()
    torch.cuda.synchronize()
    log.success("I2V ä½å™ªå£°æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½ VAE
    log.info("åŠ è½½ VAE...")
    tokenizer = Wan2pt1VAEInterface(vae_pth="checkpoints/Wan2.1_VAE.pth")
    log.success("VAE åŠ è½½å®Œæˆ")
    
    log.success("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ============================================================
# T2V ç”Ÿæˆå‡½æ•°
# ============================================================
def generate_t2v(
    prompt: str,
    num_frames: int,
    num_steps: int,
    resolution: str,
    aspect_ratio: str,
    sigma_max: float,
    sla_topk: float,
    seed: int,
    progress=gr.Progress(track_tqdm=True)
):
    """æ–‡ç”Ÿè§†é¢‘ç”Ÿæˆ"""
    global t2v_model, tokenizer
    
    if not prompt.strip():
        raise gr.Error("è¯·è¾“å…¥æç¤ºè¯")
    
    log.info(f"æ­£åœ¨ä¸ºæç¤ºè¯è®¡ç®—åµŒå…¥: {prompt}")
    with torch.no_grad():
        text_emb = get_umt5_embedding(
            checkpoint_path=text_encoder_path,
            prompts=prompt
        ).to(**tensor_kwargs)
    clear_umt5_memory()
    
    w, h = VIDEO_RES_SIZE_INFO[resolution][aspect_ratio]
    
    condition = {"crossattn_emb": text_emb.to(**tensor_kwargs)}
    
    state_shape = [
        tokenizer.latent_ch,
        tokenizer.get_latent_num_frames(num_frames),
        h // tokenizer.spatial_compression_factor,
        w // tokenizer.spatial_compression_factor,
    ]
    
    generator = torch.Generator(device=tensor_kwargs["device"])
    generator.manual_seed(int(seed))
    
    init_noise = torch.randn(
        1,
        *state_shape,
        dtype=torch.float32,
        device=tensor_kwargs["device"],
        generator=generator,
    )
    
    mid_t = [1.5, 1.4, 1.0][: num_steps - 1]
    t_steps = torch.tensor(
        [math.atan(sigma_max), *mid_t, 0],
        dtype=torch.float64,
        device=init_noise.device,
    )
    t_steps = torch.sin(t_steps) / (torch.cos(t_steps) + torch.sin(t_steps))
    
    x = init_noise.to(torch.float64) * t_steps[0]
    ones = torch.ones(x.size(0), 1, device=x.device, dtype=x.dtype)
    total_steps = t_steps.shape[0] - 1
    
    # æ¨¡å‹å·²åœ¨ GPU ä¸Šï¼Œç›´æ¥æ¨ç†
    for i, (t_cur, t_next) in enumerate(tqdm(list(zip(t_steps[:-1], t_steps[1:])), desc="é‡‡æ ·ä¸­", total=total_steps)):
        with torch.no_grad():
            v_pred = t2v_model(
                x_B_C_T_H_W=x.to(**tensor_kwargs),
                timesteps_B_T=(t_cur.float() * ones * 1000).to(**tensor_kwargs),
                **condition
            ).to(torch.float64)
            x = (1 - t_next) * (x - t_cur * v_pred) + t_next * torch.randn(
                *x.shape,
                dtype=torch.float32,
                device=tensor_kwargs["device"],
                generator=generator,
            )
    
    samples = x.float()
    
    with torch.no_grad():
        video = tokenizer.decode(samples)
    
    video_output = video.float().cpu()
    video_output = (1.0 + video_output.clamp(-1, 1)) / 2.0
    
    # æ¸…ç†ä¸´æ—¶å¼ é‡
    del x, init_noise, samples, text_emb, condition
    torch.cuda.empty_cache()
    
    # ä¿å­˜è§†é¢‘
    output_path = os.path.join(tempfile.gettempdir(), f"t2v_{uuid.uuid4().hex[:8]}.mp4")
    save_image_or_video(rearrange(video_output, "b c t h w -> c t h (b w)"), output_path, fps=16)
    
    return output_path

# ============================================================
# I2V ç”Ÿæˆå‡½æ•°
# ============================================================
def generate_i2v(
    image: Image.Image,
    prompt: str,
    num_frames: int,
    num_steps: int,
    resolution: str,
    aspect_ratio: str,
    adaptive_resolution: bool,
    use_ode: bool,
    sigma_max: float,
    boundary: float,
    sla_topk: float,
    seed: int,
    progress=gr.Progress(track_tqdm=True)
):
    """å›¾ç”Ÿè§†é¢‘ç”Ÿæˆ"""
    global i2v_high_noise_model, i2v_low_noise_model, tokenizer
    
    if image is None:
        raise gr.Error("è¯·ä¸Šä¼ è¾“å…¥å›¾ç‰‡")
    if not prompt.strip():
        raise gr.Error("è¯·è¾“å…¥æç¤ºè¯")
    
    log.info(f"æ­£åœ¨ä¸ºæç¤ºè¯è®¡ç®—åµŒå…¥: {prompt}")
    with torch.no_grad():
        text_emb = get_umt5_embedding(
            checkpoint_path=text_encoder_path,
            prompts=prompt
        ).to(**tensor_kwargs)
    clear_umt5_memory()
    
    # å¤„ç†åˆ†è¾¨ç‡
    if adaptive_resolution:
        base_w, base_h = VIDEO_RES_SIZE_INFO[resolution][aspect_ratio]
        max_resolution_area = base_w * base_h
        
        orig_w, orig_h = image.size
        image_aspect_ratio = orig_h / orig_w
        
        ideal_w = np.sqrt(max_resolution_area / image_aspect_ratio)
        ideal_h = np.sqrt(max_resolution_area * image_aspect_ratio)
        
        stride = tokenizer.spatial_compression_factor * 2
        lat_h = round(ideal_h / stride)
        lat_w = round(ideal_w / stride)
        h = lat_h * stride
        w = lat_w * stride
    else:
        w, h = VIDEO_RES_SIZE_INFO[resolution][aspect_ratio]
    
    F = num_frames
    lat_h = h // tokenizer.spatial_compression_factor
    lat_w = w // tokenizer.spatial_compression_factor
    lat_t = tokenizer.get_latent_num_frames(F)
    
    # é¢„å¤„ç†å›¾åƒ
    image_transforms = T.Compose([
        T.ToImage(),
        T.Resize(size=(h, w), antialias=True),
        T.ToDtype(torch.float32, scale=True),
        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
    ])
    image_tensor = image_transforms(image).unsqueeze(0).to(device=tensor_kwargs["device"], dtype=torch.float32)
    
    with torch.no_grad():
        frames_to_encode = torch.cat(
            [image_tensor.unsqueeze(2), torch.zeros(1, 3, F - 1, h, w, device=image_tensor.device)], dim=2
        )
        encoded_latents = tokenizer.encode(frames_to_encode)
        del frames_to_encode
        torch.cuda.empty_cache()
    
    msk = torch.zeros(1, 4, lat_t, lat_h, lat_w, device=tensor_kwargs["device"], dtype=tensor_kwargs["dtype"])
    msk[:, :, 0, :, :] = 1.0
    
    y = torch.cat([msk, encoded_latents.to(**tensor_kwargs)], dim=1)
    
    condition = {
        "crossattn_emb": text_emb.to(**tensor_kwargs),
        "y_B_C_T_H_W": y
    }
    
    state_shape = [tokenizer.latent_ch, lat_t, lat_h, lat_w]
    
    generator = torch.Generator(device=tensor_kwargs["device"])
    generator.manual_seed(int(seed))

    init_noise = torch.randn(
        1,
        *state_shape,
        dtype=torch.float32,
        device=tensor_kwargs["device"],
        generator=generator,
    )
    
    mid_t = [1.5, 1.4, 1.0][: num_steps - 1]
    t_steps = torch.tensor(
        [math.atan(sigma_max), *mid_t, 0],
        dtype=torch.float64,
        device=init_noise.device,
    )
    t_steps = torch.sin(t_steps) / (torch.cos(t_steps) + torch.sin(t_steps))
    
    x = init_noise.to(torch.float64) * t_steps[0]
    ones = torch.ones(x.size(0), 1, device=x.device, dtype=x.dtype)
    total_steps = t_steps.shape[0] - 1
    
    # æ¨¡å‹å·²åœ¨ GPU ä¸Š
    # I2V ä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹ï¼šå…ˆç”¨é«˜å™ªå£°æ¨¡å‹ï¼Œåˆ°è¾¾è¾¹ç•Œååˆ‡æ¢åˆ°ä½å™ªå£°æ¨¡å‹
    net = i2v_high_noise_model
    switched = False
    
    for i, (t_cur, t_next) in enumerate(tqdm(list(zip(t_steps[:-1], t_steps[1:])), desc="é‡‡æ ·ä¸­", total=total_steps)):
        if t_cur.item() < boundary and not switched:
            net = i2v_low_noise_model
            switched = True
            log.info("åˆ‡æ¢åˆ°ä½å™ªå£°æ¨¡å‹")
        
        with torch.no_grad():
            v_pred = net(
                x_B_C_T_H_W=x.to(**tensor_kwargs),
                timesteps_B_T=(t_cur.float() * ones * 1000).to(**tensor_kwargs),
                **condition
            ).to(torch.float64)
            
            if use_ode:
                x = x - (t_cur - t_next) * v_pred
            else:
                x = (1 - t_next) * (x - t_cur * v_pred) + t_next * torch.randn(
                    *x.shape,
                    dtype=torch.float32,
                    device=tensor_kwargs["device"],
                    generator=generator,
                )
    
    samples = x.float()
    
    with torch.no_grad():
        video = tokenizer.decode(samples)
    
    video_output = video.float().cpu()
    video_output = (1.0 + video_output.clamp(-1, 1)) / 2.0
    
    # æ¸…ç†ä¸´æ—¶å¼ é‡
    del x, init_noise, samples, text_emb, condition, encoded_latents, y, msk
    torch.cuda.empty_cache()
    
    # ä¿å­˜è§†é¢‘
    output_path = os.path.join(tempfile.gettempdir(), f"i2v_{uuid.uuid4().hex[:8]}.mp4")
    save_image_or_video(rearrange(video_output, "b c t h w -> c t h (b w)"), output_path, fps=16)
    
    return output_path

# ============================================================
# åŠ è½½ç¤ºä¾‹æ•°æ®
# ============================================================
def load_t2v_examples():
    """åŠ è½½ T2V ç¤ºä¾‹"""
    prompts_file = Path("assets/t2v_inputs/prompts.txt")
    if prompts_file.exists():
        with open(prompts_file, "r", encoding="utf-8") as f:
            prompts = [line.strip() for line in f if line.strip()]
        return [[p] for p in prompts[:10]]  # æœ€å¤š10ä¸ªç¤ºä¾‹
    return []

def load_i2v_examples():
    """åŠ è½½ I2V ç¤ºä¾‹"""
    examples = []
    prompts_file = Path("assets/i2v_inputs/prompts.txt")
    images_dir = Path("assets/i2v_inputs")
    
    if prompts_file.exists():
        with open(prompts_file, "r", encoding="utf-8") as f:
            prompts = [line.strip() for line in f if line.strip()]
        
        for i, prompt in enumerate(prompts):
            image_path = images_dir / f"i2v_input_{i}.jpg"
            if image_path.exists():
                examples.append([str(image_path), prompt])
    
    return examples

# ============================================================
# Gradio ç•Œé¢
# ============================================================
def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # YouTube é¢‘é“ä¿¡æ¯
    youtube_html = """
    <div style="text-align: center; padding: 15px; background: linear-gradient(135deg, #ff0000 0%, #cc0000 100%); border-radius: 10px; margin-bottom: 20px;">
        <a href="https://www.youtube.com/@rongyikanshijie-ai" target="_blank" style="text-decoration: none; color: white;">
            <div style="display: flex; align-items: center; justify-content: center; gap: 10px;">
                <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24" fill="white">
                    <path d="M19.615 3.184c-3.604-.246-11.631-.245-15.23 0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23 0 3.897-.266 4.356-2.62 4.385-8.816-.029-6.185-.484-8.549-4.385-8.816zm-10.615 12.816v-8l8 3.993-8 4.007z"/>
                </svg>
                <span style="font-size: 18px; font-weight: bold;">AI æŠ€æœ¯åˆ†äº«é¢‘é“</span>
            </div>
            <p style="margin: 5px 0 0 0; font-size: 14px; opacity: 0.9;">ç‚¹å‡»è®¢é˜…ï¼Œè·å–æ›´å¤š AI æŠ€æœ¯æ•™ç¨‹</p>
        </a>
    </div>
    """
    
    with gr.Blocks(title="TurboDiffusion è§†é¢‘ç”Ÿæˆ", theme=gr.themes.Soft()) as demo:
        gr.HTML(youtube_html)
        gr.Markdown("# ğŸš€ TurboDiffusion è§†é¢‘ç”Ÿæˆ")
        gr.Markdown("åŸºäº TurboDiffusion çš„é«˜é€Ÿè§†é¢‘ç”Ÿæˆï¼Œæ”¯æŒæ–‡ç”Ÿè§†é¢‘å’Œå›¾ç”Ÿè§†é¢‘ä¸¤ç§æ¨¡å¼ã€‚")
        
        with gr.Tabs():
            # ========== T2V Tab ==========
            with gr.TabItem("ğŸ“ æ–‡ç”Ÿè§†é¢‘ (T2V)"):
                with gr.Row():
                    with gr.Column(scale=1):
                        t2v_prompt = gr.Textbox(
                            label="æç¤ºè¯",
                            placeholder="è¯·è¾“å…¥æè¿°è§†é¢‘å†…å®¹çš„æç¤ºè¯...",
                            lines=4
                        )
                        
                        with gr.Row():
                            t2v_resolution = gr.Dropdown(
                                choices=["480p", "720p"],
                                value="480p",
                                label="åˆ†è¾¨ç‡"
                            )
                            t2v_aspect_ratio = gr.Dropdown(
                                choices=["16:9", "9:16", "4:3", "3:4", "1:1"],
                                value="16:9",
                                label="å®½é«˜æ¯”"
                            )
                        
                        with gr.Row():
                            t2v_num_frames = gr.Slider(
                                minimum=17,
                                maximum=129,
                                value=81,
                                step=8,
                                label="å¸§æ•°"
                            )
                            t2v_num_steps = gr.Slider(
                                minimum=1,
                                maximum=4,
                                value=4,
                                step=1,
                                label="é‡‡æ ·æ­¥æ•°"
                            )
                        
                        with gr.Accordion("é«˜çº§é€‰é¡¹", open=False):
                            t2v_sigma_max = gr.Slider(
                                minimum=10,
                                maximum=200,
                                value=80,
                                step=10,
                                label="Sigma Max (åˆå§‹å™ªå£°å¼ºåº¦)"
                            )
                            t2v_sla_topk = gr.Slider(
                                minimum=0.05,
                                maximum=0.3,
                                value=0.1,
                                step=0.05,
                                label="SLA Top-K æ¯”ä¾‹"
                            )
                            t2v_seed = gr.Number(
                                value=0,
                                label="éšæœºç§å­",
                                precision=0
                            )
                        
                        t2v_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
                    
                    with gr.Column(scale=1):
                        t2v_output = gr.Video(label="ç”Ÿæˆç»“æœ")
                
                # T2V ç¤ºä¾‹
                t2v_examples = load_t2v_examples()
                if t2v_examples:
                    gr.Examples(
                        examples=t2v_examples,
                        inputs=[t2v_prompt],
                        label="ç¤ºä¾‹æç¤ºè¯"
                    )
                
                t2v_btn.click(
                    fn=generate_t2v,
                    inputs=[
                        t2v_prompt,
                        t2v_num_frames,
                        t2v_num_steps,
                        t2v_resolution,
                        t2v_aspect_ratio,
                        t2v_sigma_max,
                        t2v_sla_topk,
                        t2v_seed
                    ],
                    outputs=t2v_output
                )
            
            # ========== I2V Tab ==========
            with gr.TabItem("ğŸ–¼ï¸ å›¾ç”Ÿè§†é¢‘ (I2V)"):
                with gr.Row():
                    with gr.Column(scale=1):
                        i2v_image = gr.Image(
                            label="è¾“å…¥å›¾ç‰‡",
                            type="pil"
                        )
                        i2v_prompt = gr.Textbox(
                            label="æç¤ºè¯",
                            placeholder="è¯·è¾“å…¥æè¿°è§†é¢‘åŠ¨ä½œçš„æç¤ºè¯...",
                            lines=4
                        )
                        
                        with gr.Row():
                            i2v_resolution = gr.Dropdown(
                                choices=["480p", "720p"],
                                value="720p",
                                label="åˆ†è¾¨ç‡"
                            )
                            i2v_aspect_ratio = gr.Dropdown(
                                choices=["16:9", "9:16", "4:3", "3:4", "1:1"],
                                value="16:9",
                                label="å®½é«˜æ¯”"
                            )
                        
                        with gr.Row():
                            i2v_num_frames = gr.Slider(
                                minimum=17,
                                maximum=129,
                                value=81,
                                step=8,
                                label="å¸§æ•°"
                            )
                            i2v_num_steps = gr.Slider(
                                minimum=1,
                                maximum=4,
                                value=4,
                                step=1,
                                label="é‡‡æ ·æ­¥æ•°"
                            )
                        
                        with gr.Row():
                            i2v_adaptive_resolution = gr.Checkbox(
                                value=True,
                                label="è‡ªé€‚åº”åˆ†è¾¨ç‡"
                            )
                            i2v_use_ode = gr.Checkbox(
                                value=True,
                                label="ä½¿ç”¨ ODE é‡‡æ · (æ›´é”åˆ©)"
                            )
                        
                        with gr.Accordion("é«˜çº§é€‰é¡¹", open=False):
                            i2v_sigma_max = gr.Slider(
                                minimum=50,
                                maximum=400,
                                value=200,
                                step=25,
                                label="Sigma Max (åˆå§‹å™ªå£°å¼ºåº¦)"
                            )
                            i2v_boundary = gr.Slider(
                                minimum=0.5,
                                maximum=1.0,
                                value=0.9,
                                step=0.05,
                                label="æ¨¡å‹åˆ‡æ¢è¾¹ç•Œ"
                            )
                            i2v_sla_topk = gr.Slider(
                                minimum=0.05,
                                maximum=0.3,
                                value=0.1,
                                step=0.05,
                                label="SLA Top-K æ¯”ä¾‹"
                            )
                            i2v_seed = gr.Number(
                                value=0,
                                label="éšæœºç§å­",
                                precision=0
                            )
                        
                        i2v_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
                    
                    with gr.Column(scale=1):
                        i2v_output = gr.Video(label="ç”Ÿæˆç»“æœ")
                
                # I2V ç¤ºä¾‹
                i2v_examples = load_i2v_examples()
                if i2v_examples:
                    gr.Examples(
                        examples=i2v_examples,
                        inputs=[i2v_image, i2v_prompt],
                        label="ç¤ºä¾‹å›¾ç‰‡å’Œæç¤ºè¯"
                    )
                
                i2v_btn.click(
                    fn=generate_i2v,
                    inputs=[
                        i2v_image,
                        i2v_prompt,
                        i2v_num_frames,
                        i2v_num_steps,
                        i2v_resolution,
                        i2v_aspect_ratio,
                        i2v_adaptive_resolution,
                        i2v_use_ode,
                        i2v_sigma_max,
                        i2v_boundary,
                        i2v_sla_topk,
                        i2v_seed
                    ],
                    outputs=i2v_output
                )
        
        gr.Markdown("""
        ---
        ### ä½¿ç”¨è¯´æ˜
        - **æ–‡ç”Ÿè§†é¢‘**: è¾“å…¥æ–‡å­—æç¤ºè¯ï¼Œæ¨¡å‹å°†æ ¹æ®æè¿°ç”Ÿæˆå¯¹åº”çš„è§†é¢‘
        - **å›¾ç”Ÿè§†é¢‘**: ä¸Šä¼ ä¸€å¼ å›¾ç‰‡å¹¶è¾“å…¥æç¤ºè¯ï¼Œæ¨¡å‹å°†åŸºäºå›¾ç‰‡å†…å®¹ç”ŸæˆåŠ¨æ€è§†é¢‘
        - **é‡‡æ ·æ­¥æ•°**: æ­¥æ•°è¶Šå¤šè´¨é‡è¶Šå¥½ï¼Œä½†ç”Ÿæˆæ—¶é—´ä¹Ÿè¶Šé•¿ (æ¨è 4 æ­¥)
        - **SLA Top-K**: å€¼è¶Šå¤§ç”Ÿæˆè´¨é‡è¶Šå¥½ï¼Œä½†é€Ÿåº¦è¶Šæ…¢ (æ¨è 0.1-0.15)
        """)
    
    return demo

# ============================================================
# ä¸»å‡½æ•°
# ============================================================
if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    load_models()
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
