import os
import math
import cv2
import gradio as gr
import torch
import numpy as np
from PIL import Image, ImageDraw
from skimage import color, img_as_float32, img_as_ubyte
from diffusers import FluxPipeline
from Genfocus.pipeline.flux import Condition, generate, seed_everything

import depth_pro


# ============== æ¨¡å‹è·¯å¾„é…ç½® ==============
# ä½¿ç”¨æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹
BASE_PATH = os.path.dirname(os.path.abspath(__file__))
MODEL_ID = os.path.join(BASE_PATH, "checkpoints", "FLUX.1-dev")
GENFOCUS_MODEL_PATH = os.path.join(BASE_PATH, "checkpoints", "Genfocus-Model")
DEBLUR_LORA_PATH = GENFOCUS_MODEL_PATH
DEBLUR_WEIGHT_NAME = "deblurNet.safetensors"
BOKEH_LORA_DIR = GENFOCUS_MODEL_PATH
BOKEH_WEIGHT_NAME = "bokehNet.safetensors"

# Depth Pro æ¨¡å‹è·¯å¾„
DEPTH_PRO_CKPT_PATH = os.path.join(GENFOCUS_MODEL_PATH, "checkpoints", "depth_pro.pt")
DEPTH_PRO_DEFAULT_PATH = os.path.join(BASE_PATH, "checkpoints", "depth_pro.pt")

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not os.path.exists(os.path.join(DEBLUR_LORA_PATH, DEBLUR_WEIGHT_NAME)):
    print(f"âŒ è­¦å‘Š: æœªæ‰¾åˆ° {DEBLUR_WEIGHT_NAME}")
if not os.path.exists(os.path.join(BOKEH_LORA_DIR, BOKEH_WEIGHT_NAME)):
    print(f"âŒ è­¦å‘Š: æœªæ‰¾åˆ° {BOKEH_WEIGHT_NAME}")
if not os.path.exists(DEPTH_PRO_CKPT_PATH):
    print(f"âŒ è­¦å‘Š: æœªæ‰¾åˆ° Depth Pro æ¨¡å‹: {DEPTH_PRO_CKPT_PATH}")

# åˆ›å»º depth_pro.pt çš„ç¬¦å·é“¾æ¥åˆ°é»˜è®¤ä½ç½®
if os.path.exists(DEPTH_PRO_CKPT_PATH) and not os.path.exists(DEPTH_PRO_DEFAULT_PATH):
    try:
        os.symlink(DEPTH_PRO_CKPT_PATH, DEPTH_PRO_DEFAULT_PATH)
        print(f"âœ… å·²åˆ›å»º Depth Pro æ¨¡å‹ç¬¦å·é“¾æ¥: {DEPTH_PRO_DEFAULT_PATH}")
    except Exception as e:
        print(f"âš ï¸ åˆ›å»ºç¬¦å·é“¾æ¥å¤±è´¥: {e}")

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32

print(f"ğŸš€ æ£€æµ‹åˆ°è®¾å¤‡: {device}")


# ============== åŠ è½½ FLUX æ¨¡å‹ ==============
print("ğŸ”„ æ­£åœ¨åŠ è½½ FLUX æ¨¡å‹...")
pipe_flux = FluxPipeline.from_pretrained(MODEL_ID, torch_dtype=dtype)
current_adapter = None

if device == "cuda":
    print("ğŸš€ æ­£åœ¨å°† FLUX ç§»åŠ¨åˆ° CUDA...")
    pipe_flux.to("cuda")


# ============== åŠ è½½ Depth Pro æ¨¡å‹ ==============
print("ğŸ”„ æ­£åœ¨åŠ è½½ Depth Pro æ¨¡å‹...")
try:
    depth_model, depth_transform = depth_pro.create_model_and_transforms(
        device=torch.device(device)
    )
    depth_model.eval()
    print("âœ… Depth Pro åŠ è½½å®Œæˆã€‚")
except Exception as e:
    print(f"âŒ åŠ è½½ Depth Pro å¤±è´¥: {e}")
    depth_model = None
    depth_transform = None


def resize_and_pad_image(img: Image.Image, force_512: bool) -> Image.Image:
    """
    æ§åˆ¶å›¾åƒé¢„å¤„ç†:
    - å¦‚æœ force_512=True: å°†é•¿è¾¹è°ƒæ•´ä¸º 512ï¼Œç„¶åè£å‰ªåˆ°æœ€è¿‘çš„ 16 çš„å€æ•°ã€‚
    - å¦‚æœ force_512=False: å°†å®½é«˜ä¸Šé‡‡æ ·/è°ƒæ•´åˆ°æœ€è¿‘çš„ 16 çš„å€æ•°ä¸Šé™ã€‚
    """
    w, h = img.size

    if force_512:
        target_max = 512
        if w >= h:
            new_w = target_max
            scale = target_max / w
            new_h = int(h * scale)
        else:
            new_h = target_max
            scale = target_max / h
            new_w = int(w * scale)
        
        img = img.resize((new_w, new_h), Image.LANCZOS)
        
        final_w = (new_w // 16) * 16
        final_h = (new_h // 16) * 16
        
        left = (new_w - final_w) // 2
        top = (new_h - final_h) // 2
        right = left + final_w
        bottom = top + final_h
        
        return img.crop((left, top, right, bottom))
    
    else:
        final_w = ((w + 15) // 16) * 16
        final_h = ((h + 15) // 16) * 16
        
        if final_w == w and final_h == h:
            return img
        
        return img.resize((final_w, final_h), Image.LANCZOS)


def switch_lora(target_mode):
    """åˆ‡æ¢ LoRA é€‚é…å™¨"""
    global pipe_flux, current_adapter
    if current_adapter == target_mode:
        return
    print(f"ğŸ”„ æ­£åœ¨åˆ‡æ¢ LoRA åˆ° [{target_mode}]...")
    pipe_flux.unload_lora_weights()
    if target_mode == "deblur":
        try:
            pipe_flux.load_lora_weights(DEBLUR_LORA_PATH, weight_name=DEBLUR_WEIGHT_NAME, adapter_name="deblurring")
            pipe_flux.set_adapters(["deblurring"])
            current_adapter = "deblur"
        except Exception as e:
            print(f"âŒ åŠ è½½ Deblur LoRA å¤±è´¥: {e}")
    elif target_mode == "bokeh":
        try:
            pipe_flux.load_lora_weights(BOKEH_LORA_DIR, weight_name=BOKEH_WEIGHT_NAME, adapter_name="bokeh")
            pipe_flux.set_adapters(["bokeh"])
            current_adapter = "bokeh"
        except Exception as e:
            print(f"âŒ åŠ è½½ Bokeh LoRA å¤±è´¥: {e}")


def preprocess_input_image(raw_img, force_512):
    """é¢„å¤„ç†è¾“å…¥å›¾åƒ"""
    if raw_img is None:
        return None, None, None
    
    mode_str = "è°ƒæ•´æœ€é•¿è¾¹ä¸º 512" if force_512 else "åŸå§‹åˆ†è¾¨ç‡ (å¯¹é½åˆ° 16)"
    print(f"ğŸ”„ æ­£åœ¨é¢„å¤„ç†è¾“å…¥... æ¨¡å¼: {mode_str}")
    
    final_input = resize_and_pad_image(raw_img, force_512)
    
    return final_input, final_input, None


def draw_red_dot_on_preview(clean_img, evt: gr.SelectData):
    """åœ¨é¢„è§ˆå›¾ä¸Šç»˜åˆ¶çº¢ç‚¹æ ‡è®°ç„¦ç‚¹"""
    if clean_img is None:
        return None, None
    
    img_copy = clean_img.copy()
    draw = ImageDraw.Draw(img_copy)
    x, y = evt.index
    r = 8
    draw.ellipse((x-r, y-r, x+r, y+r), outline="red", width=2)
    draw.line((x-r, y, x+r, y), fill="red", width=2)
    draw.line((x, y-r, x, y+r), fill="red", width=2)
    
    return img_copy, evt.index


def run_genfocus_pipeline(clean_input_processed, click_coords, K_value, cached_latents):
    """è¿è¡Œ Genfocus æµæ°´çº¿"""
    if clean_input_processed is None:
        raise gr.Error("è¯·å…ˆå®Œæˆæ­¥éª¤ 1ï¼ˆä¸Šä¼ å›¾åƒï¼‰ã€‚")

    w, h = clean_input_processed.size
    print(f"ğŸš€ å¼€å§‹ Genfocus æµæ°´çº¿... (å°ºå¯¸: {w}x{h})")
    
    # ========== é˜¶æ®µ 1: å»æ¨¡ç³Š ==========
    print("   â–º è¿è¡Œé˜¶æ®µ 1: DeblurNet")
    switch_lora("deblur")
    
    condition_0_img = Image.new("RGB", (w, h), (0, 0, 0))
    cond0 = Condition(condition_0_img, "deblurring", [0, 32], 1.0)
    cond1 = Condition(clean_input_processed, "deblurring", [0, 0], 1.0)
    
    seed_everything(42)
    deblurred_img = generate(
        pipe_flux,
        height=h, width=w,
        prompt="a sharp photo with everything in focus",
        conditions=[cond0, cond1]
    ).images[0]
    
    if K_value == 0:
        print("âœ… K=0ï¼Œè¿”å›å»æ¨¡ç³Šç»“æœã€‚")
        return deblurred_img, cached_latents

    # ========== é˜¶æ®µ 2: æ•£æ™¯ç”Ÿæˆ ==========
    print(f"   â–º è¿è¡Œé˜¶æ®µ 2: BokehNet (K={K_value})")
    
    if click_coords is None:
        click_coords = [w // 2, h // 2]
        print("   âš ï¸ æœªé€‰æ‹©ç„¦ç‚¹ã€‚é»˜è®¤ä½¿ç”¨ä¸­å¿ƒç‚¹ã€‚")

    # æ·±åº¦ä¼°è®¡
    try:
        img_t = depth_transform(deblurred_img)
        if device == "cuda":
            img_t = img_t.to("cuda")
        with torch.no_grad():
            pred = depth_model.infer(img_t, f_px=None)
        depth_map = pred["depth"].cpu().numpy().squeeze()
        safe_depth = np.where(depth_map > 0.0, depth_map, np.finfo(np.float32).max)
        disp_orig = 1.0 / safe_depth
        disp = cv2.resize(disp_orig, (w, h), interpolation=cv2.INTER_LINEAR)
    except Exception as e:
        print(f"âŒ æ·±åº¦ä¼°è®¡é”™è¯¯: {e}")
        return deblurred_img, cached_latents

    # è®¡ç®—æ•£ç„¦å›¾
    tx, ty = click_coords
    tx = min(max(int(tx), 0), w - 1)
    ty = min(max(int(ty), 0), h - 1)
    
    disp_focus = float(disp[ty, tx])
    dmf = disp - np.float32(disp_focus)
    defocus_abs = np.abs(K_value * dmf)
    MAX_COC = 100.0
    defocus_t = torch.from_numpy(defocus_abs).unsqueeze(0).float()
    cond_map = (defocus_t / MAX_COC).clamp(0, 1).repeat(3, 1, 1).unsqueeze(0)

    # å‡†å¤‡æ½œå˜é‡
    if cached_latents is None:
        print("      æ­£åœ¨ç”Ÿæˆæ–°çš„å›ºå®šæ½œå˜é‡...")
        seed_everything(42)
        gen = torch.Generator(device=pipe_flux.device).manual_seed(1234)
        latents, _ = pipe_flux.prepare_latents(
            batch_size=1, num_channels_latents=16,
            height=h, width=w,
            dtype=pipe_flux.dtype, device=pipe_flux.device, generator=gen, latents=None
        )
        current_latents = latents
    else:
        print("      ä½¿ç”¨ç¼“å­˜çš„æ½œå˜é‡...")
        current_latents = cached_latents

    # è¿è¡Œæ•£æ™¯ç”Ÿæˆ
    switch_lora("bokeh")
    cond_img = Condition(deblurred_img, "bokeh")
    cond_dmf = Condition(cond_map, "bokeh", [0, 0], 1.0, No_preprocess=True)
    
    seed_everything(42)
    gen = torch.Generator(device=pipe_flux.device).manual_seed(1234)
    
    with torch.no_grad():
        res = generate(
            pipe_flux,
            height=h, width=w,
            prompt="an excellent photo with a large aperture",
            conditions=[cond_img, cond_dmf],
            guidance_scale=1.0, kv_cache=False, generator=gen,
            latents=current_latents,
        )
    generated_bokeh = res.images[0]
    return generated_bokeh, current_latents


# ============== CSS æ ·å¼ ==============
css = """
#col-container { margin: 0 auto; max-width: 1400px; }
"""

# ============== ç¤ºä¾‹å›¾åƒåŠ è½½ ==============
example_dir = os.path.join(BASE_PATH, "example")

valid_examples = []
allowed_extensions = {".jpg", ".jpeg", ".png", ".bmp", ".webp"}

if os.path.exists(example_dir):
    files = sorted(os.listdir(example_dir))
    for filename in files:
        ext = os.path.splitext(filename)[1].lower()
        if ext in allowed_extensions:
            full_path = os.path.join(example_dir, filename)
            valid_examples.append([full_path])
    print(f"âœ… ä» '{example_dir}' åŠ è½½äº† {len(valid_examples)} ä¸ªç¤ºä¾‹")
else:
    print(f"âš ï¸ è­¦å‘Š: ç¤ºä¾‹ç›®å½• '{example_dir}' æœªæ‰¾åˆ°ã€‚")


# ============== Gradio ç•Œé¢ ==============
with gr.Blocks(css=css) as demo:
    # çŠ¶æ€å˜é‡
    clean_processed_state = gr.State(value=None)
    click_coords_state = gr.State(value=None)
    latents_state = gr.State(value=None)
    
    with gr.Column(elem_id="col-container"):
        gr.Markdown("# ğŸ“· Genfocus æµæ°´çº¿ï¼šäº¤äº’å¼é‡å¯¹ç„¦")
        
        gr.Markdown("""
        ### ğŸ“– ä½¿ç”¨æŒ‡å—
        **ç”Ÿæˆå¼é‡å¯¹ç„¦** æ”¯æŒä¸¤ç§ä¸»è¦åº”ç”¨ï¼š
        
        * **å…¨ç„¦ä¼°è®¡ (AIF)ï¼š** è®¾ç½® **K = 0**ã€‚æ¨¡å‹å°†ä»æ¨¡ç³Šè¾“å…¥ä¸­æ¢å¤å…¨ç„¦å›¾åƒã€‚
          
        * **é‡å¯¹ç„¦ï¼š** 
          1. åœ¨**æ­¥éª¤ 2** çš„å›¾åƒé¢„è§ˆä¸­**ç‚¹å‡»**æ‚¨æƒ³è¦èšç„¦çš„ä¸»ä½“ã€‚
          2. å¢åŠ  **K**ï¼ˆæ¨¡ç³Šå¼ºåº¦ï¼‰ä»¥æ ¹æ®åœºæ™¯æ·±åº¦ç”Ÿæˆé€¼çœŸçš„æ•£æ™¯æ•ˆæœã€‚
        
        > âš ï¸ **é¢„å¤„ç†è¯´æ˜ï¼š**
        > - **å‹¾é€‰ï¼ˆé»˜è®¤å…³é—­ï¼‰ï¼š** å°†é•¿è¾¹è°ƒæ•´ä¸º 512 åƒç´ ï¼Œå¹¶è£å‰ªåˆ°æœ€è¿‘çš„ 16 çš„å€æ•°ã€‚
        > - **ä¸å‹¾é€‰ï¼š** å°†é«˜åº¦å’Œå®½åº¦ä¸Šé‡‡æ ·åˆ°æœ€è¿‘çš„ 16 çš„å€æ•°ï¼ˆä¿ç•™åŸå§‹åˆ†è¾¨ç‡ï¼‰ï¼Œå¹¶ä½¿ç”¨åˆ†å—æ¨ç†ã€‚
        """)
        
        with gr.Row():
            # æ­¥éª¤ 1: ä¸Šä¼ å›¾åƒ
            with gr.Column(scale=1):
                gr.Markdown("### æ­¥éª¤ 1ï¼šä¸Šä¼ å›¾åƒ")
                gr.Markdown("ç‚¹å‡»ç¤ºä¾‹æˆ–ä¸Šä¼ æ‚¨è‡ªå·±çš„å›¾åƒã€‚")
                
                input_raw = gr.Image(label="åŸå§‹è¾“å…¥å›¾åƒ", type="pil")
                
                resize_512_check = gr.Checkbox(label="å°†é•¿è¾¹è°ƒæ•´ä¸º 512ï¼Ÿ", value=False)
                
                if valid_examples:
                    gr.Examples(examples=valid_examples, inputs=input_raw, label="ç¤ºä¾‹ï¼ˆç‚¹å‡»åŠ è½½ï¼‰", cache_examples=False)

            # æ­¥éª¤ 2: è®¾ç½®ç„¦ç‚¹å’Œ K å€¼
            with gr.Column(scale=1):
                gr.Markdown("### æ­¥éª¤ 2ï¼šè®¾ç½®ç„¦ç‚¹å’Œ K å€¼")
                gr.Markdown("ä¸‹å›¾æ˜¾ç¤ºæ¨¡å‹çš„å®é™…è¾“å…¥ã€‚**ç‚¹å‡»å›¾åƒ**è®¾ç½®ç„¦ç‚¹ã€‚")
                
                focus_preview_img = gr.Image(label="æ¨¡å‹è¾“å…¥ï¼ˆå·²å¤„ç†ï¼‰- ç‚¹å‡»æ­¤å¤„", type="pil", interactive=False)
                
                with gr.Row():
                    click_status = gr.Textbox(label="é€‰å®šåæ ‡", value="ä¸­å¿ƒï¼ˆé»˜è®¤ï¼‰", interactive=False, scale=1)
                    k_slider = gr.Slider(minimum=0, maximum=50, value=20, step=1, label="æ¨¡ç³Šå¼ºåº¦ (K)", scale=2)
                
                run_btn = gr.Button("âœ¨ è¿è¡Œ Genfocus", variant="primary", scale=1)

        # ç»“æœæ˜¾ç¤º
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ç»“æœ")
                output_img = gr.Image(label="æœ€ç»ˆè¾“å‡º", type="pil", interactive=False, elem_id="output_image")

        # äº‹ä»¶ç»‘å®š
        input_raw.change(
            fn=preprocess_input_image,
            inputs=[input_raw, resize_512_check],
            outputs=[focus_preview_img, clean_processed_state, latents_state]
        )
        input_raw.upload(
            fn=preprocess_input_image,
            inputs=[input_raw, resize_512_check],
            outputs=[focus_preview_img, clean_processed_state, latents_state]
        )
        
        resize_512_check.change(
            fn=preprocess_input_image,
            inputs=[input_raw, resize_512_check],
            outputs=[focus_preview_img, clean_processed_state, latents_state]
        )

        focus_preview_img.select(
            fn=draw_red_dot_on_preview,
            inputs=[clean_processed_state],
            outputs=[focus_preview_img, click_coords_state]
        ).then(
            fn=lambda x: f"x={x[0]}, y={x[1]}",
            inputs=[click_coords_state],
            outputs=[click_status]
        )

        run_btn.click(
            fn=run_genfocus_pipeline,
            inputs=[clean_processed_state, click_coords_state, k_slider, latents_state],
            outputs=[output_img, latents_state]
        )


if __name__ == "__main__":
    allowed_dir = os.path.join(BASE_PATH, "example")
    allowed_paths = [allowed_dir]
    demo.launch(server_name="0.0.0.0", share=True, allowed_paths=allowed_paths)
