import os
import uuid
import argparse

import gradio as gr
import torch
from diffusers.utils import load_image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration

from dreamomni2.pipeline_dreamomni2 import DreamOmni2Pipeline
from utils.vprocess import process_vision_info, resizeinput


def parse_args():
    parser = argparse.ArgumentParser(description="å¯åŠ¨ DreamOmni2 Gradio åº”ç”¨ç¨‹åºã€‚")
    parser.add_argument(
        "--vlm_path",
        type=str,
        default="checkpoints/DreamOmni2/vlm-model",
        help="Qwen2_5_VL å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç›®å½•è·¯å¾„ã€‚",
    )
    parser.add_argument(
        "--edit_lora_path",
        type=str,
        default="checkpoints/DreamOmni2/edit_lora",
        help="FLUX.1-Kontext ç¼–è¾‘ LoRA æƒé‡ç›®å½•è·¯å¾„ã€‚",
    )
    parser.add_argument(
        "--gen_lora_path",
        type=str,
        default="checkpoints/DreamOmni2/gen_lora",
        help="FLUX.1-Kontext ç”Ÿæˆ LoRA æƒé‡ç›®å½•è·¯å¾„ã€‚",
    )
    parser.add_argument(
        "--server_name",
        type=str,
        default="0.0.0.0",
        help="ç”¨äºæ‰˜ç®¡ Gradio Demo çš„æœåŠ¡å™¨ IPã€‚",
    )
    parser.add_argument(
        "--server_port",
        type=int,
        default=7860,
        help="ç”¨äºæ‰˜ç®¡ Gradio Demo çš„ç«¯å£å·ã€‚",
    )
    return parser.parse_args()


ARGS = parse_args()
vlm_path = ARGS.vlm_path
edit_lora_path = ARGS.edit_lora_path
gen_lora_path = ARGS.gen_lora_path
server_name = ARGS.server_name
server_port = ARGS.server_port
device = "cuda"


def extract_gen_content(text: str) -> str:
    return text[6:-7]


print(
    f"æ­£åœ¨åŠ è½½æ¨¡å‹ï¼švlm_path={vlm_path}, edit_lora_path={edit_lora_path}, gen_lora_path={gen_lora_path}"
)

pipe = DreamOmni2Pipeline.from_pretrained(
    "checkpoints/FLUX.1-Kontext-dev",
    torch_dtype=torch.bfloat16,
)
pipe.to(device)

AVAILABLE_ADAPTERS = set()

if edit_lora_path and os.path.exists(edit_lora_path):
    pipe.load_lora_weights(edit_lora_path, adapter_name="edit")
    AVAILABLE_ADAPTERS.add("edit")
else:
    print(f"è­¦å‘Šï¼šæœªåœ¨ {edit_lora_path} æ‰¾åˆ°ç¼–è¾‘ LoRA è·¯å¾„")

if gen_lora_path and os.path.exists(gen_lora_path):
    pipe.load_lora_weights(gen_lora_path, adapter_name="generation")
    AVAILABLE_ADAPTERS.add("generation")
else:
    print(f"è­¦å‘Šï¼šæœªåœ¨ {gen_lora_path} æ‰¾åˆ°ç”Ÿæˆ LoRA è·¯å¾„")

vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    vlm_path,
    torch_dtype="bfloat16",
    device_map="cuda",
)
processor = AutoProcessor.from_pretrained(vlm_path)


def infer_vlm(input_img_path, input_instruction, prefix):
    if not vlm_model or not processor:
        raise gr.Error("æœªåŠ è½½ VLM æ¨¡å‹ï¼Œæ— æ³•å¤„ç†æŒ‡ä»¤ã€‚")
    tp = []
    for path in input_img_path:
        tp.append({"type": "image", "image": path})
    tp.append({"type": "text", "text": input_instruction + prefix})
    messages = [{"role": "user", "content": tp}]

    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to("cuda")

    generated_ids = vlm_model.generate(**inputs, do_sample=False, max_new_tokens=4096)
    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False,
    )
    return output_text[0]


PREFERRED_KONTEXT_RESOLUTIONS = [
    (672, 1568),
    (688, 1504),
    (720, 1456),
    (752, 1392),
    (800, 1328),
    (832, 1248),
    (880, 1184),
    (944, 1104),
    (1024, 1024),
    (1104, 944),
    (1184, 880),
    (1248, 832),
    (1328, 800),
    (1392, 752),
    (1456, 720),
    (1504, 688),
    (1568, 672),
]


def find_closest_resolution(width, height, preferred_resolutions):
    input_ratio = width / height
    closest_resolution = min(
        preferred_resolutions,
        key=lambda res: abs((res[0] / res[1]) - input_ratio),
    )
    return closest_resolution


def perform_edit(input_img_paths, input_instruction, output_path):
    if "edit" not in AVAILABLE_ADAPTERS:
        raise gr.Error("æœªåŠ è½½ç¼–è¾‘é€‚é…å™¨ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°è­¦å‘Šã€‚")
    pipe.set_adapters(["edit"], adapter_weights=[1])  # åˆ‡æ¢åˆ°ç¼–è¾‘é€‚é…å™¨
    prefix = " It is editing task."
    source_imgs = []
    for path in input_img_paths:
        img = load_image(path)
        source_imgs.append(resizeinput(img))
    prompt = infer_vlm(input_img_paths, input_instruction, prefix)
    prompt = extract_gen_content(prompt)
    print(f"ç¼–è¾‘æ¨¡å¼ç”Ÿæˆæç¤ºï¼š{prompt}")

    image = pipe(
        images=source_imgs,
        height=source_imgs[0].height,
        width=source_imgs[0].width,
        prompt=prompt,
        num_inference_steps=30,
        guidance_scale=3.5,
    ).images[0]
    image.save(output_path)
    print(f"ç¼–è¾‘ç»“æœå·²ä¿å­˜è‡³ {output_path}")


def perform_generation(input_img_paths, input_instruction, output_path, height=1024, width=1024):
    if "generation" not in AVAILABLE_ADAPTERS:
        raise gr.Error("æœªåŠ è½½ç”Ÿæˆé€‚é…å™¨ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°è­¦å‘Šã€‚")
    pipe.set_adapters(["generation"], adapter_weights=[1])  # åˆ‡æ¢åˆ°ç”Ÿæˆé€‚é…å™¨
    prefix = " It is generation task."
    source_imgs = []
    for path in input_img_paths:
        img = load_image(path)
        source_imgs.append(resizeinput(img))
    prompt = infer_vlm(input_img_paths, input_instruction, prefix)
    prompt = extract_gen_content(prompt)
    print(f"ç”Ÿæˆæ¨¡å¼ç”Ÿæˆæç¤ºï¼š{prompt}")

    image = pipe(
        images=source_imgs,
        height=height,
        width=width,
        prompt=prompt,
        num_inference_steps=30,
        guidance_scale=3.5,
    ).images[0]

    image.save(output_path)
    print(f"ç”Ÿæˆç»“æœå·²ä¿å­˜è‡³ {output_path}")


def process_edit_request(image_file_1, image_file_2, instruction):
    if not image_file_1 or not image_file_2:
        raise gr.Error("è¯·ä¸Šä¼ ä¸¤å¼ å›¾åƒã€‚")
    if not instruction:
        raise gr.Error("è¯·è¾“å…¥æŒ‡ä»¤ã€‚")
    output_path = f"/tmp/{uuid.uuid4()}.png"
    input_img_paths = [image_file_1, image_file_2]
    perform_edit(input_img_paths, instruction, output_path)
    return output_path


def process_generation_request(image_file_1, image_file_2, instruction):
    if not image_file_1 or not image_file_2:
        raise gr.Error("è¯·ä¸Šä¼ ä¸¤å¼ å›¾åƒã€‚")
    if not instruction:
        raise gr.Error("è¯·è¾“å…¥æŒ‡ä»¤ã€‚")
    output_path = f"/tmp/{uuid.uuid4()}.png"
    input_img_paths = [image_file_1, image_file_2]
    perform_generation(input_img_paths, instruction, output_path)
    return output_path


css = """
.text-center { text-align: center; }
.result-img img {
    max-height: 60vh !important;
    min-height: 30vh !important;
    width: auto !important;
    object-fit: contain;
}
.input-img img {
    max-height: 30vh !important;
    width: auto !important;
    object-fit: contain;
}
"""


with gr.Blocks(theme=gr.themes.Soft(), title="DreamOmni2", css=css) as demo:
    gr.HTML(
        """
        <h1 style="text-align:center; font-size:48px; font-weight:bold; margin-bottom:20px;">
            DreamOmni2ï¼šå…¨èƒ½å›¾åƒç”Ÿæˆä¸ç¼–è¾‘
        </h1>
        """
    )
    gr.Markdown(
        "ä¸Šä¼ å‚è€ƒå›¾åƒï¼Œè¾“å…¥æŒ‡ä»¤ï¼Œç„¶åè¿è¡Œæ‰€éœ€æµç¨‹ã€‚",
        elem_classes="text-center",
    )

    with gr.Tabs():
        with gr.Tab("ç¼–è¾‘"):
            with gr.Row():
                with gr.Column(scale=2):
                    gr.Markdown("â¬†ï¸ ä¸Šä¼ å›¾åƒï¼Œå¯ç‚¹å‡»æˆ–æ‹–æ‹½ã€‚")

                    with gr.Row():
                        edit_image_uploader_1 = gr.Image(
                            label="å›¾åƒ 1",
                            type="filepath",
                            interactive=True,
                            elem_classes="input-img",
                        )
                        edit_image_uploader_2 = gr.Image(
                            label="å›¾åƒ 2",
                            type="filepath",
                            interactive=True,
                            elem_classes="input-img",
                        )

                    edit_instruction_text = gr.Textbox(
                        label="æŒ‡ä»¤",
                        lines=2,
                        placeholder="è¯·æè¿°å¦‚ä½•åˆ©ç”¨å‚è€ƒå›¾åƒç¼–è¾‘ç¬¬ä¸€å¼ å›¾åƒ...",
                    )
                    edit_run_button = gr.Button("å¼€å§‹ç¼–è¾‘", variant="primary")

                with gr.Column(scale=2):
                    gr.Markdown(
                        "âœï¸ **ç¼–è¾‘æ¨¡å¼**ï¼šæ ¹æ®æŒ‡ä»¤ä¸å‚è€ƒå›¾åƒä¿®æ”¹ç°æœ‰å›¾åƒã€‚"
                        " æç¤ºï¼šè‹¥ç»“æœä¸ç¬¦åˆé¢„æœŸï¼Œå¯å†æ¬¡ç‚¹å‡» **å¼€å§‹ç¼–è¾‘**ã€‚",
                    )
                    edit_output_image = gr.Image(
                        label="ç»“æœ",
                        type="filepath",
                        elem_classes="result-img",
                    )

            gr.Markdown("## ç¼–è¾‘ç¤ºä¾‹")
            gr.Examples(
                label="ç¼–è¾‘ç¤ºä¾‹",
                examples=[
                    [
                        "example_input/edit_tests/4/ref_0.jpg",
                        "example_input/edit_tests/4/ref_1.jpg",
                        "è®©ç¬¬ä¸€å¼ å›¾åƒæ‹¥æœ‰ä¸ç¬¬äºŒå¼ å›¾åƒç›¸åŒçš„é£æ ¼ã€‚",
                        "example_input/edit_tests/4/res.jpg",
                    ],
                    [
                        "example_input/edit_tests/5/ref_0.jpg",
                        "example_input/edit_tests/5/ref_1.jpg",
                        "è®©ç¬¬ä¸€å¼ å›¾ä¸­çš„äººç‰©æ‹¥æœ‰ç¬¬äºŒå¼ å›¾äººç‰©çš„å‘å‹ã€‚",
                        "example_input/edit_tests/5/res.jpg",
                    ],
                    [
                        "example_input/edit_tests/src.jpg",
                        "example_input/edit_tests/ref.jpg",
                        "è®©ç¬¬äºŒå¼ å›¾ä¸­çš„å¥³æ€§ç«™åœ¨ç¬¬ä¸€å¼ å›¾çš„é“è·¯ä¸Šã€‚",
                        "example_input/edit_tests/edi_res.png",
                    ],
                    [
                        "example_input/edit_tests/1/ref_0.jpg",
                        "example_input/edit_tests/1/ref_1.jpg",
                        "å°†ç¬¬ä¸€å¼ å›¾ä¸­çš„ç¯ç¬¼æ›¿æ¢ä¸ºç¬¬äºŒå¼ å›¾ä¸­çš„ç‹—ã€‚",
                        "example_input/edit_tests/1/res.jpg",
                    ],
                    [
                        "example_input/edit_tests/2/ref_0.jpg",
                        "example_input/edit_tests/2/ref_1.jpg",
                        "è®©ç¬¬ä¸€å¼ å›¾ä¸­çš„è¥¿è£…å˜ä¸ºç¬¬äºŒå¼ å›¾çš„è¡£æœã€‚",
                        "example_input/edit_tests/2/res.jpg",
                    ],
                    [
                        "example_input/edit_tests/3/ref_0.jpg",
                        "example_input/edit_tests/3/ref_1.jpg",
                        "è®©ç¬¬ä¸€å¼ å›¾æ‹¥æœ‰ä¸ç¬¬äºŒå¼ å›¾ç›¸åŒçš„å…‰ç…§ã€‚",
                        "example_input/edit_tests/3/res.jpg",
                    ],
                    [
                        "example_input/edit_tests/6/ref_0.jpg",
                        "example_input/edit_tests/6/ref_1.jpg",
                        "è®©ç¬¬ä¸€å¼ å›¾çš„æ–‡å­—é‡‡ç”¨ç¬¬äºŒå¼ å›¾çš„å­—ä½“ã€‚",
                        "example_input/edit_tests/6/res.jpg",
                    ],
                    [
                        "example_input/edit_tests/7/ref_0.jpg",
                        "example_input/edit_tests/7/ref_1.jpg",
                        "è®©ç¬¬ä¸€å¼ å›¾çš„æ±½è½¦æ‹¥æœ‰ç¬¬äºŒå¼ å›¾è€é¼ çš„èŠ±çº¹ã€‚",
                        "example_input/edit_tests/7/res.jpg",
                    ],
                    [
                        "example_input/edit_tests/8/ref_0.jpg",
                        "example_input/edit_tests/8/ref_1.jpg",
                        "è®©ç¬¬ä¸€å¼ å›¾çš„è£™å­å…·æœ‰ç¬¬äºŒå¼ å›¾çš„å›¾æ¡ˆã€‚",
                        "example_input/edit_tests/8/res.jpg",
                    ],
                ],
                inputs=[
                    edit_image_uploader_1,
                    edit_image_uploader_2,
                    edit_instruction_text,
                    edit_output_image,
                ],
                cache_examples=False,
            )

            edit_run_button.click(
                fn=process_edit_request,
                inputs=[edit_image_uploader_1, edit_image_uploader_2, edit_instruction_text],
                outputs=edit_output_image,
            )

        with gr.Tab("ç”Ÿæˆ"):
            with gr.Row():
                with gr.Column(scale=2):
                    gr.Markdown("â¬†ï¸ ä¸Šä¼ å›¾åƒï¼Œå¯ç‚¹å‡»æˆ–æ‹–æ‹½ã€‚")

                    with gr.Row():
                        gen_image_uploader_1 = gr.Image(
                            label="å›¾åƒ 1",
                            type="filepath",
                            interactive=True,
                            elem_classes="input-img",
                        )
                        gen_image_uploader_2 = gr.Image(
                            label="å›¾åƒ 2",
                            type="filepath",
                            interactive=True,
                            elem_classes="input-img",
                        )

                    gen_instruction_text = gr.Textbox(
                        label="æŒ‡ä»¤",
                        lines=2,
                        placeholder="è¯·æè¿°å¸Œæœ›åŸºäºå‚è€ƒå›¾åƒç”Ÿæˆçš„å†…å®¹...",
                    )
                    gen_run_button = gr.Button("å¼€å§‹ç”Ÿæˆ", variant="primary")

                with gr.Column(scale=2):
                    gr.Markdown(
                        "ğŸ–¼ï¸ **ç”Ÿæˆæ¨¡å¼**ï¼šæ ¹æ®å‚è€ƒå›¾åƒåˆ›å»ºå…¨æ–°åœºæ™¯ã€‚"
                        " æç¤ºï¼šè‹¥ç»“æœä¸ç¬¦åˆé¢„æœŸï¼Œå¯å†æ¬¡ç‚¹å‡» **å¼€å§‹ç”Ÿæˆ**ã€‚",
                    )
                    gen_output_image = gr.Image(
                        label="ç»“æœ",
                        type="filepath",
                        elem_classes="result-img",
                    )

            gr.Markdown("## ç”Ÿæˆç¤ºä¾‹")
            gr.Examples(
                label="ç”Ÿæˆç¤ºä¾‹",
                examples=[
                    [
                        "example_input/gen_tests/img1.jpg",
                        "example_input/gen_tests/img2.jpg",
                        "åœºæ™¯ä¸­ï¼Œç¬¬ä¸€å¼ å›¾çš„è§’è‰²ç«™åœ¨å·¦ä¾§ï¼Œç¬¬äºŒå¼ å›¾çš„è§’è‰²ç«™åœ¨å³ä¾§ï¼Œä»–ä»¬åœ¨é£èˆ¹å†…éƒ¨èƒŒæ™¯å‰æ¡æ‰‹ã€‚",
                        "example_input/gen_tests/gen_res.png",
                    ]
                ],
                inputs=[
                    gen_image_uploader_1,
                    gen_image_uploader_2,
                    gen_instruction_text,
                    gen_output_image,
                ],
                cache_examples=False,
            )

            gen_run_button.click(
                fn=process_generation_request,
                inputs=[gen_image_uploader_1, gen_image_uploader_2, gen_instruction_text],
                outputs=gen_output_image,
            )


if __name__ == "__main__":
    print("æ­£åœ¨å¯åŠ¨ Gradio Demo...")
    demo.launch(server_name=server_name, server_port=server_port)
