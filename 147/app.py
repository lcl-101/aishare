import gradio as gr
import os
import torch
import tempfile
import shutil
import subprocess
from diffsynth import ModelManager, WanVideoPipeline, save_video, load_state_dict
from PIL import Image
from peft import LoraConfig, inject_adapter_in_model
import numpy as np
from diffsynth.models.camera import CamVidEncoder
import cv2
import random
import math
import torch.nn.functional as F
from tqdm import tqdm
from imageio.v3 import imread, imwrite
import sys

# Add utility imports
sys.path.append("DepthCrafter")

try:
    from utils.nv_utils import *
    from utils.dc_utils import read_images_frames, read_video_frames
    from utils.render_utils import get_rays_from_pose
    from utils.depth_utils import DepthCrafterDemo
    UTILS_AVAILABLE = True
except ImportError as e:
    print(f"å¯¼å…¥å·¥å…·æ¨¡å—å¤±è´¥: {e}")
    UTILS_AVAILABLE = False

try:
    import trimesh
    TRIMESH_AVAILABLE = True
except ImportError:
    print("trimesh æœªå®‰è£…")
    TRIMESH_AVAILABLE = False


def load_video_frames(video_path, num_frames, height, width):
    """Load and process video frames"""
    cap = cv2.VideoCapture(video_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if frame_count < num_frames:
        print(f"Warning: Video has only {frame_count} frames, but {num_frames} required")
    
    # Get frame indices
    if frame_count >= num_frames:
        indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)
    else:
        indices = np.arange(frame_count)
    
    frames = []
    for i in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if ret:
            frame = cv2.resize(frame, (width, height))
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
    
    cap.release()
    
    # Pad or trim frames to match num_frames
    if len(frames) < num_frames:
        frames += [frames[-1]] * (num_frames - len(frames))
    else:
        frames = frames[:num_frames]
    
    frames = np.array(frames)
    video_tensor = torch.from_numpy(frames).permute(3, 0, 1, 2).float() / 255.0
    return video_tensor

def load_mask_frames(mask_path, num_frames, height, width):
    """Load and process mask frames"""
    cap = cv2.VideoCapture(mask_path)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if frame_count < num_frames:
        print(f"Warning: Mask video has only {frame_count} frames, but {num_frames} required")
    
    # Get frame indices
    if frame_count >= num_frames:
        indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)
    else:
        indices = np.arange(frame_count)
    
    frames = []
    for i in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if ret:
            frame = cv2.resize(frame, (width, height))
            # Apply random erosion like in original code
            import random
            kernel = random.randint(2, 8)
            frame = cv2.erode(frame, np.ones((kernel, kernel), np.uint8), iterations=1)
            frames.append(frame)
    
    cap.release()
    
    # Pad or trim frames to match num_frames
    if len(frames) < num_frames:
        frames += [frames[-1]] * (num_frames - len(frames))
    else:
        frames = frames[:num_frames]
    
    frames = np.array(frames)
    mask_tensor = torch.from_numpy(frames).permute(3, 0, 1, 2).float()
    mask_tensor = (mask_tensor / 255.0 > 0.5).float()  # Binarize mask
    return mask_tensor


def generate_mask_from_video(video_path, num_frames, height, width, mask_type="motion"):
    """Generate mask from input video automatically"""
    try:
        cap = cv2.VideoCapture(video_path)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if frame_count < num_frames:
            print(f"è­¦å‘Š: è§†é¢‘åªæœ‰ {frame_count} å¸§ï¼Œä½†éœ€è¦ {num_frames} å¸§")
        
        # Get frame indices
        if frame_count >= num_frames:
            indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)
        else:
            indices = np.arange(frame_count)
        
        frames = []
        for i in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if ret:
                frame = cv2.resize(frame, (width, height))
                frames.append(frame)
        
        cap.release()
        
        if len(frames) < 2:
            # If only one frame, create a partial mask (not full white)
            mask_frame = np.ones((height, width, 3), dtype=np.uint8) * 128  # Gray mask instead of white
            masks = [mask_frame] * num_frames
        else:
            # Generate motion-based mask with improved algorithm
            masks = []
            prev_frame = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
            
            for i, frame in enumerate(frames):
                curr_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                if i == 0:
                    # First frame - create partial mask
                    mask = np.ones((height, width), dtype=np.uint8) * 128
                else:
                    # Calculate frame difference with better threshold
                    diff = cv2.absdiff(prev_frame, curr_frame)
                    _, mask = cv2.threshold(diff, 20, 255, cv2.THRESH_BINARY)  # Lower threshold
                    
                    # Morphological operations to clean up the mask
                    kernel = np.ones((3, 3), np.uint8)  # Smaller kernel
                    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
                    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
                    
                    # Dilate to make mask areas larger
                    mask = cv2.dilate(mask, kernel, iterations=2)
                    
                    # Ensure mask has reasonable content but not too much
                    mask_ratio = cv2.countNonZero(mask) / (height * width)
                    if mask_ratio < 0.02:  # Less than 2% pixels
                        # Create a center mask if motion is too small
                        center_h, center_w = height // 2, width // 2
                        mask = np.zeros((height, width), dtype=np.uint8)
                        mask[center_h-height//4:center_h+height//4, 
                             center_w-width//4:center_w+width//4] = 255
                    elif mask_ratio > 0.8:  # More than 80% pixels
                        # Reduce mask if too large
                        mask = cv2.erode(mask, kernel, iterations=3)
                
                # Convert to 3-channel mask
                mask_3ch = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
                masks.append(mask_3ch)
                prev_frame = curr_frame
        
        # Pad or trim masks to match num_frames
        if len(masks) < num_frames:
            masks += [masks[-1]] * (num_frames - len(masks))
        else:
            masks = masks[:num_frames]
        
        masks = np.array(masks)
        mask_tensor = torch.from_numpy(masks).permute(3, 0, 1, 2).float()
        mask_tensor = (mask_tensor / 255.0 > 0.5).float()  # Binarize mask
        
        # Debug: Print mask statistics
        mask_sum = mask_tensor.sum().item()
        total_pixels = mask_tensor.numel()
        mask_ratio = mask_sum / total_pixels
        print(f"è’™ç‰ˆç»Ÿè®¡: {mask_ratio:.2%} çš„åƒç´ è¢«æ¿€æ´»")
        
        return mask_tensor
        
    except Exception as e:
        print(f"ç”Ÿæˆè’™ç‰ˆæ—¶å‡ºé”™: {e}")
        # Fallback: create a center mask instead of full white
        mask = np.zeros((3, num_frames, height, width), dtype=np.float32)
        center_h, center_w = height // 2, width // 2
        mask[:, :, center_h-height//4:center_h+height//4, center_w-width//4:center_w+width//4] = 1.0
        return torch.from_numpy(mask)


def add_lora_to_model(model, lora_rank=16, lora_alpha=16.0, lora_target_modules="q,k,v,o,ffn.0,ffn.2", 
                     init_lora_weights="kaiming", pretrained_path=None, state_dict_converter=None):
    """Add LoRA to model"""
    if init_lora_weights == "kaiming":
        init_lora_weights = True
        
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        init_lora_weights=init_lora_weights,
        target_modules=lora_target_modules.split(","),
    )
    model = inject_adapter_in_model(lora_config, model)
    
    for param in model.parameters():
        if param.requires_grad:
            param.data = param.to(torch.float32)
    
    if pretrained_path is not None:
        state_dict = load_state_dict(pretrained_path)
        if state_dict_converter is not None:
            state_dict = state_dict_converter(state_dict)
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        all_keys = [i for i, _ in model.named_parameters()]
        num_updated_keys = len(all_keys) - len(missing_keys)
        num_unexpected_keys = len(unexpected_keys)
        print(f"LORA: {num_updated_keys} parameters loaded from {pretrained_path}. {num_unexpected_keys} unexpected.")


class EX4DPipeline:
    def __init__(self):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.pipe = None
        self.model_loaded = False
        self.depth_estimator = None
        self.glctx = None
        
        # Default model paths
        self.ex4d_path = 'models/EX-4D/ex4d.ckpt'
        self.text_encoder_path = 'models/Wan-AI/Wan2.1-I2V-14B-480P/models_t5_umt5-xxl-enc-bf16.pth'
        self.vae_path = 'models/Wan-AI/Wan2.1-I2V-14B-480P/Wan2.1_VAE.pth'
        self.clip_path = 'models/Wan-AI/Wan2.1-I2V-14B-480P/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth'
        self.dit_dir = 'models/Wan-AI/Wan2.1-I2V-14B-480P/'
        
    def load_models(self):
        """Load all models"""
        if self.model_loaded:
            return "æ¨¡å‹å·²åŠ è½½"
            
        try:
            print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
            dit_paths = [
                os.path.join(self.dit_dir, f"diffusion_pytorch_model-{i:05d}-of-00007.safetensors")
                for i in range(1, 8)
            ]
            
            # Check if model files exist
            missing_files = []
            for path in [self.ex4d_path, self.text_encoder_path, self.vae_path, self.clip_path] + dit_paths:
                if not os.path.exists(path):
                    missing_files.append(path)
            
            if missing_files:
                return f"ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {missing_files[:3]}..."  # Show first 3 missing files
            
            model_manager = ModelManager(torch_dtype=torch.bfloat16, device=self.device)
            model_manager.load_models([dit_paths, self.text_encoder_path, self.vae_path, self.clip_path])
            
            self.pipe = WanVideoPipeline.from_model_manager(model_manager, device=self.device)
            self.pipe.camera_encoder = CamVidEncoder(16, 1024, 5120).to(self.device, dtype=torch.bfloat16)
            
            # Add LoRA
            add_lora_to_model(self.pipe.denoising_model(), pretrained_path=self.ex4d_path, lora_rank=16, lora_alpha=16.0)
            self.pipe.load_cam(self.ex4d_path)
            self.pipe.enable_vram_management(num_persistent_param_in_dit=None)
            self.pipe.camera_encoder = self.pipe.camera_encoder.to(self.device, dtype=torch.bfloat16)
            self.pipe.camera_encoder.eval()
            
            self.model_loaded = True
            return "æ¨¡å‹åŠ è½½æˆåŠŸï¼"
            
        except Exception as e:
            return f"æ¨¡å‹åŠ è½½é”™è¯¯: {str(e)}"
    
    def generate_mask_video_file(self, input_video, num_frames, height, width):
        """Generate and save mask video as a file for preview"""
        try:
            mask_tensor = generate_mask_from_video(input_video, num_frames, height, width)
            
            # Convert tensor back to video format for saving
            mask_np = mask_tensor.permute(1, 2, 3, 0).numpy()  # (frames, height, width, channels)
            mask_np = (mask_np * 255).astype(np.uint8)
            
            # Save mask video to temporary file
            mask_video_path = tempfile.mktemp(suffix='.mp4')
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(mask_video_path, fourcc, 15.0, (width, height))
            
            for frame in mask_np:
                # Convert RGB to BGR for OpenCV
                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                out.write(frame_bgr)
            
            out.release()
            return mask_video_path
            
        except Exception as e:
            print(f"ç”Ÿæˆè’™ç‰ˆè§†é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None
    
    def reconstruct_video(self, input_video, cam_angle=180, height=512, width=512, num_frames=49):
        """å†…éƒ¨é‡å»ºåŠŸèƒ½ï¼Œç›´æ¥è°ƒç”¨æ·±åº¦ä¼°è®¡å’Œæ¸²æŸ“"""
        if input_video is None:
            return None, None, "è¯·ä¸Šä¼ è¾“å…¥è§†é¢‘ï¼"
        
        try:
            print(f"å¼€å§‹é‡å»ºè§†é¢‘: {input_video}")
            print(f"å‚æ•°: cam_angle={cam_angle}, num_frames={num_frames}, åˆ†è¾¨ç‡: {width}x{height}")
            
            # åŠ è½½æ·±åº¦ä¼°è®¡æ¨¡å‹
            self.load_depth_estimator()
            
            if self.depth_estimator is None or self.glctx is None:
                print("æ·±åº¦ä¼°è®¡æ¨¡å‹æˆ–æ¸²æŸ“ä¸Šä¸‹æ–‡åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ³•")
                return self.process_input_video_fallback(input_video, cam_angle, height, width, num_frames)
            
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            temp_output_dir = tempfile.mkdtemp()
            print(f"åˆ›å»ºä¸´æ—¶ç›®å½•: {temp_output_dir}")
            
            try:
                # ç›´æ¥è¿è¡Œé‡å»ºæµç¨‹ï¼Œä¼ é€’ç›®æ ‡åˆ†è¾¨ç‡
                return self.run_render_internal(input_video, temp_output_dir, cam_angle, num_frames, height, width)
                
            except Exception as e:
                print(f"å†…éƒ¨é‡å»ºå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ³•: {e}")
                return self.process_input_video_fallback(input_video, cam_angle, height, width, num_frames)
            finally:
                # æ¸…ç†ä¸´æ—¶ç›®å½•
                shutil.rmtree(temp_output_dir, ignore_errors=True)
                
        except Exception as e:
            print(f"é‡å»ºå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ³•: {e}")
            return self.process_input_video_fallback(input_video, cam_angle, height, width, num_frames)
    
    def run_render_internal(self, video_path, output_dir, cam_angle, num_frames, target_height=512, target_width=512):
        """å†…éƒ¨æ¸²æŸ“æµç¨‹ï¼Œç§»æ¤è‡ª recon.py"""
        if not UTILS_AVAILABLE:
            raise Exception("å·¥å…·æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•è¿è¡Œå†…éƒ¨æ¸²æŸ“")
            
        print(f"å¤„ç†è§†é¢‘: {video_path}")
        print(f"ç›®æ ‡åˆ†è¾¨ç‡: {target_width}x{target_height}")
        
        # 1. åŠ è½½è§†é¢‘å¸§å¹¶è°ƒæ•´åˆ°ç›®æ ‡åˆ†è¾¨ç‡
        try:
            frames, _ = read_video_frames(video_path, process_length=num_frames, max_res=max(target_height, target_width))
            # è°ƒæ•´åˆ°ç›®æ ‡åˆ†è¾¨ç‡
            if frames.shape[1] != target_height or frames.shape[2] != target_width:
                resized_frames = []
                for frame in frames:
                    resized_frame = cv2.resize(frame, (target_width, target_height))
                    resized_frames.append(resized_frame)
                frames = np.array(resized_frames)
                print(f"è§†é¢‘å¸§å·²è°ƒæ•´åˆ°ç›®æ ‡åˆ†è¾¨ç‡: {frames.shape}")
        except:
            # å¦‚æœè¯»å–å¤±è´¥ï¼Œå°è¯•ç”¨ cv2
            cap = cv2.VideoCapture(video_path)
            frame_list = []
            while len(frame_list) < num_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # è°ƒæ•´åˆ°ç›®æ ‡åˆ†è¾¨ç‡
                frame = cv2.resize(frame, (target_width, target_height))
                frame_list.append(frame)
            cap.release()
            frames = np.array(frame_list)
            print(f"ä½¿ç”¨cv2åŠ è½½å¹¶è°ƒæ•´åˆ†è¾¨ç‡: {frames.shape}")
        
        if frames.shape[0] < num_frames:
            print(f"è§†é¢‘å¸§æ•°ä¸è¶³: {frames.shape[0]} < {num_frames}")
            # å¤åˆ¶æœ€åä¸€å¸§è¡¥é½
            last_frame = frames[-1:] if len(frames) > 0 else np.zeros((1, 512, 512, 3), dtype=np.uint8)
            frames = np.concatenate([frames] + [last_frame] * (num_frames - frames.shape[0]))
        
        frames = frames[:num_frames]
        
        # 2. è¿è¡Œæ·±åº¦ä¼°è®¡
        print("è¿è¡Œæ·±åº¦ä¼°è®¡...")
        try:
            depth_src, intrinsics = self.run_depth_crafter(frames.astype(np.float32) / 255.)
        except Exception as e:
            print(f"æ·±åº¦ä¼°è®¡å¤±è´¥: {e}")
            raise e
        
        frames = torch.from_numpy(frames).float().to(self.device)
        depth_src = depth_src.to(self.device)
        
        # 3. æ¸²æŸ“ç½‘æ ¼
        print("å¼€å§‹æ¸²æŸ“...")
        with torch.no_grad():
            old_depth_src = depth_src
            depth_src = depth_src.clone()
            
            # è¾¹ç•Œå¤„ç†
            depth_src[:, :, 0, :] = 100
            depth_src[:, :, -1, :] = 100
            depth_src[:, :, :, 0] = 100
            depth_src[:, :, :, -1] = 100
            
            depth_src = depth_src.unsqueeze(-1)
            rgbs_src = frames
            
            H, W, C = rgbs_src[0].shape
            fov_y = 2 * math.atan2(H, 2 * intrinsics[1, 1])
            fov_x = 2 * math.atan2(W, 2 * intrinsics[0, 0])
            
            # åˆ›å»ºç›¸æœºå†…å‚çŸ©é˜µ
            fx = fy = 0.5 * H / math.tan(fov_y / 2)
            K = torch.tensor([[fx, 0, W / 2], [0, fy, H / 2], [0, 0, 1]], 
                           dtype=torch.float32, device=self.device)
            pose = torch.eye(4, device=self.device)
            ro_src, rd_src = get_rays_from_pose(pose, K, H, W)
            proj = getprojection(fov_x, fov_y, n=1e-3, f=1e3, device=self.device)
            
            # ç›¸æœºè½¨è¿¹
            depth_min = depth_src[0].min().item() + 0.15
            camera_poses = self.random_camera_traj(num_frames, depth_src, str(cam_angle), num_frames, depth_min)
            
            video = []
            for idx, poses in enumerate(tqdm(camera_poses, desc="æ¸²æŸ“å¸§")):
                pts_color = rgbs_src[idx]
                pts_xyz = depth_src[idx] * rd_src + ro_src
                faces = self.generate_faces(H, W, self.device)
                
                vertices, new_faces, colors, _ = self.point_to_mesh_cuda(
                    pts_xyz, pts_color, faces, old_depth_src[idx], filter_type="angle")
                
                try:
                    img = self.render_nvdiffrast(vertices, new_faces, colors, proj, poses, fov_x, fov_y, H, W)[0]
                except Exception as e:
                    print(f"æ¸²æŸ“ç¬¬ {idx} å¸§å¤±è´¥: {e}")
                    # ä½¿ç”¨åŸå§‹å›¾åƒä½œä¸ºå¤‡ç”¨
                    img = torch.cat([pts_color, torch.ones_like(pts_color[:,:,:1]) * 255], dim=-1)
                
                if idx == 0:
                    img[..., :3] = pts_color
                    img[..., 3:] = 255
                else:
                    mask = img[..., 3:]
                    mask[mask > 125] = 255
                    mask[mask <= 125] = 0
                    img[..., 3:] = mask
                    img[..., :3] = img[..., :3] * (mask / 255)
                
                video.append(img.cpu().numpy().astype(np.uint8))
            
            # ä¿å­˜è§†é¢‘
            os.makedirs(output_dir, exist_ok=True)
            
            cond_path = os.path.join(output_dir, f"color_{cam_angle}.mp4")
            mask_path = os.path.join(output_dir, f"mask_{cam_angle}.mp4")
            
            video = np.stack(video, axis=0).astype(np.uint8)
            
            # ä¿å­˜å½©è‰²å’Œè’™ç‰ˆè§†é¢‘
            imwrite(cond_path, video[..., :3], fps=30)
            imwrite(mask_path, video[..., 3:], fps=30)
            
            print(f"é‡å»ºå®Œæˆ: {cond_path}, {mask_path}")
            
            # å¤åˆ¶åˆ°ä¸´æ—¶æ–‡ä»¶ä¾› Gradio ä½¿ç”¨
            final_color_path = tempfile.mktemp(suffix='.mp4')
            final_mask_path = tempfile.mktemp(suffix='.mp4')
            
            shutil.copy2(cond_path, final_color_path)
            shutil.copy2(mask_path, final_mask_path)
            
            return final_color_path, final_mask_path, f"é‡å»ºæˆåŠŸï¼ä½¿ç”¨ç›¸æœºè§’åº¦: {cam_angle}Â°"
    
    def process_input_video_fallback(self, input_video, cam_angle=180, height=512, width=512, num_frames=49):
        """å›é€€æ–¹æ³•ï¼šç®€å•çš„è’™ç‰ˆç”Ÿæˆï¼ˆå½“çœŸæ­£çš„é‡å»ºå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        try:
            print(f"ä½¿ç”¨ç®€åŒ–æ–¹æ³•ï¼Œç›®æ ‡åˆ†è¾¨ç‡: {width}x{height}")
            
            # Generate mask video file for preview (using cam_angle parameter)
            mask_video_path = self.generate_mask_video_file(input_video, num_frames, height, width)
            
            # Resize input video to match target resolution
            color_video_path = self.resize_video_to_target(input_video, height, width, num_frames)
            
            return color_video_path, mask_video_path, f"ä½¿ç”¨ç®€åŒ–æ–¹æ³•å¤„ç†æˆåŠŸï¼ç›¸æœºè§’åº¦: {cam_angle}Â°"
            
        except Exception as e:
            return None, None, f"å¤„ç†é”™è¯¯: {str(e)}"
    
    def resize_video_to_target(self, input_video, target_height, target_width, num_frames):
        """å°†è¾“å…¥è§†é¢‘è°ƒæ•´åˆ°ç›®æ ‡åˆ†è¾¨ç‡"""
        try:
            cap = cv2.VideoCapture(input_video)
            
            # Create temporary output video
            output_path = tempfile.mktemp(suffix='.mp4')
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, 15.0, (target_width, target_height))
            
            frame_count = 0
            while frame_count < num_frames:
                ret, frame = cap.read()
                if not ret:
                    if frame_count == 0:
                        # If no frames read, create a black frame
                        frame = np.zeros((target_height, target_width, 3), dtype=np.uint8)
                    else:
                        # Use last frame if video is shorter
                        break
                
                # Resize frame to target resolution
                frame_resized = cv2.resize(frame, (target_width, target_height))
                out.write(frame_resized)
                frame_count += 1
            
            # Pad with last frame if needed
            if frame_count < num_frames and frame_count > 0:
                for _ in range(num_frames - frame_count):
                    out.write(frame_resized)
            
            cap.release()
            out.release()
            
            print(f"è§†é¢‘å·²è°ƒæ•´åˆ°ç›®æ ‡åˆ†è¾¨ç‡: {target_width}x{target_height}")
            return output_path
            
        except Exception as e:
            print(f"è°ƒæ•´è§†é¢‘åˆ†è¾¨ç‡å¤±è´¥: {e}")
            return input_video  # Return original if resize fails
    
    def generate_video(self, color_video, mask_video, height=512, width=512, num_frames=49, 
                      num_inference_steps=25, seed=0, progress=gr.Progress()):
        """Generate video using EX-4D"""
        if color_video is None or mask_video is None:
            return None, "è¯·ä¸Šä¼ å½©è‰²è§†é¢‘å’Œè’™ç‰ˆè§†é¢‘ï¼"
        
        # Auto-load models if not loaded
        if not self.model_loaded:
            progress(0.05, desc="æ­£åœ¨åŠ è½½æ¨¡å‹...")
            load_result = self.load_models()
            if "é”™è¯¯" in load_result or "ç¼ºå°‘" in load_result:
                return None, f"æ¨¡å‹åŠ è½½å¤±è´¥: {load_result}"
        
        try:
            progress(0.2, desc="æ­£åœ¨å¤„ç†è¾“å…¥è§†é¢‘...")
            
            print(f"åŠ è½½è§†é¢‘ï¼Œç›®æ ‡åˆ†è¾¨ç‡: {width}x{height}, å¸§æ•°: {num_frames}")
            
            # Load mask and color videos with specified resolution
            mask_tensor = load_mask_frames(mask_video, num_frames, height, width)
            color_tensor = load_video_frames(color_video, num_frames, height, width)
            
            print(f"å½©è‰²è§†é¢‘å¼ é‡å°ºå¯¸: {color_tensor.shape}")
            print(f"è’™ç‰ˆè§†é¢‘å¼ é‡å°ºå¯¸: {mask_tensor.shape}")
            
            # Ensure both tensors have the same dimensions
            if color_tensor.shape != mask_tensor.shape:
                print(f"è­¦å‘Š: å½©è‰²è§†é¢‘å’Œè’™ç‰ˆè§†é¢‘å°ºå¯¸ä¸åŒ¹é…ï¼Œå°†è°ƒæ•´è’™ç‰ˆå°ºå¯¸")
                # Resize mask to match color video
                mask_np = mask_tensor.permute(1, 2, 3, 0).numpy()  # (frames, height, width, channels)
                target_frames, target_channels, target_height, target_width = color_tensor.shape
                
                resized_mask_frames = []
                for i in range(target_frames):
                    if i < mask_np.shape[0]:
                        mask_frame = mask_np[i]
                    else:
                        mask_frame = mask_np[-1]  # Use last frame if not enough frames
                    
                    # Resize mask frame
                    mask_frame_resized = cv2.resize(mask_frame, (target_width, target_height))
                    if len(mask_frame_resized.shape) == 2:
                        mask_frame_resized = np.stack([mask_frame_resized] * target_channels, axis=-1)
                    resized_mask_frames.append(mask_frame_resized)
                
                resized_mask_np = np.array(resized_mask_frames)
                mask_tensor = torch.from_numpy(resized_mask_np).permute(3, 0, 1, 2).float()
                mask_tensor = (mask_tensor / 255.0 > 0.5).float()  # Binarize mask
                print(f"è°ƒæ•´åè’™ç‰ˆè§†é¢‘å¼ é‡å°ºå¯¸: {mask_tensor.shape}")
            
            color_tensor = (color_tensor * mask_tensor).to(torch.bfloat16) * 2 - 1
            mask_tensor = mask_tensor.to(torch.bfloat16) * 2 - 1
            
            progress(0.4, desc="è®¾ç½®æç¤ºè¯...")
            
            # Set prompts
            prompt = "4K ultra HD, surround motion, realistic tone, panoramic shot, wide-angle view, cinematic quality"
            negative_prompt = "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"
            
            progress(0.6, desc="æ­£åœ¨è¿è¡Œæ¨ç†...")
            
            # Run inference
            with torch.no_grad(), torch.amp.autocast(dtype=torch.bfloat16, device_type=self.device):
                input_cond = color_tensor.to(self.device)[None]  # Add batch dimension
                input_mask = mask_tensor.to(self.device)[None]
                
                output_video = self.pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    video=input_cond,
                    mask=input_mask,
                    num_inference_steps=num_inference_steps,
                    seed=seed,
                    tiled=False,
                    height=height,
                    width=width,
                    num_frames=num_frames,
                )
            
            progress(0.9, desc="æ­£åœ¨ä¿å­˜è¾“å‡ºè§†é¢‘...")
            
            # Save output video to temporary file
            output_path = tempfile.mktemp(suffix='.mp4')
            save_video(output_video, output_path, fps=15, quality=8)
            
            progress(1.0, desc="å®Œæˆ!")
            return output_path, "è§†é¢‘ç”ŸæˆæˆåŠŸï¼"
            
        except Exception as e:
            return None, f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
    
    def load_depth_estimator(self):
        """Load depth estimation model"""
        if not UTILS_AVAILABLE:
            print("å·¥å…·æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½æ·±åº¦ä¼°è®¡å™¨")
            return
            
        if self.depth_estimator is None:
            try:
                self.depth_estimator = DepthCrafterDemo(
                    unet_path="models/DepthCrafter",
                    pre_train_path="models/stable-video-diffusion-img2vid",
                    cpu_offload=None,
                    device=self.device
                )
                print("æ·±åº¦ä¼°è®¡æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"åŠ è½½æ·±åº¦ä¼°è®¡æ¨¡å‹å¤±è´¥: {e}")
                self.depth_estimator = None
        
        if self.glctx is None:
            try:
                import nvdiffrast.torch as dr
                self.glctx = dr.RasterizeCudaContext(device=self.device)
                print("æ¸²æŸ“ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ")
            except ImportError as e:
                print(f"nvdiffrast æœªå®‰è£…: {e}")
                self.glctx = None
            except Exception as e:
                print(f"åˆ›å»ºæ¸²æŸ“ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
                self.glctx = None
    
    def point_to_mesh_cuda(self, pts, rgbs, faces, old_depth_src, min_angle_deg=2.5, filter_type="angle"):
        """Convert point cloud to mesh with filtering"""
        h, w = rgbs.shape[:2]
        vertices = pts.reshape(-1, 3)
        masks = torch.ones((h, w, 1), dtype=torch.uint8).to(rgbs.device) * 255
        rgbs = torch.cat([rgbs, masks], axis=-1)
        colors = rgbs.reshape(-1, 4)
        
        v0 = vertices[faces[:, 0]]
        v1 = vertices[faces[:, 1]]
        v2 = vertices[faces[:, 2]]
        face_normals = F.normalize(torch.cross(v1 - v0, v2 - v0), dim=1)
        
        if filter_type == "angle":
            def angle_between(v1, v2):
                cos_theta = torch.sum(v1 * v2, -1) / (
                    torch.norm(v1, dim=1) * torch.norm(v2, dim=1) + 1e-12
                )
                return torch.arccos(torch.clip(cos_theta, -1.0, 1.0)) * 180 / np.pi

            a = angle_between(v1 - v0, v2 - v0)
            b = angle_between(v2 - v1, v0 - v1)
            c = angle_between(v0 - v2, v1 - v2)
            min_angles = torch.minimum(torch.minimum(a, b), c)

            valid_faces = min_angles >= min_angle_deg
            z_range = vertices[:, 2].max() - vertices[:, 2].min()

            z01, z12, z20 = torch.abs((v0 - v1)[:, 2]), torch.abs((v1 - v2)[:, 2]), torch.abs((v2 - v0)[:, 2])
            y01, y12, y20 = torch.abs((v0 - v1)[:, 1]), torch.abs((v1 - v2)[:, 1]), torch.abs((v2 - v0)[:, 1])
            x01, x12, x20 = torch.abs((v0 - v1)[:, 0]), torch.abs((v1 - v2)[:, 0]), torch.abs((v2 - v0)[:, 0])
            z_max = torch.maximum(torch.maximum(z01, z12), z20)
            y_max = torch.maximum(torch.maximum(y01, y12), y20)
            x_max = torch.maximum(torch.maximum(x01, x12), x20)
            proj_max = torch.maximum(torch.maximum(x_max, y_max), z_max)
            valid_faces2 = (proj_max / z_range < 0.013)
            valid_faces_final = valid_faces & valid_faces2
            faces = faces[valid_faces_final]
        
        return vertices, faces, colors, None
    
    def render_nvdiffrast(self, vertices, faces, colors, proj, poses, fovx, fovy, h, w, near=1e-3, far=1e3):
        """Render mesh using nvdiffrast"""
        try:
            import nvdiffrast.torch as dr
        except ImportError:
            raise ImportError("nvdiffrast æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œæ¸²æŸ“")
        
        def transform_pos(mtx, pos):
            t_mtx = torch.from_numpy(mtx).to(pos.device) if isinstance(mtx, np.ndarray) else mtx
            posw = torch.cat([pos, torch.ones([pos.shape[0], 1]).to(pos.device)], axis=1)
            return torch.matmul(posw, t_mtx.t())[None, ...]

        def render(glctx, mtx, pos, pos_idx, vtx_col, col_idx, h, w):
            pos_clip = transform_pos(mtx, pos)
            rast_out, _ = dr.rasterize(glctx, pos_clip, pos_idx, resolution=[h, w])
            color, _ = dr.interpolate(vtx_col[None, ...], rast_out, col_idx)
            color = dr.antialias(color, rast_out, pos_clip, pos_idx)
            return color
            
        poses_copy = poses.clone()
        poses_copy[0,:] *= -1
        poses_copy[1,:] *= -1
        poses_copy[2,:] *= -1
        mvp = proj @ poses_copy
        return render(self.glctx, mvp, vertices, faces, colors, faces, h, w)
    
    def get_camera_pose(self, eye, center):
        """Calculate camera pose matrix"""
        up = np.array((0, 1, 0), dtype=np.float32)
        def normalize(v):
            norm = np.linalg.norm(v)
            if norm < 1e-8:
                return v
            return v / norm
            
        forward = normalize(center - eye)
        right = normalize(np.cross(forward, up))
        new_up = normalize(np.cross(right, forward))
        view = np.zeros((4, 4), dtype=np.float32)
        view[0, 0:3] = right
        view[1, 0:3] = new_up
        view[2, 0:3] = forward
        view[0:3, 3] = -np.array([np.dot(right, eye), np.dot(new_up, eye), np.dot(forward, eye)])
        view[3, 3] = 1.0
        return torch.from_numpy(view)
    
    def random_camera_traj(self, n_frames, depth_src, random_type, num_frames, depth_min):
        """Generate random camera trajectory"""
        rounds = (49 / num_frames) if 49 > num_frames else num_frames / 49
        
        if random_type == "180":
            radius = depth_min
            eyes = np.zeros((num_frames, 3))
            angle = np.linspace(0, 2 * rounds * np.pi, num_frames)
            eyes[:, 0] = np.sin(angle) * radius
            eyes[:, 1] = (np.cos(angle) - 1) * radius * 0.2
            centers = np.zeros((num_frames, 3))
            centers[:, 2] = radius
        else:
            ta = int(random_type)
            angle = np.linspace(0, (ta / 180 * np.pi) * rounds, num_frames)
            eyes = np.zeros((num_frames, 3))
            radius = depth_min
            eyes[:, 0] = np.sin(angle) * radius
            eyes[:, 1] = (np.abs(np.cos(angle)) - 1) * radius * 0.2
            eyes[:, 2] = radius - radius * np.abs(np.cos(angle))
            centers = np.zeros((num_frames, 3))
            centers[:, -1] = radius
            
        camera_poses = torch.stack([self.get_camera_pose(eye, center) for eye, center in zip(eyes, centers)], 0)
        camera_poses = camera_poses.to(depth_src.device)
        return camera_poses
    
    def generate_faces(self, H, W, device):
        """Generate mesh faces for the grid"""
        idx = np.arange(H * W).reshape(H, W)
        faces = torch.from_numpy(np.concatenate([
                np.stack([idx[:-1, :-1].ravel(), idx[1:, :-1].ravel(), idx[:-1, 1:].ravel()], axis=-1),
                np.stack([idx[:-1, 1:].ravel(), idx[1:, :-1].ravel(), idx[1:, 1:].ravel()], axis=-1)
            ], axis=0)).int().to(device)
        faces = faces[:,[1,0,2]]
        faces = torch.cat([
                faces,
                torch.tensor([[W - 1, 0, H * W - 1]]).int().to(device),
                torch.tensor([[H * W - 1, 0, (H - 1) * W]]).int().to(device),
            ], 0)
        return faces
    
    def run_depth_crafter(self, frames, near=0.0001, far=10000, 
                         depth_inference_steps=5, depth_guidance_scale=1.0, 
                         window_size=110, overlap=25):
        """Run depth estimation using DepthCrafter"""
        depths, _ = self.depth_estimator.infer(
            frames, near, far, depth_inference_steps, depth_guidance_scale,
            window_size=window_size, overlap=overlap
        )
        f = 500
        cx = depths.shape[-1] // 2
        cy = depths.shape[-2] // 2
        intrinsics = torch.tensor([[f, 0.0, cx], [0.0, f, cy], [0.0, 0.0, 1.0]])
        return depths, intrinsics


# Initialize pipeline
pipeline = EX4DPipeline()


def create_interface():
    """Create Gradio interface"""
    
    # Default settings (hidden from user, automatically managed)
    DEFAULT_HEIGHT = 512
    DEFAULT_WIDTH = 512
    
    with gr.Blocks(title="EX-4D è§†é¢‘ç”Ÿæˆ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# EX-4D è§†é¢‘ç”Ÿæˆ WebUI")
        gr.Markdown("åŸºäº EX-4D æ¨¡å‹çš„ä¸¤æ­¥è§†é¢‘ç”Ÿæˆæµç¨‹ï¼šç¬¬ä¸€æ­¥é‡å»ºå½©è‰²å’Œè’™ç‰ˆè§†é¢‘ï¼Œç¬¬äºŒæ­¥ç”Ÿæˆæœ€ç»ˆå¢å¼ºè§†é¢‘ã€‚")
        gr.Markdown("ğŸ¯ **æ™ºèƒ½åˆ†è¾¨ç‡é€‚é…**ï¼šæ”¯æŒä»»æ„åˆ†è¾¨ç‡è§†é¢‘ä¸Šä¼ ï¼Œç³»ç»Ÿè‡ªåŠ¨ä¼˜åŒ–å¤„ç†ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ç½®ï¼")
        
        # Hidden components for internal use
        height = gr.State(value=DEFAULT_HEIGHT)
        width = gr.State(value=DEFAULT_WIDTH)
        
        with gr.Row():
            # Left panel - Step 1: Reconstruction
            with gr.Column(scale=1):
                gr.Markdown("## ç¬¬ä¸€æ­¥ï¼šè§†é¢‘é‡å»º")
                gr.Markdown("ä¸Šä¼ ä»»æ„åˆ†è¾¨ç‡çš„è¾“å…¥è§†é¢‘ï¼Œç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆå½©è‰²è§†é¢‘å’Œè’™ç‰ˆè§†é¢‘")
                
                input_video = gr.Video(label="ğŸ“ è¾“å…¥è§†é¢‘ï¼ˆæ”¯æŒä»»æ„åˆ†è¾¨ç‡ï¼‰", height=200)
                cam_angle = gr.Dropdown(
                    choices=[30, 60, 90, 180],
                    value=180,
                    label="ğŸ“· ç›¸æœºè§’åº¦",
                    info="æ§åˆ¶è§†é‡èŒƒå›´ï¼š30Â°=çª„è§†é‡ï¼Œ180Â°=å¹¿è§†é‡"
                )
                
                reconstruct_btn = gr.Button("ğŸ” å¼€å§‹é‡å»ºè§†é¢‘", variant="secondary", size="lg")
                
                gr.Markdown("### é‡å»ºç»“æœé¢„è§ˆ")
                with gr.Row():
                    color_video_preview = gr.Video(label="ç”Ÿæˆçš„å½©è‰²è§†é¢‘", height=150)
                    mask_video_preview = gr.Video(label="ç”Ÿæˆçš„è’™ç‰ˆè§†é¢‘", height=150)
                
                reconstruct_status = gr.Textbox(label="é‡å»ºçŠ¶æ€", interactive=False)
                
            # Right panel - Step 2: Generation  
            with gr.Column(scale=1):
                gr.Markdown("## ç¬¬äºŒæ­¥ï¼šè§†é¢‘ç”Ÿæˆ")
                gr.Markdown("ä½¿ç”¨é‡å»ºçš„å½©è‰²å’Œè’™ç‰ˆè§†é¢‘ç”Ÿæˆæœ€ç»ˆå¢å¼ºè§†é¢‘")
                
                gr.Markdown("### è¾“å…¥è§†é¢‘ï¼ˆä»ç¬¬ä¸€æ­¥è‡ªåŠ¨å¡«å……ï¼‰")
                color_video = gr.Video(label="å½©è‰²è§†é¢‘", height=150, interactive=True)
                mask_video = gr.Video(label="è’™ç‰ˆè§†é¢‘", height=150, interactive=True)
                
                gr.Markdown("### ç”Ÿæˆå‚æ•°")
                with gr.Row():
                    num_frames = gr.Slider(1, 100, value=49, step=1, label="ğŸ¬ å¸§æ•°")
                    num_inference_steps = gr.Slider(10, 50, value=25, step=1, label="âš™ï¸ æ¨ç†æ­¥æ•°")
                
                seed = gr.Number(value=0, label="ğŸ² éšæœºç§å­", precision=0)
                
                generate_btn = gr.Button("ğŸš€ ç”Ÿæˆæœ€ç»ˆè§†é¢‘", variant="primary", size="lg")
                
                gr.Markdown("### æœ€ç»ˆè¾“å‡º")
                output_video = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘", height=300)
                generation_status = gr.Textbox(label="ç”ŸæˆçŠ¶æ€", interactive=False)
                
                gr.Markdown("## ğŸ“– ä½¿ç”¨è¯´æ˜")
                gr.Markdown("""
                **ğŸ¯ å®Œå…¨è‡ªåŠ¨åŒ–æµç¨‹ï¼š**
                1. **åªéœ€ä¸Šä¼ **ï¼šä¸Šä¼ ä»»æ„åˆ†è¾¨ç‡çš„è§†é¢‘ï¼ˆ720pã€1080pã€4Kç­‰éƒ½æ”¯æŒï¼‰
                2. **è‡ªåŠ¨å¤„ç†**ï¼šç³»ç»Ÿè‡ªåŠ¨è°ƒæ•´åˆ†è¾¨ç‡ï¼Œä¿è¯æµç¨‹å…¼å®¹æ€§
                3. **ä¸€é”®é‡å»º**ï¼šé€‰æ‹©ç›¸æœºè§’åº¦ï¼Œç‚¹å‡»"å¼€å§‹é‡å»ºè§†é¢‘" 
                4. **ä¸€é”®ç”Ÿæˆ**ï¼šè°ƒæ•´å‚æ•°ï¼Œç‚¹å‡»"ç”Ÿæˆæœ€ç»ˆè§†é¢‘"
                
                **ğŸ”§ é«˜çº§ç”¨æ³•ï¼š**
                - æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥åœ¨å³ä¾§ä¸Šä¼ è‡ªå·±çš„å½©è‰²è§†é¢‘å’Œè’™ç‰ˆè§†é¢‘
                - æˆ–ä½¿ç”¨ä¸‹æ–¹ç¤ºä¾‹å¿«é€Ÿå¼€å§‹
                
                **ğŸ’¡ æ™ºèƒ½ç‰¹æ€§ï¼š** 
                - âœ… **åˆ†è¾¨ç‡è‡ªé€‚åº”**ï¼šæ— éœ€å…³å¿ƒè¾“å…¥è§†é¢‘åˆ†è¾¨ç‡
                - âœ… **è‡ªåŠ¨å¯¹é½**ï¼šå½©è‰²è§†é¢‘å’Œè’™ç‰ˆè§†é¢‘åˆ†è¾¨ç‡è‡ªåŠ¨åŒ¹é…
                - âœ… **ç›¸æœºè§’åº¦**ï¼š30Â°-180Â°ï¼Œè§’åº¦è¶Šå¤§è§†é‡è¶Šå¹¿
                - âœ… **è’™ç‰ˆå«ä¹‰**ï¼šç™½è‰²=ä¿®æ”¹åŒºåŸŸï¼Œé»‘è‰²=ä¿æŒåŸæ ·
                """)
        
        # Examples section
        gr.Markdown("## ğŸ® ç¤ºä¾‹")
        gr.Markdown("ç‚¹å‡»ä¸‹æ–¹ç¤ºä¾‹å¿«é€Ÿä½“éªŒï¼š")
        
        # Two sets of examples: one for input, one for color+mask
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ç¤ºä¾‹1ï¼šä»è¾“å…¥è§†é¢‘å¼€å§‹")
                examples_input = gr.Examples(
                    examples=[
                        [
                            "examples/flower/input.mp4",  # input_video
                            180,  # cam_angle
                            49,   # num_frames
                            25,   # num_inference_steps
                            0     # seed
                        ]
                    ],
                    inputs=[
                        input_video, cam_angle, num_frames, num_inference_steps, seed
                    ],
                    label="èŠ±æœµè§†é¢‘ç¤ºä¾‹"
                )
            
            with gr.Column():
                gr.Markdown("### ç¤ºä¾‹2ï¼šç›´æ¥ä½¿ç”¨é‡å»ºç»“æœ")
                examples_output = gr.Examples(
                    examples=[
                        [
                            "examples/flower/render_180.mp4",  # color_video
                            "examples/flower/mask_180.mp4",    # mask_video
                            49,   # num_frames
                            25,   # num_inference_steps
                            0     # seed
                        ]
                    ],
                    inputs=[
                        color_video, mask_video, num_frames, num_inference_steps, seed
                    ],
                    label="å½©è‰²+è’™ç‰ˆè§†é¢‘ç¤ºä¾‹"
                )
        
        # Event handlers
        def reconstruct_and_auto_fill(input_video, cam_angle, num_frames):
            """Reconstruct videos and auto-fill generation inputs"""
            # Use default resolution (hidden from user)
            height_val = DEFAULT_HEIGHT
            width_val = DEFAULT_WIDTH
            
            # First, run the reconstruction
            color_preview, mask_preview, status = pipeline.reconstruct_video(
                input_video, cam_angle, height_val, width_val, num_frames
            )
            
            # Then auto-fill the generation inputs
            if color_preview and mask_preview and "æˆåŠŸ" in status:
                fill_status = "ç¬¬ä¸€æ­¥å®Œæˆï¼å·²è‡ªåŠ¨å¡«å……ç¬¬äºŒæ­¥çš„è¾“å…¥è§†é¢‘ï¼Œåˆ†è¾¨ç‡å·²ä¼˜åŒ–"
                return color_preview, mask_preview, status, color_preview, mask_preview, fill_status
            else:
                return color_preview, mask_preview, status, None, None, "ç¬¬ä¸€æ­¥å¤±è´¥ï¼Œæ— æ³•è‡ªåŠ¨å¡«å……"
        
        def generate_with_defaults(color_video, mask_video, num_frames, num_inference_steps, seed):
            """Generate video with default resolution"""
            height_val = DEFAULT_HEIGHT
            width_val = DEFAULT_WIDTH
            return pipeline.generate_video(
                color_video, mask_video, height_val, width_val, num_frames, 
                num_inference_steps, seed
            )
        
        reconstruct_btn.click(
            fn=reconstruct_and_auto_fill,
            inputs=[input_video, cam_angle, num_frames],
            outputs=[color_video_preview, mask_video_preview, reconstruct_status, 
                    color_video, mask_video, generation_status]
        )
        
        generate_btn.click(
            fn=generate_with_defaults,
            inputs=[
                color_video, mask_video, num_frames, 
                num_inference_steps, seed
            ],
            outputs=[output_video, generation_status]
        )
    
    return demo


if __name__ == "__main__":
    # Launch configuration
    HOST = "0.0.0.0"
    PORT = 7860
    SHARE = False  # Set to True to create public link
    
    demo = create_interface()
    demo.launch(
        server_name=HOST,
        server_port=PORT,
        share=SHARE,
        show_api=True,
        debug=False
    )