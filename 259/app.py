import gradio as gr
from diffsynth.pipelines.qwen_image import (
    QwenImagePipeline, ModelConfig,
    QwenImageUnit_Image2LoRAEncode, QwenImageUnit_Image2LoRADecode
)
from safetensors.torch import save_file
import torch
from PIL import Image
import os
from datetime import datetime
import glob

from diffsynth.utils.lora import merge_lora
from diffsynth.core.loader.file import load_state_dict

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
CHECKPOINTS_DIR = os.path.join(PROJECT_ROOT, "checkpoints")

# VRAM é…ç½®
vram_config_disk_offload = {
    "offload_dtype": "disk",
    "offload_device": "disk",
    "onload_dtype": "disk",
    "onload_device": "disk",
    "preparing_dtype": torch.bfloat16,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}

# ========== å…¨å±€ Pipeline ==========
style_pipe = None      # Style æ¨¡å¼ä¸“ç”¨
cfb_pipe = None        # Coarse+Fine+Bias æ¨¡å¼ä¸“ç”¨
gen_pipe = None        # å›¾ç‰‡ç”Ÿæˆä¸“ç”¨


def load_style_model():
    """åŠ è½½ Style æ¨¡å¼æ‰€éœ€çš„æ¨¡å‹"""
    global style_pipe
    if style_pipe is not None:
        return
    print("æ­£åœ¨åŠ è½½ Style æ¨¡å‹...")
    style_pipe = QwenImagePipeline.from_pretrained(
        torch_dtype=torch.bfloat16,
        device="cuda",
        model_configs=[
            ModelConfig(path=os.path.join(CHECKPOINTS_DIR, "General-Image-Encoders/SigLIP2-G384/model.safetensors"), **vram_config_disk_offload),
            ModelConfig(path=os.path.join(CHECKPOINTS_DIR, "General-Image-Encoders/DINOv3-7B/model.safetensors"), **vram_config_disk_offload),
            ModelConfig(path=os.path.join(CHECKPOINTS_DIR, "Qwen-Image-i2L/Qwen-Image-i2L-Style.safetensors"), **vram_config_disk_offload),
        ],
        tokenizer_config=None,
        vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
    )
    print("âœ… Style æ¨¡å‹åŠ è½½å®Œæˆï¼")


def load_cfb_model():
    """åŠ è½½ Coarse+Fine+Bias æ¨¡å¼æ‰€éœ€çš„æ¨¡å‹"""
    global cfb_pipe
    if cfb_pipe is not None:
        return
    print("æ­£åœ¨åŠ è½½ Coarse+Fine+Bias æ¨¡å‹...")
    text_encoder_files = sorted(glob.glob(os.path.join(CHECKPOINTS_DIR, "Qwen-Image/text_encoder/model-*.safetensors")))
    cfb_pipe = QwenImagePipeline.from_pretrained(
        torch_dtype=torch.bfloat16,
        device="cuda",
        model_configs=[
            ModelConfig(path=text_encoder_files, **vram_config_disk_offload),
            ModelConfig(path=os.path.join(CHECKPOINTS_DIR, "General-Image-Encoders/SigLIP2-G384/model.safetensors"), **vram_config_disk_offload),
            ModelConfig(path=os.path.join(CHECKPOINTS_DIR, "General-Image-Encoders/DINOv3-7B/model.safetensors"), **vram_config_disk_offload),
            ModelConfig(path=os.path.join(CHECKPOINTS_DIR, "Qwen-Image-i2L/Qwen-Image-i2L-Coarse.safetensors"), **vram_config_disk_offload),
            ModelConfig(path=os.path.join(CHECKPOINTS_DIR, "Qwen-Image-i2L/Qwen-Image-i2L-Fine.safetensors"), **vram_config_disk_offload),
        ],
        tokenizer_config=None,
        # Coarse+Fine éœ€è¦ processorï¼ˆQwen2VLProcessorï¼‰æ¥å¤„ç†å›¾åƒç¼–ç 
        processor_config=ModelConfig(model_id="Qwen/Qwen-Image-Edit", origin_file_pattern="processor/"),
        vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
    )
    print("âœ… Coarse+Fine+Bias æ¨¡å‹åŠ è½½å®Œæˆï¼")


def load_generation_model():
    """åŠ è½½ç”¨äºç”Ÿæˆå›¾ç‰‡çš„ Qwen-Image æ¨¡å‹"""
    global gen_pipe
    if gen_pipe is not None:
        return
    print("æ­£åœ¨åŠ è½½å›¾ç‰‡ç”Ÿæˆæ¨¡å‹...")
    transformer_files = sorted(glob.glob(os.path.join(CHECKPOINTS_DIR, "Qwen-Image/transformer/diffusion_pytorch_model*.safetensors")))
    text_encoder_files = sorted(glob.glob(os.path.join(CHECKPOINTS_DIR, "Qwen-Image/text_encoder/model-*.safetensors")))
    vae_files = sorted(glob.glob(os.path.join(CHECKPOINTS_DIR, "Qwen-Image/vae/diffusion_pytorch_model*.safetensors")))
    model_configs = []
    if transformer_files:
        model_configs.append(ModelConfig(path=transformer_files, **vram_config_disk_offload))
    if text_encoder_files:
        model_configs.append(ModelConfig(path=text_encoder_files, **vram_config_disk_offload))
    if vae_files:
        model_configs.append(ModelConfig(path=vae_files, **vram_config_disk_offload))
    tokenizer_path = os.path.join(CHECKPOINTS_DIR, "Qwen-Image/tokenizer")
    tokenizer_config = ModelConfig(path=tokenizer_path) if os.path.exists(tokenizer_path) else None
    gen_pipe = QwenImagePipeline.from_pretrained(
        torch_dtype=torch.bfloat16,
        device="cuda",
        model_configs=model_configs,
        tokenizer_config=tokenizer_config,
        vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
    )
    print("âœ… å›¾ç‰‡ç”Ÿæˆæ¨¡å‹åŠ è½½å®Œæˆï¼")


# ========== å·¥å…·å‡½æ•° ==========

def parse_gallery_images(images):
    """å°† Gradio Gallery çš„è¾“å…¥è§£æä¸º PIL Image åˆ—è¡¨"""
    pil_images = []
    if images is None:
        return pil_images
    for img in images:
        if isinstance(img, tuple):
            img = img[0]
        if isinstance(img, str):
            pil_images.append(Image.open(img).convert("RGB"))
        elif isinstance(img, Image.Image):
            pil_images.append(img.convert("RGB"))
    return pil_images


def load_style_examples(style_num):
    """åŠ è½½ Style ç¤ºä¾‹å›¾ç‰‡ (assets/style/*)"""
    style_dir = os.path.join(CHECKPOINTS_DIR, f"Qwen-Image-i2L/assets/style/{style_num}")
    if not os.path.exists(style_dir):
        return None, f"âŒ ç›®å½•ä¸å­˜åœ¨: {style_dir}"
    images = sorted([os.path.join(style_dir, f) for f in os.listdir(style_dir) if f.endswith('.jpg') and not f.startswith('image_')])
    return images, f"âœ… å·²åŠ è½½ {len(images)} å¼ ç¤ºä¾‹å›¾ç‰‡"


def load_lora_examples(lora_num):
    """åŠ è½½ Coarse+Fine+Bias ç¤ºä¾‹å›¾ç‰‡ (assets/lora/*)"""
    lora_dir = os.path.join(CHECKPOINTS_DIR, f"Qwen-Image-i2L/assets/lora/{lora_num}")
    if not os.path.exists(lora_dir):
        return None, f"âŒ ç›®å½•ä¸å­˜åœ¨: {lora_dir}"
    images = sorted([os.path.join(lora_dir, f) for f in os.listdir(lora_dir) if f.endswith('.jpg') and not f.startswith('image_')])
    return images, f"âœ… å·²åŠ è½½ {len(images)} å¼ ç¤ºä¾‹å›¾ç‰‡"


# ========== LoRA ç”Ÿæˆ ==========

def generate_style_lora(images):
    """Style æ¨¡å¼ï¼šä»å›¾ç‰‡ç”Ÿæˆ LoRA"""
    global style_pipe
    pil_images = parse_gallery_images(images)
    if not pil_images:
        return None, "âŒ è¯·ä¸Šä¼ è‡³å°‘ä¸€å¼ å›¾ç‰‡ï¼"
    try:
        if style_pipe is None:
            load_style_model()
        with torch.no_grad():
            embs = QwenImageUnit_Image2LoRAEncode().process(style_pipe, image2lora_images=pil_images)
            lora = QwenImageUnit_Image2LoRADecode().process(style_pipe, **embs)["lora"]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(PROJECT_ROOT, "outputs")
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, f"lora_style_{timestamp}.safetensors")
        save_file(lora, output_path)
        return output_path, f"âœ… Style LoRA ç”ŸæˆæˆåŠŸï¼\nè·¯å¾„: {output_path}"
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def generate_cfb_lora(images):
    """Coarse+Fine+Bias æ¨¡å¼ï¼šä»å›¾ç‰‡ç”Ÿæˆ LoRA"""
    global cfb_pipe
    pil_images = parse_gallery_images(images)
    if not pil_images:
        return None, "âŒ è¯·ä¸Šä¼ è‡³å°‘ä¸€å¼ å›¾ç‰‡ï¼"
    try:
        if cfb_pipe is None:
            load_cfb_model()
        with torch.no_grad():
            embs = QwenImageUnit_Image2LoRAEncode().process(cfb_pipe, image2lora_images=pil_images)
            lora = QwenImageUnit_Image2LoRADecode().process(cfb_pipe, **embs)["lora"]
        # åˆå¹¶ Bias
        bias_path = os.path.join(CHECKPOINTS_DIR, "Qwen-Image-i2L/Qwen-Image-i2L-Bias.safetensors")
        if os.path.exists(bias_path):
            lora_bias = load_state_dict(bias_path, torch_dtype=torch.bfloat16, device="cuda")
            lora = merge_lora([lora, lora_bias])
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(PROJECT_ROOT, "outputs")
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, f"lora_cfb_{timestamp}.safetensors")
        save_file(lora, output_path)
        return output_path, f"âœ… Coarse+Fine+Bias LoRA ç”ŸæˆæˆåŠŸï¼\nè·¯å¾„: {output_path}"
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


# ========== å›¾ç‰‡ç”Ÿæˆ ==========

def generate_image_with_lora(prompt, lora_path_text, height, width, steps, seed):
    """ä½¿ç”¨ LoRA ç”Ÿæˆå›¾ç‰‡"""
    global gen_pipe
    try:
        if gen_pipe is None:
            load_generation_model()
        # è§£æ LoRA è·¯å¾„
        lora_path = lora_path_text.strip() if lora_path_text else None
        if not lora_path or not os.path.exists(lora_path):
            return None, "âŒ è¯·å¡«å†™æœ‰æ•ˆçš„ LoRA æ–‡ä»¶è·¯å¾„"
        # æ¸…é™¤æ—§çš„ LoRAï¼Œå†åŠ è½½æ–°çš„
        gen_pipe.clear_lora()
        gen_pipe.load_lora(gen_pipe.dit, lora_path)
        # ç”Ÿæˆå›¾ç‰‡
        seed_val = int(seed) if seed is not None and str(seed).strip() != "" else None
        image = gen_pipe(prompt, seed=seed_val, height=int(height), width=int(width), num_inference_steps=int(steps))
        # ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(PROJECT_ROOT, "outputs")
        os.makedirs(output_dir, exist_ok=True)
        out_path = os.path.join(output_dir, f"generated_{timestamp}.png")
        if isinstance(image, list):
            image = image[0]
        if hasattr(image, "save"):
            image.save(out_path)
        else:
            Image.fromarray(image).save(out_path)
        return out_path, f"âœ… å›¾ç‰‡ç”ŸæˆæˆåŠŸ: {out_path}"
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


# ========== Gradio UI ==========

def create_ui():
    with gr.Blocks(title="Qwen-Image-i2L", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¨ Qwen-Image-i2L: Image to LoRA
        
        å°†ä¸€ç»„å›¾ç‰‡è½¬æ¢ä¸º LoRA æ¨¡å‹ï¼Œç”¨äºé£æ ¼è¿ç§»æˆ–å†…å®¹ä¿ç•™ï¼Œç„¶åç”¨ç”Ÿæˆçš„ LoRA ç”Ÿæˆæ–°å›¾ç‰‡ã€‚
        """)
        
        with gr.Tabs():
            # ==================== Tab 1: Style ====================
            with gr.Tab("ğŸ¨ Styleï¼ˆé£æ ¼è¿ç§»ï¼‰"):
                gr.Markdown("""
                ### Style æ¨¡å¼
                - **ç‰¹ç‚¹**ï¼šæå–å›¾ç‰‡é£æ ¼ï¼ˆå¼±ç»†èŠ‚ï¼‰ï¼Œé€‚åˆé£æ ¼è¿ç§»
                - **æ¨è**ï¼š3~5 å¼ é£æ ¼ç»Ÿä¸€çš„å›¾ç‰‡
                """)
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### ğŸ“ ç¤ºä¾‹é£æ ¼")
                        style_dropdown = gr.Dropdown(
                            choices=["1 - Rough Sketch", "2 - Abstract Vector", "3 - Black & White Sketch", "4 - Blue Flat"],
                            label="é€‰æ‹©ç¤ºä¾‹",
                            value="1 - Rough Sketch"
                        )
                        style_load_btn = gr.Button("åŠ è½½ç¤ºä¾‹å›¾ç‰‡")
                        style_example_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
                    with gr.Column(scale=2):
                        gr.Markdown("#### ğŸ–¼ï¸ ä¸Šä¼ é£æ ¼å›¾ç‰‡")
                        style_gallery = gr.Gallery(label="é£æ ¼å›¾ç‰‡", columns=5, rows=2, height="auto", interactive=True, type="filepath")
                        with gr.Row():
                            style_clear_btn = gr.Button("æ¸…ç©º")
                            style_gen_lora_btn = gr.Button("ğŸš€ ç”Ÿæˆ Style LoRA", variant="primary")
                
                gr.Markdown("---")
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### ğŸ“¥ LoRA è¾“å‡º")
                        style_lora_file = gr.File(label="ä¸‹è½½ LoRA")
                        style_lora_status = gr.Textbox(label="ç”ŸæˆçŠ¶æ€", interactive=False, lines=2)
                    with gr.Column():
                        gr.Markdown("#### ğŸ–¼ï¸ ç”¨ LoRA ç”Ÿæˆå›¾ç‰‡")
                        style_prompt = gr.Textbox(label="Prompt", value="a cat", lines=1)
                        style_lora_path = gr.Textbox(label="LoRA æ–‡ä»¶è·¯å¾„", value="", lines=1)
                        with gr.Row():
                            style_height = gr.Number(label="é«˜åº¦", value=1024)
                            style_width = gr.Number(label="å®½åº¦", value=1024)
                        with gr.Row():
                            style_steps = gr.Number(label="æ­¥æ•°", value=50)
                            style_seed = gr.Number(label="ç§å­", value=0)
                        style_gen_img_btn = gr.Button("ğŸ–¼ï¸ ç”Ÿæˆå›¾ç‰‡", variant="primary")
                        style_output_img = gr.Image(label="ç”Ÿæˆç»“æœ")
                        style_img_status = gr.Textbox(label="çŠ¶æ€", interactive=False, lines=2)
                
                # äº‹ä»¶ç»‘å®š
                style_load_btn.click(
                    fn=lambda x: load_style_examples(x.split(" - ")[0]),
                    inputs=[style_dropdown],
                    outputs=[style_gallery, style_example_status]
                )
                style_clear_btn.click(fn=lambda: (None, ""), outputs=[style_gallery, style_lora_status])
                style_gen_lora_btn.click(fn=generate_style_lora, inputs=[style_gallery], outputs=[style_lora_file, style_lora_status])
                style_gen_img_btn.click(
                    fn=generate_image_with_lora,
                    inputs=[style_prompt, style_lora_path, style_height, style_width, style_steps, style_seed],
                    outputs=[style_output_img, style_img_status]
                )
            
            # ==================== Tab 2: Coarse+Fine+Bias ====================
            with gr.Tab("ğŸ”§ Coarse+Fine+Biasï¼ˆå†…å®¹ä¿ç•™ï¼‰"):
                gr.Markdown("""
                ### Coarse+Fine+Bias æ¨¡å¼
                - **ç‰¹ç‚¹**ï¼šä¿ç•™å›¾ç‰‡å†…å®¹å’Œç»†èŠ‚ï¼Œç”Ÿæˆçš„ LoRA å¯ä½œä¸ºè®­ç»ƒåˆå§‹åŒ–æƒé‡
                - **æ¨è**ï¼š5~10 å¼ æˆ–æ›´å¤šç›¸ä¼¼å†…å®¹çš„å›¾ç‰‡
                - **è¯´æ˜**ï¼šä¼šè‡ªåŠ¨åˆå¹¶ Bias LoRA ä»¥æ ¡æ­£é£æ ¼
                """)
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### ğŸ“ ç¤ºä¾‹æ•°æ®")
                        cfb_dropdown = gr.Dropdown(
                            choices=["1 - Puppy Backpack", "2 - Teddy Bear", "3 - Blueberries"],
                            label="é€‰æ‹©ç¤ºä¾‹",
                            value="3 - Blueberries"
                        )
                        cfb_load_btn = gr.Button("åŠ è½½ç¤ºä¾‹å›¾ç‰‡")
                        cfb_example_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
                    with gr.Column(scale=2):
                        gr.Markdown("#### ğŸ–¼ï¸ ä¸Šä¼ å›¾ç‰‡")
                        cfb_gallery = gr.Gallery(label="è¾“å…¥å›¾ç‰‡", columns=5, rows=2, height="auto", interactive=True, type="filepath")
                        with gr.Row():
                            cfb_clear_btn = gr.Button("æ¸…ç©º")
                            cfb_gen_lora_btn = gr.Button("ğŸš€ ç”Ÿæˆ Coarse+Fine+Bias LoRA", variant="primary")
                
                gr.Markdown("---")
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### ğŸ“¥ LoRA è¾“å‡º")
                        cfb_lora_file = gr.File(label="ä¸‹è½½ LoRA")
                        cfb_lora_status = gr.Textbox(label="ç”ŸæˆçŠ¶æ€", interactive=False, lines=2)
                    with gr.Column():
                        gr.Markdown("#### ğŸ–¼ï¸ ç”¨ LoRA ç”Ÿæˆå›¾ç‰‡")
                        cfb_prompt = gr.Textbox(label="Prompt", value="a bowl of blueberries", lines=1)
                        cfb_lora_path = gr.Textbox(label="LoRA æ–‡ä»¶è·¯å¾„", value="", lines=1)
                        with gr.Row():
                            cfb_height = gr.Number(label="é«˜åº¦", value=1024)
                            cfb_width = gr.Number(label="å®½åº¦", value=1024)
                        with gr.Row():
                            cfb_steps = gr.Number(label="æ­¥æ•°", value=50)
                            cfb_seed = gr.Number(label="ç§å­", value=0)
                        cfb_gen_img_btn = gr.Button("ğŸ–¼ï¸ ç”Ÿæˆå›¾ç‰‡", variant="primary")
                        cfb_output_img = gr.Image(label="ç”Ÿæˆç»“æœ")
                        cfb_img_status = gr.Textbox(label="çŠ¶æ€", interactive=False, lines=2)
                
                # äº‹ä»¶ç»‘å®š
                cfb_load_btn.click(
                    fn=lambda x: load_lora_examples(x.split(" - ")[0]),
                    inputs=[cfb_dropdown],
                    outputs=[cfb_gallery, cfb_example_status]
                )
                cfb_clear_btn.click(fn=lambda: (None, ""), outputs=[cfb_gallery, cfb_lora_status])
                cfb_gen_lora_btn.click(fn=generate_cfb_lora, inputs=[cfb_gallery], outputs=[cfb_lora_file, cfb_lora_status])
                cfb_gen_img_btn.click(
                    fn=generate_image_with_lora,
                    inputs=[cfb_prompt, cfb_lora_path, cfb_height, cfb_width, cfb_steps, cfb_seed],
                    outputs=[cfb_output_img, cfb_img_status]
                )
        
    return demo


if __name__ == "__main__":
    # å¯åŠ¨æ—¶é¢„åŠ è½½ Style æ¨¡å‹ï¼ˆè½»é‡çº§ï¼Œå¯å¿«é€Ÿå¯åŠ¨ï¼‰
    load_style_model()
    
    # å¯åŠ¨ Web ç•Œé¢
    demo = create_ui()
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
