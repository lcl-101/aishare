#!/usr/bin/env python3
"""
SongGeneration WebUI - A Gradio interface for song generation
Based on generate.py but with flash_attn disabled for compatibility

é…ç½®è¯´æ˜:
- ä¿®æ”¹ä¸‹é¢çš„ CONFIG éƒ¨åˆ†æ¥é…ç½®å‚æ•°
- ç›´æ¥è¿è¡Œ: python webui_app.py
"""

# ===================== é…ç½®åŒºåŸŸ =====================
CONFIG = {
    # æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ (å¿…é¡»ä¿®æ”¹ä¸ºæ­£ç¡®è·¯å¾„)
    "CHECKPOINT_PATH": "ckpt/songgeneration_base",
    
    # æ€§èƒ½è®¾ç½®
    "USE_FLASH_ATTN": True,      # æ˜¯å¦å¯ç”¨ Flash Attention (æ¨è True)
    "LOW_MEMORY_MODE": False,    # æ˜¯å¦å¯ç”¨ä½å†…å­˜æ¨¡å¼
    
    # æœåŠ¡å™¨é…ç½®
    "HOST": "0.0.0.0",           # ç›‘å¬åœ°å€ï¼Œ0.0.0.0 è¡¨ç¤ºæ‰€æœ‰ç½‘ç»œæ¥å£
    "PORT": 7860,                # ç«¯å£å·
    "SHARE": False,              # æ˜¯å¦åˆ›å»ºå…¬å¼€é“¾æ¥ (Gradio share)
    
    # å…¶ä»–è®¾ç½®
    "AUTO_OPEN_BROWSER": True,   # æ˜¯å¦è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
}
# =================================================

import os
import sys
import gc
import json
import time
import torch
import torchaudio
import numpy as np
import gradio as gr
from datetime import datetime
from omegaconf import OmegaConf
from codeclm.models import builders
from codeclm.trainer.codec_song_pl import CodecLM_PL
from codeclm.models import CodecLM
from third_party.demucs.models.pretrained import get_model_from_yaml

def load_examples_from_jsonl(jsonl_path="sample/lyrics.jsonl"):
    """ä» JSONL æ–‡ä»¶åŠ è½½ç¤ºä¾‹æ•°æ®"""
    examples = []
    if os.path.exists(jsonl_path):
        try:
            with open(jsonl_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        # æ„å»ºç¤ºä¾‹æ ¼å¼ï¼š[æ­Œè¯, æè¿°, éŸ³é¢‘è·¯å¾„, è‡ªåŠ¨æç¤ºç±»å‹]
                        lyric = data.get('gt_lyric', '')
                        description = data.get('descriptions', '')
                        audio_path = data.get('prompt_audio_path', '')
                        auto_prompt = data.get('auto_prompt_audio_type', 'None')
                        
                        # ç¤ºä¾‹åç§°åŸºäº idx
                        idx = data.get('idx', f'sample_{len(examples)+1}')
                        
                        examples.append([
                            lyric,                          # æ­Œè¯
                            description,                    # æè¿°
                            audio_path if audio_path and os.path.exists(audio_path) else None,  # éŸ³é¢‘è·¯å¾„
                            auto_prompt if auto_prompt != 'Auto' else 'Auto',  # è‡ªåŠ¨æç¤ºç±»å‹
                            "mixed",                        # ç”Ÿæˆç±»å‹
                            1.5,                           # CFG ç³»æ•°
                            0.9,                           # æ¸©åº¦
                            50,                            # Top-K
                            True,                          # å¯ç”¨ Flash Attention
                            idx                            # ç¤ºä¾‹åç§°
                        ])
        except Exception as e:
            print(f"åŠ è½½ç¤ºä¾‹æ–‡ä»¶å¤±è´¥: {e}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¤ºä¾‹æ–‡ä»¶æˆ–ç¤ºä¾‹ä¸ºç©ºï¼Œæ·»åŠ é»˜è®¤ç¤ºä¾‹
    if not examples:
        examples = [
            [
                "[intro-short]\n\n[verse]\nå¤œæ™šçš„è¡—ç¯é—ªçƒ\næˆ‘æ¼«æ­¥åœ¨ç†Ÿæ‚‰çš„è§’è½\nå›å¿†åƒæ½®æ°´èˆ¬æ¶Œæ¥\nä½ çš„ç¬‘å®¹å¦‚æ­¤æ¸…æ™°\n\n[chorus]\nåœ¨å¿ƒå¤´æ— æ³•æŠ¹å»\né‚£äº›æ›¾ç»çš„ç”œèœœ\nå¦‚ä»Šåªå‰©æˆ‘ç‹¬è‡ªå›å¿†\néŸ³ä¹çš„èŠ‚å¥åœ¨å¥å“\n\n[outro-short]",
                "female, dark, pop, sad, piano and drums",
                None,
                "Auto",
                "mixed",
                1.5,
                0.9,
                50,
                True,
                "é»˜è®¤ç¤ºä¾‹"
            ]
        ]
    
    return examples

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['USER'] = 'root'
os.environ['PYTHONDONTWRITEBYTECODE'] = '1'
os.environ['TRANSFORMERS_CACHE'] = os.path.join(os.getcwd(), 'third_party/hub')
os.environ['NCCL_HOME'] = '/usr/local/tccl'

# æ·»åŠ åˆ° Python è·¯å¾„
current_dir = os.getcwd()
sys.path.extend([
    os.path.join(current_dir, 'codeclm/tokenizer'),
    current_dir,
    os.path.join(current_dir, 'codeclm/tokenizer/Flow1dVAE'),
])

# ç¦ç”¨ cudnn å’Œæ³¨å†Œ OmegaConf è§£æå™¨
torch.backends.cudnn.enabled = False
OmegaConf.register_new_resolver("eval", lambda x: eval(x))
OmegaConf.register_new_resolver("concat", lambda *x: [xxx for xx in x for xxx in xx])
OmegaConf.register_new_resolver("get_fname", lambda: os.path.splitext(os.path.basename(sys.argv[1]))[0] if len(sys.argv) > 1 else 'default')
OmegaConf.register_new_resolver("load_yaml", lambda x: list(OmegaConf.load(x)))

# å…¨å±€å˜é‡
auto_prompt_type = ['Pop', 'R&B', 'Dance', 'Jazz', 'Folk', 'Rock', 'Chinese Style', 'Chinese Tradition', 'Metal', 'Reggae', 'Chinese Opera', 'Auto']

class Separator:
    def __init__(self, dm_model_path='third_party/demucs/ckpt/htdemucs.pth', dm_config_path='third_party/demucs/ckpt/htdemucs.yaml', gpu_id=0) -> None:
        if torch.cuda.is_available() and gpu_id < torch.cuda.device_count():
            self.device = torch.device(f"cuda:{gpu_id}")
        else:
            self.device = torch.device("cpu")
        self.demucs_model = self.init_demucs_model(dm_model_path, dm_config_path)

    def init_demucs_model(self, model_path, config_path):
        model = get_model_from_yaml(config_path, model_path)
        model.to(self.device)
        model.eval()
        return model
    
    def load_audio(self, f):
        a, fs = torchaudio.load(f)
        if (fs != 48000):
            a = torchaudio.functional.resample(a, fs, 48000)
        if a.shape[-1] >= 48000*10:
            a = a[..., :48000*10]
        return a[:, 0:48000*10]
    
    def run(self, audio_path, output_dir='tmp', ext=".flac"):
        os.makedirs(output_dir, exist_ok=True)
        name, _ = os.path.splitext(os.path.split(audio_path)[-1])
        output_paths = []

        for stem in self.demucs_model.sources:
            output_path = os.path.join(output_dir, f"{name}_{stem}{ext}")
            if os.path.exists(output_path):
                output_paths.append(output_path)
        if len(output_paths) == 4:
            drums_path, bass_path, other_path, vocal_path = output_paths
        else:
            drums_path, bass_path, other_path, vocal_path = self.demucs_model.separate(audio_path, output_dir, device=self.device)
            for path in [drums_path, bass_path, other_path]:
                os.remove(path)
        full_audio = self.load_audio(audio_path)
        vocal_audio = self.load_audio(vocal_path)
        bgm_audio = full_audio - vocal_audio
        return full_audio, vocal_audio, bgm_audio


class SongGenerator:
    def __init__(self, ckpt_path, use_flash_attn=True):
        self.ckpt_path = ckpt_path
        self.cfg_path = os.path.join(ckpt_path, 'config.yaml')
        self.pt_path = os.path.join(ckpt_path, 'model.pt')
        
        # åŠ è½½é…ç½®
        self.cfg = OmegaConf.load(self.cfg_path)
        self.cfg.mode = 'inference'
        
        # é…ç½® flash_attn - ç°åœ¨å¯ç”¨å®ƒæ¥æå‡æ€§èƒ½
        if hasattr(self.cfg, 'lm'):
            if use_flash_attn:
                # ä¼˜å…ˆä½¿ç”¨ flash_attn_2ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°åŸç‰ˆ
                if hasattr(self.cfg.lm, 'use_flash_attn_2'):
                    self.cfg.lm.use_flash_attn_2 = True
                elif hasattr(self.cfg.lm, 'use_flash_attn'):
                    self.cfg.lm.use_flash_attn = True
                print("Flash Attention enabled for better performance")
            else:
                if hasattr(self.cfg.lm, 'use_flash_attn_2'):
                    self.cfg.lm.use_flash_attn_2 = False
                if hasattr(self.cfg.lm, 'use_flash_attn'):
                    self.cfg.lm.use_flash_attn = False
                print("Flash Attention disabled for compatibility")
        
        self.max_duration = self.cfg.max_dur
        self.separator = Separator()
        
        # åŠ è½½è‡ªåŠ¨æç¤º
        try:
            self.auto_prompt = torch.load('ckpt/prompt.pt')
            self.merge_prompt = [item for sublist in self.auto_prompt.values() for item in sublist]
        except:
            print("Warning: Could not load auto prompt file")
            self.auto_prompt = {}
            self.merge_prompt = []

    def generate_song(self, lyric, description=None, prompt_audio_path=None, auto_prompt_type_select=None, 
                     generate_type="mixed", cfg_coef=1.5, temperature=0.9, top_k=50, progress=gr.Progress()):
        """ç”Ÿæˆæ­Œæ›²çš„ä¸»è¦å‡½æ•°"""
        try:
            progress(0.1, "å‡†å¤‡ç”Ÿæˆå‚æ•°...")
            
            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
            save_dir = f"tmp_output_{int(time.time())}"
            os.makedirs(save_dir, exist_ok=True)
            os.makedirs(f"{save_dir}/audios", exist_ok=True)
            
            # å‡†å¤‡ç”Ÿæˆé¡¹ç›®
            item = {
                'idx': f'webui_{int(time.time())}',
                'gt_lyric': lyric.replace("  ", " "),
                'descriptions': description if description else None
            }
            
            progress(0.2, "å¤„ç†éŸ³é¢‘æç¤º...")
            
            # å¤„ç†éŸ³é¢‘æç¤º
            if prompt_audio_path and os.path.exists(prompt_audio_path):
                with torch.no_grad():
                    pmt_wav, vocal_wav, bgm_wav = self.separator.run(prompt_audio_path)
                    
                # ç¼–ç éŸ³é¢‘
                audio_tokenizer = builders.get_audio_tokenizer_model(self.cfg.audio_tokenizer_checkpoint, self.cfg)
                audio_tokenizer = audio_tokenizer.eval().cuda()
                
                if pmt_wav.dim() == 2:
                    pmt_wav = pmt_wav[None]
                if vocal_wav.dim() == 2:
                    vocal_wav = vocal_wav[None]
                if bgm_wav.dim() == 2:
                    bgm_wav = bgm_wav[None]
                    
                with torch.no_grad():
                    pmt_wav, _ = audio_tokenizer.encode(pmt_wav.cuda())
                    
                del audio_tokenizer
                torch.cuda.empty_cache()
                
                # åˆ†ç¦»éŸ³é¢‘ç¼–ç 
                if "audio_tokenizer_checkpoint_sep" in self.cfg.keys():
                    seperate_tokenizer = builders.get_audio_tokenizer_model(self.cfg.audio_tokenizer_checkpoint_sep, self.cfg)
                    seperate_tokenizer = seperate_tokenizer.eval().cuda()
                    with torch.no_grad():
                        vocal_wav, bgm_wav = seperate_tokenizer.encode(vocal_wav.cuda(), bgm_wav.cuda())
                    del seperate_tokenizer
                    torch.cuda.empty_cache()
                
                melody_is_wav = False
                
            elif auto_prompt_type_select and auto_prompt_type_select != "None" and self.auto_prompt:
                if auto_prompt_type_select == "Auto": 
                    prompt_token = self.merge_prompt[np.random.randint(0, len(self.merge_prompt))]
                else:
                    prompt_token = self.auto_prompt[auto_prompt_type_select][np.random.randint(0, len(self.auto_prompt[auto_prompt_type_select]))]
                pmt_wav = prompt_token[:,[0],:]
                vocal_wav = prompt_token[:,[1],:]
                bgm_wav = prompt_token[:,[2],:]
                melody_is_wav = False
            else:
                pmt_wav = None
                vocal_wav = None
                bgm_wav = None
                melody_is_wav = True

            progress(0.4, "åŠ è½½è¯­è¨€æ¨¡å‹...")
            
            # åŠ è½½è¯­è¨€æ¨¡å‹
            audiolm = builders.get_lm_model(self.cfg)
            checkpoint = torch.load(self.pt_path, map_location='cpu')
            audiolm_state_dict = {k.replace('audiolm.', ''): v for k, v in checkpoint.items() if k.startswith('audiolm')}
            audiolm.load_state_dict(audiolm_state_dict, strict=False)
            audiolm = audiolm.eval().cuda().to(torch.float16)

            model = CodecLM(
                name="webui_tmp",
                lm=audiolm,
                audiotokenizer=None,
                max_duration=self.max_duration,
                seperate_tokenizer=None,
            )
            
            # è®¾ç½®ç”Ÿæˆå‚æ•°
            model.set_generation_params(
                duration=self.max_duration, 
                extend_stride=5, 
                temperature=temperature, 
                cfg_coef=cfg_coef,
                top_k=top_k, 
                top_p=0.0, 
                record_tokens=True, 
                record_window=50
            )

            progress(0.6, "ç”ŸæˆéŸ³ä¹...")
            
            # ç”Ÿæˆ
            generate_inp = {
                'lyrics': [lyric.replace("  ", " ")],
                'descriptions': [description],
                'melody_wavs': pmt_wav,
                'vocal_wavs': vocal_wav,
                'bgm_wavs': bgm_wav,
                'melody_is_wav': melody_is_wav,
            }
            
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                with torch.no_grad():
                    tokens = model.generate(**generate_inp, return_tokens=True)

            # æ¸…ç†å†…å­˜
            del model
            audiolm = audiolm.cpu()
            del audiolm
            del checkpoint
            gc.collect()
            torch.cuda.empty_cache()

            progress(0.8, "ç”ŸæˆéŸ³é¢‘...")
            
            # ç”ŸæˆéŸ³é¢‘
            seperate_tokenizer = builders.get_audio_tokenizer_model_cpu(self.cfg.audio_tokenizer_checkpoint_sep, self.cfg)
            device = "cuda:0"
            seperate_tokenizer.model.device = device
            seperate_tokenizer.model.vae = seperate_tokenizer.model.vae.to(device)
            seperate_tokenizer.model.model.device = torch.device(device)
            seperate_tokenizer.model.model = seperate_tokenizer.model.model.to(device)
            seperate_tokenizer = seperate_tokenizer.eval()

            model = CodecLM(
                name="webui_tmp",
                lm=None,
                audiotokenizer=None,
                max_duration=self.max_duration,
                seperate_tokenizer=seperate_tokenizer,
            )

            with torch.no_grad():
                if generate_type == 'separate':
                    wav_vocal = model.generate_audio(tokens, chunked=True, gen_type='vocal')
                    wav_bgm = model.generate_audio(tokens, chunked=True, gen_type='bgm')
                    wav_mixed = model.generate_audio(tokens, chunked=True, gen_type='mixed')
                    
                    # ä¿å­˜æ–‡ä»¶
                    output_files = {}
                    output_files['vocal'] = f"{save_dir}/audios/vocal.flac"
                    output_files['bgm'] = f"{save_dir}/audios/bgm.flac"
                    output_files['mixed'] = f"{save_dir}/audios/mixed.flac"
                    
                    torchaudio.save(output_files['vocal'], wav_vocal[0].cpu().float(), self.cfg.sample_rate)
                    torchaudio.save(output_files['bgm'], wav_bgm[0].cpu().float(), self.cfg.sample_rate)
                    torchaudio.save(output_files['mixed'], wav_mixed[0].cpu().float(), self.cfg.sample_rate)
                    
                    # è¿”å›æ··åˆéŸ³é¢‘
                    result_audio = (self.cfg.sample_rate, wav_mixed[0].cpu().float().numpy().T)
                    
                else:
                    wav_result = model.generate_audio(tokens, chunked=True, gen_type=generate_type)
                    output_file = f"{save_dir}/audios/result.flac"
                    torchaudio.save(output_file, wav_result[0].cpu().float(), self.cfg.sample_rate)
                    result_audio = (self.cfg.sample_rate, wav_result[0].cpu().float().numpy().T)

            torch.cuda.empty_cache()
            
            progress(1.0, "ç”Ÿæˆå®Œæˆ!")
            
            # åˆ›å»ºç»“æœä¿¡æ¯
            result_info = {
                "lyric": lyric,
                "description": description,
                "prompt_audio": prompt_audio_path,
                "auto_prompt_type": auto_prompt_type_select,
                "generate_type": generate_type,
                "cfg_coef": cfg_coef,
                "temperature": temperature,
                "top_k": top_k,
                "timestamp": datetime.now().isoformat(),
                "sample_rate": self.cfg.sample_rate
            }
            
            return result_audio, json.dumps(result_info, indent=2, ensure_ascii=False)
            
        except Exception as e:
            error_msg = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
            print(error_msg)
            return None, json.dumps({"error": error_msg}, ensure_ascii=False)


# å…¨å±€ç”Ÿæˆå™¨å®ä¾‹
generator = None

def initialize_generator(ckpt_path, use_flash_attn=True):
    """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
    global generator
    if generator is None:
        generator = SongGenerator(ckpt_path, use_flash_attn)
    return generator

def generate_song_wrapper(lyric, description, prompt_audio, auto_prompt_type_select, 
                         generate_type, cfg_coef, temperature, top_k, use_flash_attn, progress=gr.Progress()):
    """Gradio åŒ…è£…å‡½æ•°"""
    global generator
    
    if generator is None:
        return None, json.dumps({"error": "ç”Ÿæˆå™¨æœªåˆå§‹åŒ–"}, ensure_ascii=False)
    
    try:
        # å¦‚æœ flash_attn è®¾ç½®ä¸å½“å‰ä¸åŒï¼Œé‡æ–°åˆå§‹åŒ–ç”Ÿæˆå™¨
        current_flash_attn = getattr(generator.cfg.lm, 'use_flash_attn_2', getattr(generator.cfg.lm, 'use_flash_attn', False))
        if current_flash_attn != use_flash_attn:
            generator = None
            initialize_generator(CONFIG["CHECKPOINT_PATH"], use_flash_attn)
            print(f"Flash Attention è®¾ç½®å·²æ›´æ–°ä¸º: {'å¯ç”¨' if use_flash_attn else 'ç¦ç”¨'}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"outputs/webui_{timestamp}"
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ­Œæ›²
        result_audio, info = generator.generate_song(
            lyric=lyric,
            description=description,
            prompt_audio_path=prompt_audio,
            auto_prompt_type_select=auto_prompt_type_select,
            generate_type=generate_type,
            cfg_coef=cfg_coef,
            temperature=temperature,
            top_k=top_k,
            progress=progress
        )
        
        # å¦‚æœç”ŸæˆæˆåŠŸï¼Œä¿å­˜åˆ°è¾“å‡ºç›®å½•
        if result_audio is not None:
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            audio_filename = f"generated_{timestamp}.wav"
            audio_path = os.path.join(output_dir, audio_filename)
            
            # ä½¿ç”¨ torchaudio ä¿å­˜
            sample_rate, audio_data = result_audio
            audio_tensor = torch.from_numpy(audio_data.T).float()  # è½¬æ¢ä¸º (channels, samples) æ ¼å¼
            torchaudio.save(audio_path, audio_tensor, sample_rate)
            
            # ä¿å­˜ç”Ÿæˆä¿¡æ¯
            info_filename = f"info_{timestamp}.json"
            info_path = os.path.join(output_dir, info_filename)
            with open(info_path, 'w', encoding='utf-8') as f:
                f.write(info)
            
            # æ›´æ–°ä¿¡æ¯ï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„
            info_dict = json.loads(info)
            info_dict["output_directory"] = output_dir
            info_dict["audio_file"] = audio_path
            info_dict["info_file"] = info_path
            info_dict["flash_attn_enabled"] = use_flash_attn
            updated_info = json.dumps(info_dict, indent=2, ensure_ascii=False)
            
            return result_audio, updated_info
        
        return result_audio, info
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(error_msg)
        return None, json.dumps({"error": error_msg}, ensure_ascii=False)

# ç¤ºä¾‹æ­Œè¯
EXAMPLE_LYRICS = """[intro-short]

[verse]
å¤œæ™šçš„è¡—ç¯é—ªçƒ
æˆ‘æ¼«æ­¥åœ¨ç†Ÿæ‚‰çš„è§’è½
å›å¿†åƒæ½®æ°´èˆ¬æ¶Œæ¥
ä½ çš„ç¬‘å®¹å¦‚æ­¤æ¸…æ™°

[chorus]
åœ¨å¿ƒå¤´æ— æ³•æŠ¹å»
é‚£äº›æ›¾ç»çš„ç”œèœœ
å¦‚ä»Šåªå‰©æˆ‘ç‹¬è‡ªå›å¿†
éŸ³ä¹çš„èŠ‚å¥åœ¨å¥å“

[verse]
æ‰‹æœºå±å¹•äº®èµ·
æ˜¯ä½ å‘æ¥çš„æ¶ˆæ¯
ç®€å•çš„å‡ ä¸ªå­—
å´è®©æˆ‘æ³ªæµæ»¡é¢

[chorus]
å›å¿†çš„æ¸©åº¦è¿˜åœ¨
ä½ å´å·²ä¸åœ¨
æˆ‘çš„å¿ƒè¢«çˆ±å¡«æ»¡
å´åˆè¢«æ€å¿µåˆºç—›

[outro-short]"""

def create_gradio_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    # åŠ è½½ç¤ºä¾‹æ•°æ®
    examples_data = load_examples_from_jsonl()
    
    with gr.Blocks(title="SongGeneration WebUI", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸµ SongGeneration WebUI")
        gr.Markdown("åŸºäºè…¾è®¯ AI Lab çš„ SongGeneration æ¨¡å‹çš„ç½‘é¡µç•Œé¢ï¼Œæ”¯æŒ Flash Attention åŠ é€Ÿ")
        
        with gr.Row():
            with gr.Column(scale=1):
                # æ­Œè¯è¾“å…¥
                lyric_input = gr.Textbox(
                    label="æ­Œè¯ (Lyrics)",
                    lines=10,
                    max_lines=20,
                    value=EXAMPLE_LYRICS,
                    placeholder="è¯·è¾“å…¥æ­Œè¯...",
                    info="æ¯ä¸ªæ®µè½ä»£è¡¨ä¸€ä¸ªç»“æ„æ®µï¼Œä»¥ç»“æ„æ ‡ç­¾å¼€å§‹"
                )
                
                # æç¤ºè®¾ç½®
                with gr.Tabs():
                    with gr.Tab("æ–‡æœ¬æè¿°"):
                        description_input = gr.Textbox(
                            label="æ­Œæ›²æè¿° (å¯é€‰)",
                            placeholder="ä¾‹å¦‚: female, dark, pop, sad, piano and drums, the bpm is 125",
                            info="æè¿°æ­Œæ›²çš„æ€§åˆ«ã€éŸ³è‰²ã€é£æ ¼ã€æƒ…ç»ªã€ä¹å™¨å’Œ BPM"
                        )
                    
                    with gr.Tab("éŸ³é¢‘æç¤º"):
                        prompt_audio_input = gr.Audio(
                            label="æç¤ºéŸ³é¢‘ (å¯é€‰)",
                            type="filepath"
                        )
                    
                    with gr.Tab("è‡ªåŠ¨æç¤º"):
                        auto_prompt_select = gr.Dropdown(
                            choices=["None"] + auto_prompt_type,
                            value="Auto",
                            label="è‡ªåŠ¨æç¤ºç±»å‹",
                            info="é€‰æ‹©éŸ³ä¹é£æ ¼è¿›è¡Œè‡ªåŠ¨æç¤º"
                        )
                
                # ç”Ÿæˆè®¾ç½®
                with gr.Accordion("ç”Ÿæˆè®¾ç½®", open=False):
                    generate_type_input = gr.Radio(
                        choices=["mixed", "vocal", "bgm", "separate"],
                        value="mixed",
                        label="ç”Ÿæˆç±»å‹",
                        info="mixed: å®Œæ•´æ­Œæ›², vocal: äººå£°, bgm: èƒŒæ™¯éŸ³ä¹, separate: åˆ†ç¦»ç”Ÿæˆ"
                    )
                    
                    with gr.Row():
                        cfg_coef_input = gr.Slider(
                            minimum=0.1,
                            maximum=3.0,
                            step=0.1,
                            value=1.5,
                            label="CFG ç³»æ•°"
                        )
                        
                        temperature_input = gr.Slider(
                            minimum=0.1,
                            maximum=2.0,
                            step=0.1,
                            value=0.9,
                            label="æ¸©åº¦"
                        )
                    
                    with gr.Row():
                        top_k_input = gr.Slider(
                            minimum=1,
                            maximum=100,
                            step=1,
                            value=50,
                            label="Top-K"
                        )
                        
                        use_flash_attn_input = gr.Checkbox(
                            value=CONFIG.get("USE_FLASH_ATTN", True),
                            label="å¯ç”¨ Flash Attention",
                            info="æå‡ç”Ÿæˆé€Ÿåº¦ï¼Œéœ€è¦å…¼å®¹çš„ GPU"
                        )
                
                # ç”ŸæˆæŒ‰é’®
                generate_btn = gr.Button("ğŸµ ç”Ÿæˆæ­Œæ›²", variant="primary", size="lg")
            
            with gr.Column(scale=1):
                # è¾“å‡º
                output_audio = gr.Audio(
                    label="ç”Ÿæˆçš„éŸ³é¢‘",
                    type="numpy"
                )
                
                output_info = gr.JSON(
                    label="ç”Ÿæˆä¿¡æ¯"
                )
        
        # ç¤ºä¾‹åŒºåŸŸ
        with gr.Row():
            gr.Markdown("## ğŸ“š ç¤ºä¾‹åº“")
        
        with gr.Row():
            # åˆ›å»ºç¤ºä¾‹ç»„ä»¶
            example_inputs = [
                lyric_input,
                description_input, 
                prompt_audio_input,
                auto_prompt_select,
                generate_type_input,
                cfg_coef_input,
                temperature_input,
                top_k_input,
                use_flash_attn_input
            ]
            
            # åªä½¿ç”¨å‰9ä¸ªå…ƒç´ ä½œä¸ºç¤ºä¾‹è¾“å…¥ï¼ˆå»æ‰ç¤ºä¾‹åç§°ï¼‰
            examples_for_gradio = [example[:9] for example in examples_data]
            
            gr.Examples(
                examples=examples_for_gradio,
                inputs=example_inputs,
                label="ç‚¹å‡»ä¸‹æ–¹ç¤ºä¾‹å¿«é€Ÿå¡«å……å‚æ•°:",
                examples_per_page=5
            )
        
        # ç»‘å®šäº‹ä»¶
        generate_btn.click(
            fn=generate_song_wrapper,
            inputs=[
                lyric_input,
                description_input,
                prompt_audio_input,
                auto_prompt_select,
                generate_type_input,
                cfg_coef_input,
                temperature_input,
                top_k_input,
                use_flash_attn_input
            ],
            outputs=[output_audio, output_info]
        )
        
        # æ·»åŠ ä½¿ç”¨è¯´æ˜
        with gr.Accordion("ï¿½ ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            ### ğŸ¯ åŸºæœ¬ä½¿ç”¨
            1. **é€‰æ‹©ç¤ºä¾‹**: ç‚¹å‡»ä¸‹æ–¹ç¤ºä¾‹åº“ä¸­çš„ä»»ä¸€ç¤ºä¾‹å¿«é€Ÿå¡«å……å‚æ•°
            2. **ä¿®æ”¹å‚æ•°**: æ ¹æ®éœ€è¦è°ƒæ•´æ­Œè¯ã€æè¿°å’Œç”Ÿæˆè®¾ç½®
            3. **ç”Ÿæˆæ­Œæ›²**: ç‚¹å‡»"ç”Ÿæˆæ­Œæ›²"æŒ‰é’®å¼€å§‹åˆ›ä½œ
            4. **é¢„è§ˆæ’­æ”¾**: ç”Ÿæˆå®Œæˆåå¯åœ¨å³ä¾§ç›´æ¥æ’­æ”¾é¢„è§ˆ
            
            ### ğŸ“ æ­Œè¯æ ¼å¼
            - ä½¿ç”¨ç»“æ„æ ‡ç­¾ï¼š`[intro-short]`, `[intro-medium]`, `[intro-long]`
            - ä¸»æ­Œï¼š`[verse]`ï¼Œå‰¯æ­Œï¼š`[chorus]`ï¼Œæ¡¥æ®µï¼š`[bridge]`
            - é—´å¥ï¼š`[inst-short]`, `[inst-medium]`, `[inst-long]`
            - ç»“å°¾ï¼š`[outro-short]`, `[outro-medium]`, `[outro-long]`
            - é™éŸ³ï¼š`[silence]`
            
            ### ğŸ›ï¸ æç¤ºæ–¹å¼
            - **æ–‡æœ¬æè¿°**: æè¿°æ€§åˆ«ã€é£æ ¼ã€æƒ…ç»ªã€ä¹å™¨ç­‰
            - **éŸ³é¢‘æç¤º**: ä¸Šä¼ å‚è€ƒéŸ³é¢‘æ–‡ä»¶
            - **è‡ªåŠ¨æç¤º**: é€‰æ‹©é¢„è®¾çš„éŸ³ä¹é£æ ¼
            
            ### ğŸµ ç”Ÿæˆç±»å‹
            - **mixed**: å®Œæ•´æ­Œæ›²ï¼ˆæ¨èï¼‰
            - **vocal**: åªç”Ÿæˆäººå£°éƒ¨åˆ†  
            - **bgm**: åªç”ŸæˆèƒŒæ™¯éŸ³ä¹
            - **separate**: åŒæ—¶ç”Ÿæˆäººå£°ã€èƒŒæ™¯éŸ³ä¹å’Œæ··åˆç‰ˆæœ¬
            
            ### âš¡ æ€§èƒ½ä¼˜åŒ–
            - **Flash Attention**: ç°å·²æ”¯æŒå¹¶é»˜è®¤å¯ç”¨ï¼Œæ˜¾è‘—æå‡ç”Ÿæˆé€Ÿåº¦
            - **ä½å†…å­˜æ¨¡å¼**: é€‚ç”¨äºæ˜¾å­˜è¾ƒå°çš„è®¾å¤‡
            - **CUDA åŠ é€Ÿ**: è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ GPU åŠ é€Ÿ
            - **åŠ¨æ€å†…å­˜ç®¡ç†**: è‡ªåŠ¨æ¸…ç†æ˜¾å­˜ï¼Œé¿å…å†…å­˜æº¢å‡º
            
            ### ğŸ”§ é«˜çº§è®¾ç½®
            - **CFG ç³»æ•°**: æ§åˆ¶ç”Ÿæˆä¸æç¤ºçš„ç¬¦åˆåº¦ (1.0-3.0)
            - **æ¸©åº¦**: æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ (0.1-2.0ï¼Œè¶Šä½è¶Šä¿å®ˆ)
            - **Top-K**: é™åˆ¶å€™é€‰è¯æ±‡æ•°é‡ (1-100)
            - **Flash Attention**: å¯åŠ¨æ€å¼€å…³ï¼Œé‡å¯æ—¶ç”Ÿæ•ˆ
            """)
    
    return demo

def main():
    # ä»é…ç½®ä¸­è¯»å–å‚æ•°
    CHECKPOINT_PATH = CONFIG["CHECKPOINT_PATH"]
    USE_FLASH_ATTN = CONFIG["USE_FLASH_ATTN"]
    LOW_MEMORY_MODE = CONFIG["LOW_MEMORY_MODE"]
    HOST = CONFIG["HOST"]
    PORT = CONFIG["PORT"]
    SHARE = CONFIG["SHARE"]
    AUTO_OPEN_BROWSER = CONFIG["AUTO_OPEN_BROWSER"]
    
    # æ£€æŸ¥æ£€æŸ¥ç‚¹è·¯å¾„
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"é”™è¯¯: æ£€æŸ¥ç‚¹è·¯å¾„ä¸å­˜åœ¨: {CHECKPOINT_PATH}")
        print("è¯·ä¿®æ”¹ webui_app.py ä¸­çš„ CONFIG['CHECKPOINT_PATH'] å˜é‡")
        print("ä¾‹å¦‚: CONFIG['CHECKPOINT_PATH'] = 'ckpt/songgeneration_base'")
        sys.exit(1)
    
    if not os.path.exists(os.path.join(CHECKPOINT_PATH, 'config.yaml')):
        print(f"é”™è¯¯: åœ¨ {CHECKPOINT_PATH} ä¸­æ‰¾ä¸åˆ° config.yaml")
        sys.exit(1)
    
    if not os.path.exists(os.path.join(CHECKPOINT_PATH, 'model.pt')):
        print(f"é”™è¯¯: åœ¨ {CHECKPOINT_PATH} ä¸­æ‰¾ä¸åˆ° model.pt")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("outputs", exist_ok=True)
    
    print("=== SongGeneration WebUI ===")
    print(f"æ£€æŸ¥ç‚¹è·¯å¾„: {CHECKPOINT_PATH}")
    print(f"Flash Attention: {'å¯ç”¨' if USE_FLASH_ATTN else 'ç¦ç”¨'}")
    print(f"ä½å†…å­˜æ¨¡å¼: {'å¯ç”¨' if LOW_MEMORY_MODE else 'ç¦ç”¨'}")
    print(f"å¯åŠ¨åœ°å€: http://{HOST}:{PORT}")
    print(f"è¾“å‡ºç›®å½•: outputs/")
    if SHARE:
        print("å…¬å¼€é“¾æ¥: å¯ç”¨")
    print("============================")
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(int(time.time()))
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    try:
        print("æ­£åœ¨åˆå§‹åŒ–ç”Ÿæˆå™¨...")
        initialize_generator(CHECKPOINT_PATH, USE_FLASH_ATTN)
        print("ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ!")
    except Exception as e:
        print(f"ç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥:")
        print("1. æ£€æŸ¥ç‚¹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. æ˜¯å¦å®‰è£…äº†æ‰€æœ‰ä¾èµ–")
        print("3. æ˜¯å¦æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜")
        print("4. Flash Attention æ˜¯å¦æ­£ç¡®å®‰è£…")
        sys.exit(1)
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_gradio_interface()
    
    demo.launch(
        server_name=HOST,
        server_port=PORT,
        share=SHARE,
        inbrowser=AUTO_OPEN_BROWSER
    )

if __name__ == "__main__":
    main()
