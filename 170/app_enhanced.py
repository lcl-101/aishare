# =============================================================================
# HYPIR å¢å¼ºç‰ˆ Gradio WebUI
# =============================================================================
# 
# ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„ HYPIR å›¾åƒå¢å¼º Web ç•Œé¢
# 
# åŠŸèƒ½ç‰¹æ€§:
# - å†…ç½®é…ç½®ï¼ˆæ— éœ€å¤–éƒ¨é…ç½®æ–‡ä»¶ï¼‰
# - ç¤ºä¾‹å›¾ç‰‡å’Œæç¤ºè¯åŠ è½½
# - å¢å¼ºçš„ç”¨æˆ·ç•Œé¢å’Œå‚æ•°æ§åˆ¶
# - æœ¬åœ°æ¨¡å‹æ”¯æŒ
# 
# ä½¿ç”¨æ–¹æ³•:
#   python app_enhanced.py
# 
# ç¯å¢ƒè¦æ±‚:
# - æ¨¡å‹æ–‡ä»¶åœ¨ checkpoints/ ç›®å½•ä¸­:
#   - checkpoints/stable-diffusion-2-1-base/
#   - checkpoints/HYPIR/HYPIR_sd2.pth
# - ç¤ºä¾‹æ–‡ä»¶åœ¨ examples/ ç›®å½•ä¸­ï¼ˆå¯é€‰ï¼‰:
#   - examples/lq/ (ä½è´¨é‡å›¾ç‰‡)  
#   - examples/prompt/ (æ–‡æœ¬æç¤ºè¯)
# 
# =============================================================================

import random
import os
from pathlib import Path

import gradio as gr
import torchvision.transforms as transforms
from accelerate.utils import set_seed
from dotenv import load_dotenv
from PIL import Image

from HYPIR.enhancer.sd2 import SD2Enhancer
from HYPIR.utils.captioner import GPTCaptioner

print("ğŸš€ HYPIR Enhanced WebUI - Starting...")
print("=" * 50)


# =============================================================================
# é…ç½® - æ‰€æœ‰è®¾ç½®éƒ½åœ¨è¿™é‡Œ
# =============================================================================

# æ¨¡å‹é…ç½®
CONFIG = {
    "base_model_type": "sd2",
    "base_model_path": "checkpoints/stable-diffusion-2-1-base",
    "weight_path": "checkpoints/HYPIR/HYPIR_sd2.pth",
    "lora_modules": ["to_k", "to_q", "to_v", "to_out.0", "conv", "conv1", "conv2", 
                     "conv_shortcut", "conv_out", "proj_in", "proj_out", "ff.net.2", "ff.net.0.proj"],
    "lora_rank": 256,
    "model_t": 200,
    "coeff_t": 200,
    "device": "cuda",
}

# WebUI é…ç½®
WEBUI_CONFIG = {
    "port": 7860,
    "server_name": "0.0.0.0",  # ä½¿ç”¨ "127.0.0.1" åªå…è®¸æœ¬åœ°è®¿é—®
    "share": False,
    "max_size": None,  # è®¾ç½®ä¸º (å®½åº¦, é«˜åº¦) å…ƒç»„æ¥é™åˆ¶è¾“å‡ºå°ºå¯¸ï¼Œä¾‹å¦‚ (2048, 2048)
    "enable_gpt_caption": False,  # å¦‚æœé…ç½®äº† GPT APIï¼Œè®¾ç½®ä¸º True
}

# é»˜è®¤å‚æ•°
DEFAULT_PARAMS = {
    "upscale": 4,
    "patch_size": 512,
    "stride": 256,
    "seed": -1,
}

# =============================================================================
# åˆå§‹åŒ–
# =============================================================================

load_dotenv()
error_image = Image.open(os.path.join("assets", "gradio_error_img.png"))

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
def check_model_files():
    """æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    base_model_path = Path(CONFIG["base_model_path"])
    weight_path = Path(CONFIG["weight_path"])
    
    print("ğŸ“‚ æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
    
    if not base_model_path.exists():
        print(f"âŒ åŸºç¡€æ¨¡å‹æœªæ‰¾åˆ°: {base_model_path}")
        print("è¯·ä¸‹è½½ Stable Diffusion 2.1 åŸºç¡€æ¨¡å‹åˆ° checkpoints/stable-diffusion-2-1-base/")
        raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹æœªæ‰¾åˆ°: {base_model_path}")
    
    if not weight_path.exists():
        print(f"âŒ HYPIR æƒé‡æœªæ‰¾åˆ°: {weight_path}")
        print("è¯·ä¸‹è½½ HYPIR æ¨¡å‹æƒé‡åˆ° checkpoints/HYPIR/HYPIR_sd2.pth")
        raise FileNotFoundError(f"HYPIR æƒé‡æœªæ‰¾åˆ°: {weight_path}")
    
    print(f"âœ… åŸºç¡€æ¨¡å‹å·²æ‰¾åˆ°: {base_model_path}")
    print(f"âœ… HYPIR æƒé‡å·²æ‰¾åˆ°: {weight_path}")
    
    # æ£€æŸ¥ç¤ºä¾‹
    examples_dir = Path("examples")
    if (examples_dir / "lq").exists() and (examples_dir / "prompt").exists():
        example_count = len(list((examples_dir / "lq").glob("*.png")))
        print(f"ğŸ“¸ æ‰¾åˆ° {example_count} ä¸ªç¤ºä¾‹å›¾ç‰‡")
    else:
        print("âš ï¸  ç¤ºä¾‹ç›®å½•æœªæ‰¾åˆ° - ç¤ºä¾‹ç”»å»Šå°†ä¸ºç©º")

print("ğŸ” è¿è¡Œé¢„æ£€æŸ¥...")
check_model_files()

# è®¾ç½®æœ€å¤§å°ºå¯¸é™åˆ¶
max_size = WEBUI_CONFIG["max_size"]
if max_size is not None:
    print(f"æœ€å¤§å°ºå¯¸è®¾ç½®ä¸º {max_size}ï¼Œæœ€å¤§åƒç´ æ•°: {max_size[0] * max_size[1]}")

# å¦‚æœå¯ç”¨åˆ™è®¾ç½® GPT å­—å¹•ç”Ÿæˆå™¨
captioner = None
if WEBUI_CONFIG["enable_gpt_caption"]:
    if (
        "GPT_API_KEY" not in os.environ
        or "GPT_BASE_URL" not in os.environ
        or "GPT_MODEL" not in os.environ
    ):
        print("è­¦å‘Š: GPT å­—å¹•åŠŸèƒ½å·²å¯ç”¨ä½†ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® GPT_API_KEY, GPT_BASE_URL å’Œ GPT_MODELã€‚")
        WEBUI_CONFIG["enable_gpt_caption"] = False
    else:
        captioner = GPTCaptioner(
            api_key=os.getenv("GPT_API_KEY"),
            base_url=os.getenv("GPT_BASE_URL"),
            model=os.getenv("GPT_MODEL"),
        )

to_tensor = transforms.ToTensor()

# åˆå§‹åŒ–æ¨¡å‹
print("ğŸš€ åˆå§‹åŒ– HYPIR æ¨¡å‹...")
print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {CONFIG['device']}")
print(f"ğŸ§  æ¨¡å‹é…ç½®: {CONFIG['base_model_type']}")
print(f"ğŸ“‚ åŸºç¡€æ¨¡å‹: {CONFIG['base_model_path']}")
print(f"âš¡ LoRA ç§©: {CONFIG['lora_rank']}")

model = SD2Enhancer(
    base_model_path=CONFIG["base_model_path"],
    weight_path=CONFIG["weight_path"],
    lora_modules=CONFIG["lora_modules"],
    lora_rank=CONFIG["lora_rank"],
    model_t=CONFIG["model_t"],
    coeff_t=CONFIG["coeff_t"],
    device=CONFIG["device"],
)

print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
model.init_models()
print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")


def load_examples():
    """åŠ è½½ç¤ºä¾‹å›¾ç‰‡å’Œå¯¹åº”çš„æç¤ºè¯"""
    examples_dir = Path("examples")
    lq_dir = examples_dir / "lq"
    prompt_dir = examples_dir / "prompt"
    
    examples = []
    if lq_dir.exists() and prompt_dir.exists():
        for img_file in sorted(lq_dir.glob("*.png")):
            prompt_file = prompt_dir / f"{img_file.stem}.txt"
            if prompt_file.exists():
                with open(prompt_file, 'r', encoding='utf-8') as f:
                    prompt = f.read().strip()
                examples.append({
                    "name": img_file.stem,
                    "image_path": str(img_file),
                    "prompt": prompt
                })
    return examples


def get_example_list():
    """è·å–ç¤ºä¾‹åç§°åˆ—è¡¨ç”¨äºä¸‹æ‹‰èœå•"""
    examples = load_examples()
    return [ex["name"] for ex in examples]


def load_example(example_name):
    """åŠ è½½é€‰ä¸­çš„ç¤ºä¾‹å›¾ç‰‡å’Œæç¤ºè¯"""
    if not example_name:
        return None, ""
    
    examples = load_examples()
    for ex in examples:
        if ex["name"] == example_name:
            try:
                image = Image.open(ex["image_path"])
                return image, ex["prompt"]
            except Exception as e:
                print(f"åŠ è½½ç¤ºä¾‹ {example_name} æ—¶å‡ºé”™: {e}")
                return None, ""
    return None, ""


def process(
    image,
    prompt,
    upscale,
    patch_size,
    stride,
    seed,
    progress=gr.Progress(track_tqdm=True),
):
    if seed == -1:
        seed = random.randint(0, 2**32 - 1)
    set_seed(seed)
    image = image.convert("RGB")
    # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸
    if max_size is not None:
        out_w, out_h = tuple(int(x * upscale) for x in image.size)
        if out_w * out_h > max_size[0] * max_size[1]:
            return error_image, (
                "å¤±è´¥: è¯·æ±‚çš„åˆ†è¾¨ç‡è¶…è¿‡æœ€å¤§åƒç´ é™åˆ¶ã€‚"
                f"æ‚¨è¯·æ±‚çš„åˆ†è¾¨ç‡æ˜¯ ({out_h}, {out_w})ã€‚"
                f"æœ€å¤§å…è®¸çš„åƒç´ æ•°æ˜¯ {max_size[0]} x {max_size[1]} "
                f"= {max_size[0] * max_size[1]} :("
            )
    if prompt == "auto":
        if WEBUI_CONFIG["enable_gpt_caption"] and captioner is not None:
            prompt = captioner(image)
        else:
            return error_image, "å¤±è´¥: æ­¤ Gradio æœªå¯ç”¨ GPT å­—å¹•æ”¯æŒ :("

    image_tensor = to_tensor(image).unsqueeze(0)
    try:
        pil_image = model.enhance(
            lq=image_tensor,
            prompt=prompt,
            upscale=upscale,
            patch_size=patch_size,
            stride=stride,
            return_type="pil",
        )[0]
    except Exception as e:
        return error_image, f"å¤±è´¥: {e} :("

    return pil_image, f"æˆåŠŸ! :)\nä½¿ç”¨çš„æç¤ºè¯: {prompt}\nä½¿ç”¨çš„ç§å­: {seed}"


MARKDOWN = """
## HYPIR: åˆ©ç”¨æ‰©æ•£äº§ç”Ÿçš„åˆ†æ•°å…ˆéªŒè¿›è¡Œå›¾åƒæ¢å¤

[GitHub](https://github.com/XPixelGroup/HYPIR) | [è®ºæ–‡](TODO) | [é¡¹ç›®é¡µé¢](TODO)

å¦‚æœ HYPIR å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·å¸®åŠ©ä¸º GitHub ä»“åº“ç‚¹æ˜Ÿã€‚è°¢è°¢ï¼

### ä½¿ç”¨è¯´æ˜:
1. **ä¸Šä¼ æ‚¨è‡ªå·±çš„å›¾ç‰‡** æˆ– **ä»ä¸‹é¢çš„ç¤ºä¾‹ä¸­é€‰æ‹©**
2. **è¾“å…¥æè¿°å›¾ç‰‡çš„æç¤ºè¯** æˆ–ä½¿ç”¨ "auto" è¿›è¡Œ GPT ç”Ÿæˆçš„å­—å¹•
3. **æ ¹æ®éœ€è¦è°ƒæ•´å‚æ•°**ï¼ˆæ”¾å¤§å€æ•°ã€è¡¥ä¸å¤§å°ã€æ­¥é•¿ï¼‰
4. **ç‚¹å‡»è¿è¡Œ** æ¥å¢å¼ºæ‚¨çš„å›¾ç‰‡
"""

# è‡ªå®šä¹‰ CSS æ ·å¼
css = """
.example-gallery {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
    gap: 10px;
    padding: 10px;
}
.example-item {
    border: 1px solid #ddd;
    border-radius: 8px;
    padding: 8px;
    text-align: center;
}
.example-item img {
    max-width: 100%;
    height: auto;
    border-radius: 4px;
}
"""

block = gr.Blocks(css=css).queue()
with block:
    gr.Markdown(MARKDOWN)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### è¾“å…¥")
            
            # ç¤ºä¾‹é€‰æ‹©
            with gr.Group():
                gr.Markdown("**åŠ è½½ç¤ºä¾‹:**")
                example_dropdown = gr.Dropdown(
                    choices=get_example_list(),
                    label="é€‰æ‹©ç¤ºä¾‹",
                    value=None,
                    interactive=True
                )
                load_example_btn = gr.Button("åŠ è½½é€‰ä¸­çš„ç¤ºä¾‹", variant="secondary")
            
            # å›¾ç‰‡è¾“å…¥
            image = gr.Image(type="pil", label="è¾“å…¥å›¾ç‰‡")
            
            # æç¤ºè¯è¾“å…¥
            prompt = gr.Textbox(
                label=(
                    "æç¤ºè¯ (è¾“å…¥ 'auto' ä½¿ç”¨ GPT ç”Ÿæˆçš„å­—å¹•)"
                    if WEBUI_CONFIG["enable_gpt_caption"] else "æç¤ºè¯"
                ),
                placeholder="è¾“å…¥å›¾ç‰‡çš„æè¿°...",
                lines=3
            )
            
            # å‚æ•°è®¾ç½®
            with gr.Group():
                gr.Markdown("**å‚æ•°è®¾ç½®:**")
                upscale = gr.Slider(
                    minimum=1, 
                    maximum=8, 
                    value=DEFAULT_PARAMS["upscale"], 
                    label="æ”¾å¤§å€æ•°", 
                    step=1,
                    info="å›¾ç‰‡æ”¾å¤§çš„å€æ•°"
                )
                patch_size = gr.Slider(
                    minimum=512, 
                    maximum=1024, 
                    value=DEFAULT_PARAMS["patch_size"], 
                    label="è¡¥ä¸å¤§å°", 
                    step=128,
                    info="å¤„ç†è¡¥ä¸çš„å¤§å°"
                )
                stride = gr.Slider(
                    minimum=256, 
                    maximum=1024, 
                    value=DEFAULT_PARAMS["stride"], 
                    label="è¡¥ä¸æ­¥é•¿", 
                    step=128,
                    info="è¡¥ä¸ä¹‹é—´çš„æ­¥é•¿"
                )
                seed = gr.Number(
                    label="éšæœºç§å­", 
                    value=DEFAULT_PARAMS["seed"],
                    info="éšæœºç§å­ (-1 ä¸ºéšæœº)"
                )
            
            run = gr.Button(value="ğŸš€ å¢å¼ºå›¾ç‰‡", variant="primary", size="lg")
        
        with gr.Column(scale=1):
            gr.Markdown("### è¾“å‡º")
            result = gr.Image(type="pil", format="png", label="å¢å¼ºåçš„å›¾ç‰‡")
            status = gr.Textbox(label="çŠ¶æ€", interactive=False, lines=3)
    
    # ç¤ºä¾‹ç”»å»Š
    with gr.Row():
        gr.Markdown("### ğŸ“¸ ç¤ºä¾‹ç”»å»Š")
    
    # åˆ›å»ºç¤ºä¾‹ç”»å»Š
    examples = load_examples()
    if examples:
        with gr.Row():
            for i in range(0, len(examples), 3):  # æ¯è¡Œæ˜¾ç¤º3ä¸ªç¤ºä¾‹
                with gr.Column():
                    for j in range(3):
                        if i + j < len(examples):
                            ex = examples[i + j]
                            with gr.Group():
                                gr.Image(
                                    value=ex["image_path"],
                                    label=ex["name"],
                                    show_label=True,
                                    interactive=False,
                                    height=150
                                )
                                gr.Textbox(
                                    value=ex["prompt"][:100] + "..." if len(ex["prompt"]) > 100 else ex["prompt"],
                                    label="æç¤ºè¯é¢„è§ˆ",
                                    interactive=False,
                                    lines=2,
                                    max_lines=2
                                )
    
    # äº‹ä»¶å¤„ç†å™¨
    def load_example_handler(example_name):
        return load_example(example_name)
    
    load_example_btn.click(
        fn=load_example_handler,
        inputs=[example_dropdown],
        outputs=[image, prompt]
    )
    
    run.click(
        fn=process,
        inputs=[image, prompt, upscale, patch_size, stride, seed],
        outputs=[result, status],
    )

if __name__ == "__main__":
    print("ğŸŒ å¯åŠ¨ Gradio ç•Œé¢...")
    print(f"ğŸ”— è®¿é—®åœ°å€: http://{WEBUI_CONFIG['server_name']}:{WEBUI_CONFIG['port']}")
    print("ğŸ“¸ ç¤ºä¾‹å·²ä» examples/ ç›®å½•åŠ è½½")
    print("ğŸ‰ å‡†å¤‡å¢å¼ºå›¾ç‰‡!")
    
    block.launch(
        server_name=WEBUI_CONFIG["server_name"], 
        server_port=WEBUI_CONFIG["port"],
        share=WEBUI_CONFIG["share"]
    )
