"""
EgoX Gradio Web åº”ç”¨ç¨‹åº
åŸºäº Wan2.1 çš„ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ç”Ÿæˆ Demo
"""

import os
import json
import random
import tempfile
import shutil
from pathlib import Path
from typing import Optional

import cv2
import gradio as gr
import numpy as np
import torch

# å…¨å±€å˜é‡å­˜å‚¨ pipeline
PIPE = None
MODEL_LOADED = False

# é»˜è®¤è·¯å¾„é…ç½®
DEFAULT_MODEL_PATH = "./checkpoints/Wan2.1-I2V-14B-480P-Diffusers"
DEFAULT_LORA_PATH = "./checkpoints/EgoX/pytorch_lora_weights.safetensors"
ITW_META_PATH = "./example/in_the_wild/meta.json"
EGO4D_META_PATH = "./example/egoexo4D/meta.json"


def set_seed(seed: Optional[int]) -> None:
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    if seed is None:
        return
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def load_model(model_path: str, lora_path: str):
    """åŠ è½½æ¨¡å‹"""
    global PIPE, MODEL_LOADED
    
    if MODEL_LOADED:
        return "âœ… æ¨¡å‹å·²åŠ è½½"
    
    try:
        from transformers import CLIPVisionModel
        from core.finetune.models.wan_i2v.custom_transformer import WanTransformer3DModel_GGA as WanTransformer3DModel
        from core.finetune.models.wan_i2v.sft_trainer import WanWidthConcatImageToVideoPipeline
        
        dtype = torch.bfloat16
        transformer_path = os.path.join(model_path, 'transformer')
        
        print("æ­£åœ¨åŠ è½½ Transformer...")
        transformer = WanTransformer3DModel.from_pretrained(transformer_path, torch_dtype=dtype)
        
        print("æ­£åœ¨åŠ è½½ Image Encoder...")
        image_encoder = CLIPVisionModel.from_pretrained(model_path, subfolder="image_encoder", torch_dtype=torch.float32)
        
        print("æ­£åœ¨åˆ›å»º Pipeline...")
        PIPE = WanWidthConcatImageToVideoPipeline.from_pretrained(
            model_path, 
            image_encoder=image_encoder, 
            transformer=transformer, 
            torch_dtype=dtype
        )
        
        if lora_path and os.path.exists(lora_path):
            print("æ­£åœ¨åŠ è½½ LoRA æƒé‡...")
            PIPE.load_lora_weights(lora_path, weight_name="pytorch_lora_weights.safetensors")
            PIPE.fuse_lora(components=["transformer"], lora_scale=1.0)
        
        PIPE.to("cuda")
        MODEL_LOADED = True
        
        return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼"
    except Exception as e:
        return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"


def load_meta_data(meta_file: str):
    """åŠ è½½å…ƒæ•°æ®æ–‡ä»¶"""
    with open(meta_file, 'r') as f:
        meta_data = json.load(f)
    return meta_data['test_datasets']


def get_example_choices(meta_file: str):
    """è·å–ç¤ºä¾‹é€‰é¡¹åˆ—è¡¨"""
    try:
        meta_data = load_meta_data(meta_file)
        choices = []
        for i, meta in enumerate(meta_data):
            exo_path = meta['exo_path']
            take_name = exo_path.split('/')[-2]
            choices.append(f"{i}: {take_name}")
        return choices
    except:
        return []


def compute_gga_attention(meta, is_in_the_wild: bool = False):
    """è®¡ç®— GGA æ³¨æ„åŠ›å›¾"""
    from core.finetune.datasets.utils import iproj_disp
    
    device = 'cpu'
    C, F, H, W = 16, 13, 56, 154
    exo_H, exo_W = H, W - H
    W = H
    
    # åŠ è½½æ·±åº¦å›¾
    take_name = meta['exo_path'].split('/')[-2]
    depth_root = "/".join(meta['exo_path'].split('/')[:3])
    depth_map_path = Path(os.path.join(depth_root, 'depth_maps', take_name))
    
    depth_maps = []
    for depth_map_file in sorted(depth_map_path.glob("*.npy")):
        depth_map = np.load(depth_map_file)
        depth_maps.append(torch.from_numpy(depth_map).unsqueeze(0))
    depth_maps = torch.cat(depth_maps, dim=0)
    
    # è·å–ç›¸æœºå‚æ•°
    ego_intrinsic = torch.tensor(meta['ego_intrinsics'])
    ego_extrinsic = torch.tensor(meta['ego_extrinsics'])
    camera_extrinsic = torch.tensor(meta['camera_extrinsics'])
    camera_intrinsic = torch.tensor(meta['camera_intrinsics'])
    
    # å¤„ç†å¤–å‚çŸ©é˜µ
    if ego_extrinsic.shape[1] == 3 and ego_extrinsic.shape[2] == 4:
        ego_extrinsic = torch.cat([
            ego_extrinsic, 
            torch.tensor([[[0, 0, 0, 1]]], dtype=ego_extrinsic.dtype).expand(ego_extrinsic.shape[0], -1, -1)
        ], dim=1)
    if camera_extrinsic.shape == (3, 4):
        camera_extrinsic = torch.cat([
            torch.tensor(camera_extrinsic, dtype=ego_extrinsic.dtype), 
            torch.tensor([[0, 0, 0, 1]], dtype=ego_extrinsic.dtype)
        ], dim=0)
    
    # ç¼©æ”¾å†…å‚
    scale = 1/8
    scaled_intrinsic = ego_intrinsic.clone()
    scaled_intrinsic[0, 0] *= scale
    scaled_intrinsic[1, 1] *= scale
    scaled_intrinsic[0, 2] *= scale
    scaled_intrinsic[1, 2] *= scale
    
    # åˆ›å»ºåƒç´ åæ ‡
    ys, xs = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device))
    ones = torch.ones_like(xs)
    pixel_coords = torch.stack([xs, ys, ones], dim=-1).view(-1, 3).to(dtype=ego_intrinsic.dtype)
    
    pixel_coords_cv = pixel_coords[..., :2].cpu().numpy().reshape(-1, 1, 2).astype(np.float32)
    K = scaled_intrinsic.cpu().numpy().astype(np.float32)
    
    # Ego cam ç•¸å˜ç³»æ•° (Project Aria) 
    distortion_coeffs = np.array([[-0.02340373583137989, 0.09388021379709244, -0.06088035926222801, 
                                   0.0053304750472307205, 0.003342868760228157, -0.0006356257363222539,
                                   0.0005087381578050554, -0.0004747129278257489, -0.0011330085108056664,
                                   -0.00025734835071489215, 0.00009328465239377692, 0.00009424977179151028]])
    D = distortion_coeffs.astype(np.float32)
    normalized_points = cv2.undistortPoints(pixel_coords_cv, K, D, R=np.eye(3), P=np.eye(3))
    
    normalized_points = torch.from_numpy(normalized_points).squeeze(1).to(device)
    ones = torch.ones_like(normalized_points[..., :1])
    cam_rays_fish = torch.cat([normalized_points, ones], dim=-1)
    cam_rays = cam_rays_fish / torch.norm(cam_rays_fish, dim=-1, keepdim=True)
    cam_rays = cam_rays @ ego_extrinsic[::4, :3, :3]
    cam_rays = cam_rays.view(F, H, W, 3)
    
    # å¤„ç†ç›¸æœºå†…å‚
    height, width = depth_maps.shape[1], depth_maps.shape[2]
    cx = width / 2.0
    cy = height / 2.0
    camera_intrinsic_scale_y = cy / camera_intrinsic[1, 2]
    camera_intrinsic_scale_x = cx / camera_intrinsic[0, 2]
    camera_intrinsic[0, 0] = camera_intrinsic[0, 0] * camera_intrinsic_scale_x
    camera_intrinsic[1, 1] = camera_intrinsic[1, 1] * camera_intrinsic_scale_y
    camera_intrinsic[0, 2] = cx
    camera_intrinsic[1, 2] = cy
    
    camera_intrinsic_array = np.array([camera_intrinsic[0, 0], camera_intrinsic[1, 1], cx, cy])
    
    disp_v, disp_u = torch.meshgrid(
        torch.arange(depth_maps.shape[1], device=device).float(),
        torch.arange(depth_maps.shape[2], device=device).float(),
        indexing="ij",
    )
    
    disp = torch.ones_like(disp_v)
    pts, _, _ = iproj_disp(torch.from_numpy(camera_intrinsic_array), disp.cpu(), disp_u.cpu(), disp_v.cpu())
    
    if isinstance(pts, torch.Tensor):
        pts = pts.to(device)
    else:
        pts = torch.from_numpy(pts).to(device).float()
    
    rays = pts[..., :3]
    rays = rays / rays[..., 2:3]
    rays = rays.unsqueeze(0).expand(depth_maps.size(0), -1, -1, -1)
    camera_extrinsics_c2w = torch.linalg.inv(camera_extrinsic)
    
    pcd_camera = rays * depth_maps.unsqueeze(-1)
    point_map = pcd_camera.to(dtype=camera_extrinsics_c2w.dtype)
    point_map = torch.tensor(point_map)
    
    p_f, p_h, p_w, p_p = point_map.shape
    point_map_world = point_map.reshape(-1, 3)
    
    camera_extrinsics_c2w = torch.linalg.inv(camera_extrinsic)
    ones_point = torch.ones(point_map_world.shape[0], 1, device=point_map_world.device)
    point_map_world = torch.cat([point_map_world, ones_point], dim=-1)
    point_map_world = (camera_extrinsics_c2w @ point_map_world.T).T[..., :3]
    point_map = point_map_world.reshape(p_f, p_h, p_w, 3).permute(0, 3, 1, 2)
    
    point_map = point_map[:, :, (point_map.shape[2] - 448)//2:(point_map.shape[2] + 448)//2, 
                          (point_map.shape[3] - 784)//2:(point_map.shape[3] + 784)//2]
    point_map = torch.nn.functional.interpolate(point_map, size=(exo_H, exo_W), mode='bilinear', align_corners=False).permute(0, 2, 3, 1)
    
    ego_extrinsic_c2w = torch.linalg.inv(ego_extrinsic)
    cam_origins = ego_extrinsic_c2w[::4, :3, 3].unsqueeze(1).expand(-1, exo_H * exo_W, -1)
    cam_origins = cam_origins.view(F, exo_H, exo_W, 3)
    
    if point_map.size(0) != ego_extrinsic_c2w.size(0):
        min_size = min(point_map.size(0), ego_extrinsic_c2w.size(0))
        point_map = point_map[:min_size]
    
    point_vecs_per_frame = []
    for j in range(cam_origins.size(0)):
        point_vec = point_map[::4] - cam_origins[j].unsqueeze(0)
        point_vec = point_vec / torch.norm(point_vec, dim=-1, keepdim=True)
        point_vecs_per_frame.append(point_vec)
    point_vecs_per_frame = torch.stack(point_vecs_per_frame, dim=0)
    
    point_vecs = point_map[::4] - cam_origins
    point_vecs = point_vecs / torch.norm(point_vecs, dim=-1, keepdim=True)
    
    cam_rays = torch.rot90(cam_rays, k=-1, dims=[1, 2])
    
    attn_maps = torch.cat((point_vecs, cam_rays), dim=2)
    attn_masks = torch.cat((torch.ones_like(point_vecs), torch.zeros_like(cam_rays)), dim=2)
    
    return attn_maps, attn_masks, cam_rays, point_vecs_per_frame


def generate_video_core(
    prompt: str,
    exo_video_path: str,
    ego_prior_video_path: str,
    output_path: str,
    seed: int,
    use_gga: bool,
    cos_sim_scaling_factor: float,
    meta: dict = None,
    is_in_the_wild: bool = False,
    num_inference_steps: int = 50,
    guidance_scale: float = 5.0,
):
    """æ ¸å¿ƒè§†é¢‘ç”Ÿæˆå‡½æ•°"""
    from core.inference.wan import generate_video
    
    global PIPE
    
    if PIPE is None:
        raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹")
    
    set_seed(seed)
    
    # è®¡ç®— GGA æ³¨æ„åŠ›
    if use_gga and meta is not None:
        attn_maps, attn_masks, cam_rays, point_vecs_per_frame = compute_gga_attention(meta, is_in_the_wild)
    else:
        attn_maps = None
        attn_masks = None
        cam_rays = None
        point_vecs_per_frame = None
    
    # ç”Ÿæˆè§†é¢‘
    video = generate_video(
        prompt=prompt,
        exo_video_path=exo_video_path,
        ego_prior_video_path=ego_prior_video_path,
        output_path=output_path,
        num_frames=49,
        width=784 + 448,
        height=448,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        fps=30,
        num_videos_per_prompt=1,
        seed=seed,
        attention_GGA=attn_maps.unsqueeze(0) if attn_maps is not None else None,
        attention_mask_GGA=attn_masks.unsqueeze(0) if attn_masks is not None else None,
        point_vecs_per_frame=point_vecs_per_frame,
        cam_rays=cam_rays,
        do_kv_cache=False,
        cos_sim_scaling_factor=cos_sim_scaling_factor,
        pipe=PIPE,
    )
    
    return output_path


def run_in_the_wild_inference(
    example_idx: str,
    seed: int,
    use_gga: bool,
    cos_sim_scaling_factor: float,
    num_inference_steps: int,
    guidance_scale: float,
    progress=gr.Progress()
):
    """In-the-wild æ¨ç†"""
    global MODEL_LOADED
    
    if not MODEL_LOADED:
        return None, "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥å¯åŠ¨æ—¥å¿—ï¼"
    
    try:
        # è§£æç¤ºä¾‹ç´¢å¼•
        idx = int(example_idx.split(":")[0])
        
        # åŠ è½½å…ƒæ•°æ®
        meta_data = load_meta_data(ITW_META_PATH)
        meta = meta_data[idx]
        
        prompt = meta['prompt']
        exo_video_path = meta['exo_path']
        ego_prior_video_path = meta['ego_prior_path']
        take_name = exo_video_path.split('/')[-2]
        
        progress(0.1, desc="å‡†å¤‡æ•°æ®...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "./results/gradio_outputs"
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, f"{take_name}_itw_{seed}.mp4")
        
        progress(0.2, desc="å¼€å§‹ç”Ÿæˆè§†é¢‘...")
        
        # ç”Ÿæˆè§†é¢‘
        result_path = generate_video_core(
            prompt=prompt,
            exo_video_path=exo_video_path,
            ego_prior_video_path=ego_prior_video_path,
            output_path=output_path,
            seed=seed,
            use_gga=use_gga,
            cos_sim_scaling_factor=cos_sim_scaling_factor,
            meta=meta,
            is_in_the_wild=True,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
        )
        
        progress(1.0, desc="å®Œæˆï¼")
        
        return result_path, f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼ä¿å­˜è‡³: {result_path}"
    
    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        return None, error_msg


def run_ego4d_inference(
    example_idx: str,
    seed: int,
    use_gga: bool,
    cos_sim_scaling_factor: float,
    num_inference_steps: int,
    guidance_scale: float,
    progress=gr.Progress()
):
    """Ego-Exo4D æ¨ç†"""
    global MODEL_LOADED
    
    if not MODEL_LOADED:
        return None, "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥å¯åŠ¨æ—¥å¿—ï¼"
    
    try:
        # è§£æç¤ºä¾‹ç´¢å¼•
        idx = int(example_idx.split(":")[0])
        
        # åŠ è½½å…ƒæ•°æ®
        meta_data = load_meta_data(EGO4D_META_PATH)
        meta = meta_data[idx]
        
        prompt = meta['prompt']
        exo_video_path = meta['exo_path']
        ego_prior_video_path = meta['ego_prior_path']
        take_name = exo_video_path.split('/')[-2]
        
        progress(0.1, desc="å‡†å¤‡æ•°æ®...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "./results/gradio_outputs"
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, f"{take_name}_ego4d_{seed}.mp4")
        
        progress(0.2, desc="å¼€å§‹ç”Ÿæˆè§†é¢‘...")
        
        # ç”Ÿæˆè§†é¢‘
        result_path = generate_video_core(
            prompt=prompt,
            exo_video_path=exo_video_path,
            ego_prior_video_path=ego_prior_video_path,
            output_path=output_path,
            seed=seed,
            use_gga=use_gga,
            cos_sim_scaling_factor=cos_sim_scaling_factor,
            meta=meta,
            is_in_the_wild=False,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
        )
        
        progress(1.0, desc="å®Œæˆï¼")
        
        return result_path, f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼ä¿å­˜è‡³: {result_path}"
    
    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        return None, error_msg


def preview_example_itw(example_idx: str):
    """é¢„è§ˆ In-the-wild ç¤ºä¾‹"""
    if not example_idx:
        return None, None, ""
    
    try:
        idx = int(example_idx.split(":")[0])
        meta_data = load_meta_data(ITW_META_PATH)
        meta = meta_data[idx]
        
        exo_video_path = meta['exo_path']
        ego_prior_video_path = meta['ego_prior_path']
        prompt = meta['prompt']
        
        return exo_video_path, ego_prior_video_path, prompt
    except Exception as e:
        return None, None, f"åŠ è½½å¤±è´¥: {str(e)}"


def preview_example_ego4d(example_idx: str):
    """é¢„è§ˆ Ego-Exo4D ç¤ºä¾‹"""
    if not example_idx:
        return None, None, ""
    
    try:
        idx = int(example_idx.split(":")[0])
        meta_data = load_meta_data(EGO4D_META_PATH)
        meta = meta_data[idx]
        
        exo_video_path = meta['exo_path']
        ego_prior_video_path = meta['ego_prior_path']
        prompt = meta['prompt']
        
        return exo_video_path, ego_prior_video_path, prompt
    except Exception as e:
        return None, None, f"åŠ è½½å¤±è´¥: {str(e)}"


def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # è·å–ç¤ºä¾‹é€‰é¡¹
    itw_choices = get_example_choices(ITW_META_PATH)
    ego4d_choices = get_example_choices(EGO4D_META_PATH)
    
    with gr.Blocks(title="EgoX - ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ç”Ÿæˆ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¬ EgoX - ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ç”Ÿæˆ
        
        åŸºäº Wan2.1 æ¨¡å‹ï¼Œå°†ç¬¬ä¸‰äººç§°ï¼ˆå¤–éƒ¨è§†è§’ï¼‰è§†é¢‘è½¬æ¢ä¸ºç¬¬ä¸€äººç§°ï¼ˆè‡ªæˆ‘è§†è§’ï¼‰è§†é¢‘ã€‚
        
        **ä½¿ç”¨è¯´æ˜ï¼š**
        1. é€‰æ‹©ä¸€ä¸ªç¤ºä¾‹æ•°æ®
        2. è°ƒæ•´å‚æ•°ï¼ˆå¯é€‰ï¼‰
        3. ç‚¹å‡»"ç”Ÿæˆè§†é¢‘"å¼€å§‹ç”Ÿæˆ
        
        âœ… **æ¨¡å‹å·²è‡ªåŠ¨åŠ è½½å®Œæˆ**
        """)
        
        # ä¸»è¦åŠŸèƒ½åŒºåŸŸ - ä½¿ç”¨ Tabs
        with gr.Tabs():
            # In-the-wild Tab
            with gr.TabItem("ğŸ­ In-the-Wild ç¤ºä¾‹"):
                gr.Markdown("""
                **In-the-Wild æ•°æ®é›†** åŒ…å«æ¥è‡ªç”µå½±ã€åŠ¨ç”»ç­‰å¤šæ ·åŒ–åœºæ™¯çš„è§†é¢‘ç¤ºä¾‹ã€‚
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### ğŸ“‹ é€‰æ‹©ç¤ºä¾‹")
                        itw_example_dropdown = gr.Dropdown(
                            choices=itw_choices,
                            label="é€‰æ‹©ç¤ºä¾‹",
                            value=itw_choices[0] if itw_choices else None,
                            interactive=True
                        )
                        
                        gr.Markdown("#### ğŸ›ï¸ ç”Ÿæˆå‚æ•°")
                        itw_seed = gr.Number(label="éšæœºç§å­", value=846514, precision=0)
                        itw_use_gga = gr.Checkbox(label="ä½¿ç”¨ GGA (Geometry-Guided Attention)", value=True)
                        itw_cos_sim = gr.Slider(
                            label="ä½™å¼¦ç›¸ä¼¼åº¦ç¼©æ”¾å› å­",
                            minimum=0.1,
                            maximum=10.0,
                            value=3.0,
                            step=0.1
                        )
                        itw_steps = gr.Slider(
                            label="æ¨ç†æ­¥æ•°",
                            minimum=10,
                            maximum=100,
                            value=50,
                            step=5
                        )
                        itw_guidance = gr.Slider(
                            label="å¼•å¯¼å¼ºåº¦ (Guidance Scale)",
                            minimum=1.0,
                            maximum=15.0,
                            value=5.0,
                            step=0.5
                        )
                        
                        itw_generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary")
                    
                    with gr.Column(scale=2):
                        gr.Markdown("#### ğŸ“¹ è¾“å…¥è§†é¢‘é¢„è§ˆ")
                        with gr.Row():
                            itw_exo_video = gr.Video(label="å¤–éƒ¨è§†è§’ (Exo View)", interactive=False)
                            itw_ego_prior_video = gr.Video(label="å…ˆéªŒè‡ªæˆ‘è§†è§’ (Ego Prior)", interactive=False)
                        
                        gr.Markdown("#### ğŸ“ æç¤ºè¯")
                        itw_prompt_display = gr.Textbox(
                            label="Prompt",
                            lines=5,
                            interactive=False
                        )
                
                gr.Markdown("#### ğŸï¸ ç”Ÿæˆç»“æœ")
                itw_output_video = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘")
                itw_status = gr.Textbox(label="çŠ¶æ€ä¿¡æ¯", interactive=False)
                
                # äº‹ä»¶ç»‘å®š
                itw_example_dropdown.change(
                    fn=preview_example_itw,
                    inputs=[itw_example_dropdown],
                    outputs=[itw_exo_video, itw_ego_prior_video, itw_prompt_display]
                )
                
                itw_generate_btn.click(
                    fn=run_in_the_wild_inference,
                    inputs=[
                        itw_example_dropdown,
                        itw_seed,
                        itw_use_gga,
                        itw_cos_sim,
                        itw_steps,
                        itw_guidance
                    ],
                    outputs=[itw_output_video, itw_status]
                )
            
            # Ego-Exo4D Tab
            with gr.TabItem("âš½ Ego-Exo4D ç¤ºä¾‹"):
                gr.Markdown("""
                **Ego-Exo4D æ•°æ®é›†** åŒ…å«æ¥è‡ª Ego-Exo4D æ•°æ®é›†çš„çœŸå®æ´»åŠ¨è§†é¢‘ï¼Œå¦‚è¶³çƒã€ç¯®çƒã€çƒ¹é¥ªã€èˆè¹ˆç­‰ã€‚
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### ğŸ“‹ é€‰æ‹©ç¤ºä¾‹")
                        ego4d_example_dropdown = gr.Dropdown(
                            choices=ego4d_choices,
                            label="é€‰æ‹©ç¤ºä¾‹",
                            value=ego4d_choices[0] if ego4d_choices else None,
                            interactive=True
                        )
                        
                        gr.Markdown("#### ğŸ›ï¸ ç”Ÿæˆå‚æ•°")
                        ego4d_seed = gr.Number(label="éšæœºç§å­", value=42, precision=0)
                        ego4d_use_gga = gr.Checkbox(label="ä½¿ç”¨ GGA (Geometry-Guided Attention)", value=True)
                        ego4d_cos_sim = gr.Slider(
                            label="ä½™å¼¦ç›¸ä¼¼åº¦ç¼©æ”¾å› å­",
                            minimum=0.1,
                            maximum=10.0,
                            value=3.0,
                            step=0.1
                        )
                        ego4d_steps = gr.Slider(
                            label="æ¨ç†æ­¥æ•°",
                            minimum=10,
                            maximum=100,
                            value=50,
                            step=5
                        )
                        ego4d_guidance = gr.Slider(
                            label="å¼•å¯¼å¼ºåº¦ (Guidance Scale)",
                            minimum=1.0,
                            maximum=15.0,
                            value=5.0,
                            step=0.5
                        )
                        
                        ego4d_generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary")
                    
                    with gr.Column(scale=2):
                        gr.Markdown("#### ğŸ“¹ è¾“å…¥è§†é¢‘é¢„è§ˆ")
                        with gr.Row():
                            ego4d_exo_video = gr.Video(label="å¤–éƒ¨è§†è§’ (Exo View)", interactive=False)
                            ego4d_ego_prior_video = gr.Video(label="å…ˆéªŒè‡ªæˆ‘è§†è§’ (Ego Prior)", interactive=False)
                        
                        gr.Markdown("#### ğŸ“ æç¤ºè¯")
                        ego4d_prompt_display = gr.Textbox(
                            label="Prompt",
                            lines=5,
                            interactive=False
                        )
                
                gr.Markdown("#### ğŸï¸ ç”Ÿæˆç»“æœ")
                ego4d_output_video = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘")
                ego4d_status = gr.Textbox(label="çŠ¶æ€ä¿¡æ¯", interactive=False)
                
                # äº‹ä»¶ç»‘å®š
                ego4d_example_dropdown.change(
                    fn=preview_example_ego4d,
                    inputs=[ego4d_example_dropdown],
                    outputs=[ego4d_exo_video, ego4d_ego_prior_video, ego4d_prompt_display]
                )
                
                ego4d_generate_btn.click(
                    fn=run_ego4d_inference,
                    inputs=[
                        ego4d_example_dropdown,
                        ego4d_seed,
                        ego4d_use_gga,
                        ego4d_cos_sim,
                        ego4d_steps,
                        ego4d_guidance
                    ],
                    outputs=[ego4d_output_video, ego4d_status]
                )
        
        gr.Markdown("""
        ---
        ### ğŸ“– å‚æ•°è¯´æ˜
        
        | å‚æ•° | è¯´æ˜ |
        |------|------|
        | **éšæœºç§å­** | æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼Œç›¸åŒç§å­ä¼šäº§ç”Ÿç›¸åŒç»“æœ |
        | **GGA** | Geometry-Guided Attentionï¼Œåˆ©ç”¨å‡ ä½•ä¿¡æ¯å¼•å¯¼è§†è§’è½¬æ¢ |
        | **ä½™å¼¦ç›¸ä¼¼åº¦ç¼©æ”¾å› å­** | æ§åˆ¶ GGA æ³¨æ„åŠ›çš„å¼ºåº¦ |
        | **æ¨ç†æ­¥æ•°** | æ‰©æ•£æ¨¡å‹çš„å»å™ªæ­¥æ•°ï¼Œè¶Šå¤šè´¨é‡è¶Šå¥½ä½†è¶Šæ…¢ |
        | **å¼•å¯¼å¼ºåº¦** | æ§åˆ¶ç”Ÿæˆç»“æœä¸æç¤ºè¯çš„åŒ¹é…ç¨‹åº¦ |
        
        ---
        **æ³¨æ„ï¼š** è§†é¢‘ç”Ÿæˆéœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆçº¦ 5-10 åˆ†é’Ÿï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚
        """)
    
    return demo


if __name__ == "__main__":
    print("="*50)
    print("ğŸš€ EgoX å¯åŠ¨ä¸­...")
    print("="*50)
    
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
    result = load_model(DEFAULT_MODEL_PATH, DEFAULT_LORA_PATH)
    print(result)
    
    if not MODEL_LOADED:
        print("\nâš ï¸ è­¦å‘Šï¼šæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
    else:
        print("\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œæ­£åœ¨å¯åŠ¨ Web ç•Œé¢...")
    
    print("="*50)
    
    demo = create_ui()
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
