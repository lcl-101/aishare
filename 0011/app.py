#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ACE-Step 1.5 ç®€åŒ–ç‰ˆæ¨ç†ç•Œé¢
- ä½¿ç”¨ transformers åç«¯ï¼ˆä¸ä½¿ç”¨ vllmï¼‰
- æœ¬åœ°æ¨¡å‹è·¯å¾„: checkpoints/Ace-Step1.5
- ä¸­æ–‡ç•Œé¢
"""

import os
import sys
import random
import tempfile
import traceback
from typing import Optional, List, Dict, Any

import torch
import gradio as gr

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# å¯¼å…¥ ACE-Step æ¨¡å—
from acestep.handler import AceStepHandler
from acestep.llm_inference import LLMHandler
from acestep.constants import VALID_LANGUAGES, DEFAULT_DIT_INSTRUCTION

# ==================== å…¨å±€é…ç½® ====================
# æ¨¡å‹åœ¨ checkpoints/Ace-Step1.5/ ä¸‹ï¼Œéœ€è¦åˆ›å»ºç¬¦å·é“¾æ¥æˆ–ç›´æ¥æŒ‡å®šè·¯å¾„
CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, "checkpoints", "Ace-Step1.5")
DIT_MODEL_PATH = "acestep-v15-turbo"
LM_MODEL_PATH = "acestep-5Hz-lm-1.7B"

# ç¡®ä¿ handler èƒ½æ‰¾åˆ°æ¨¡å‹ï¼šåˆ›å»ºç¬¦å·é“¾æ¥
def setup_model_paths():
    """è®¾ç½®æ¨¡å‹è·¯å¾„ï¼Œåˆ›å»ºç¬¦å·é“¾æ¥è®© handler èƒ½æ‰¾åˆ°æ¨¡å‹"""
    src_dir = os.path.join(PROJECT_ROOT, "checkpoints", "Ace-Step1.5")
    dst_dir = os.path.join(PROJECT_ROOT, "checkpoints")
    
    # éœ€è¦é“¾æ¥çš„ç›®å½•
    dirs_to_link = ["acestep-v15-turbo", "acestep-5Hz-lm-1.7B", "vae", "Qwen3-Embedding-0.6B"]
    
    for dir_name in dirs_to_link:
        src = os.path.join(src_dir, dir_name)
        dst = os.path.join(dst_dir, dir_name)
        
        if os.path.exists(src) and not os.path.exists(dst):
            try:
                os.symlink(src, dst)
                print(f"åˆ›å»ºç¬¦å·é“¾æ¥: {dst} -> {src}")
            except Exception as e:
                print(f"åˆ›å»ºç¬¦å·é“¾æ¥å¤±è´¥ {dir_name}: {e}")

# å¯åŠ¨æ—¶è®¾ç½®è·¯å¾„
setup_model_paths()

# å…¨å±€ handler å®ä¾‹
dit_handler: Optional[AceStepHandler] = None
llm_handler: Optional[LLMHandler] = None

# ==================== åˆå§‹åŒ–å‡½æ•° ====================

def initialize_service(
    device: str = "auto",
    use_flash_attention: bool = True,
    offload_to_cpu: bool = False,
    init_llm: bool = True,
) -> str:
    """åˆå§‹åŒ–æ¨¡å‹æœåŠ¡"""
    global dit_handler, llm_handler
    
    try:
        # è®¾å¤‡æ£€æµ‹
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
        
        status_msgs = []
        
        # åˆå§‹åŒ– DiT Handler
        dit_handler = AceStepHandler()
        dit_status, success = dit_handler.initialize_service(
            project_root=PROJECT_ROOT,
            config_path=DIT_MODEL_PATH,
            device=device,
            use_flash_attention=use_flash_attention,
            compile_model=False,
            offload_to_cpu=offload_to_cpu,
            offload_dit_to_cpu=offload_to_cpu,
        )
        status_msgs.append(dit_status)
        
        if not success:
            return f"âŒ DiT æ¨¡å‹åˆå§‹åŒ–å¤±è´¥:\n{dit_status}"
        
        # åˆå§‹åŒ– LLM Handlerï¼ˆä½¿ç”¨ PyTorch åç«¯ï¼‰
        if init_llm:
            llm_handler = LLMHandler()
            llm_status, llm_success = llm_handler.initialize(
                checkpoint_dir=CHECKPOINT_DIR,
                lm_model_path=LM_MODEL_PATH,
                backend="pt",  # ä½¿ç”¨ PyTorch åç«¯ï¼Œä¸ä½¿ç”¨ vllm
                device=device,
                offload_to_cpu=offload_to_cpu,
            )
            status_msgs.append(llm_status)
            
            if not llm_success:
                return f"âŒ LLM æ¨¡å‹åˆå§‹åŒ–å¤±è´¥:\n{llm_status}"
        else:
            llm_handler = LLMHandler()
        
        return "\n\n".join(status_msgs)
        
    except Exception as e:
        error_msg = f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        return error_msg


def generate_music(
    caption: str,
    lyrics: str,
    vocal_language: str = "unknown",
    instrumental: bool = False,
    bpm: Optional[float] = None,
    key_scale: str = "",
    time_signature: str = "",
    audio_duration: float = -1,
    batch_size: int = 1,
    inference_steps: int = 8,
    guidance_scale: float = 3.5,
    seed: int = -1,
    use_thinking: bool = True,
    reference_audio: Optional[str] = None,
    progress=gr.Progress(track_tqdm=True),
) -> tuple:
    """ç”ŸæˆéŸ³ä¹"""
    global dit_handler, llm_handler
    
    if dit_handler is None or dit_handler.model is None:
        return None, "âŒ è¯·å…ˆåˆå§‹åŒ–æ¨¡å‹æœåŠ¡"
    
    try:
        # å¤„ç†æ­Œè¯
        if instrumental:
            lyrics = "[Instrumental]"
        
        # å¤„ç†ç§å­
        if seed < 0:
            seed = random.randint(0, 2**32 - 1)
        
        # å¤„ç†æ—¶é•¿
        if audio_duration <= 0:
            audio_duration = 30.0
        
        # å¤„ç† BPM
        bpm_value = int(bpm) if bpm and bpm > 0 else None
        
        # å¤„ç†å‚è€ƒéŸ³é¢‘
        processed_refer_audio = None
        if reference_audio:
            processed_refer_audio = dit_handler.process_reference_audio(reference_audio)
        
        # å‡†å¤‡å‚è€ƒéŸ³é¢‘åˆ—è¡¨
        if processed_refer_audio is not None:
            refer_audios = [[processed_refer_audio]]
        else:
            refer_audios = [[torch.zeros(2, 30 * 48000)]]
        
        # LLM Chain-of-Thought ç”Ÿæˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
        audio_code_string = ""
        lm_metadata = None
        
        if use_thinking and llm_handler and llm_handler.llm_initialized:
            # ä½¿ç”¨ LLM ç”Ÿæˆå…ƒæ•°æ®å’ŒéŸ³é¢‘ä»£ç 
            try:
                result = llm_handler.generate_with_stop_condition(
                    caption=caption or "",
                    lyrics=lyrics or "",
                    infer_type="llm_dit",
                    temperature=0.85,
                    cfg_scale=2.0,
                    negative_prompt="NO USER INPUT",
                    top_k=None,
                    top_p=0.9,
                    target_duration=audio_duration,
                    user_metadata={"bpm": bpm_value} if bpm_value else None,
                    use_cot_caption=True,
                    use_cot_language=True,
                    use_cot_metas=True,
                    use_constrained_decoding=True,
                    batch_size=batch_size,
                    seeds=[seed + i for i in range(batch_size)],
                )
                
                if result.get("success", False):
                    lm_metadata = result.get("metadata", {})
                    audio_codes = result.get("audio_codes", [])
                    if audio_codes:
                        audio_code_string = audio_codes if isinstance(audio_codes, list) else [audio_codes]
                    
                    # æ›´æ–°å…ƒæ•°æ®
                    if lm_metadata:
                        if not bpm_value and lm_metadata.get("bpm"):
                            try:
                                bpm_value = int(lm_metadata["bpm"])
                            except:
                                pass
                        if not key_scale and lm_metadata.get("keyscale"):
                            key_scale = lm_metadata["keyscale"]
                        if not time_signature and lm_metadata.get("timesignature"):
                            time_signature = lm_metadata["timesignature"]
                        if lm_metadata.get("caption"):
                            caption = lm_metadata["caption"]
                        if lm_metadata.get("vocal_language"):
                            vocal_language = lm_metadata["vocal_language"]
                            
            except Exception as e:
                print(f"LLM ç”Ÿæˆè­¦å‘Š: {e}")
        
        # åˆ›å»ºç›®æ ‡éŸ³é¢‘
        target_wavs = dit_handler.create_target_wavs(audio_duration)
        target_wavs = target_wavs.unsqueeze(0).repeat(batch_size, 1, 1)
        
        # è°ƒç”¨ DiT ç”Ÿæˆ
        result = dit_handler.generate_music(
            captions=caption,
            lyrics=lyrics,
            bpm=bpm_value,
            key_scale=key_scale,
            time_signature=time_signature,
            vocal_language=vocal_language,
            inference_steps=inference_steps,
            guidance_scale=guidance_scale,
            use_random_seed=False,
            seed=str(seed),
            reference_audio=reference_audio,
            audio_duration=audio_duration,
            batch_size=batch_size,
            src_audio=None,
            audio_code_string=audio_code_string if audio_code_string else "",
            repainting_start=None,
            repainting_end=None,
            instruction=DEFAULT_DIT_INSTRUCTION,
            audio_cover_strength=1.0,
            task_type="text2music",
            use_adg=False,
            cfg_interval_start=0.0,
            cfg_interval_end=1.0,
            shift=3.0,
        )
        
        if not result.get("success", False):
            return None, f"âŒ ç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        # è·å–ç”Ÿæˆçš„éŸ³é¢‘
        audios = result.get("audios", [])
        if not audios:
            return None, "âŒ æœªç”Ÿæˆä»»ä½•éŸ³é¢‘"
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        output_files = []
        for i, audio_info in enumerate(audios):
            audio_tensor = audio_info.get("tensor")
            if audio_tensor is not None:
                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
                import torchaudio
                torchaudio.save(temp_file.name, audio_tensor.cpu(), 48000)
                output_files.append(temp_file.name)
        
        status_msg = f"âœ… æˆåŠŸç”Ÿæˆ {len(output_files)} ä¸ªéŸ³é¢‘\n"
        status_msg += f"ç§å­: {seed}\n"
        status_msg += f"æ—¶é•¿: {audio_duration}ç§’\n"
        if bpm_value:
            status_msg += f"BPM: {bpm_value}\n"
        if key_scale:
            status_msg += f"è°ƒæ€§: {key_scale}\n"
        
        # è¿”å›ç¬¬ä¸€ä¸ªéŸ³é¢‘
        if output_files:
            return output_files[0], status_msg
        else:
            return None, "âŒ éŸ³é¢‘ä¿å­˜å¤±è´¥"
            
    except Exception as e:
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        return None, error_msg


def random_caption() -> str:
    """ç”Ÿæˆéšæœºç¤ºä¾‹æè¿°"""
    examples = [
        "A gentle acoustic guitar melody with warm fingerpicking patterns, soft ambient pads, and a dreamy atmosphere",
        "Upbeat electronic dance music with punchy bass, catchy synth hooks, and energetic drums",
        "Classical piano piece with emotional arpeggios, dynamic crescendos, and romantic harmonies",
        "Lo-fi hip hop beat with dusty vinyl textures, mellow keys, and relaxed drums",
        "Epic orchestral soundtrack with dramatic strings, powerful brass, and cinematic percussion",
        "Jazz fusion with smooth saxophone, walking bass, and sophisticated chord progressions",
        "Ambient soundscape with ethereal pads, nature sounds, and meditative textures",
        "Rock anthem with distorted guitars, driving drums, and powerful vocals",
    ]
    return random.choice(examples)


def load_example(language: str = "zh") -> tuple:
    """åŠ è½½ç¤ºä¾‹æ•°æ®
    
    Args:
        language: è¯­è¨€ä»£ç  "zh" æˆ– "en"
    
    Returns:
        (caption, lyrics, vocal_language, bpm, key_scale, time_signature, duration)
    """
    import json
    import glob
    import re
    
    examples_dir = os.path.join(PROJECT_ROOT, "examples", "text2music")
    
    # è·å–æ‰€æœ‰ç¤ºä¾‹æ–‡ä»¶
    all_files = glob.glob(os.path.join(examples_dir, "example_*.json"))
    
    # ç­›é€‰æŒ‡å®šè¯­è¨€çš„ç¤ºä¾‹
    matching_files = []
    for f in all_files:
        try:
            with open(f, 'r', encoding='utf-8') as fp:
                data = json.load(fp)
                if data.get("language", "") == language:
                    lyrics = data.get("lyrics", "")
                    # å¯¹äºä¸­æ–‡ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«çœŸæ­£çš„ä¸­æ–‡å­—ç¬¦ï¼ˆæ’é™¤æ‹¼éŸ³æ ¼å¼ï¼‰
                    if language == "zh":
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
                        has_chinese = bool(re.search(r'[\u4e00-\u9fff]', lyrics))
                        # æ’é™¤æ‹¼éŸ³æ ¼å¼ [zh] xxx
                        has_pinyin = bool(re.search(r'\[zh\]\s*[a-z]', lyrics))
                        if has_chinese and not has_pinyin:
                            matching_files.append(f)
                    else:
                        matching_files.append(f)
        except:
            continue
    
    if not matching_files:
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤å€¼
        return "", "", "unknown", None, "", "", 30
    
    # éšæœºé€‰æ‹©ä¸€ä¸ª
    chosen_file = random.choice(matching_files)
    
    try:
        with open(chosen_file, 'r', encoding='utf-8') as fp:
            data = json.load(fp)
        
        caption = data.get("caption", "")
        lyrics = data.get("lyrics", "")
        vocal_language = data.get("language", "unknown")
        bpm = data.get("bpm", None)
        key_scale = data.get("keyscale", "")
        time_signature = str(data.get("timesignature", "")) if data.get("timesignature") else ""
        duration = data.get("duration", 30)
        
        return caption, lyrics, vocal_language, bpm, key_scale, time_signature, duration
    except Exception as e:
        print(f"åŠ è½½ç¤ºä¾‹å¤±è´¥: {e}")
        return "", "", "unknown", None, "", "", 30


def load_chinese_example() -> tuple:
    """åŠ è½½ä¸­æ–‡ç¤ºä¾‹"""
    return load_example("zh")


def load_english_example() -> tuple:
    """åŠ è½½è‹±æ–‡ç¤ºä¾‹"""
    return load_example("en")


# ==================== Gradio ç•Œé¢ ====================

def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # è‡ªå®šä¹‰ CSS
    custom_css = """
    .header-banner {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 20px;
        border-radius: 12px;
        margin-bottom: 15px;
        text-align: center;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    }
    .header-banner h1 {
        color: white;
        margin: 0 0 5px 0;
        font-size: 28px;
    }
    .header-banner p {
        color: rgba(255,255,255,0.9);
        margin: 0;
        font-size: 14px;
    }
    .youtube-link {
        display: inline-flex;
        align-items: center;
        gap: 8px;
        background: rgba(255,0,0,0.9);
        padding: 8px 16px;
        border-radius: 20px;
        color: white;
        text-decoration: none;
        font-size: 13px;
        margin-top: 10px;
        transition: transform 0.2s;
    }
    .youtube-link:hover {
        transform: scale(1.05);
        color: white;
    }
    .example-buttons {
        display: flex;
        gap: 8px;
        margin-bottom: 10px;
    }
    .section-header {
        font-size: 16px;
        font-weight: 600;
        margin-bottom: 8px;
        color: #374151;
    }
    """
    
    # é¡¶éƒ¨æ¨ªå¹…
    header_html = """
    <div class="header-banner">
        <h1>ğŸµ ACE-Step 1.5 éŸ³ä¹ç”Ÿæˆ</h1>
        <p>åŸºäº ACE-Step 1.5 æ¨¡å‹çš„æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆç³»ç»Ÿ</p>
        <a href="https://www.youtube.com/@rongyi-ai" target="_blank" class="youtube-link">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="white">
                <path d="M19.615 3.184c-3.604-.246-11.631-.245-15.23 0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23 0 3.897-.266 4.356-2.62 4.385-8.816-.029-6.185-.484-8.549-4.385-8.816zm-10.615 12.816v-8l8 3.993-8 4.007z"/>
            </svg>
            å…³æ³¨ YouTube é¢‘é“ @rongyi-ai
        </a>
    </div>
    """
    
    with gr.Blocks(
        title="ACE-Step 1.5 éŸ³ä¹ç”Ÿæˆ",
        theme=gr.themes.Soft(),
        css=custom_css,
    ) as demo:
        
        # é¡¶éƒ¨æ¨ªå¹…
        gr.HTML(header_html)
        
        # ============ ç¬¬ä¸€è¡Œï¼šå¿«æ·æ“ä½œ ============
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("**ğŸ’¡ å¿«é€Ÿå¼€å§‹ï¼šåŠ è½½ç¤ºä¾‹æˆ–éšæœºç”Ÿæˆæè¿°**")
            with gr.Column(scale=1):
                with gr.Row():
                    load_zh_btn = gr.Button("ğŸ“– ä¸­æ–‡ç¤ºä¾‹", variant="secondary", size="sm")
                    load_en_btn = gr.Button("ğŸ“– è‹±æ–‡ç¤ºä¾‹", variant="secondary", size="sm")
                    random_btn = gr.Button("ğŸ² éšæœºæè¿°", variant="secondary", size="sm")
        
        gr.Markdown("---")
        
        # ============ ä¸»å†…å®¹åŒº ============
        with gr.Row():
            # ========== å·¦åˆ—ï¼šè¾“å…¥åŒº ==========
            with gr.Column(scale=3):
                # éŸ³ä¹æè¿°
                caption_input = gr.Textbox(
                    label="ğŸ“ éŸ³ä¹æè¿°",
                    placeholder="æè¿°ä½ æƒ³è¦ç”Ÿæˆçš„éŸ³ä¹é£æ ¼ã€ä¹å™¨ã€æƒ…ç»ªç­‰...\nä¾‹å¦‚: A cheerful pop song with acoustic guitar and piano...",
                    lines=4,
                )
                
                # æ­Œè¯
                lyrics_input = gr.Textbox(
                    label="ğŸ¤ æ­Œè¯",
                    placeholder="è¾“å…¥æ­Œè¯ï¼Œä½¿ç”¨ [Verse], [Chorus] ç­‰æ ‡è®°æ®µè½ç»“æ„...\nå¦‚æœæ˜¯çº¯éŸ³ä¹ï¼Œè¯·å‹¾é€‰å³ä¾§ã€Œçº¯éŸ³ä¹ã€é€‰é¡¹",
                    lines=8,
                )
                
                # æ­Œè¯é€‰é¡¹è¡Œ
                with gr.Row():
                    instrumental_checkbox = gr.Checkbox(
                        label="ğŸ¹ çº¯éŸ³ä¹ï¼ˆæ— äººå£°ï¼‰",
                        value=False,
                    )
                    vocal_language = gr.Dropdown(
                        choices=VALID_LANGUAGES,
                        value="unknown",
                        label="äººå£°è¯­è¨€",
                        scale=2,
                    )
                
                # å‚è€ƒéŸ³é¢‘ï¼ˆæŠ˜å ï¼‰
                with gr.Accordion("ğŸ§ å‚è€ƒéŸ³é¢‘ï¼ˆå¯é€‰ï¼‰", open=False):
                    reference_audio = gr.Audio(
                        label="ä¸Šä¼ å‚è€ƒéŸ³é¢‘",
                        type="filepath",
                    )
            
            # ========== å³åˆ—ï¼šå‚æ•°åŒº ==========
            with gr.Column(scale=2):
                # åŸºæœ¬éŸ³ä¹å‚æ•°
                gr.Markdown("**ğŸ›ï¸ éŸ³ä¹å‚æ•°**")
                with gr.Row():
                    bpm_input = gr.Number(
                        label="BPM",
                        value=None,
                        step=1,
                        info="èŠ‚æ‹é€Ÿåº¦",
                    )
                    duration_input = gr.Number(
                        label="æ—¶é•¿ï¼ˆç§’ï¼‰",
                        value=30,
                        minimum=10,
                        maximum=600,
                        step=1,
                    )
                with gr.Row():
                    key_scale_input = gr.Textbox(
                        label="è°ƒæ€§",
                        placeholder="å¦‚: C major",
                        value="",
                    )
                    time_sig_input = gr.Dropdown(
                        choices=["", "2", "3", "4", "6"],
                        value="",
                        label="æ‹å·",
                    )
                
                gr.Markdown("---")
                
                # ç”Ÿæˆå‚æ•°
                gr.Markdown("**âš¡ ç”Ÿæˆå‚æ•°**")
                with gr.Row():
                    steps_input = gr.Slider(
                        minimum=1,
                        maximum=8,
                        value=8,
                        step=1,
                        label="æ¨ç†æ­¥æ•°",
                    )
                    guidance_input = gr.Slider(
                        minimum=1.0,
                        maximum=10.0,
                        value=3.5,
                        step=0.5,
                        label="å¼•å¯¼å¼ºåº¦",
                    )
                
                with gr.Row():
                    batch_size_input = gr.Number(
                        label="æ‰¹é‡æ•°",
                        value=1,
                        minimum=1,
                        maximum=8,
                        step=1,
                    )
                    seed_input = gr.Number(
                        label="ç§å­ (-1éšæœº)",
                        value=-1,
                        step=1,
                    )
                
                thinking_checkbox = gr.Checkbox(
                    label="ğŸ§  å¯ç”¨ LLM æ€ç»´é“¾ï¼ˆæ™ºèƒ½ç”Ÿæˆå…ƒæ•°æ®ï¼‰",
                    value=True,
                )
                
                # ç”ŸæˆæŒ‰é’®
                gr.Markdown("")
                generate_btn = gr.Button(
                    "ğŸµ ç”ŸæˆéŸ³ä¹",
                    variant="primary",
                    size="lg",
                )
        
        gr.Markdown("---")
        
        # ============ è¾“å‡ºåŒº ============
        gr.Markdown("### ğŸ¶ ç”Ÿæˆç»“æœ")
        with gr.Row():
            with gr.Column(scale=3):
                audio_output = gr.Audio(
                    label="ç”Ÿæˆçš„éŸ³ä¹",
                    type="filepath",
                )
            with gr.Column(scale=2):
                status_output = gr.Textbox(
                    label="ç”ŸæˆçŠ¶æ€",
                    lines=6,
                    interactive=False,
                )
        
        # äº‹ä»¶ç»‘å®š
        random_btn.click(
            fn=random_caption,
            inputs=[],
            outputs=[caption_input]
        )
        
        # åŠ è½½ä¸­æ–‡ç¤ºä¾‹
        load_zh_btn.click(
            fn=load_chinese_example,
            inputs=[],
            outputs=[caption_input, lyrics_input, vocal_language, bpm_input, key_scale_input, time_sig_input, duration_input]
        )
        
        # åŠ è½½è‹±æ–‡ç¤ºä¾‹
        load_en_btn.click(
            fn=load_english_example,
            inputs=[],
            outputs=[caption_input, lyrics_input, vocal_language, bpm_input, key_scale_input, time_sig_input, duration_input]
        )
        
        generate_btn.click(
            fn=generate_music,
            inputs=[
                caption_input,
                lyrics_input,
                vocal_language,
                instrumental_checkbox,
                bpm_input,
                key_scale_input,
                time_sig_input,
                duration_input,
                batch_size_input,
                steps_input,
                guidance_input,
                seed_input,
                thinking_checkbox,
                reference_audio,
            ],
            outputs=[audio_output, status_output]
        )
    
    return demo


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ACE-Step 1.5 éŸ³ä¹ç”Ÿæˆ")
    parser.add_argument("--port", type=int, default=7860, help="æœåŠ¡ç«¯å£")
    parser.add_argument("--server-name", type=str, default="0.0.0.0", help="æœåŠ¡åœ°å€")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")
    
    args = parser.parse_args()
    
    # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
    print("\n" + "="*60)
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– ACE-Step 1.5 æ¨¡å‹...")
    print("="*60 + "\n")
    
    status = initialize_service(
        device="auto",
        use_flash_attention=False,
        offload_to_cpu=False,
        init_llm=True,
    )
    print(status)
    print("\n" + "="*60 + "\n")
    
    demo = create_ui()
    demo.queue()
    demo.launch(
        server_name=args.server_name,
        server_port=args.port,
        share=args.share,
    )


if __name__ == "__main__":
    main()
