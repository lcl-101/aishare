###!/usr/bin/env python3

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from snac import SNAC
import soundfile as sf
import numpy as np
import gradio as gr
import tempfile
import os

CODE_START_TOKEN_ID = 128257
CODE_END_TOKEN_ID = 128258
CODE_TOKEN_OFFSET = 128266
SNAC_MIN_ID = 128266
SNAC_MAX_ID = 156937
SNAC_TOKENS_PER_FRAME = 7

SOH_ID = 128259
EOH_ID = 128260
SOA_ID = 128261
BOS_ID = 128000
TEXT_EOT_ID = 128009


def build_prompt(tokenizer, description: str, text: str) -> str:
    """Build formatted prompt for Maya1."""
    soh_token = tokenizer.decode([SOH_ID])
    eoh_token = tokenizer.decode([EOH_ID])
    soa_token = tokenizer.decode([SOA_ID])
    sos_token = tokenizer.decode([CODE_START_TOKEN_ID])
    eot_token = tokenizer.decode([TEXT_EOT_ID])
    bos_token = tokenizer.bos_token
    
    formatted_text = f'<description="{description}"> {text}'
    
    prompt = (
        soh_token + bos_token + formatted_text + eot_token +
        eoh_token + soa_token + sos_token
    )
    
    return prompt


def extract_snac_codes(token_ids: list) -> list:
    """Extract SNAC codes from generated tokens."""
    try:
        eos_idx = token_ids.index(CODE_END_TOKEN_ID)
    except ValueError:
        eos_idx = len(token_ids)
    
    snac_codes = [
        token_id for token_id in token_ids[:eos_idx]
        if SNAC_MIN_ID <= token_id <= SNAC_MAX_ID
    ]
    
    return snac_codes


def unpack_snac_from_7(snac_tokens: list) -> list:
    """Unpack 7-token SNAC frames to 3 hierarchical levels."""
    if snac_tokens and snac_tokens[-1] == CODE_END_TOKEN_ID:
        snac_tokens = snac_tokens[:-1]
    
    frames = len(snac_tokens) // SNAC_TOKENS_PER_FRAME
    snac_tokens = snac_tokens[:frames * SNAC_TOKENS_PER_FRAME]
    
    if frames == 0:
        return [[], [], []]
    
    l1, l2, l3 = [], [], []
    
    for i in range(frames):
        slots = snac_tokens[i*7:(i+1)*7]
        l1.append((slots[0] - CODE_TOKEN_OFFSET) % 4096)
        l2.extend([
            (slots[1] - CODE_TOKEN_OFFSET) % 4096,
            (slots[4] - CODE_TOKEN_OFFSET) % 4096,
        ])
        l3.extend([
            (slots[2] - CODE_TOKEN_OFFSET) % 4096,
            (slots[3] - CODE_TOKEN_OFFSET) % 4096,
            (slots[5] - CODE_TOKEN_OFFSET) % 4096,
            (slots[6] - CODE_TOKEN_OFFSET) % 4096,
        ])
    
    return [l1, l2, l3]


# Global variables to store loaded models
model = None
tokenizer = None
snac_model = None


def load_models():
    """Load models once at startup."""
    global model, tokenizer, snac_model
    
    if model is not None:
        return
    
    print("\n[1/2] Loading Maya1 model...")
    model = AutoModelForCausalLM.from_pretrained(
        "checkpoints/maya1", 
        torch_dtype=torch.bfloat16, 
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(
        "checkpoints/maya1",
        trust_remote_code=True
    )
    print(f"Model loaded: {len(tokenizer)} tokens in vocabulary")
    
    print("\n[2/2] Loading SNAC audio decoder...")
    snac_model = SNAC.from_pretrained("checkpoints/snac_24khz").eval()
    if torch.cuda.is_available():
        snac_model = snac_model.to("cuda")
    print("SNAC decoder loaded")


def generate_speech(description: str, text: str, progress=gr.Progress()):
    """Generate speech from description and text."""
    global model, tokenizer, snac_model
    
    try:
        # Load models if not loaded
        progress(0.1, desc="Loading models...")
        load_models()
        
        progress(0.2, desc="Building prompt...")
        print(f"\nGenerating speech...")
        print(f"Description: {description}")
        print(f"Text: {text}")
        
        # Create prompt with proper formatting
        prompt = build_prompt(tokenizer, description, text)
        
        # Generate emotional speech
        progress(0.3, desc="Tokenizing input...")
        inputs = tokenizer(prompt, return_tensors="pt")
        print(f"Input token count: {inputs['input_ids'].shape[1]} tokens")
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        progress(0.4, desc="Generating audio tokens...")
        with torch.inference_mode():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=2048,
                min_new_tokens=28,
                temperature=0.4, 
                top_p=0.9, 
                repetition_penalty=1.1,
                do_sample=True,
                eos_token_id=CODE_END_TOKEN_ID,
                pad_token_id=tokenizer.pad_token_id,
            )
        
        # Extract generated tokens
        generated_ids = outputs[0, inputs['input_ids'].shape[1]:].tolist()
        print(f"Generated {len(generated_ids)} tokens")
        
        # Extract SNAC audio tokens
        progress(0.6, desc="Extracting audio codes...")
        snac_tokens = extract_snac_codes(generated_ids)
        print(f"Extracted {len(snac_tokens)} SNAC tokens")
        
        if len(snac_tokens) < 7:
            return None, "é”™è¯¯: ç”Ÿæˆçš„ SNAC ä»¤ç‰Œä¸è¶³ï¼Œè¯·é‡è¯•ã€‚"
        
        # Unpack SNAC tokens
        progress(0.7, desc="Unpacking audio frames...")
        levels = unpack_snac_from_7(snac_tokens)
        frames = len(levels[0])
        print(f"Unpacked to {frames} frames")
        
        # Convert to tensors
        device = "cuda" if torch.cuda.is_available() else "cpu"
        codes_tensor = [
            torch.tensor(level, dtype=torch.long, device=device).unsqueeze(0)
            for level in levels
        ]
        
        # Generate final audio
        progress(0.8, desc="Decoding audio...")
        with torch.inference_mode():
            z_q = snac_model.quantizer.from_codes(codes_tensor)
            audio = snac_model.decoder(z_q)[0, 0].cpu().numpy()
        
        # Trim warmup samples
        if len(audio) > 2048:
            audio = audio[2048:]
        
        duration_sec = len(audio) / 24000
        print(f"Audio generated: {len(audio)} samples ({duration_sec:.2f}s)")
        
        # Save to temporary file
        progress(0.9, desc="Saving audio...")
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
            sf.write(f.name, audio, 24000)
            output_path = f.name
        
        progress(1.0, desc="Complete!")
        status_msg = f"âœ“ å·²ç”Ÿæˆ {duration_sec:.2f}s çš„éŸ³é¢‘ ({frames} å¸§)"
        return output_path, status_msg
        
    except Exception as e:
        import traceback
        error_msg = f"é”™è¯¯: {str(e)}\n{traceback.format_exc()}"
        print(error_msg)
        return None, error_msg


def create_ui():
    """Create Gradio interface."""
    with gr.Blocks(title="Maya1 è¯­éŸ³ç”Ÿæˆ", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸ¤ Maya1 è¯­éŸ³ç”Ÿæˆ
            ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå¸¦æœ‰æƒ…æ„Ÿçš„è¯­éŸ³ã€‚
            
            **æ”¯æŒçš„æƒ…æ„Ÿæ ‡ç­¾**: `<laugh>`, `<laugh_harder>`, `<sigh>`, `<gasp>` ç­‰
            """
        )
        
        with gr.Row():
            with gr.Column():
                description = gr.Textbox(
                    label="å£°éŸ³æè¿°",
                    placeholder="ä¾‹å¦‚: Realistic female voice in the 20s age with british accent. Normal pitch, warm timbre, conversational pacing.",
                    value="Realistic male voice in the 30s age with american accent. Normal pitch, warm timbre, conversational pacing.",
                    lines=3,
                )
                
                text = gr.Textbox(
                    label="è¦æœ—è¯»çš„æ–‡æœ¬",
                    placeholder="è¾“å…¥æƒ³è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬...",
                    value="Hello! This is Maya1 <laugh_harder> the best open source voice AI model with emotions.",
                    lines=4,
                )
                
                generate_btn = gr.Button("ğŸµ ç”Ÿæˆè¯­éŸ³", variant="primary", size="lg")
                
            with gr.Column():
                audio_output = gr.Audio(
                    label="ç”Ÿæˆçš„éŸ³é¢‘",
                    type="filepath",
                )
                
                status = gr.Textbox(
                    label="çŠ¶æ€",
                    interactive=False,
                    lines=2,
                )
        
        gr.Markdown(
            """
            ### æç¤º:
            - æè¿°å£°éŸ³ç‰¹å¾ï¼šå¹´é¾„ã€æ€§åˆ«ã€å£éŸ³ã€éŸ³é«˜ã€éŸ³è‰²ã€è¯­é€Ÿ
            - ä½¿ç”¨æ ‡ç­¾æ·»åŠ æƒ…æ„Ÿï¼Œå¦‚ `<laugh>`, `<sigh>`, `<gasp>`
            - ä¿æŒæ–‡æœ¬ç®€æ´ä»¥è·å¾—æ›´å¥½çš„è´¨é‡
            - ç”Ÿæˆå¯èƒ½éœ€è¦ 10-30 ç§’ï¼Œå…·ä½“å–å†³äºæ–‡æœ¬é•¿åº¦
            """
        )
        
        gr.Examples(
            examples=[
                [
                    "Young female voice with british accent. High pitch, bright timbre, energetic pacing.",
                    "Welcome to the future of AI voice technology! <laugh>"
                ],
                [
                    "Deep male voice with authoritative tone. Low pitch, rich timbre, slow pacing.",
                    "This is an important announcement. <pause> Please pay attention."
                ],
                [
                    "Cheerful female voice in the 20s. Normal pitch, warm and friendly tone.",
                    "Hi there! How are you doing today? <laugh_harder>"
                ],
            ],
            inputs=[description, text],
            label="ç¤ºä¾‹æç¤ºè¯"
        )
        
        generate_btn.click(
            fn=generate_speech,
            inputs=[description, text],
            outputs=[audio_output, status],
        )
    
    return demo


def main():
    """Main entry point."""
    print("å¯åŠ¨ Maya1 è¯­éŸ³ç”Ÿæˆ Web ç•Œé¢...")
    demo = create_ui()
    demo.queue()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
    )


if __name__ == "__main__":
    main()
