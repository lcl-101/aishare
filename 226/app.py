import re
import gradio as gr
from tqdm import tqdm
from argparse import ArgumentParser
from typing import Literal, List, Tuple
import sys
import importlib.util
import os
from datetime import datetime

import torch
import numpy as np  
import random    
import s3tokenizer

from soulxpodcast.models.soulxpodcast import SoulXPodcast
from soulxpodcast.config import Config, SoulXPodcastLLMConfig, SamplingParams
from soulxpodcast.utils.dataloader import (
    PodcastInferHandler,
    SPK_DICT, TEXT_START, TEXT_END, AUDIO_START, TASK_PODCAST
)


S1_PROMPT_WAV = "example/audios/female_mandarin.wav"  
S2_PROMPT_WAV = "example/audios/male_mandarin.wav"  

# æ¨¡å‹é…ç½®
AVAILABLE_MODELS = {
    "SoulX-Podcast-1.7B (åŸºç¡€æ¨¡å‹)": "checkpoints/SoulX-Podcast-1.7B",
    "SoulX-Podcast-1.7B-dialect (æ–¹è¨€æ¨¡å‹)": "checkpoints/SoulX-Podcast-1.7B-dialect",
}

def load_dialect_prompt_data():
    """
    åŠ è½½æ–¹è¨€æç¤ºæ–‡æœ¬æ–‡ä»¶å¹¶æ ¼å¼åŒ–ä¸ºåµŒå¥—å­—å…¸ã€‚
    è¿”å›ç»“æ„: {dialect_key: {display_name: full_text, ...}, ...}
    """
    dialect_data = {}
    
    dialect_files = [
        ("sichuan", "example/dialect_prompt/sichuan.txt", "<|Sichuan|>"),
        ("yueyu", "example/dialect_prompt/yueyu.txt", "<|Yue|>"),
        ("henan", "example/dialect_prompt/henan.txt", "<|Henan|>"),
    ]
    
    for key, file_path, prefix in dialect_files:
        dialect_data[key] = {"(æ— )": ""} 
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for i, line in enumerate(lines):
                    line = line.strip()
                    if line:
                        full_text = f"{prefix}{line}"
                        display_name = f"ä¾‹{i+1}: {line[:20]}..."
                        dialect_data[key][display_name] = full_text
        except FileNotFoundError:
            print(f"[WARNING] æ–¹è¨€æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        except Exception as e:
            print(f"[WARNING] è¯»å–æ–¹è¨€æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            
    return dialect_data

DIALECT_PROMPT_DATA = load_dialect_prompt_data()
DIALECT_CHOICES = ["(æ— )", "sichuan", "yueyu", "henan"]


EXAMPLES_LIST = [
    [
        None, "", "", None, "", "", ""
    ],
    [
        S1_PROMPT_WAV,
        "å–œæ¬¢æ”€å²©ã€å¾’æ­¥ã€æ»‘é›ªçš„è¯­è¨€çˆ±å¥½è€…ï¼Œä»¥åŠè¿‡ä¸¤å¤©è¦å¸¦ç€å…¨éƒ¨å®¶å½“å»æ™¯å¾·é•‡åšé™¶ç“·çš„ç™½æ—¥æ¢¦æƒ³å®¶ã€‚",
        "",
        S2_PROMPT_WAV,
        "å‘ƒï¼Œè¿˜æœ‰ä¸€ä¸ªå°±æ˜¯è¦è·Ÿå¤§å®¶çº æ­£ä¸€ç‚¹ï¼Œå°±æ˜¯æˆ‘ä»¬åœ¨çœ‹ç”µå½±çš„æ—¶å€™ï¼Œå°¤å…¶æ˜¯æ¸¸æˆç©å®¶ï¼Œçœ‹ç”µå½±çš„æ—¶å€™ï¼Œåœ¨çœ‹åˆ°é‚£ä¸ªåˆ°è¥¿åŒ—é‚£è¾¹çš„è¿™ä¸ªé™•åŒ—æ°‘è°£ï¼Œå—¯ï¼Œè¿™ä¸ªå¯èƒ½åœ¨æƒ³ï¼Œå“ï¼Œæ˜¯ä¸æ˜¯ä»–æ˜¯å—åˆ°äº†é»‘ç¥è¯çš„å¯å‘ï¼Ÿ",
        "",
        "[S1] å“ˆå–½ï¼ŒAIæ—¶ä»£çš„å†²æµªå…ˆé”‹ä»¬ï¼æ¬¢è¿æ”¶å¬ã€ŠAIç”Ÿæ´»è¿›è¡Œæ—¶ã€‹ã€‚å•Šï¼Œä¸€ä¸ªå……æ»¡äº†æœªæ¥æ„Ÿï¼Œç„¶åï¼Œè¿˜æœ‰ä¸€ç‚¹ç‚¹ï¼Œ<|laughter|>ç¥ç»è´¨çš„æ’­å®¢èŠ‚ç›®ï¼Œæˆ‘æ˜¯ä¸»æŒäººå°å¸Œã€‚\n[S2] å“ï¼Œå¤§å®¶å¥½å‘€ï¼æˆ‘æ˜¯èƒ½å” ï¼Œçˆ±å” ï¼Œå¤©å¤©éƒ½æƒ³å” çš„å” å—‘ï¼\n[S1] æœ€è¿‘æ´»å¾—ç‰¹åˆ«èµ›åšæœ‹å…‹å“ˆï¼ä»¥å‰è€æ˜¯è§‰å¾—AIæ˜¯ç§‘å¹»ç‰‡å„¿é‡Œçš„ï¼Œ<|sigh|> ç°åœ¨ï¼Œç°åœ¨è¿æˆ‘å¦ˆéƒ½ç”¨AIå†™å¹¿åœºèˆæ–‡æ¡ˆäº†ã€‚\n[S2] è¿™ä¸ªä¾‹å­å¾ˆç”ŸåŠ¨å•Šã€‚æ˜¯çš„ï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆå¼AIå“ˆï¼Œæ„Ÿè§‰éƒ½è¦ç‚¸äº†ï¼ è¯¶ï¼Œé‚£æˆ‘ä»¬ä»Šå¤©å°±èŠèŠAIæ˜¯æ€ä¹ˆèµ°è¿›æˆ‘ä»¬çš„ç”Ÿæ´»çš„å“ˆï¼",
    ],
    [
        S1_PROMPT_WAV,
        "å–œæ¬¢æ”€å²©ã€å¾’æ­¥ã€æ»‘é›ªçš„è¯­è¨€çˆ±å¥½è€…ï¼Œä»¥åŠè¿‡ä¸¤å¤©è¦å¸¦ç€å…¨éƒ¨å®¶å½“å»æ™¯å¾·é•‡åšé™¶ç“·çš„ç™½æ—¥æ¢¦æƒ³å®¶ã€‚",
        "<|Sichuan|>è¦å¾—è¦å¾—ï¼å‰å¤´å‡ ä¸ªè€æ´‹ç›˜ï¼Œæˆ‘åè„šå°±èƒŒèµ·é“ºç›–å·å»æ™¯å¾·é•‡è€æ³¥å·´ï¼Œå·´é€‚å¾—å–Šè€å¤©çˆ·ï¼",
        S2_PROMPT_WAV,
        "å‘ƒï¼Œè¿˜æœ‰ä¸€ä¸ªå°±æ˜¯è¦è·Ÿå¤§å®¶çº æ­£ä¸€ç‚¹ï¼Œå°±æ˜¯æˆ‘ä»¬åœ¨çœ‹ç”µå½±çš„æ—¶å€™ï¼Œå°¤å…¶æ˜¯æ¸¸æˆç©å®¶ï¼Œçœ‹ç”µå½±çš„æ—¶å€™ï¼Œåœ¨çœ‹åˆ°é‚£ä¸ªåˆ°è¥¿åŒ—é‚£è¾¹çš„è¿™ä¸ªé™•åŒ—æ°‘è°£ï¼Œå—¯ï¼Œè¿™ä¸ªå¯èƒ½åœ¨æƒ³ï¼Œå“ï¼Œæ˜¯ä¸æ˜¯ä»–æ˜¯å—åˆ°äº†é»‘ç¥è¯çš„å¯å‘ï¼Ÿ",
        "<|Sichuan|>å“å“Ÿå–‚ï¼Œè¿™ä¸ªæåäº†å™»ï¼é»‘ç¥è¯é‡Œå¤´å”±æ›²å­çš„ç‹äºŒæµªæ—©å…«ç™¾å¹´å°±åœ¨é»„åœŸé«˜å¡å¼ç§¦è…”å–½ï¼Œæ¸¸æˆç»„ä¸“é—¨è·‘åˆ‡å½•çš„åŸæ±¤åŸæ°´ï¼Œå¬å¾—äººæ±—æ¯›å„¿éƒ½ç«‹èµ·æ¥ï¼",
        "[S1] <|Sichuan|>å„ä½ã€Šå·´é€‚å¾—æ¿ã€‹çš„å¬ä¼—äº›ï¼Œå¤§å®¶å¥½å™»ï¼æˆ‘æ˜¯ä½ ä»¬ä¸»æŒäººæ™¶æ™¶ã€‚ä»Šå„¿å¤©æ°”ç¡¬æ˜¯å·´é€‚ï¼Œä¸æ™“å¾—å¤§å®¶æ˜¯åœ¨èµ¶è·¯å˜›ï¼Œè¿˜æ˜¯èŒ¶éƒ½æ³¡èµ·å’¯ï¼Œå‡†å¤‡è·Ÿæˆ‘ä»¬å¥½ç”Ÿæ‘†ä¸€å“ˆé¾™é—¨é˜µå–ƒï¼Ÿ\n[S2] <|Sichuan|>æ™¶æ™¶å¥½å“¦ï¼Œå¤§å®¶å®‰é€¸å™»ï¼æˆ‘æ˜¯æè€å€Œã€‚ä½ åˆšå¼€å£å°±å·å‘³åè¶³ï¼Œæ‘†é¾™é—¨é˜µå‡ ä¸ªå­—ä¸€ç”©å‡ºæ¥ï¼Œæˆ‘é¼»å­å¤´éƒ½é—»åˆ°èŒ¶é¦™è·Ÿç«é”…é¦™å’¯ï¼\n[S1] <|Sichuan|>å°±æ˜¯å¾—å˜›ï¼æè€å€Œï¼Œæˆ‘å‰äº›å¤©å¸¦ä¸ªå¤–åœ°æœ‹å‹åˆ‡äººæ°‘å…¬å›­é¹¤é¸£èŒ¶ç¤¾åäº†ä¸€å“ˆã€‚ä»–ç¡¬æ˜¯æä¸é†’è±ï¼Œä¸ºå•¥å­æˆ‘ä»¬ä¸€å †äººå›´åˆ°æ¯èŒ¶å°±å¯ä»¥å¹ä¸€ä¸‹åˆå£³å­ï¼Œä»éš”å£å­ç‹å¬¢å¬¢å¨ƒå„¿è€æœ‹å‹ï¼Œæ‰¯åˆ°ç¾å›½å¤§é€‰ï¼Œä¸­é—´è¿˜æºå‡ ç›˜æ–—åœ°ä¸»ã€‚ä»–è¯´æˆ‘ä»¬å››å·äººç®€ç›´æ˜¯æŠŠæ‘¸é±¼åˆ»è¿›éª¨å­é‡Œå¤´å’¯ï¼\n[S2] <|Sichuan|>ä½ é‚£ä¸ªæœ‹å‹è¯´å¾—å€’æ˜¯æœ‰ç‚¹å„¿è¶£ï¼Œä½†ä»–è«çœ‹åˆ°ç²¾é«“å™»ã€‚æ‘†é¾™é—¨é˜µå“ªæ˜¯æ‘¸é±¼å˜›ï¼Œè¿™æ˜¯æˆ‘ä»¬å·æ¸äººç‰¹æœ‰çš„äº¤é™…æ–¹å¼ï¼Œæ›´æ˜¯ä¸€ç§æ´»æ³•ã€‚å¤–çœäººå¤©å¤©è¯´çš„æ¾å¼›æ„Ÿï¼Œæ ¹æ ¹å„¿å°±åœ¨è¿™é¾™é—¨é˜µé‡Œå¤´ã€‚ä»Šå¤©æˆ‘ä»¬å°±è¦å¥½ç”Ÿæ‘†ä¸€å“ˆï¼Œä¸ºå•¥å­å››å·äººæ´»å¾—è¿™ä¹ˆèˆ’å¦ã€‚å°±å…ˆä»èŒ¶é¦†è¿™ä¸ªè€çªå­è¯´èµ·ï¼Œçœ‹å®ƒå’‹ä¸ªæˆäº†æˆ‘ä»¬å››å·äººçš„é­‚å„¿ï¼",
    ],
    [
        S1_PROMPT_WAV,
        "å–œæ¬¢æ”€å²©ã€å¾’æ­¥ã€æ»‘é›ªçš„è¯­è¨€çˆ±å¥½è€…ï¼Œä»¥åŠè¿‡ä¸¤å¤©è¦å¸¦ç€å…¨éƒ¨å®¶å½“å»æ™¯å¾·é•‡åšé™¶ç“·çš„ç™½æ—¥æ¢¦æƒ³å®¶ã€‚",
        "<|Yue|>çœŸä¿‚å†‡è®²é”™å•Šï¼æ”€å±±æ»‘é›ªå˜…è¯­è¨€ä¸“å®¶å‡ å·´é—­ï¼Œéƒ½å””åŠæˆ‘å¬æ—¥æ‹–æˆå‰¯èº«å®¶å»æ™¯å¾·é•‡ç©æ³¥å·´ï¼Œå‘¢é“ºçœŸç³»å‘å“‚ç™½æ—¥æ¢¦å’¯ï¼",
        S2_PROMPT_WAV,
        "å‘ƒï¼Œè¿˜æœ‰ä¸€ä¸ªå°±æ˜¯è¦è·Ÿå¤§å®¶çº æ­£ä¸€ç‚¹ï¼Œå°±æ˜¯æˆ‘ä»¬åœ¨çœ‹ç”µå½±çš„æ—¶å€™ï¼Œå°¤å…¶æ˜¯æ¸¸æˆç©å®¶ï¼Œçœ‹ç”µå½±çš„æ—¶å€™ï¼Œåœ¨çœ‹åˆ°é‚£ä¸ªåˆ°è¥¿åŒ—é‚£è¾¹çš„è¿™ä¸ªé™•åŒ—æ°‘è°£ï¼Œå—¯ï¼Œè¿™ä¸ªå¯èƒ½åœ¨æƒ³ï¼Œå“ï¼Œæ˜¯ä¸æ˜¯ä»–æ˜¯å—åˆ°äº†é»‘ç¥è¯çš„å¯å‘ï¼Ÿ",
        "<|Yue|>å’ªæé”™å•Šï¼é™•åŒ—æ°‘è°£å“åº¦å”±å’—å‡ åå¹´ï¼Œé»‘ç¥è¯è¾¹æœ‰å’å¤§é¢å•Šï¼Ÿä½ ä¼°ä½¢å“‹æŠ„æ¸¸æˆå’©ï¼",
        "[S1] <|Yue|>å“ˆå›‰å¤§å®¶å¥½å•Šï¼Œæ­¡è¿æ”¶è½æˆ‘å“‹å˜…ç¯€ç›®ã€‚å–‚ï¼Œæˆ‘ä»Šæ—¥æƒ³å•ä½ æ¨£å˜¢å•Šï¼Œä½ è¦ºå””è¦ºå¾—ï¼Œå—¯ï¼Œè€Œå®¶æ¸é›»å‹•è»Šï¼Œæœ€ç…©ï¼Œæœ€ç…©å˜…ä¸€æ¨£å˜¢ä¿‚å’©å•Šï¼Ÿ\n[S2] <|Yue|>æ¢—ä¿‚å……é›»å•¦ã€‚å¤§ä½¬å•Šï¼Œæµå€‹ä½éƒ½å·²ç¶“å¥½ç…©ï¼Œæµåˆ°å€‹ä½ä»²è¦å–ºåº¦ç­‰ï¼Œä½ è©±å¿«æ¥µéƒ½è¦åŠå€‹é˜ä¸€å€‹é˜ï¼ŒçœŸä¿‚ï¼Œæœ‰æ™‚è«—èµ·éƒ½è¦ºå¾—å¥½å†‡ç™®ã€‚\n[S1] <|Yue|>ä¿‚å’ªå…ˆã€‚å¦‚æœæˆ‘è€Œå®¶åŒä½ è¬›ï¼Œå……é›»å¯ä»¥å¿«åˆ°åŒå…¥æ²¹å·®å””å¤šæ™‚é–“ï¼Œä½ ä¿¡å””ä¿¡å…ˆï¼Ÿå–‚ä½ å¹³æ™‚å–ºæ²¹ç«™å…¥æ»¿ä¸€ç¼¸æ²¹ï¼Œè¦å¹¾è€å•Šï¼Ÿäº”å…­åˆ†é˜ï¼Ÿ\n[S2] <|Yue|>å·®å””å¤šå•¦ï¼Œä¸ƒå…«åˆ†é˜ï¼Œé»éƒ½èµ°å¾—å•¦ã€‚é›»è»Šå–ï¼Œå¯ä»¥åšåˆ°å’å¿«ï¼Ÿä½ å’ªç©å•¦ã€‚",
    ],
    [
        S1_PROMPT_WAV,
        "å–œæ¬¢æ”€å²©ã€å¾’æ­¥ã€æ»‘é›ªçš„è¯­è¨€çˆ±å¥½è€…ï¼Œä»¥åŠè¿‡ä¸¤å¤©è¦å¸¦ç€å…¨éƒ¨å®¶å½“å»æ™¯å¾·é•‡åšé™¶ç“·çš„ç™½æ—¥æ¢¦æƒ³å®¶ã€‚",
        "<|Henan|>ä¿ºè¿™ä¸æ˜¯æ€•æè·¯ä¸Šä¸å¾—åŠ²å„¿å˜›ï¼é‚£æ™¯å¾·é•‡ç“·æ³¥å¯å¨‡è´µç€å“©ï¼Œå¾—å…ˆæ‹¿å’±æ²³å—äººè¿™å®è¯šåŠ²å„¿ç»™å®ƒæ‰é€å–½ã€‚",
        S2_PROMPT_WAV,
        "å‘ƒï¼Œè¿˜æœ‰ä¸€ä¸ªå°±æ˜¯è¦è·Ÿå¤§å®¶çº æ­£ä¸€ç‚¹ï¼Œå°±æ˜¯æˆ‘ä»¬åœ¨çœ‹ç”µå½±çš„æ—¶å€™ï¼Œå°¤å…¶æ˜¯æ¸¸æˆç©å®¶ï¼Œçœ‹ç”µå½±çš„æ—¶å€™ï¼Œåœ¨çœ‹åˆ°é‚£ä¸ªåˆ°è¥¿åŒ—é‚£è¾¹çš„è¿™ä¸ªé™•åŒ—æ°‘è°£ï¼Œå—¯ï¼Œè¿™ä¸ªå¯èƒ½åœ¨æƒ³ï¼Œå“ï¼Œæ˜¯ä¸æ˜¯ä»–æ˜¯å—åˆ°äº†é»‘ç¥è¯çš„å¯å‘ï¼Ÿ",
        "<|Henan|>æè¿™æƒ³æ³•çœŸé—¹æŒºï¼é™•åŒ—æ°‘è°£æ¯”é»‘ç¥è¯æ—©å‡ ç™¾å¹´éƒ½æœ‰äº†ï¼Œå’±å¯ä¸å…´è¿™å¼„é¢ å€’å•Šï¼Œä¸­ä¸ï¼Ÿæè¿™æƒ³æ³•çœŸé—¹æŒºï¼é‚£é™•åŒ—æ°‘è°£åœ¨é»„åœŸé«˜å¡å“äº†å‡ ç™¾å¹´ï¼Œå’‹èƒ½è¯´æ˜¯è·Ÿé»‘ç¥è¯å­¦çš„å’§ï¼Ÿå’±å¾—æŠŠè¿™äº‹å„¿æ‹ç›´å–½ï¼Œä¸­ä¸ä¸­ï¼",
        "[S1] <|Henan|>å“ï¼Œå¤§å®¶å¥½å•Šï¼Œæ¬¢è¿æ”¶å¬å’±è¿™ä¸€æœŸå˜ã€ŠçèŠå‘—ï¼Œå°±è¿™ä¹ˆè¯´ã€‹ï¼Œæˆ‘æ˜¯æå˜è€æœ‹å‹ï¼Œç‡•å­ã€‚\n[S2] <|Henan|>å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯è€å¼ ã€‚ç‡•å­å•Šï¼Œä»Šå„¿ç…ç…ä½ è¿™ä¸ªåŠ²å„¿ï¼Œå’‹ç€ï¼Œæ˜¯æœ‰å•¥å¯å¾—åŠ²å˜äº‹å„¿æƒ³è·Ÿå’±å” å” ï¼Ÿ\n[S1] <|Henan|>å“å“Ÿï¼Œè€å¼ ï¼Œä½ å’‹ææ‡‚æˆ‘å˜ï¼æˆ‘è·Ÿä½ è¯´å•Šï¼Œæœ€è¿‘æˆ‘åˆ·æ‰‹æœºï¼Œè€æ˜¯åˆ·ä½äº›å¯é€—å˜æ–¹è¨€è§†é¢‘ï¼Œç‰¹åˆ«æ˜¯å’±æ²³å—è¯ï¼Œå’¦ï½æˆ‘å“©ä¸ªä¹–ä¹–ï¼Œä¸€å¬æˆ‘éƒ½æ†‹ä¸ä½ç¬‘ï¼Œå’‹è¯´å˜ï¼Œå¾—åŠ²å„¿å“©å¾ˆï¼Œè·Ÿå›åˆ°å®¶ä¸€æ ·ã€‚\n[S2] <|Henan|>ä½ è¿™å›å¯ç®—è¯´åˆ°æ ¹å„¿ä¸Šäº†ï¼æ²³å—è¯ï¼Œå’±å¾€å¤§å¤„è¯´è¯´ï¼Œä¸­åŸå®˜è¯ï¼Œå®ƒçœŸå˜æ˜¯æœ‰ä¸€è‚¡åŠ²å„¿æé‡Œå¤´ã€‚å®ƒå¯ä¸å…‰æ˜¯è¯´è¯ï¼Œå®ƒè„Šæ¢éª¨åå¤´è—å˜ï¼Œæ˜¯å’±ä¸€æ•´å¥—ã€é²œé²œæ´»æ´»å˜è¿‡æ³•å„¿ï¼Œä¸€ç§æ´»äººå˜é“ç†ã€‚\n[S1] <|Henan|>æ´»äººå˜é“ç†ï¼Ÿå“ï¼Œè¿™ä½ è¿™ä¸€è¯´ï¼Œæˆ‘å˜å…´è‡´â€œè…¾â€ä¸€ä¸‹å°±ä¸Šæ¥å•¦ï¼è§‰ä½å’±è¿™å—‘å„¿ï¼Œä¸€ä¸‹å„¿ä»æç¬‘è§†é¢‘è¹¿åˆ°æ–‡åŒ–é¡¶ä¸Šäº†ã€‚é‚£ä½ èµ¶ç´§ç»™æˆ‘ç™½è¯ç™½è¯ï¼Œè¿™é‡Œå¤´åˆ°åº•æœ‰å•¥é“é“å„¿ï¼Ÿæˆ‘ç‰¹åˆ«æƒ³çŸ¥é“â€”â€”ä¸ºå•¥ä¸€æèµ·å’±æ²³å—äººï¼Œå¥½äº›äººè„‘å­é‡Œâ€œè¹¦â€å‡ºæ¥å˜å¤´ä¸€ä¸ªè¯å„¿ï¼Œå°±æ˜¯å®åœ¨ï¼Ÿè¿™ä¸ªå®åœ¨ï¼Œéª¨å­é‡Œåˆ°åº•æ˜¯å•¥å˜ï¼Ÿ",
    ],
]


model: SoulXPodcast = None
dataset: PodcastInferHandler = None
current_model_path: str = None

def initiate_model(config: Config, enable_tn: bool=False):
    global model, dataset
    if model is None:
        model = SoulXPodcast(config)

    if dataset is None:
        dataset = PodcastInferHandler(model.llm.tokenizer, None, config)

def load_model(model_name: str, llm_engine: str, fp16_flow: bool, seed: int):
    """åŠ è½½é€‰å®šçš„æ¨¡å‹"""
    global model, dataset, current_model_path
    
    if model_name not in AVAILABLE_MODELS:
        return f"âŒ æœªçŸ¥æ¨¡å‹: {model_name}"
    
    model_path = AVAILABLE_MODELS[model_name]
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"
    
    # å¦‚æœå·²ç»åŠ è½½äº†ç›¸åŒçš„æ¨¡å‹ï¼Œè·³è¿‡
    if current_model_path == model_path and model is not None:
        return f"âœ… æ¨¡å‹å·²åŠ è½½: {model_name}"
    
    try:
        # æ¸…ç†æ—§æ¨¡å‹
        model = None
        dataset = None
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # åŠ è½½æ–°æ¨¡å‹
        hf_config = SoulXPodcastLLMConfig.from_initial_and_json(
            initial_values={"fp16_flow": fp16_flow}, 
            json_file=f"{model_path}/soulxpodcast_config.json"
        )
        
        # æ£€æŸ¥vllmæ˜¯å¦å¯ç”¨
        actual_llm_engine = llm_engine
        if llm_engine == "vllm":
            if not importlib.util.find_spec("vllm"):
                actual_llm_engine = "hf"
                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')[:-3]
                print(f"[{timestamp}] - [WARNING]: No install VLLM, switch to hf engine.")
        
        config = Config(
            model=model_path, 
            enforce_eager=True, 
            llm_engine=actual_llm_engine,
            hf_config=hf_config
        )
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(seed)
        np.random.seed(seed)
        random.seed(seed)
        
        initiate_model(config)
        current_model_path = model_path
        
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}\nè·¯å¾„: {model_path}\nå¼•æ“: {actual_llm_engine}"
    
    except Exception as e:
        return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"


_i18n_key2lang_dict = dict(
    # Speaker1 Prompt
    spk1_prompt_audio_label=dict(
        en="Speaker 1 Prompt Audio",
        zh="è¯´è¯äºº 1 å‚è€ƒè¯­éŸ³",
    ),
    spk1_prompt_text_label=dict(
        en="Speaker 1 Prompt Text",
        zh="è¯´è¯äºº 1 å‚è€ƒæ–‡æœ¬",
    ),
    spk1_prompt_text_placeholder=dict(
        en="text of speaker 1 Prompt audio.",
        zh="è¯´è¯äºº 1 å‚è€ƒæ–‡æœ¬",
    ),
    spk1_dialect_prompt_text_label=dict(
        en="Speaker 1 Dialect Prompt Text",
        zh="è¯´è¯äºº 1 æ–¹è¨€æç¤ºæ–‡æœ¬",
    ),
    spk1_dialect_prompt_text_placeholder=dict(
        en="Dialect prompt text with prefix: <|Sichuan|>/<|Yue|>/<|Henan|> ",
        zh="å¸¦å‰ç¼€æ–¹è¨€æç¤ºè¯æ€ç»´é“¾æ–‡æœ¬ï¼Œå‰ç¼€å¦‚ä¸‹ï¼š<|Sichuan|>/<|Yue|>/<|Henan|>ï¼Œå¦‚ï¼š<|Sichuan|>èµ°å˜›ï¼Œåˆ‡åƒé‚£å®¶æ–°å¼€çš„éº»è¾£çƒ«ï¼Œå¬åˆ«ä¸ªè¯´å‘³é“ç¡¬æ˜¯éœ¸é“å¾—å¾ˆï¼Œå¥½åƒåˆ°ä¸æ‘†äº†ï¼Œå»æ™šäº†è¿˜å¾—æ’é˜Ÿï¼",
    ),
    # Speaker2 Prompt
    spk2_prompt_audio_label=dict(
        en="Speaker 2 Prompt Audio",
        zh="è¯´è¯äºº 2 å‚è€ƒè¯­éŸ³",
    ),
    spk2_prompt_text_label=dict(
        en="Speaker 2 Prompt Text",
        zh="è¯´è¯äºº 2 å‚è€ƒæ–‡æœ¬",
    ),
    spk2_prompt_text_placeholder=dict(
        en="text of speaker 2 prompt audio.",
        zh="è¯´è¯äºº 2 å‚è€ƒæ–‡æœ¬",
    ),
    spk2_dialect_prompt_text_label=dict(
        en="Speaker 2 Dialect Prompt Text",
        zh="è¯´è¯äºº 2 æ–¹è¨€æç¤ºæ–‡æœ¬",
    ),
    spk2_dialect_prompt_text_placeholder=dict(
        en="Dialect prompt text with prefix: <|Sichuan|>/<|Yue|>/<|Henan|> ",
        zh="å¸¦å‰ç¼€æ–¹è¨€æç¤ºè¯æ€ç»´é“¾æ–‡æœ¬ï¼Œå‰ç¼€å¦‚ä¸‹ï¼š<|Sichuan|>/<|Yue|>/<|Henan|>ï¼Œå¦‚ï¼š<|Sichuan|>èµ°å˜›ï¼Œåˆ‡åƒé‚£å®¶æ–°å¼€çš„éº»è¾£çƒ«ï¼Œå¬åˆ«ä¸ªè¯´å‘³é“ç¡¬æ˜¯éœ¸é“å¾—å¾ˆï¼Œå¥½åƒåˆ°ä¸æ‘†äº†ï¼Œå»æ™šäº†è¿˜å¾—æ’é˜Ÿï¼",
    ),
    # Dialogue input textbox
    dialogue_text_input_label=dict(
        en="Dialogue Text Input",
        zh="åˆæˆæ–‡æœ¬è¾“å…¥",
    ),
    dialogue_text_input_placeholder=dict(
        en="[S1]text[S2]text[S1]text...",
        zh="[S1]æ–‡æœ¬[S2]æ–‡æœ¬[S1]æ–‡æœ¬...",
    ),
    # Generate button
    generate_btn_label=dict(
        en="Generate Audio",
        zh="åˆæˆ",
    ),
    # Generated audio
    generated_audio_label=dict(
        en="Generated Dialogue Audio",
        zh="åˆæˆçš„å¯¹è¯éŸ³é¢‘",
    ),
    # Warining1: invalid text for prompt
    warn_invalid_spk1_prompt_text=dict(
        en='Invalid speaker 1 prompt text, should not be empty and strictly follow: "xxx"',
        zh='è¯´è¯äºº 1 å‚è€ƒæ–‡æœ¬ä¸åˆè§„ï¼Œä¸èƒ½ä¸ºç©ºï¼Œæ ¼å¼ï¼š"xxx"',
    ),
    warn_invalid_spk2_prompt_text=dict(
        en='Invalid speaker 2 prompt text, should strictly follow: "[S2]xxx"',
        zh='è¯´è¯äºº 2 å‚è€ƒæ–‡æœ¬ä¸åˆè§„ï¼Œæ ¼å¼ï¼š"[S2]xxx"',
    ),
    warn_invalid_dialogue_text=dict(
        en='Invalid dialogue input text, should strictly follow: "[S1]xxx[S2]xxx..."',
        zh='å¯¹è¯æ–‡æœ¬è¾“å…¥ä¸åˆè§„ï¼Œæ ¼å¼ï¼š"[S1]xxx[S2]xxx..."',
    ),
    # Warining3: incomplete prompt info
    warn_incomplete_prompt=dict(
        en="Please provide prompt audio and text for both speaker 1 and speaker 2",
        zh="è¯·æä¾›è¯´è¯äºº 1 ä¸è¯´è¯äºº 2 çš„å‚è€ƒè¯­éŸ³ä¸å‚è€ƒæ–‡æœ¬",
    ),
)


global_lang: Literal["zh", "en"] = "zh"

def i18n(key):
    global global_lang
    return _i18n_key2lang_dict[key][global_lang]

def check_monologue_text(text: str, prefix: str = None) -> bool:
    text = text.strip()
    # Check speaker tags
    if prefix is not None and (not text.startswith(prefix)):
        return False
    # Remove prefix
    if prefix is not None:
        text = text.removeprefix(prefix)
    text = text.strip()
    # If empty?
    if len(text) == 0:
        return False
    return True

def check_dialect_prompt_text(text: str, prefix: str = None) -> bool:
    text = text.strip()
    # Check Dialect Prompt prefix tags
    if prefix is not None and (not text.startswith(prefix)):
        return False
    text = text.strip()
    # If empty?
    if len(text) == 0:
        return False
    return True

def check_dialogue_text(text_list: List[str]) -> bool:
    if len(text_list) == 0:
        return False
    for text in text_list:
        if not (
            check_monologue_text(text, "[S1]")
            or check_monologue_text(text, "[S2]")
            or check_monologue_text(text, "[S3]")
            or check_monologue_text(text, "[S4]")
        ):
            return False
    return True

def process_single(target_text_list, prompt_wav_list, prompt_text_list, use_dialect_prompt, dialect_prompt_text):
    spks, texts = [], []
    for target_text in target_text_list:
        pattern = r'(\[S[1-9]\])(.+)'
        match = re.match(pattern, target_text)
        text, spk = match.group(2), int(match.group(1)[2])-1
        spks.append(spk)
        texts.append(text)
    
    global dataset
    dataitem = {"key": "001", "prompt_text": prompt_text_list, "prompt_wav": prompt_wav_list, 
             "text": texts, "spk": spks, }
    if use_dialect_prompt:
        dataitem.update({
            "dialect_prompt_text": dialect_prompt_text
        })
    dataset.update_datasource(
        [
           dataitem 
        ]
    )        

    # assert one data only;
    data = dataset[0]
    prompt_mels_for_llm, prompt_mels_lens_for_llm = s3tokenizer.padding(data["log_mel"])  # [B, num_mels=128, T]
    spk_emb_for_flow = torch.tensor(data["spk_emb"])
    prompt_mels_for_flow = torch.nn.utils.rnn.pad_sequence(data["mel"], batch_first=True, padding_value=0)  # [B, T', num_mels=80]
    prompt_mels_lens_for_flow = torch.tensor(data['mel_len'])
    text_tokens_for_llm = data["text_tokens"]
    prompt_text_tokens_for_llm = data["prompt_text_tokens"]
    spk_ids = data["spks_list"]
    sampling_params = SamplingParams(use_ras=True,win_size=25,tau_r=0.2)
    infos = [data["info"]]
    processed_data = {
        "prompt_mels_for_llm": prompt_mels_for_llm,
        "prompt_mels_lens_for_llm": prompt_mels_lens_for_llm,
        "prompt_text_tokens_for_llm": prompt_text_tokens_for_llm,
        "text_tokens_for_llm": text_tokens_for_llm,
        "prompt_mels_for_flow_ori": prompt_mels_for_flow,
        "prompt_mels_lens_for_flow": prompt_mels_lens_for_flow,
        "spk_emb_for_flow": spk_emb_for_flow,
        "sampling_params": sampling_params,
        "spk_ids": spk_ids,
        "infos": infos,
        "use_dialect_prompt": use_dialect_prompt,
    }
    if use_dialect_prompt:
        processed_data.update({
            "dialect_prompt_text_tokens_for_llm": data["dialect_prompt_text_tokens"],
            "dialect_prefix": data["dialect_prefix"],
        })
    return processed_data


def dialogue_synthesis_function(
    target_text: str,
    spk1_prompt_text: str | None = "",
    spk1_prompt_audio: str | None = None,
    spk1_dialect_prompt_text: str | None = "",
    spk2_prompt_text: str | None = "",
    spk2_prompt_audio: str | None = None,
    spk2_dialect_prompt_text: str | None = "",
    seed: int = 1988,
):
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    global model
    if model is None:
        gr.Warning(message="è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½æ¨¡å‹ï¼")
        return None
    
    seed = int(seed)
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

    # Check prompt info
    target_text_list: List[str] = re.findall(r"(\[S[0-9]\][^\[\]]*)", target_text)
    target_text_list = [text.strip() for text in target_text_list]
    if not check_dialogue_text(target_text_list):
        gr.Warning(message=i18n("warn_invalid_dialogue_text"))
        return None

    # Go synthesis
    progress_bar = gr.Progress(track_tqdm=True)
    prompt_wav_list = [spk1_prompt_audio, spk2_prompt_audio]
    prompt_text_list = [spk1_prompt_text, spk2_prompt_text] 
    use_dialect_prompt = spk1_dialect_prompt_text.strip()!="" or spk2_dialect_prompt_text.strip()!=""
    dialect_prompt_text_list = [spk1_dialect_prompt_text, spk2_dialect_prompt_text]
    data = process_single(
        target_text_list,
        prompt_wav_list,
        prompt_text_list,
        use_dialect_prompt,
        dialect_prompt_text_list,
    )
    results_dict = model.forward_longform(
        **data
    )
    target_audio = None
    for i in range(len(results_dict['generated_wavs'])):
        if target_audio is None:
            target_audio = results_dict['generated_wavs'][i]
        else:
            target_audio = torch.concat([target_audio, results_dict['generated_wavs'][i]], axis=1)
    return (24000, target_audio.cpu().squeeze(0).numpy())


def update_example_choices(dialect_key: str):

    if dialect_key == "(æ— )":
        choices = ["(è¯·å…ˆé€‰æ‹©æ–¹è¨€)"]

        return gr.update(choices=choices, value="(æ— )"), gr.update(choices=choices, value="(æ— )")
    
    choices = list(DIALECT_PROMPT_DATA.get(dialect_key, {}).keys())

    return gr.update(choices=choices, value="(æ— )"), gr.update(choices=choices, value="(æ— )")

def update_prompt_text(dialect_key: str, example_key: str):
    if dialect_key == "(æ— )" or example_key in ["(æ— )", "(è¯·å…ˆé€‰æ‹©æ–¹è¨€)"]:
        return gr.update(value="")
    

    full_text = DIALECT_PROMPT_DATA.get(dialect_key, {}).get(example_key, "")
    return gr.update(value=full_text)


def render_interface() -> gr.Blocks:
    with gr.Blocks(title="SoulX-Podcast - æ¨¡å‹é€‰æ‹©å™¨", theme=gr.themes.Soft()) as page:
        
        gr.Markdown("# ğŸ™ï¸ SoulX-Podcast æ’­å®¢ç”Ÿæˆç³»ç»Ÿ")
        gr.Markdown("é€‰æ‹©æ¨¡å‹åï¼Œç³»ç»Ÿå°†è‡ªåŠ¨åŠ è½½ï¼Œç„¶åæ‚¨å¯ä»¥å¼€å§‹ç”Ÿæˆæ’­å®¢éŸ³é¢‘ã€‚")
        
        # æ¨¡å‹é€‰æ‹©åŒºåŸŸ
        with gr.Group():
            gr.Markdown("## ğŸ“¦ æ¨¡å‹é…ç½®")
            with gr.Row():
                model_selector = gr.Dropdown(
                    choices=list(AVAILABLE_MODELS.keys()),
                    value=list(AVAILABLE_MODELS.keys())[0],
                    label="é€‰æ‹©æ¨¡å‹",
                    interactive=True,
                    scale=3,
                )
                model_seed_input = gr.Number(
                    label="æ¨¡å‹éšæœºç§å­",
                    value=1988,
                    step=1,
                    interactive=True,
                    scale=1,
                )
                load_model_btn = gr.Button(
                    value="ğŸš€ åŠ è½½æ¨¡å‹",
                    variant="primary",
                    size="lg",
                    scale=1,
                )
            
            model_status = gr.Textbox(
                label="æ¨¡å‹çŠ¶æ€",
                value="æœªåŠ è½½æ¨¡å‹",
                interactive=False,
                lines=2,
            )
            
            # éšè—çš„é»˜è®¤å‚æ•°
            llm_engine_selector = gr.Textbox(value="hf", visible=False)
            fp16_flow_checkbox = gr.Checkbox(value=False, visible=False)
        
        gr.Markdown("---")
        
        # åŸæœ‰çš„ç•Œé¢å…ƒç´ 
        with gr.Row():
            lang_choice = gr.Radio(
                choices=["ä¸­æ–‡", "English"],
                value="ä¸­æ–‡",
                label="Display Language/æ˜¾ç¤ºè¯­è¨€",
                type="index",
                interactive=True,
                scale=3,
            )
            seed_input = gr.Number(
                label="Seed (ç§å­)",
                value=1988,
                step=1,
                interactive=True,
                scale=1,
            )

        with gr.Row():

            with gr.Column(scale=1):
                with gr.Group(visible=True) as spk1_prompt_group:
                    spk1_prompt_audio = gr.Audio(
                        label=i18n("spk1_prompt_audio_label"),
                        type="filepath",
                        editable=False,
                        interactive=True,
                    )
                    spk1_prompt_text = gr.Textbox(
                        label=i18n("spk1_prompt_text_label"),
                        placeholder=i18n("spk1_prompt_text_placeholder"),
                        lines=3,
                    )
                    spk1_dialect_prompt_text = gr.Textbox(
                        label=i18n("spk1_dialect_prompt_text_label"),
                        placeholder=i18n("spk1_dialect_prompt_text_placeholder"),
                        value="",
                        lines=3,
                    )

            with gr.Column(scale=1, visible=True):
                with gr.Group(visible=True) as spk2_prompt_group:
                    spk2_prompt_audio = gr.Audio(
                        label=i18n("spk2_prompt_audio_label"),
                        type="filepath",
                        editable=False,
                        interactive=True,
                    )
                    spk2_prompt_text = gr.Textbox(
                        label=i18n("spk2_prompt_text_label"),
                        placeholder=i18n("spk2_prompt_text_placeholder"),
                        lines=3,
                    )
                    spk2_dialect_prompt_text = gr.Textbox(
                        label=i18n("spk2_dialect_prompt_text_label"),
                        placeholder=i18n("spk2_dialect_prompt_text_placeholder"),
                        value="",
                        lines=3,
                    )

            with gr.Column(scale=2):
                with gr.Row():
                    dialogue_text_input = gr.Textbox(
                        label=i18n("dialogue_text_input_label"),
                        placeholder=i18n("dialogue_text_input_placeholder"),
                        lines=18,
                    )

        # Generate button
        with gr.Row():
            generate_btn = gr.Button(
                value=i18n("generate_btn_label"), 
                variant="primary", 
                scale=3,
                size="lg",
            )
        
        # Long output audio
        generate_audio = gr.Audio(
            label=i18n("generated_audio_label"),
            interactive=False,
        )


        with gr.Row():
            inputs_for_examples = [
                spk1_prompt_audio,
                spk1_prompt_text,
                spk1_dialect_prompt_text,
                spk2_prompt_audio,
                spk2_prompt_text,
                spk2_dialect_prompt_text,
                dialogue_text_input,
            ]
            
            gr.Examples(
                examples=EXAMPLES_LIST,
                inputs=inputs_for_examples,
                label="æ’­å®¢æ¨¡æ¿ç¤ºä¾‹ (ç‚¹å‡»åŠ è½½)",
                examples_per_page=5,
            )
        
        # æ¨¡å‹åŠ è½½äº‹ä»¶
        load_model_btn.click(
            fn=load_model,
            inputs=[model_selector, llm_engine_selector, fp16_flow_checkbox, model_seed_input],
            outputs=[model_status],
        )
        
        def _change_component_language(lang):
            global global_lang
            global_lang = ["zh", "en"][lang]
            return [
                
                # spk1_prompt_{audio,text,dialect_prompt_text}
                gr.update(label=i18n("spk1_prompt_audio_label")),
                gr.update(
                    label=i18n("spk1_prompt_text_label"),
                    placeholder=i18n("spk1_prompt_text_placeholder"),
                ),
                gr.update(
                    label=i18n("spk1_dialect_prompt_text_label"),
                    placeholder=i18n("spk1_dialect_prompt_text_placeholder"),
                ),
                # spk2_prompt_{audio,text}
                gr.update(label=i18n("spk2_prompt_audio_label")),
                gr.update(
                    label=i18n("spk2_prompt_text_label"),
                    placeholder=i18n("spk2_prompt_text_placeholder"),
                ),
                gr.update(
                    label=i18n("spk2_dialect_prompt_text_label"),
                    placeholder=i18n("spk2_dialect_prompt_text_placeholder"),
                ),
                # dialogue_text_input
                gr.update(
                    label=i18n("dialogue_text_input_label"),
                    placeholder=i18n("dialogue_text_input_placeholder"),
                ),
                # generate_btn
                gr.update(value=i18n("generate_btn_label")),
                # generate_audio
                gr.update(label=i18n("generated_audio_label")),
            ]

        lang_choice.change(
            fn=_change_component_language,
            inputs=[lang_choice],
            outputs=[
                spk1_prompt_audio,
                spk1_prompt_text,
                spk1_dialect_prompt_text,
                spk2_prompt_audio,
                spk2_prompt_text,
                spk2_dialect_prompt_text,
                dialogue_text_input,
                generate_btn,
                generate_audio,
            ],
        )
        
        generate_btn.click(
            fn=dialogue_synthesis_function,
            inputs=[
                dialogue_text_input,
                spk1_prompt_text,
                spk1_prompt_audio,
                spk1_dialect_prompt_text,
                spk2_prompt_text,
                spk2_prompt_audio,
                spk2_dialect_prompt_text,
                seed_input,
            ],
            outputs=[generate_audio],
        )
    return page


def get_args():
    parser = ArgumentParser()
    parser.add_argument('--port',
                        type=int,
                        default=7860,
                        help='gradio port for web app')
    parser.add_argument('--share',
                        action='store_true',
                        help='enable gradio share link')
    args = parser.parse_args()
    return args


if __name__ == "__main__":
    args = get_args()

    print("[INFO] SoulX-Podcast æ¨¡å‹é€‰æ‹©å™¨å¯åŠ¨")
    print("[INFO] å¯ç”¨æ¨¡å‹:")
    for name, path in AVAILABLE_MODELS.items():
        exists = "âœ…" if os.path.exists(path) else "âŒ"
        print(f"  {exists} {name}: {path}")
    
    page = render_interface()
    page.queue()
    page.launch(share=args.share, server_name="0.0.0.0", server_port=args.port)
