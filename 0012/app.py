import gc
import logging
import os
import random
import subprocess
import time

import gradio as gr
import imageio
import numpy as np
import requests
import torch
from diffusers.image_processor import VaeImageProcessor
from diffusers.utils import load_image
from PIL import Image

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    force=True,
    handlers=[logging.StreamHandler()],
)


class _ListWithDevice(list):
    """list å­ç±»ï¼Œæ·»åŠ  .device å±æ€§ä»¥å…¼å®¹ x.device è®¿é—®"""
    @property
    def device(self):
        return self[0].device if len(self) > 0 else torch.device("cpu")


def patch_transformer_forward(transformer):
    """
    ä¿®å¤ WanModel.forward ä¸­ x ä¸º list æ—¶ x.device æŠ¥é”™çš„é—®é¢˜ã€‚
    åŸä»£ç ç¬¬ 738 è¡Œ: if not block_offload and self.freqs.device != x.device and isinstance(x, torch.Tensor)
    çŸ­è·¯æ±‚å€¼é¡ºåºæœ‰è¯¯ï¼Œx.device åœ¨ isinstance æ£€æŸ¥ä¹‹å‰å°±è¢«æ‰§è¡Œäº†ã€‚
    é€šè¿‡å°† list åŒ…è£…ä¸ºå¸¦ .device å±æ€§çš„å­ç±»æ¥é¿å…å´©æºƒã€‚
    """
    cls = type(transformer)
    if getattr(cls, '_forward_patched', False):
        return

    _original_forward = cls.forward

    def _patched_forward(self, x, *args, **kwargs):
        if isinstance(x, list) and not isinstance(x, _ListWithDevice):
            x = _ListWithDevice(x)
        return _original_forward(self, x, *args, **kwargs)

    cls.forward = _patched_forward
    cls._forward_patched = True

# ============ æ¨¡å‹è·¯å¾„ ============
MODEL_PATH_R2V = "checkpoints/SkyReels-V3-R2V-14B"
MODEL_PATH_V2V = "checkpoints/SkyReels-V3-V2V-14B"
MODEL_PATH_A2V = "checkpoints/SkyReels-V3-A2V-19B"

# ============ å…¨å±€ pipeline ç¼“å­˜ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰ ============
_pipelines = {}

# ============ ç¤ºä¾‹èµ„æºä¸‹è½½ ============
EXAMPLE_DIR = "assets/examples"
os.makedirs(EXAMPLE_DIR, exist_ok=True)

EXAMPLE_URLS = {
    "ref_1.png": "https://skyreels-api.oss-accelerate.aliyuncs.com/examples/subject_reference/0_1.png",
    "ref_2.png": "https://skyreels-api.oss-accelerate.aliyuncs.com/examples/subject_reference/0_2.png",
    "test_video.mp4": "https://skyreels-api.oss-accelerate.aliyuncs.com/examples/video_extension/test.mp4",
    "avatar_woman.JPEG": "https://skyreels-api.oss-accelerate.aliyuncs.com/examples/talking_avatar_video/woman.JPEG",
    "avatar_woman_speech.mp3": "https://skyreels-api.oss-accelerate.aliyuncs.com/examples/talking_avatar_video/single_actor/woman_speech.mp3",
}


def download_example_file(filename, url):
    filepath = os.path.join(EXAMPLE_DIR, filename)
    if not os.path.exists(filepath):
        print(f"æ­£åœ¨ä¸‹è½½ç¤ºä¾‹æ–‡ä»¶: {filename}")
        r = requests.get(url, stream=True)
        with open(filepath, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"ä¸‹è½½å®Œæˆ: {filename}")
    return filepath


print("æ­£åœ¨æ£€æŸ¥å¹¶ä¸‹è½½ç¤ºä¾‹èµ„æº...")
for filename, url in EXAMPLE_URLS.items():
    download_example_file(filename, url)
print("ç¤ºä¾‹èµ„æºå‡†å¤‡å®Œæˆï¼")

# ============ ç¤ºä¾‹æç¤ºè¯ ============
PROMPT_REF_TO_VIDEO = "In a dimly lit, cluttered occult club room adorned with shelves full of books, skulls, and mysterious dolls, two young Asian girls are talking. One girl has vibrant teal pigtails with bangs, wearing a white collared polo shirt, while the other has a sleek black bob with bangs, also in a white polo shirt, conversing under the hum of fluorescent lights, a high-quality and detailed cinematic shot."

PROMPT_SINGLE_SHOT = "A man is making his way forward slowly, leaning on a white cane to prop himself up."

PROMPT_SHOT_SWITCHING = "[ZOOM_IN_CUT] The scene cuts from a medium shot of a visually impaired man walking on a path in a park. The shot then cut in to a close-up of the man's face and upper torso. The visually impaired Black man is shown from the chest up, wearing dark sunglasses, a grey turtleneck scarf, and a light olive green jacket. His head is held straight, looking forward towards the camera, continuing his walk. The lighting is natural and bright. The background is a soft blur of green trees and foliage from the park."

PROMPT_TALKING_AVATAR = "A woman is giving a speech. She is confident, poised, and joyful. Use a static shot."


# ============ Pipeline åŠ è½½/å¸è½½ ============
def unload_all_pipelines():
    """å¸è½½æ‰€æœ‰å·²åŠ è½½çš„ pipelineï¼Œé‡Šæ”¾æ˜¾å­˜"""
    global _pipelines
    for name in list(_pipelines.keys()):
        del _pipelines[name]
    _pipelines.clear()
    gc.collect()
    torch.cuda.empty_cache()
    logging.info("å·²å¸è½½æ‰€æœ‰ pipelineï¼Œé‡Šæ”¾æ˜¾å­˜")


def get_r2v_pipeline():
    global _pipelines
    if "r2v" not in _pipelines:
        unload_all_pipelines()
        from skyreels_v3.pipelines import ReferenceToVideoPipeline
        logging.info("æ­£åœ¨åŠ è½½ Reference-to-Video pipeline...")
        _pipelines["r2v"] = ReferenceToVideoPipeline(
            model_path=MODEL_PATH_R2V,
            offload=True,
        )
        logging.info("Reference-to-Video pipeline åŠ è½½å®Œæˆ")
    return _pipelines["r2v"]


def get_single_shot_pipeline():
    global _pipelines
    if "single_shot" not in _pipelines:
        unload_all_pipelines()
        from skyreels_v3.pipelines import SingleShotExtensionPipeline
        logging.info("æ­£åœ¨åŠ è½½ Single-shot Extension pipeline...")
        _pipelines["single_shot"] = SingleShotExtensionPipeline(
            model_path=MODEL_PATH_V2V,
            offload=True,
        )
        patch_transformer_forward(_pipelines["single_shot"].transformer)
        logging.info("Single-shot Extension pipeline åŠ è½½å®Œæˆ")
    return _pipelines["single_shot"]


def get_shot_switching_pipeline():
    global _pipelines
    if "shot_switching" not in _pipelines:
        unload_all_pipelines()
        from skyreels_v3.pipelines import ShotSwitchingExtensionPipeline
        logging.info("æ­£åœ¨åŠ è½½ Shot Switching Extension pipeline...")
        _pipelines["shot_switching"] = ShotSwitchingExtensionPipeline(
            model_path=MODEL_PATH_V2V,
            offload=True,
        )
        patch_transformer_forward(_pipelines["shot_switching"].transformer)
        logging.info("Shot Switching Extension pipeline åŠ è½½å®Œæˆ")
    return _pipelines["shot_switching"]


def get_talking_avatar_pipeline():
    global _pipelines
    if "talking_avatar" not in _pipelines:
        unload_all_pipelines()
        from skyreels_v3.configs import WAN_CONFIGS
        from skyreels_v3.pipelines import TalkingAvatarPipeline
        config = WAN_CONFIGS["talking-avatar-19B"]
        logging.info("æ­£åœ¨åŠ è½½ Talking Avatar pipeline...")
        _pipelines["talking_avatar"] = TalkingAvatarPipeline(
            config=config,
            model_path=MODEL_PATH_A2V,
            device_id=0,
            rank=0,
            offload=True,
        )
        logging.info("Talking Avatar pipeline åŠ è½½å®Œæˆ")
    return _pipelines["talking_avatar"]


# ============ ä¿å­˜è§†é¢‘ ============
def save_video(video_frames, task_type, seed, fps=24, input_data=None):
    """ä¿å­˜è§†é¢‘å¸§ä¸º mp4 æ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶è·¯å¾„"""
    save_dir = os.path.join("result", task_type)
    os.makedirs(save_dir, exist_ok=True)
    current_time = time.strftime("%Y-%m-%d_%H-%M-%S", time.localtime())
    video_out_file = f"{seed}_{current_time}.mp4"
    output_path = os.path.join(save_dir, video_out_file)

    imageio.mimwrite(
        output_path,
        video_frames,
        fps=fps,
        quality=8,
        output_params=["-loglevel", "error"],
    )

    # å¯¹äº talking_avatarï¼Œåˆå¹¶éŸ³é¢‘
    if task_type == "talking_avatar" and input_data is not None:
        video_with_audio_path = os.path.join(save_dir, video_out_file.replace(".mp4", "_with_audio.mp4"))
        audio_path = input_data["video_audio"]
        video_in = os.path.abspath(output_path)
        audio_in = os.path.abspath(audio_path)
        video_out_with_audio = os.path.abspath(video_with_audio_path)
        cmd = [
            "ffmpeg", "-y",
            "-i", video_in,
            "-i", audio_in,
            "-map", "0:v",
            "-map", "1:a",
            "-c:v", "copy",
            "-shortest",
            video_out_with_audio,
        ]
        try:
            subprocess.run(
                cmd, check=True,
                stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
            )
            logging.info(f"å¸¦éŸ³é¢‘è§†é¢‘ç”ŸæˆæˆåŠŸ: {video_with_audio_path}")
            os.remove(video_in)
            return video_with_audio_path
        except subprocess.CalledProcessError as e:
            logging.error(f"ffmpeg åˆå¹¶éŸ³é¢‘å¤±è´¥: {e.stdout}")
            return output_path

    return output_path


# ============ å‚è€ƒå›¾ç”Ÿæˆè§†é¢‘ ============
def reference_to_video(ref_img1, ref_img2, ref_img3, ref_img4, prompt, duration, seed, progress=gr.Progress()):
    ref_img_paths = [img for img in [ref_img1, ref_img2, ref_img3, ref_img4] if img is not None and img != ""]
    if len(ref_img_paths) == 0:
        return None, "âŒ é”™è¯¯ï¼šè¯·è‡³å°‘ä¸Šä¼ ä¸€å¼ å‚è€ƒå›¾ç‰‡"

    try:
        progress(0.1, desc="æ­£åœ¨åŠ è½½æ¨¡å‹...")
        pipe = get_r2v_pipeline()

        # åŠ è½½å‚è€ƒå›¾ç‰‡
        ref_imgs = [load_image(p) for p in ref_img_paths]

        progress(0.3, desc="æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
        video_frames = pipe.generate_video(
            ref_imgs=ref_imgs,
            prompt=prompt,
            duration=int(duration),
            seed=int(seed),
        )

        progress(0.9, desc="æ­£åœ¨ä¿å­˜è§†é¢‘...")
        output_path = save_video(video_frames, "reference_to_video", int(seed), fps=24)
        return output_path, f"âœ… ç”ŸæˆæˆåŠŸï¼è§†é¢‘ä¿å­˜è‡³: {output_path}"
    except Exception as e:
        logging.exception("å‚è€ƒå›¾ç”Ÿæˆè§†é¢‘å¤±è´¥")
        return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


# ============ å•æ®µè§†é¢‘æ‰©å±• ============
def single_shot_extension(input_video, prompt, duration, seed, progress=gr.Progress()):
    if input_video is None or input_video == "":
        return None, "âŒ é”™è¯¯ï¼šè¯·ä¸Šä¼ è¾“å…¥è§†é¢‘"

    try:
        progress(0.1, desc="æ­£åœ¨åŠ è½½æ¨¡å‹...")
        pipe = get_single_shot_pipeline()

        progress(0.3, desc="æ­£åœ¨æ‰©å±•è§†é¢‘...")
        video_frames = pipe.extend_video(
            raw_video=input_video,
            prompt=prompt,
            duration=int(duration),
            seed=int(seed),
        )

        progress(0.9, desc="æ­£åœ¨ä¿å­˜è§†é¢‘...")
        output_path = save_video(video_frames, "single_shot_extension", int(seed), fps=24)
        return output_path, f"âœ… ç”ŸæˆæˆåŠŸï¼è§†é¢‘ä¿å­˜è‡³: {output_path}"
    except Exception as e:
        logging.exception("å•æ®µè§†é¢‘æ‰©å±•å¤±è´¥")
        return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


# ============ é•œå¤´åˆ‡æ¢æ‰©å±• ============
def shot_switching_extension(input_video, prompt, seed, progress=gr.Progress()):
    if input_video is None or input_video == "":
        return None, "âŒ é”™è¯¯ï¼šè¯·ä¸Šä¼ è¾“å…¥è§†é¢‘"

    try:
        progress(0.1, desc="æ­£åœ¨åŠ è½½æ¨¡å‹...")
        pipe = get_shot_switching_pipeline()

        progress(0.3, desc="æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
        video_frames = pipe.extend_video(
            raw_video=input_video,
            prompt=prompt,
            duration=5,
            seed=int(seed),
        )

        progress(0.9, desc="æ­£åœ¨ä¿å­˜è§†é¢‘...")
        output_path = save_video(video_frames, "shot_switching_extension", int(seed), fps=24)
        return output_path, f"âœ… ç”ŸæˆæˆåŠŸï¼è§†é¢‘ä¿å­˜è‡³: {output_path}"
    except Exception as e:
        logging.exception("é•œå¤´åˆ‡æ¢æ‰©å±•å¤±è´¥")
        return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


# ============ è¯´è¯å¤´åƒç”Ÿæˆ ============
def talking_avatar(input_image, input_audio, prompt, seed, progress=gr.Progress()):
    if input_image is None or input_image == "":
        return None, "âŒ é”™è¯¯ï¼šè¯·ä¸Šä¼ è‚–åƒå›¾ç‰‡"
    if input_audio is None or input_audio == "":
        return None, "âŒ é”™è¯¯ï¼šè¯·ä¸Šä¼ é©±åŠ¨éŸ³é¢‘"

    try:
        progress(0.1, desc="æ­£åœ¨åŠ è½½æ¨¡å‹...")
        pipe = get_talking_avatar_pipeline()

        # å‡†å¤‡è¾“å…¥æ•°æ®
        input_data = {
            "prompt": prompt,
            "cond_image": input_image,
            "cond_audio": {"person1": input_audio},
        }

        progress(0.2, desc="æ­£åœ¨é¢„å¤„ç†éŸ³é¢‘...")
        from skyreels_v3.utils.avatar_preprocess import preprocess_audio
        input_data, _ = preprocess_audio(MODEL_PATH_A2V, input_data, "processed_audio")

        progress(0.3, desc="æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
        kwargs = {
            "input_data": input_data,
            "size_buckget": "720P",
            "motion_frame": 5,
            "frame_num": 81,
            "drop_frame": 12,
            "shift": 11,
            "text_guide_scale": 1.0,
            "audio_guide_scale": 1.0,
            "seed": int(seed),
            "sampling_steps": 4,
            "max_frames_num": 5000,
        }
        video_frames = pipe.generate(**kwargs)

        progress(0.9, desc="æ­£åœ¨ä¿å­˜è§†é¢‘...")
        output_path = save_video(video_frames, "talking_avatar", int(seed), fps=25, input_data=input_data)
        return output_path, f"âœ… ç”ŸæˆæˆåŠŸï¼è§†é¢‘ä¿å­˜è‡³: {output_path}"
    except Exception as e:
        logging.exception("è¯´è¯å¤´åƒç”Ÿæˆå¤±è´¥")
        return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


# ============ ä½¿ç”¨ç¤ºä¾‹ ============
def use_example_ref_imgs():
    return (
        os.path.join(EXAMPLE_DIR, "ref_1.png"),
        os.path.join(EXAMPLE_DIR, "ref_2.png"),
        None,
        None,
    )


def use_example_video():
    return os.path.join(EXAMPLE_DIR, "test_video.mp4")


def use_example_avatar():
    return (
        os.path.join(EXAMPLE_DIR, "avatar_woman.JPEG"),
        os.path.join(EXAMPLE_DIR, "avatar_woman_speech.mp3"),
    )


# ============ Gradio ç•Œé¢ ============
with gr.Blocks(title="SkyReels-V3 è§†é¢‘ç”Ÿæˆ") as demo:

    # é¡¶éƒ¨ YouTube é¢‘é“ä¿¡æ¯
    gr.HTML("""
    <div style='text-align:center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 10px; margin-bottom: 20px;'>
        <h1 style='color: white; margin: 0;'>ğŸ¬ SkyReels-V3 è§†é¢‘ç”Ÿæˆ</h1>
        <p style='color: white; margin: 10px 0 0 0; font-size: 16px;'>
            ğŸ“º <b>AI æŠ€æœ¯åˆ†äº«é¢‘é“</b> |
            <a href='https://www.youtube.com/@rongyi-ai' target='_blank' style='color: #ffeb3b; text-decoration: none;'>
                https://www.youtube.com/@rongyi-ai
            </a>
        </p>
    </div>
    """)

    with gr.Tabs():
        # ============ Tab 1: å‚è€ƒå›¾ç”Ÿæˆè§†é¢‘ ============
        with gr.TabItem("ğŸ“· å‚è€ƒå›¾ç”Ÿæˆè§†é¢‘"):
            gr.Markdown("""
            ### å‚è€ƒå›¾ç”Ÿæˆè§†é¢‘ (Reference-to-Video)
            ä»1-4å¼ å‚è€ƒå›¾ç‰‡å’Œæ–‡å­—æç¤ºç”Ÿæˆè¿è´¯çš„è§†é¢‘åºåˆ—ï¼Œæ“…é•¿ä¿æŒè§’è‰²ã€ç‰©ä½“å’ŒèƒŒæ™¯çš„èº«ä»½ä¸€è‡´æ€§ã€‚
            - **æ¨èè¾“å‡º**: 5ç§’è§†é¢‘ï¼Œ720pï¼Œ24fps
            """)

            with gr.Row():
                with gr.Column(scale=1):
                    ref_img1 = gr.Image(label="å‚è€ƒå›¾ç‰‡ 1", type="filepath")
                    ref_img2 = gr.Image(label="å‚è€ƒå›¾ç‰‡ 2", type="filepath")
                with gr.Column(scale=1):
                    ref_img3 = gr.Image(label="å‚è€ƒå›¾ç‰‡ 3 (å¯é€‰)", type="filepath")
                    ref_img4 = gr.Image(label="å‚è€ƒå›¾ç‰‡ 4 (å¯é€‰)", type="filepath")

            example_btn1 = gr.Button("ğŸ“¥ ä½¿ç”¨ç¤ºä¾‹å›¾ç‰‡", variant="secondary")

            prompt_ref = gr.Textbox(label="æç¤ºè¯", value=PROMPT_REF_TO_VIDEO, lines=4)

            with gr.Row():
                duration_ref = gr.Slider(label="è§†é¢‘æ—¶é•¿ (ç§’)", minimum=1, maximum=5, value=5, step=1)
                seed_ref = gr.Number(label="éšæœºç§å­", value=42)

            btn_ref = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
            status_ref = gr.Textbox(label="çŠ¶æ€", interactive=False)
            output_ref = gr.Video(label="ç”Ÿæˆç»“æœ")

            example_btn1.click(use_example_ref_imgs, outputs=[ref_img1, ref_img2, ref_img3, ref_img4])
            btn_ref.click(
                reference_to_video,
                inputs=[ref_img1, ref_img2, ref_img3, ref_img4, prompt_ref, duration_ref, seed_ref],
                outputs=[output_ref, status_ref],
            )

        # ============ Tab 2: å•æ®µè§†é¢‘æ‰©å±• ============
        with gr.TabItem("ğŸï¸ å•æ®µè§†é¢‘æ‰©å±•"):
            gr.Markdown("""
            ### å•æ®µè§†é¢‘æ‰©å±• (Single-shot Extension)
            æ‰©å±•ç°æœ‰è§†é¢‘ï¼ŒåŒæ—¶ä¿æŒè¿åŠ¨è¿ç»­æ€§ã€åœºæ™¯ä¸€è‡´æ€§å’Œä¸»ä½“èº«ä»½ã€‚
            - **æ‰©å±•æ—¶é•¿**: 5-30ç§’
            """)

            input_video_single = gr.Video(label="è¾“å…¥è§†é¢‘")
            example_btn2 = gr.Button("ğŸ“¥ ä½¿ç”¨ç¤ºä¾‹è§†é¢‘", variant="secondary")

            prompt_single = gr.Textbox(label="æç¤ºè¯", value=PROMPT_SINGLE_SHOT, lines=3)

            with gr.Row():
                duration_single = gr.Slider(label="æ‰©å±•æ—¶é•¿ (ç§’)", minimum=5, maximum=30, value=5, step=1)
                seed_single = gr.Number(label="éšæœºç§å­", value=42)

            btn_single = gr.Button("ğŸ¬ æ‰©å±•è§†é¢‘", variant="primary", size="lg")
            status_single = gr.Textbox(label="çŠ¶æ€", interactive=False)
            output_single = gr.Video(label="ç”Ÿæˆç»“æœ")

            example_btn2.click(use_example_video, outputs=input_video_single)
            btn_single.click(
                single_shot_extension,
                inputs=[input_video_single, prompt_single, duration_single, seed_single],
                outputs=[output_single, status_single],
            )

        # ============ Tab 3: é•œå¤´åˆ‡æ¢æ‰©å±• ============
        with gr.TabItem("ğŸ¥ é•œå¤´åˆ‡æ¢æ‰©å±•"):
            gr.Markdown("""
            ### é•œå¤´åˆ‡æ¢æ‰©å±• (Shot Switching Extension)
            æ”¯æŒç”µå½±çº§é•œå¤´è½¬æ¢ï¼Œå¦‚æ¨è¿› (Cut-In)ã€æ‹‰è¿œ (Cut-Out)ã€æ­£åæ‰“ (Shot/Reverse Shot) ç­‰ã€‚
            - **æœ€å¤§æ—¶é•¿**: 5ç§’
            - **æç¤ºè¯å‰ç¼€**: `[ZOOM_IN_CUT]`, `[ZOOM_OUT_CUT]`, `[SHOT_REVERSE_SHOT]`, `[MULTI_ANGLE]`, `[CUT_AWAY]`
            """)

            input_video_switch = gr.Video(label="è¾“å…¥è§†é¢‘")
            example_btn3 = gr.Button("ğŸ“¥ ä½¿ç”¨ç¤ºä¾‹è§†é¢‘", variant="secondary")

            prompt_switch = gr.Textbox(label="æç¤ºè¯", value=PROMPT_SHOT_SWITCHING, lines=5)

            seed_switch = gr.Number(label="éšæœºç§å­", value=42)

            btn_switch = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
            status_switch = gr.Textbox(label="çŠ¶æ€", interactive=False)
            output_switch = gr.Video(label="ç”Ÿæˆç»“æœ")

            example_btn3.click(use_example_video, outputs=input_video_switch)
            btn_switch.click(
                shot_switching_extension,
                inputs=[input_video_switch, prompt_switch, seed_switch],
                outputs=[output_switch, status_switch],
            )

        # ============ Tab 4: è¯´è¯å¤´åƒç”Ÿæˆ ============
        with gr.TabItem("ğŸ—£ï¸ è¯´è¯å¤´åƒç”Ÿæˆ"):
            gr.Markdown("""
            ### è¯´è¯å¤´åƒç”Ÿæˆ (Talking Avatar)
            ä»å•å¼ è‚–åƒå’ŒéŸ³é¢‘ç‰‡æ®µç”Ÿæˆé€¼çœŸçš„è¯´è¯å¤´åƒè§†é¢‘ã€‚
            - **æ”¯æŒå›¾ç‰‡æ ¼å¼**: jpg/jpeg, png, gif, bmp
            - **æ”¯æŒéŸ³é¢‘æ ¼å¼**: mp3, wav
            - **æœ€å¤§éŸ³é¢‘æ—¶é•¿**: 200ç§’
            """)

            with gr.Row():
                input_image_avatar = gr.Image(label="è‚–åƒå›¾ç‰‡", type="filepath")
                input_audio_avatar = gr.Audio(label="é©±åŠ¨éŸ³é¢‘", type="filepath")

            example_btn4 = gr.Button("ğŸ“¥ ä½¿ç”¨ç¤ºä¾‹å›¾ç‰‡å’ŒéŸ³é¢‘", variant="secondary")

            prompt_avatar = gr.Textbox(label="æç¤ºè¯", value=PROMPT_TALKING_AVATAR, lines=2)

            seed_avatar = gr.Number(label="éšæœºç§å­", value=42)

            btn_avatar = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
            status_avatar = gr.Textbox(label="çŠ¶æ€", interactive=False)
            output_avatar = gr.Video(label="ç”Ÿæˆç»“æœ")

            example_btn4.click(use_example_avatar, outputs=[input_image_avatar, input_audio_avatar])
            btn_avatar.click(
                talking_avatar,
                inputs=[input_image_avatar, input_audio_avatar, prompt_avatar, seed_avatar],
                outputs=[output_avatar, status_avatar],
            )

    gr.Markdown("""
    ---
    <div style='text-align: center; color: #666;'>
        åŸºäº <b>SkyReels-V3</b> | æ¨¡å‹è·¯å¾„: checkpoints/
    </div>
    """)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
