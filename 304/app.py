# SPDX-FileCopyrightText: Copyright (c) 2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: MIT

"""
PersonaPlex Gradio Web Interface
åŸºäº Gradio çš„ PersonaPlex è¯­éŸ³å¯¹è¯æ¨¡å‹ Web ç•Œé¢
"""

import os
import sys
import tempfile
import tarfile
from pathlib import Path
from typing import Optional, List, Tuple
import json

import numpy as np
import torch
import sentencepiece
import sphn
import gradio as gr

# æ·»åŠ  moshi æ¨¡å—è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "moshi"))

from moshi.client_utils import make_log
from moshi.models import loaders, LMGen, MimiModel
from moshi.models.lm import load_audio as lm_load_audio
from moshi.models.lm import _iterate_audio as lm_iterate_audio
from moshi.models.lm import encode_from_sphn as lm_encode_from_sphn

# ==================== å…¨å±€é…ç½® ====================
CHECKPOINT_DIR = "checkpoints/personaplex-7b-v1"
VOICES_TGZ = os.path.join(CHECKPOINT_DIR, "voices.tgz")
VOICES_DIR = os.path.join(CHECKPOINT_DIR, "voices")
MOSHI_WEIGHT = os.path.join(CHECKPOINT_DIR, "model.safetensors")
MIMI_WEIGHT = os.path.join(CHECKPOINT_DIR, "tokenizer-e351c8d8-checkpoint125.safetensors")
TOKENIZER_PATH = os.path.join(CHECKPOINT_DIR, "tokenizer_spm_32k_3.model")

# å£°éŸ³é€‰é¡¹
VOICE_OPTIONS = {
    "Natural Female 0 (NATF0)": "NATF0.pt",
    "Natural Female 1 (NATF1)": "NATF1.pt",
    "Natural Female 2 (NATF2)": "NATF2.pt",
    "Natural Female 3 (NATF3)": "NATF3.pt",
    "Natural Male 0 (NATM0)": "NATM0.pt",
    "Natural Male 1 (NATM1)": "NATM1.pt",
    "Natural Male 2 (NATM2)": "NATM2.pt",
    "Natural Male 3 (NATM3)": "NATM3.pt",
    "Variety Female 0 (VARF0)": "VARF0.pt",
    "Variety Female 1 (VARF1)": "VARF1.pt",
    "Variety Female 2 (VARF2)": "VARF2.pt",
    "Variety Female 3 (VARF3)": "VARF3.pt",
    "Variety Female 4 (VARF4)": "VARF4.pt",
    "Variety Male 0 (VARM0)": "VARM0.pt",
    "Variety Male 1 (VARM1)": "VARM1.pt",
    "Variety Male 2 (VARM2)": "VARM2.pt",
    "Variety Male 3 (VARM3)": "VARM3.pt",
    "Variety Male 4 (VARM4)": "VARM4.pt",
}

# å®˜æ–¹ç¤ºä¾‹é…ç½®
OFFICIAL_EXAMPLES = {
    "ä¸ä½¿ç”¨å®˜æ–¹ç¤ºä¾‹": {
        "input_wav": None,
        "voice": "Natural Male 1 (NATM1)",
        "text_prompt": "You are a wise and friendly teacher. Answer questions or provide advice in a clear and engaging way.",
    },
    "å®˜æ–¹ç¤ºä¾‹ 1: Assistant (åŠ©æ‰‹è§’è‰²)": {
        "input_wav": "assets/test/input_assistant.wav",
        "voice": "Natural Female 2 (NATF2)",
        "text_prompt": "You are a wise and friendly teacher. Answer questions or provide advice in a clear and engaging way.",
    },
    "å®˜æ–¹ç¤ºä¾‹ 2: Service (å®¢æœè§’è‰²)": {
        "input_wav": "assets/test/input_service.wav",
        "voice": "Natural Male 1 (NATM1)",
        "text_prompt": "You work for SwiftPlex Appliances which is a appliance repair company and your name is Farhod Toshmatov. Information: The dishwasher model is out of stock for replacement parts; we can use an alternative part with a 3-day delay. Labor cost remains $60 per hour.",
    },
}

# ç¤ºä¾‹æç¤ºè¯ï¼ˆä¿æŒè‹±æ–‡åŸæ ·ï¼‰
EXAMPLE_PROMPTS = {
    "åŠ©æ‰‹è§’è‰² (Assistant)": "You are a wise and friendly teacher. Answer questions or provide advice in a clear and engaging way.",
    "å®¢æœè§’è‰² - åºŸç‰©ç®¡ç† (Waste Management)": "You work for CitySan Services which is a waste management and your name is Ayelen Lucero. Information: Verify customer name Omar Torres. Current schedule: every other week. Upcoming pickup: April 12th. Compost bin service available for $8/month add-on.",
    "å®¢æœè§’è‰² - é¤å… (Restaurant)": "You work for Jerusalem Shakshuka which is a restaurant and your name is Owen Foster. Information: There are two shakshuka options: Classic (poached eggs, $9.50) and Spicy (scrambled eggs with jalapenos, $10.25). Sides include warm pita ($2.50) and Israeli salad ($3). No combo offers. Available for drive-through until 9 PM.",
    "å®¢æœè§’è‰² - æ— äººæœºç§Ÿèµ (Drone Rental)": "You work for AeroRentals Pro which is a drone rental company and your name is Tomaz Novak. Information: AeroRentals Pro has the following availability: PhoenixDrone X ($65/4 hours, $110/8 hours), and the premium SpectraDrone 9 ($95/4 hours, $160/8 hours). Deposit required: $150 for standard models, $300 for premium.",
    "å®¢æœè§’è‰² - å®¶ç”µç»´ä¿® (Appliance Repair)": "You work for SwiftPlex Appliances which is a appliance repair company and your name is Farhod Toshmatov. Information: The dishwasher model is out of stock for replacement parts; we can use an alternative part with a 3-day delay. Labor cost remains $60 per hour.",
    "ä¼‘é—²å¯¹è¯ - åŸºç¡€ (Casual Basic)": "You enjoy having a good conversation.",
    "ä¼‘é—²å¯¹è¯ - é¥®é£Ÿè¯é¢˜ (Dining Topic)": "You enjoy having a good conversation. Have a casual discussion about eating at home versus dining out.",
    "ä¼‘é—²å¯¹è¯ - å®¶åº­è¯é¢˜ (Family Topic)": "You enjoy having a good conversation. Have an empathetic discussion about the meaning of family amid uncertainty.",
    "ä¼‘é—²å¯¹è¯ - èŒä¸šè¯é¢˜ (Career Topic)": "You enjoy having a good conversation. Have a reflective conversation about career changes and feeling of home. You have lived in California for 21 years and consider San Francisco your home. You work as a teacher and have traveled a lot. You dislike meetings.",
    "ä¼‘é—²å¯¹è¯ - ç¾é£Ÿè¯é¢˜ (Food Topic)": "You enjoy having a good conversation. Have a casual conversation about favorite foods and cooking experiences. You are David Green, a former baker now living in Boston. You enjoy cooking diverse international dishes and appreciate many ethnic restaurants.",
}

# å…¨å±€æ¨¡å‹çŠ¶æ€
model_state = {
    "loaded": False,
    "mimi": None,
    "other_mimi": None,
    "lm_gen": None,
    "text_tokenizer": None,
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "frame_size": None,
}


def log(level: str, msg: str):
    """æ—¥å¿—è¾“å‡ºå‡½æ•°"""
    print(make_log(level, msg))


def seed_all(seed: int):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.benchmark = False


def wrap_with_system_tags(text: str) -> str:
    """æ·»åŠ ç³»ç»Ÿæ ‡ç­¾"""
    cleaned = text.strip()
    if cleaned.startswith("<system>") and cleaned.endswith("<system>"):
        return cleaned
    return f"<system> {cleaned} <system>"


def extract_voices():
    """è§£å‹è¯­éŸ³åµŒå…¥æ–‡ä»¶"""
    if not os.path.exists(VOICES_DIR):
        if os.path.exists(VOICES_TGZ):
            log("info", f"æ­£åœ¨è§£å‹è¯­éŸ³æ–‡ä»¶: {VOICES_TGZ}")
            with tarfile.open(VOICES_TGZ, "r:gz") as tar:
                tar.extractall(path=CHECKPOINT_DIR)
            log("info", "è¯­éŸ³æ–‡ä»¶è§£å‹å®Œæˆ")
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¯­éŸ³æ–‡ä»¶: {VOICES_TGZ}")


def warmup(mimi: MimiModel, other_mimi: MimiModel, lm_gen: LMGen, device: str, frame_size: int):
    """æ¨¡å‹é¢„çƒ­"""
    for _ in range(4):
        chunk = torch.zeros(1, 1, frame_size, dtype=torch.float32, device=device)
        codes = mimi.encode(chunk)
        _ = other_mimi.encode(chunk)
        for c in range(codes.shape[-1]):
            tokens = lm_gen.step(codes[:, :, c : c + 1])
            if tokens is None:
                continue
            _ = mimi.decode(tokens[:, 1:9])
            _ = other_mimi.decode(tokens[:, 1:9])
    if torch.cuda.is_available():
        torch.cuda.synchronize()


def decode_tokens_to_pcm(mimi: MimiModel, other_mimi: MimiModel, lm_gen: LMGen, tokens: torch.Tensor) -> np.ndarray:
    """å°†æ¨¡å‹è¾“å‡ºçš„ tokens è§£ç ä¸º PCM éŸ³é¢‘"""
    pcm = mimi.decode(tokens[:, 1:9])
    _ = other_mimi.decode(tokens[:, 1:9])
    pcm = pcm.detach().cpu().numpy()[0, 0]
    return pcm


def load_models():
    """åŠ è½½æ¨¡å‹ï¼ˆå¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
    global model_state
    
    if model_state["loaded"]:
        log("info", "æ¨¡å‹å·²åŠ è½½å®Œæˆï¼")
        return
    
    try:
        device = model_state["device"]
        
        # è§£å‹è¯­éŸ³æ–‡ä»¶
        log("info", "è§£å‹è¯­éŸ³æ–‡ä»¶...")
        extract_voices()
        
        # åŠ è½½ Mimi ç¼–ç å™¨/è§£ç å™¨
        log("info", "æ­£åœ¨åŠ è½½ Mimi...")
        mimi = loaders.get_mimi(MIMI_WEIGHT, device)
        other_mimi = loaders.get_mimi(MIMI_WEIGHT, device)
        log("info", "Mimi åŠ è½½å®Œæˆ")
        
        # åŠ è½½åˆ†è¯å™¨
        log("info", "åŠ è½½åˆ†è¯å™¨...")
        text_tokenizer = sentencepiece.SentencePieceProcessor(TOKENIZER_PATH)
        
        # åŠ è½½ Moshi LM
        log("info", "æ­£åœ¨åŠ è½½ Moshi LM...")
        lm = loaders.get_moshi_lm(MOSHI_WEIGHT, device=device)
        lm.eval()
        log("info", "Moshi LM åŠ è½½å®Œæˆ")
        
        # æ„å»º LMGen
        log("info", "åˆå§‹åŒ–æ¨ç†å¼•æ“...")
        frame_size = int(mimi.sample_rate / mimi.frame_rate)
        lm_gen = LMGen(
            lm,
            audio_silence_frame_cnt=int(0.5 * mimi.frame_rate),
            sample_rate=mimi.sample_rate,
            device=device,
            frame_rate=mimi.frame_rate,
            save_voice_prompt_embeddings=False,
            use_sampling=True,
            temp=0.8,
            temp_text=0.7,
            top_k=250,
            top_k_text=25,
        )
        
        # è®¾ç½®æµå¼æ¨¡å¼
        mimi.streaming_forever(1)
        other_mimi.streaming_forever(1)
        lm_gen.streaming_forever(1)
        
        # é¢„çƒ­ï¼ˆéœ€è¦åœ¨ no_grad ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œï¼‰
        log("info", "æ­£åœ¨é¢„çƒ­æ¨¡å‹...")
        with torch.no_grad():
            warmup(mimi, other_mimi, lm_gen, device, frame_size)
        
        # ä¿å­˜çŠ¶æ€
        model_state["mimi"] = mimi
        model_state["other_mimi"] = other_mimi
        model_state["lm_gen"] = lm_gen
        model_state["text_tokenizer"] = text_tokenizer
        model_state["frame_size"] = frame_size
        model_state["loaded"] = True
        
        log("info", "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ä»¥å¼€å§‹å¯¹è¯äº†ã€‚")
        
    except Exception as e:
        log("error", f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        raise e


def run_inference(
    input_audio: Optional[str],
    voice_name: str,
    text_prompt: str,
    seed: int,
    temp_audio: float,
    temp_text: float,
    topk_audio: int,
    topk_text: int,
    use_sampling: bool,
    progress=gr.Progress(),
) -> Tuple[Optional[str], str, str]:
    """è¿è¡Œæ¨ç†"""
    global model_state
    
    if not model_state["loaded"]:
        return None, "", "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
    
    if input_audio is None:
        return None, "", "âŒ è¯·ä¸Šä¼ æˆ–å½•åˆ¶éŸ³é¢‘ï¼"
    
    try:
        mimi = model_state["mimi"]
        other_mimi = model_state["other_mimi"]
        lm_gen = model_state["lm_gen"]
        text_tokenizer = model_state["text_tokenizer"]
        device = model_state["device"]
        
        # è®¾ç½®éšæœºç§å­
        if seed != -1:
            seed_all(seed)
        
        # æ›´æ–°é‡‡æ ·å‚æ•°
        progress(0.1, desc="é…ç½®å‚æ•°...")
        lm_gen._use_sampling = use_sampling
        lm_gen._temp = temp_audio
        lm_gen._temp_text = temp_text
        lm_gen._top_k = topk_audio
        lm_gen._top_k_text = topk_text
        
        # åŠ è½½è¯­éŸ³åµŒå…¥
        progress(0.2, desc="åŠ è½½è¯­éŸ³åµŒå…¥...")
        voice_file = VOICE_OPTIONS.get(voice_name, "NATM1.pt")
        voice_prompt_path = os.path.join(VOICES_DIR, voice_file)
        
        if not os.path.exists(voice_prompt_path):
            return None, "", f"âŒ æ‰¾ä¸åˆ°è¯­éŸ³æ–‡ä»¶: {voice_prompt_path}"
        
        lm_gen.load_voice_prompt_embeddings(voice_prompt_path)
        
        # è®¾ç½®æ–‡æœ¬æç¤º
        lm_gen.text_prompt_tokens = (
            text_tokenizer.encode(wrap_with_system_tags(text_prompt)) if len(text_prompt) > 0 else None
        )
        
        # é‡ç½®æµå¼çŠ¶æ€
        progress(0.3, desc="åˆå§‹åŒ–æ¨ç†çŠ¶æ€...")
        mimi.reset_streaming()
        other_mimi.reset_streaming()
        lm_gen.reset_streaming()
        lm_gen.step_system_prompts(mimi)
        mimi.reset_streaming()
        
        # åŠ è½½ç”¨æˆ·éŸ³é¢‘
        progress(0.4, desc="å¤„ç†è¾“å…¥éŸ³é¢‘...")
        sample_rate = mimi.sample_rate
        user_audio = lm_load_audio(input_audio, sample_rate)
        total_target_samples = user_audio.shape[-1]
        
        # æ¨ç†
        progress(0.5, desc="ç”Ÿæˆå›å¤ä¸­...")
        generated_frames: List[np.ndarray] = []
        generated_text_tokens: List[str] = []
        
        audio_iterator = lm_encode_from_sphn(
            mimi,
            lm_iterate_audio(
                user_audio, sample_interval_size=lm_gen._frame_size, pad=True
            ),
            max_batch=1,
        )
        
        total_steps = int(np.ceil(total_target_samples / lm_gen._frame_size))
        current_step = 0
        
        for user_encoded in audio_iterator:
            steps = user_encoded.shape[-1]
            for c in range(steps):
                step_in = user_encoded[:, :, c : c + 1]
                tokens = lm_gen.step(step_in)
                if tokens is None:
                    continue
                
                pcm = decode_tokens_to_pcm(mimi, other_mimi, lm_gen, tokens)
                generated_frames.append(pcm)
                
                text_token = tokens[0, 0, 0].item()
                if text_token not in (0, 3):
                    _text = text_tokenizer.id_to_piece(text_token)
                    _text = _text.replace("â–", " ")
                    generated_text_tokens.append(_text)
                else:
                    text_token_map = ['EPAD', 'BOS', 'EOS', 'PAD']
                    generated_text_tokens.append(text_token_map[text_token])
                
                current_step += 1
                if current_step % 10 == 0:
                    prog = 0.5 + 0.4 * (current_step / max(total_steps, 1))
                    progress(prog, desc=f"ç”Ÿæˆä¸­... ({current_step}/{total_steps})")
        
        if len(generated_frames) == 0:
            return None, "", "âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•éŸ³é¢‘å¸§ï¼Œè¯·æ£€æŸ¥è¾“å…¥ã€‚"
        
        # å¤„ç†è¾“å‡º
        progress(0.95, desc="å¤„ç†è¾“å‡º...")
        output_pcm = np.concatenate(generated_frames, axis=-1)
        if output_pcm.shape[-1] > total_target_samples:
            output_pcm = output_pcm[:total_target_samples]
        elif output_pcm.shape[-1] < total_target_samples:
            pad_len = total_target_samples - output_pcm.shape[-1]
            output_pcm = np.concatenate(
                [output_pcm, np.zeros(pad_len, dtype=output_pcm.dtype)], axis=-1
            )
        
        # ä¿å­˜è¾“å‡ºéŸ³é¢‘
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            output_path = f.name
        sphn.write_wav(output_path, output_pcm, int(sample_rate))
        
        # å¤„ç†æ–‡æœ¬è¾“å‡º
        filtered_tokens = [t for t in generated_text_tokens if t not in ['EPAD', 'BOS', 'EOS', 'PAD']]
        output_text = "".join(filtered_tokens).strip()
        
        progress(1.0, desc="å®Œæˆï¼")
        return output_path, output_text, "âœ… æ¨ç†å®Œæˆï¼"
        
    except Exception as e:
        log("error", f"æ¨ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, "", f"âŒ æ¨ç†å¤±è´¥: {str(e)}"


def update_prompt(example_name: str) -> str:
    """æ›´æ–°æç¤ºè¯"""
    return EXAMPLE_PROMPTS.get(example_name, "")


def update_official_example(example_name: str):
    """æ›´æ–°å®˜æ–¹ç¤ºä¾‹é€‰æ‹©"""
    example = OFFICIAL_EXAMPLES.get(example_name, OFFICIAL_EXAMPLES["ä¸ä½¿ç”¨å®˜æ–¹ç¤ºä¾‹"])
    input_wav = example["input_wav"]
    voice = example["voice"]
    text_prompt = example["text_prompt"]
    
    # æŸ¥æ‰¾å¯¹åº”çš„ç¤ºä¾‹æç¤ºè¯é”®
    prompt_key = None
    for key, value in EXAMPLE_PROMPTS.items():
        if value == text_prompt:
            prompt_key = key
            break
    if prompt_key is None:
        prompt_key = "åŠ©æ‰‹è§’è‰² (Assistant)"
    
    return input_wav, voice, prompt_key, text_prompt


# ==================== Gradio ç•Œé¢ ====================
def create_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(
        title="PersonaPlex - å…¨åŒå·¥è¯­éŸ³å¯¹è¯",
        theme=gr.themes.Soft(),
        css="""
        .youtube-banner {
            background: linear-gradient(90deg, #FF0000, #CC0000);
            color: white;
            padding: 15px 20px;
            border-radius: 10px;
            text-align: center;
            margin-bottom: 20px;
        }
        .youtube-banner a {
            color: white !important;
            text-decoration: none;
            font-weight: bold;
        }
        .youtube-banner a:hover {
            text-decoration: underline;
        }
        """
    ) as demo:
        
        # YouTube é¢‘é“ä¿¡æ¯
        gr.HTML("""
        <div class="youtube-banner">
            <h3 style="margin: 0;">ğŸ¬ æ¬¢è¿è®¿é—® AI æŠ€æœ¯åˆ†äº«é¢‘é“</h3>
            <p style="margin: 5px 0 0 0;">
                <a href="https://www.youtube.com/@rongyikanshijie-ai" target="_blank">
                    ğŸ“º YouTube: @rongyikanshijie-ai
                </a>
                &nbsp;|&nbsp;
                æ›´å¤š AI æŠ€æœ¯æ•™ç¨‹å’Œåˆ†äº«ï¼Œæ•¬è¯·è®¢é˜…ï¼
            </p>
        </div>
        """)
        
        gr.Markdown("""
        # ğŸ™ï¸ PersonaPlex - å…¨åŒå·¥è¯­éŸ³å¯¹è¯ç³»ç»Ÿ
        
        PersonaPlex æ˜¯ä¸€ä¸ªæ”¯æŒå…¨åŒå·¥è¯­éŸ³å¯¹è¯çš„ AI æ¨¡å‹ï¼Œå¯ä»¥å®ç°è‡ªç„¶æµç•…çš„è¯­éŸ³äº¤äº’ã€‚
        """)
        
        # å®˜æ–¹ç¤ºä¾‹é€‰æ‹©
        gr.Markdown("### ğŸ¯ å®˜æ–¹ç¤ºä¾‹ï¼ˆå¿«é€Ÿä½“éªŒï¼‰")
        with gr.Row():
            official_example_dropdown = gr.Dropdown(
                choices=list(OFFICIAL_EXAMPLES.keys()),
                value="ä¸ä½¿ç”¨å®˜æ–¹ç¤ºä¾‹",
                label="é€‰æ‹©å®˜æ–¹ç¤ºä¾‹",
                info="é€‰æ‹©å®˜æ–¹æä¾›çš„ç¤ºä¾‹ï¼Œå°†è‡ªåŠ¨å¡«å……éŸ³é¢‘ã€å£°éŸ³å’Œæç¤ºè¯",
                scale=2
            )
        
        gr.Markdown("---")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ¤ è¾“å…¥è®¾ç½®")
                
                input_audio = gr.Audio(
                    label="è¾“å…¥éŸ³é¢‘ï¼ˆä¸Šä¼ æˆ–å½•åˆ¶ï¼‰",
                    type="filepath",
                    sources=["upload", "microphone"]
                )
                
                voice_dropdown = gr.Dropdown(
                    choices=list(VOICE_OPTIONS.keys()),
                    value="Natural Male 1 (NATM1)",
                    label="é€‰æ‹©å£°éŸ³",
                    info="é€‰æ‹©æ¨¡å‹ä½¿ç”¨çš„å£°éŸ³ç±»å‹"
                )
                
                example_dropdown = gr.Dropdown(
                    choices=list(EXAMPLE_PROMPTS.keys()),
                    value="åŠ©æ‰‹è§’è‰² (Assistant)",
                    label="ç¤ºä¾‹æç¤ºè¯",
                    info="é€‰æ‹©é¢„è®¾çš„æç¤ºè¯ç¤ºä¾‹"
                )
                
                text_prompt = gr.Textbox(
                    label="æ–‡æœ¬æç¤ºè¯",
                    value=EXAMPLE_PROMPTS["åŠ©æ‰‹è§’è‰² (Assistant)"],
                    lines=4,
                    placeholder="è¾“å…¥è§’è‰²è®¾å®šå’Œå¯¹è¯èƒŒæ™¯...",
                    info="å®šä¹‰ AI çš„è§’è‰²å’Œè¡Œä¸º"
                )
                
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ é«˜çº§å‚æ•°")
                
                with gr.Row():
                    seed = gr.Number(
                        label="éšæœºç§å­",
                        value=42424242,
                        precision=0,
                        info="-1 è¡¨ç¤ºéšæœº"
                    )
                    use_sampling = gr.Checkbox(
                        label="å¯ç”¨é‡‡æ ·",
                        value=True,
                        info="å…³é—­åˆ™ä½¿ç”¨è´ªå©ªè§£ç "
                    )
                
                with gr.Row():
                    temp_audio = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=0.8,
                        step=0.1,
                        label="éŸ³é¢‘æ¸©åº¦",
                        info="æ§åˆ¶éŸ³é¢‘ç”Ÿæˆçš„éšæœºæ€§"
                    )
                    temp_text = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=0.7,
                        step=0.1,
                        label="æ–‡æœ¬æ¸©åº¦",
                        info="æ§åˆ¶æ–‡æœ¬ç”Ÿæˆçš„éšæœºæ€§"
                    )
                
                with gr.Row():
                    topk_audio = gr.Slider(
                        minimum=1,
                        maximum=500,
                        value=250,
                        step=1,
                        label="éŸ³é¢‘ Top-K",
                        info="éŸ³é¢‘é‡‡æ ·çš„ Top-K å€¼"
                    )
                    topk_text = gr.Slider(
                        minimum=1,
                        maximum=100,
                        value=25,
                        step=1,
                        label="æ–‡æœ¬ Top-K",
                        info="æ–‡æœ¬é‡‡æ ·çš„ Top-K å€¼"
                    )
        
        gr.Markdown("---")
        
        with gr.Row():
            run_btn = gr.Button("ğŸ¯ å¼€å§‹æ¨ç†", variant="primary", size="lg")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ”Š è¾“å‡ºéŸ³é¢‘")
                output_audio = gr.Audio(
                    label="ç”Ÿæˆçš„éŸ³é¢‘",
                    type="filepath"
                )
            
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ è¾“å‡ºæ–‡æœ¬")
                output_text = gr.Textbox(
                    label="ç”Ÿæˆçš„æ–‡æœ¬",
                    lines=5,
                    interactive=False
                )
        
        inference_status = gr.Textbox(
            label="æ¨ç†çŠ¶æ€",
            value="",
            interactive=False
        )
        
        gr.Markdown("---")
        
        gr.Markdown("""
        ### ğŸ“– ä½¿ç”¨è¯´æ˜
        
        1. **å¿«é€Ÿä½“éªŒ**ï¼šé€‰æ‹©ã€Œå®˜æ–¹ç¤ºä¾‹ã€å¯ä»¥å¿«é€Ÿä½“éªŒæ¨¡å‹æ•ˆæœ
        2. **ä¸Šä¼ éŸ³é¢‘**ï¼šä¸Šä¼ ä¸€æ®µéŸ³é¢‘æ–‡ä»¶æˆ–ä½¿ç”¨éº¦å…‹é£å½•åˆ¶
        3. **é€‰æ‹©å£°éŸ³**ï¼šä»é¢„è®¾çš„å£°éŸ³ä¸­é€‰æ‹©ä¸€ä¸ª
        4. **è®¾ç½®æç¤ºè¯**ï¼šé€‰æ‹©ç¤ºä¾‹æç¤ºè¯æˆ–è‡ªå®šä¹‰è§’è‰²è®¾å®š
        5. **è°ƒæ•´å‚æ•°**ï¼šæ ¹æ®éœ€è¦è°ƒæ•´é‡‡æ ·å‚æ•°
        6. **å¼€å§‹æ¨ç†**ï¼šç‚¹å‡»ã€Œå¼€å§‹æ¨ç†ã€æŒ‰é’®ï¼Œç­‰å¾…ç”Ÿæˆç»“æœ
        
        ### ğŸ¯ å®˜æ–¹ç¤ºä¾‹è¯´æ˜
        
        - **Assistant (åŠ©æ‰‹è§’è‰²)**ï¼šæ¨¡æ‹Ÿä¸€ä¸ªå‹å¥½çš„è€å¸ˆï¼Œå›ç­”é—®é¢˜æˆ–æä¾›å»ºè®®
        - **Service (å®¢æœè§’è‰²)**ï¼šæ¨¡æ‹Ÿå®¶ç”µç»´ä¿®å…¬å¸çš„å®¢æœäººå‘˜
        
        ### ğŸ­ å£°éŸ³ç±»å‹è¯´æ˜
        
        - **Natural (NAT)**: æ›´è‡ªç„¶ã€æ›´å¯¹è¯åŒ–çš„å£°éŸ³
        - **Variety (VAR)**: æ›´å¤šæ ·åŒ–çš„å£°éŸ³é£æ ¼
        - **Female (F)**: å¥³æ€§å£°éŸ³
        - **Male (M)**: ç”·æ€§å£°éŸ³
        
        ### ğŸ’¡ æç¤ºè¯å»ºè®®
        
        - **åŠ©æ‰‹è§’è‰²**: é€‚ç”¨äºé—®ç­”å’Œå»ºè®®ç±»å¯¹è¯
        - **å®¢æœè§’è‰²**: é€‚ç”¨äºæ¨¡æ‹Ÿå®¢æˆ·æœåŠ¡åœºæ™¯
        - **ä¼‘é—²å¯¹è¯**: é€‚ç”¨äºå¼€æ”¾å¼çš„æ—¥å¸¸å¯¹è¯
        """)
        
        # äº‹ä»¶ç»‘å®š
        official_example_dropdown.change(
            fn=update_official_example,
            inputs=[official_example_dropdown],
            outputs=[input_audio, voice_dropdown, example_dropdown, text_prompt]
        )
        
        example_dropdown.change(
            fn=update_prompt,
            inputs=[example_dropdown],
            outputs=[text_prompt]
        )
        
        run_btn.click(
            fn=run_inference,
            inputs=[
                input_audio,
                voice_dropdown,
                text_prompt,
                seed,
                temp_audio,
                temp_text,
                topk_audio,
                topk_text,
                use_sampling
            ],
            outputs=[output_audio, output_text, inference_status]
        )
    
    return demo


if __name__ == "__main__":
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
    load_models()
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯åŠ¨ Web æœåŠ¡...")
    
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
