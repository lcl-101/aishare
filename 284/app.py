"""
LongVie - é•¿è§†é¢‘ç”Ÿæˆ Web åº”ç”¨
åŸºäº Gradio çš„äº¤äº’å¼ç•Œé¢
"""

import os
import json
import torch
import gradio as gr
import decord
import tempfile
import shutil
from PIL import Image
from datetime import datetime

from diffsynth import save_video
from diffsynth.pipelines.wan_video_new_longvie import LongViePipeline, ModelConfig

# ==================== é…ç½®å‚æ•° ====================
TARGET_SIZE = (640, 352)  # ç›®æ ‡åˆ†è¾¨ç‡ (å®½, é«˜)

# æ¨¡å‹è·¯å¾„é…ç½®ï¼ˆæ ¹æ®å®é™…ä¸‹è½½ä½ç½®è°ƒæ•´ï¼‰
BASE_MODEL_PATH = "./checkpoints"
CONTROL_WEIGHT_PATH = "./checkpoints/LongVie2/control.safetensors"
DIT_WEIGHT_PATH = "./checkpoints/LongVie2/dit.safetensors"

# é»˜è®¤è´Ÿå‘æç¤ºè¯
DEFAULT_NEGATIVE_PROMPT = (
    "è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œ"
    "æ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œ"
    "ç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œ"
    "é™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
)

# ç¤ºä¾‹æç¤ºè¯ï¼ˆæ¥è‡ªå®˜æ–¹ç¤ºä¾‹ï¼‰
EXAMPLE_PROMPTS = {
    "ride_horse": "The video captures a serene journey through a snowy landscape. A lone rider, clad in a dark jacket and a wide-brimmed hat, is seen from behind, riding a dark-colored horse along a snow-covered trail. The path meanders through a winter wonderland, flanked by frosty bushes and evergreen trees dusted with snow. In the distance, majestic snow-capped mountains rise against a backdrop of a partly cloudy sky, with sunlight filtering through the clouds, casting a soft glow over the scene. The rider's steady pace and the tranquil surroundings evoke a sense of peaceful solitude and the beauty of nature in its winter guise.",
    "valley": "The video opens with an aerial view of a stunning autumnal valley. The camera begins at a high altitude, providing a broad overview of the landscape. It then gradually descends, moving closer to the ground, giving a more intimate look at the river that winds through the valley. The river is a vibrant green, contrasting beautifully with the surrounding dense forest of trees adorned with fiery hues of orange, red, and yellow."
}

# ==================== å…¨å±€å˜é‡ ====================
pipe = None


def load_pipeline():
    """åŠ è½½æ¨¡å‹ç®¡é“"""
    global pipe
    
    if pipe is not None:
        return "âœ… æ¨¡å‹å·²åŠ è½½"
    
    try:
        import glob as glob_module
        
        # è®¾ç½® ModelScope æ¨¡å‹ç›®å½•ä¸ºæœ¬åœ° checkpoints
        # æ¨¡å‹ ID æ˜ å°„åˆ°æœ¬åœ°ç›®å½•å
        model_id = "Wan2.1-I2V-14B-480P"
        model_base_path = f"{BASE_MODEL_PATH}/{model_id}"
        
        # å±•å¼€ glob æ¨¡å¼è·å– DiT æƒé‡æ–‡ä»¶åˆ—è¡¨
        dit_files = sorted(glob_module.glob(f"{model_base_path}/diffusion_pytorch_model*.safetensors"))
        
        pipe = LongViePipeline.from_pretrained(
            torch_dtype=torch.bfloat16,
            device="cuda",
            use_usp=False,
            model_configs=[
                ModelConfig(
                    path=dit_files,  # åˆ—è¡¨å½¢å¼çš„å¤šä¸ªæ–‡ä»¶
                    offload_device="cpu",
                    skip_download=True,
                ),
                ModelConfig(
                    path=f"{model_base_path}/models_t5_umt5-xxl-enc-bf16.pth",
                    offload_device="cpu",
                    skip_download=True,
                ),
                ModelConfig(
                    path=f"{model_base_path}/Wan2.1_VAE.pth",
                    offload_device="cpu",
                    skip_download=True,
                ),
                ModelConfig(
                    path=f"{model_base_path}/models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth",
                    offload_device="cpu",
                    skip_download=True,
                ),
            ],
            redirect_common_files=False,
            control_weight_path=CONTROL_WEIGHT_PATH,
            dit_weight_path=DIT_WEIGHT_PATH,
        )
        pipe.enable_vram_management()
        return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼"
    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"


def load_image(path):
    """åŠ è½½å¹¶è°ƒæ•´å›¾ç‰‡å°ºå¯¸"""
    return Image.open(path).convert("RGB").resize(TARGET_SIZE)


def resize_video_frames(video_np):
    """è°ƒæ•´è§†é¢‘å¸§å°ºå¯¸"""
    return [Image.fromarray(frame).resize(TARGET_SIZE) for frame in video_np]


def load_json_file(json_path):
    """åŠ è½½ JSON é…ç½®æ–‡ä»¶"""
    with open(json_path, "r", encoding="utf-8") as f:
        return json.load(f)


def generate_video_segments(
    first_image,
    json_path,
    seed,
    negative_prompt,
    progress=gr.Progress(track_tqdm=True)
):
    """
    ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
    
    Args:
        first_image: é¦–å¸§å›¾ç‰‡è·¯å¾„
        json_path: æ¡ä»¶ JSON æ–‡ä»¶è·¯å¾„
        seed: éšæœºç§å­
        negative_prompt: è´Ÿå‘æç¤ºè¯
        progress: è¿›åº¦æ¡
    
    Returns:
        ç”Ÿæˆçš„è§†é¢‘åˆ—è¡¨, çŠ¶æ€ä¿¡æ¯
    """
    global pipe
    
    if pipe is None:
        return None, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹"
    
    if first_image is None or first_image == "":
        return None, "âŒ è¯·ä¸Šä¼ é¦–å¸§å›¾ç‰‡æˆ–é€‰æ‹©ç¤ºä¾‹"
    
    if json_path is None or json_path == "":
        return None, "âŒ è¯·è¾“å…¥æ¡ä»¶é…ç½®æ–‡ä»¶è·¯å¾„ (JSON) æˆ–é€‰æ‹©ç¤ºä¾‹"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(json_path):
        return None, f"âŒ JSON é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {json_path}"
    
    if not os.path.exists(first_image):
        return None, f"âŒ é¦–å¸§å›¾ç‰‡ä¸å­˜åœ¨: {first_image}"
    
    try:
        # è¯»å– JSON é…ç½®
        samples = load_json_file(json_path)
        
        # å¤„ç†é¦–å¸§å›¾ç‰‡
        image = load_image(first_image)
        
        # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(tempfile.gettempdir(), f"longvie_{timestamp}")
        os.makedirs(output_dir, exist_ok=True)
        
        history = []
        noise = None
        video_paths = []
        
        progress(0, desc="å¼€å§‹ç”Ÿæˆè§†é¢‘...")
        
        for i, sample in enumerate(samples):
            progress((i + 0.1) / len(samples), desc=f"æ­£åœ¨ç”Ÿæˆç¬¬ {i + 1}/{len(samples)} ä¸ªç‰‡æ®µ...")
            
            # åŠ è½½æ·±åº¦å’Œè½¨è¿¹è§†é¢‘
            dense_vr = decord.VideoReader(sample["depth"])
            sparse_vr = decord.VideoReader(sample["track"])
            
            dense_frames = resize_video_frames(dense_vr[:].asnumpy())
            sparse_frames = resize_video_frames(sparse_vr[:].asnumpy())
            
            # ç”Ÿæˆè§†é¢‘
            video, noise = pipe(
                input_image=image,
                prompt=sample["text"],
                negative_prompt=negative_prompt,
                seed=seed,
                tiled=False,
                height=TARGET_SIZE[1],
                width=TARGET_SIZE[0],
                dense_video=dense_frames,
                sparse_video=sparse_frames,
                history=history,
                noise=noise,
            )
            
            # æ›´æ–°å›¾ç‰‡å’Œå†å²
            image = video[-1]
            history = video[-8:]
            
            # ä¿å­˜è§†é¢‘
            save_path = os.path.join(output_dir, f"segment_{i:02d}.mp4")
            save_video(video, save_path, fps=16, quality=10)
            video_paths.append(save_path)
            
            progress((i + 1) / len(samples), desc=f"ç¬¬ {i + 1}/{len(samples)} ä¸ªç‰‡æ®µç”Ÿæˆå®Œæˆ")
        
        # åˆå¹¶æ‰€æœ‰è§†é¢‘ç‰‡æ®µ
        if len(video_paths) > 1:
            merged_path = os.path.join(output_dir, "merged_video.mp4")
            merge_videos(video_paths, merged_path)
            return merged_path, f"âœ… æˆåŠŸç”Ÿæˆ {len(video_paths)} ä¸ªè§†é¢‘ç‰‡æ®µå¹¶åˆå¹¶ï¼ä¿å­˜è‡³: {output_dir}"
        elif len(video_paths) == 1:
            return video_paths[0], f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼ä¿å­˜è‡³: {video_paths[0]}"
        else:
            return None, "âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•è§†é¢‘ç‰‡æ®µ"
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def merge_videos(video_paths, output_path):
    """åˆå¹¶å¤šä¸ªè§†é¢‘ç‰‡æ®µ"""
    import subprocess
    
    # åˆ›å»ºæ–‡ä»¶åˆ—è¡¨
    list_file = output_path.replace(".mp4", "_list.txt")
    with open(list_file, "w") as f:
        for path in video_paths:
            f.write(f"file '{path}'\n")
    
    # ä½¿ç”¨ ffmpeg åˆå¹¶
    cmd = [
        "ffmpeg", "-y", "-f", "concat", "-safe", "0",
        "-i", list_file, "-c", "copy", output_path
    ]
    subprocess.run(cmd, capture_output=True)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove(list_file)


def generate_single_segment(
    first_image,
    prompt,
    depth_video,
    track_video,
    seed,
    negative_prompt,
    progress=gr.Progress(track_tqdm=True)
):
    """
    ç”Ÿæˆå•ä¸ªè§†é¢‘ç‰‡æ®µï¼ˆç®€æ˜“æ¨¡å¼ï¼‰
    
    Args:
        first_image: é¦–å¸§å›¾ç‰‡è·¯å¾„
        prompt: æ­£å‘æç¤ºè¯
        depth_video: æ·±åº¦è§†é¢‘è·¯å¾„
        track_video: è½¨è¿¹è§†é¢‘è·¯å¾„
        seed: éšæœºç§å­
        negative_prompt: è´Ÿå‘æç¤ºè¯
        progress: è¿›åº¦æ¡
    
    Returns:
        ç”Ÿæˆçš„è§†é¢‘, çŠ¶æ€ä¿¡æ¯
    """
    global pipe
    
    if pipe is None:
        return None, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹"
    
    if first_image is None or first_image == "":
        return None, "âŒ è¯·ä¸Šä¼ é¦–å¸§å›¾ç‰‡æˆ–è¾“å…¥å›¾ç‰‡è·¯å¾„"
    
    if depth_video is None or depth_video == "" or track_video is None or track_video == "":
        return None, "âŒ è¯·è¾“å…¥æ·±åº¦è§†é¢‘å’Œè½¨è¿¹è§†é¢‘è·¯å¾„"
    
    if not prompt or prompt.strip() == "":
        return None, "âŒ è¯·è¾“å…¥æç¤ºè¯"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(first_image):
        return None, f"âŒ é¦–å¸§å›¾ç‰‡ä¸å­˜åœ¨: {first_image}"
    
    if not os.path.exists(depth_video):
        return None, f"âŒ æ·±åº¦è§†é¢‘ä¸å­˜åœ¨: {depth_video}"
    
    if not os.path.exists(track_video):
        return None, f"âŒ è½¨è¿¹è§†é¢‘ä¸å­˜åœ¨: {track_video}"
    
    try:
        # å¤„ç†é¦–å¸§å›¾ç‰‡
        image = load_image(first_image)
        
        progress(0.1, desc="åŠ è½½æ§åˆ¶è§†é¢‘...")
        
        # åŠ è½½æ·±åº¦å’Œè½¨è¿¹è§†é¢‘
        dense_vr = decord.VideoReader(depth_video)
        sparse_vr = decord.VideoReader(track_video)
        
        dense_frames = resize_video_frames(dense_vr[:].asnumpy())
        sparse_frames = resize_video_frames(sparse_vr[:].asnumpy())
        
        progress(0.2, desc="æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
        
        # ç”Ÿæˆè§†é¢‘
        video, _ = pipe(
            input_image=image,
            prompt=prompt,
            negative_prompt=negative_prompt,
            seed=seed,
            tiled=False,
            height=TARGET_SIZE[1],
            width=TARGET_SIZE[0],
            dense_video=dense_frames,
            sparse_video=sparse_frames,
            history=[],
            noise=None,
        )
        
        progress(0.9, desc="ä¿å­˜è§†é¢‘...")
        
        # ä¿å­˜è§†é¢‘
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(tempfile.gettempdir(), f"longvie_{timestamp}")
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, "output.mp4")
        save_video(video, save_path, fps=16, quality=10)
        
        progress(1.0, desc="å®Œæˆï¼")
        
        return save_path, f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼ä¿å­˜è‡³: {save_path}"
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def load_example(example_name):
    """åŠ è½½ç¤ºä¾‹æ•°æ®"""
    examples_map = {
        "ride_horse": {
            "image": "./example/ride_horse/first.png",
            "json": "./example/ride_horse/cond.json"
        },
        "valley": {
            "image": "./example/valley/first.png",
            "json": "./example/valley/cond.json"
        }
    }
    
    if example_name not in examples_map:
        return None, None, ""
    
    example = examples_map[example_name]
    image_path = example["image"]
    json_path = example["json"]
    
    # è¯»å– JSON è·å–ç¬¬ä¸€ä¸ªç‰‡æ®µçš„æç¤ºè¯
    try:
        samples = load_json_file(json_path)
        first_prompt = samples[0]["text"] if samples else ""
    except:
        first_prompt = ""
    
    return image_path, json_path, first_prompt


# ==================== Gradio ç•Œé¢ ====================
def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(
        title="LongVie - é•¿è§†é¢‘ç”Ÿæˆ",
        theme=gr.themes.Soft(),
        css="""
        .title { text-align: center; margin-bottom: 20px; }
        .description { text-align: center; color: #666; margin-bottom: 30px; }
        .example-img { max-height: 150px; }
        """
    ) as demo:
        
        gr.HTML("""
        <div class="title">
            <h1>ğŸ¬ LongVie - é•¿è§†é¢‘ç”Ÿæˆ</h1>
        </div>
        <div class="description">
            <p>åŸºäºæ·±åº¦å’Œè½¨è¿¹æ§åˆ¶çš„é•¿è§†é¢‘ç”Ÿæˆå·¥å…·</p>
        </div>
        """)
        
        with gr.Tabs():
            # ==================== é•¿è§†é¢‘ç”Ÿæˆæ¨¡å¼ ====================
            with gr.TabItem("ğŸ¬ é•¿è§†é¢‘ç”Ÿæˆï¼ˆæ¨èï¼‰"):
                gr.Markdown("""
                ### ğŸ¥ é•¿è§†é¢‘ç”Ÿæˆæ¨¡å¼
                
                æ­¤æ¨¡å¼é€šè¿‡**åˆ†æ®µè¿ç»­ç”Ÿæˆ**å®ç°é•¿è§†é¢‘ï¼š
                - ç³»ç»Ÿä¼šæŒ‰é¡ºåºç”Ÿæˆå¤šä¸ªè§†é¢‘ç‰‡æ®µ
                - æ¯ä¸ªç‰‡æ®µä½¿ç”¨å‰ä¸€ç‰‡æ®µçš„æœ€å8å¸§ä½œä¸ºå†å²ï¼Œä¿è¯è¿ç»­æ€§
                - æœ€ç»ˆè‡ªåŠ¨åˆå¹¶ä¸ºå®Œæ•´çš„é•¿è§†é¢‘
                
                **å·¥ä½œåŸç†ï¼š** `é¦–å¸§å›¾ç‰‡ â†’ ç‰‡æ®µ1 â†’ ç‰‡æ®µ2 â†’ ... â†’ ç‰‡æ®µN â†’ åˆå¹¶ â†’ é•¿è§†é¢‘`
                
                **éœ€è¦å‡†å¤‡ï¼š**
                1. **é¦–å¸§å›¾ç‰‡**ï¼šè§†é¢‘çš„èµ·å§‹ç”»é¢
                2. **æ¡ä»¶é…ç½®æ–‡ä»¶ï¼ˆJSONï¼‰**ï¼šåŒ…å«æ¯ä¸ªç‰‡æ®µçš„æç¤ºè¯ã€æ·±åº¦è§†é¢‘å’Œè½¨è¿¹è§†é¢‘è·¯å¾„
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        # è¾“å…¥åŒºåŸŸ
                        batch_image = gr.Image(
                            label="é¦–å¸§å›¾ç‰‡",
                            type="filepath",
                            height=200
                        )
                        batch_json = gr.Textbox(
                            label="æ¡ä»¶é…ç½®æ–‡ä»¶è·¯å¾„ (JSON)",
                            placeholder="ä¾‹å¦‚: ./example/ride_horse/cond.json"
                        )
                        
                    with gr.Column(scale=1):
                        batch_seed = gr.Number(
                            label="éšæœºç§å­",
                            value=0,
                            precision=0
                        )
                        batch_negative = gr.Textbox(
                            label="è´Ÿå‘æç¤ºè¯",
                            value=DEFAULT_NEGATIVE_PROMPT,
                            lines=4
                        )
                        batch_generate_btn = gr.Button(
                            "ğŸ¬ å¼€å§‹ç”Ÿæˆé•¿è§†é¢‘",
                            variant="primary",
                            size="lg"
                        )
                
                with gr.Row():
                    batch_output = gr.Video(label="ç”Ÿæˆçš„é•¿è§†é¢‘")
                    batch_status = gr.Textbox(label="çŠ¶æ€ä¿¡æ¯", lines=3)
                
                # å®˜æ–¹é•¿è§†é¢‘ç¤ºä¾‹
                gr.Markdown("""
                ### ğŸ¯ å®˜æ–¹ç¤ºä¾‹ï¼ˆç‚¹å‡»åŠ è½½ï¼‰
                
                | ç¤ºä¾‹ | ç‰‡æ®µæ•° | é¢„è®¡æ—¶é•¿ | æè¿° |
                |------|--------|----------|------|
                | éª‘é©¬é›ªæ™¯ | 10 ä¸ªç‰‡æ®µ | ~40ç§’ | éª‘é©¬ç©¿è¶Šé›ªå±±çš„ç¬¬ä¸‰äººç§°è§†è§’ |
                | ç§‹å¤©å±±è°· | 12 ä¸ªç‰‡æ®µ | ~48ç§’ | ç§‹å¤©å±±è°·æ²³æµçš„èˆªæ‹è§†è§’ |
                """)
                gr.Examples(
                    examples=[
                        ["./example/ride_horse/first.png", "./example/ride_horse/cond.json"],
                        ["./example/valley/first.png", "./example/valley/cond.json"],
                    ],
                    inputs=[batch_image, batch_json],
                    label="é€‰æ‹©ç¤ºä¾‹",
                    examples_per_page=2,
                )
            
            # ==================== å•ç‰‡æ®µæµ‹è¯•æ¨¡å¼ ====================
            with gr.TabItem("ğŸ§ª å•ç‰‡æ®µæµ‹è¯•"):
                gr.Markdown("""
                ### ğŸ§ª å•ç‰‡æ®µæµ‹è¯•æ¨¡å¼
                
                æ­¤æ¨¡å¼ç”¨äº**å¿«é€Ÿæµ‹è¯•å•ä¸ªç‰‡æ®µ**çš„ç”Ÿæˆæ•ˆæœï¼Œé€‚åˆï¼š
                - è°ƒè¯•å‚æ•°å’Œæç¤ºè¯
                - é¢„è§ˆæ§åˆ¶è§†é¢‘çš„æ•ˆæœ
                - å¿«é€ŸéªŒè¯æƒ³æ³•
                
                **æ³¨æ„ï¼š** å¦‚éœ€ç”Ÿæˆå®Œæ•´é•¿è§†é¢‘ï¼Œè¯·ä½¿ç”¨ã€Œé•¿è§†é¢‘ç”Ÿæˆã€æ¨¡å¼ã€‚
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        single_image = gr.Image(
                            label="é¦–å¸§å›¾ç‰‡",
                            type="filepath",
                            height=200
                        )
                        single_prompt = gr.Textbox(
                            label="æç¤ºè¯",
                            placeholder="è¯·è¾“å…¥æè¿°è§†é¢‘å†…å®¹çš„æç¤ºè¯...",
                            lines=4,
                            value=EXAMPLE_PROMPTS["ride_horse"]
                        )
                        
                    with gr.Column(scale=1):
                        single_depth = gr.Video(
                            label="æ·±åº¦è§†é¢‘ (MP4)",
                            height=150
                        )
                        single_track = gr.Video(
                            label="è½¨è¿¹è§†é¢‘ (MP4)",
                            height=150
                        )
                
                with gr.Row():
                    with gr.Column(scale=1):
                        single_seed = gr.Number(
                            label="éšæœºç§å­",
                            value=0,
                            precision=0
                        )
                    with gr.Column(scale=2):
                        single_negative = gr.Textbox(
                            label="è´Ÿå‘æç¤ºè¯",
                            value=DEFAULT_NEGATIVE_PROMPT,
                            lines=2
                        )
                
                single_generate_btn = gr.Button(
                    "ğŸ¬ å¼€å§‹ç”Ÿæˆ",
                    variant="primary",
                    size="lg"
                )
                
                with gr.Row():
                    single_output = gr.Video(label="ç”Ÿæˆç»“æœ")
                    single_status = gr.Textbox(label="çŠ¶æ€ä¿¡æ¯", lines=3)
                
                # å®˜æ–¹ç¤ºä¾‹
                gr.Markdown("### ğŸ¯ å®˜æ–¹ç¤ºä¾‹ï¼ˆç‚¹å‡»åŠ è½½ï¼‰")
                gr.Examples(
                    examples=[
                        [
                            "./example/ride_horse/first.png",
                            EXAMPLE_PROMPTS["ride_horse"],
                            "./example/ride_horse/depth_00.mp4",
                            "./example/ride_horse/track_00.mp4"
                        ],
                        [
                            "./example/valley/first.png",
                            EXAMPLE_PROMPTS["valley"],
                            "./example/valley/depth_00.mp4",
                            "./example/valley/track_00.mp4"
                        ],
                    ],
                    inputs=[single_image, single_prompt, single_depth, single_track],
                    label="é€‰æ‹©ç¤ºä¾‹",
                    examples_per_page=2,
                )
            
            # ==================== ä½¿ç”¨å¸®åŠ© ====================
            with gr.TabItem("â“ ä½¿ç”¨å¸®åŠ©"):
                gr.Markdown("""
                ## LongVie é•¿è§†é¢‘ç”Ÿæˆä½¿ç”¨æŒ‡å—
                
                ### ä»€ä¹ˆæ˜¯ LongVie?
                LongVie æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å’Œè½¨è¿¹æ§åˆ¶çš„é•¿è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®é¦–å¸§å›¾ç‰‡å’Œæ§åˆ¶ä¿¡å·ç”Ÿæˆé«˜è´¨é‡çš„è¿ç»­è§†é¢‘ã€‚
                
                ### å‡†å¤‡å·¥ä½œ
                
                #### 1. é¦–å¸§å›¾ç‰‡
                - æ ¼å¼ï¼šPNG æˆ– JPG
                - å»ºè®®å°ºå¯¸ï¼š640x352 æˆ–ç±»ä¼¼æ¯”ä¾‹
                - å›¾ç‰‡å°†è¢«è‡ªåŠ¨è°ƒæ•´ä¸º 640x352
                
                #### 2. æ·±åº¦è§†é¢‘ (Depth Video)
                - ç”¨äºæ§åˆ¶è§†é¢‘çš„ç©ºé—´ç»“æ„å’Œæ™¯æ·±
                - å¯ä»¥ä½¿ç”¨æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼ˆå¦‚ MiDaSï¼‰ä»åŸè§†é¢‘ç”Ÿæˆ
                - æ ¼å¼ï¼šMP4
                
                #### 3. è½¨è¿¹è§†é¢‘ (Track Video)
                - ç”¨äºæ§åˆ¶ç‰©ä½“çš„è¿åŠ¨è½¨è¿¹
                - å¯ä»¥ä½¿ç”¨å…‰æµæˆ–ç‚¹è¿½è¸ªç®—æ³•ä»åŸè§†é¢‘ç”Ÿæˆ
                - æ ¼å¼ï¼šMP4
                
                #### 4. JSON é…ç½®æ–‡ä»¶ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰
                ```json
                [
                    {
                        "text": "æè¿°ç¬¬ä¸€ä¸ªç‰‡æ®µçš„æç¤ºè¯",
                        "depth": "./path/to/depth_00.mp4",
                        "track": "./path/to/track_00.mp4"
                    },
                    {
                        "text": "æè¿°ç¬¬äºŒä¸ªç‰‡æ®µçš„æç¤ºè¯",
                        "depth": "./path/to/depth_01.mp4",
                        "track": "./path/to/track_01.mp4"
                    }
                ]
                ```
                
                ### æç¤ºè¯æŠ€å·§
                - ä½¿ç”¨è¯¦ç»†çš„åœºæ™¯æè¿°
                - åŒ…å«ç›¸æœºè¿åŠ¨æ–¹å‘ï¼ˆå¦‚ï¼šç›¸æœºå‰è¿›ã€å‘å·¦è½¬ç­‰ï¼‰
                - æè¿°å…‰ç…§å’Œæ°›å›´
                - è‹±æ–‡æç¤ºè¯æ•ˆæœé€šå¸¸æ›´å¥½
                
                ### å¸¸è§é—®é¢˜
                
                **Q: ä¸ºä»€ä¹ˆç”Ÿæˆé€Ÿåº¦å¾ˆæ…¢ï¼Ÿ**
                A: LongVie ä½¿ç”¨ 14B å‚æ•°çš„å¤§æ¨¡å‹ï¼Œéœ€è¦è¾ƒå¼ºçš„ GPUã€‚å»ºè®®ä½¿ç”¨è‡³å°‘ 24GB æ˜¾å­˜çš„æ˜¾å¡ã€‚
                
                **Q: å¦‚ä½•è·å–æ·±åº¦å’Œè½¨è¿¹è§†é¢‘ï¼Ÿ**
                A: å¯ä»¥ä½¿ç”¨é¡¹ç›®ä¸­çš„ `utils/get_depth.py` å’Œ `utils/get_track.py` è„šæœ¬ç”Ÿæˆã€‚
                
                **Q: ç”Ÿæˆçš„è§†é¢‘ä¸è¿ç»­æ€ä¹ˆåŠï¼Ÿ**
                A: ä½¿ç”¨æ‰¹é‡æ¨¡å¼ï¼Œç¡®ä¿ JSON ä¸­çš„ç‰‡æ®µé¡ºåºæ­£ç¡®ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨å†å²å¸§ä¿æŒè¿ç»­æ€§ã€‚
                """)
        
        # ==================== äº‹ä»¶ç»‘å®š ====================
        
        # æ‰¹é‡ç”Ÿæˆ
        batch_generate_btn.click(
            fn=generate_video_segments,
            inputs=[batch_image, batch_json, batch_seed, batch_negative],
            outputs=[batch_output, batch_status]
        )
        
        # å•ç‰‡æ®µç”Ÿæˆ
        single_generate_btn.click(
            fn=generate_single_segment,
            inputs=[single_image, single_prompt, single_depth, single_track, single_seed, single_negative],
            outputs=[single_output, single_status]
        )
    
    return demo


# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["MODELSCOPE_CACHE"] = "./checkpoints"
    os.environ["HF_HOME"] = "./checkpoints"
    
    # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
    status = load_pipeline()
    print(status)
    
    if pipe is None:
        print("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        exit(1)
    
    # åˆ›å»ºå¹¶å¯åŠ¨ UI
    demo = create_ui()
    demo.queue(max_size=10)
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
