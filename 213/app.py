import gradio as gr
import torch
import argparse
from ovi.ovi_fusion_engine import OviFusionEngine, DEFAULT_CONFIG
from diffusers import FluxPipeline
import tempfile
from ovi.utils.io_utils import save_video
from ovi.utils.processing_utils import clean_text, scale_hw_to_area_divisible
import csv
import os

# ----------------------------
# è§£æå‘½ä»¤è¡Œå‚æ•°
# ----------------------------
parser = argparse.ArgumentParser(description="Ovi è”åˆè§†é¢‘ + éŸ³é¢‘ Gradio æ¼”ç¤º")
parser.add_argument(
    "--use_image_gen",
    action="store_true",
    help="å¯ç”¨å›¾åƒç”Ÿæˆç•Œé¢ (ä½¿ç”¨ FluxPipeline)"
)
parser.add_argument(
    "--cpu_offload",
    action="store_true",
    help="ä¸º OviFusionEngine å’Œ FluxPipeline å¯ç”¨ CPU å¸è½½"
)
parser.add_argument(
    "--fp8",
    action="store_true",
    help="å¯ç”¨èåˆæ¨¡å‹çš„ 8 ä½é‡åŒ–",
)
parser.add_argument(
    "--qint8",
    action="store_true",
    help="å¯ç”¨èåˆæ¨¡å‹çš„ 8 ä½é‡åŒ–ã€‚æ— éœ€ä¸‹è½½é¢å¤–çš„æ¨¡å‹ã€‚",
)
parser.add_argument("--server_name", type=str, default="0.0.0.0", help="IP åœ°å€ï¼Œå±€åŸŸç½‘è®¿é—®è¯·æ”¹ä¸º 0.0.0.0")
parser.add_argument("--server_port", type=int, default=7860, help="ä½¿ç”¨ç«¯å£")
parser.add_argument("--share", action="store_true", help="å¯ç”¨ gradio åˆ†äº«")
parser.add_argument("--mcp_server", action="store_true", help="å¯ç”¨ MCP æœåŠ¡")
args = parser.parse_args()


# åˆå§‹åŒ– OviFusionEngine
enable_cpu_offload = args.cpu_offload or args.use_image_gen
use_image_gen = args.use_image_gen
fp8 = args.fp8
qint8 = args.qint8
print(f"æ­£åœ¨åŠ è½½æ¨¡å‹... {enable_cpu_offload=}, {use_image_gen=}, {fp8=}, {qint8=} for gradio demo")
DEFAULT_CONFIG["cpu_offload"] = (
    enable_cpu_offload  # å¦‚æœå¯ç”¨å›¾åƒç”Ÿæˆï¼Œåˆ™å§‹ç»ˆä½¿ç”¨ cpu å¸è½½
)
DEFAULT_CONFIG["mode"] = "t2v"  # ç¡¬ç¼–ç ï¼Œå› ä¸ºå®ƒæ€»æ˜¯ä½¿ç”¨ cpu å¸è½½
DEFAULT_CONFIG["fp8"] = fp8
DEFAULT_CONFIG["qint8"] = qint8
ovi_engine = OviFusionEngine()
flux_model = None
if fp8 or qint8:
    assert not use_image_gen, "ä½¿ç”¨ FluxPipeline çš„å›¾åƒç”Ÿæˆä¸æ”¯æŒ fp8 é‡åŒ–ã€‚è¿™æ˜¯å› ä¸ºå¦‚æœæ‚¨æ— æ³•è¿è¡Œ bf16 æ¨¡å‹ï¼Œæ‚¨å¯èƒ½ä¹Ÿæ— æ³•è¿è¡Œå›¾åƒç”Ÿæˆæ¨¡å‹"
    
if use_image_gen:
    flux_model = FluxPipeline.from_pretrained("black-forest-labs/FLUX.1-Krea-dev", torch_dtype=torch.bfloat16)
    flux_model.enable_model_cpu_offload() # é€šè¿‡å°†æ¨¡å‹å¸è½½åˆ° CPU æ¥èŠ‚çœä¸€äº› VRAMã€‚å¦‚æœæ‚¨æœ‰è¶³å¤Ÿçš„ GPU VRAMï¼Œè¯·åˆ é™¤æ­¤é¡¹
print("æ¨¡å‹åŠ è½½å®Œæˆ")


# åŠ è½½ç¤ºä¾‹æ•°æ®
def load_t2v_examples():
    """åŠ è½½æ–‡æœ¬åˆ°è§†é¢‘ç¤ºä¾‹"""
    examples = []
    csv_path = "example_prompts/gpt_examples_t2v.csv"
    if os.path.exists(csv_path):
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # è¿”å›æ ¼å¼: [video_text_prompt, image, video_height, video_width, ...]
                examples.append([
                    row['text_prompt'],  # video_text_prompt
                    None,  # image (T2V æ²¡æœ‰å›¾åƒ)
                    512,  # video_height
                    992,  # video_width
                    100,  # video_seed
                    "unipc",  # solver_name
                    50,  # sample_steps
                    5.0,  # shift
                    4.0,  # video_guidance_scale
                    3.0,  # audio_guidance_scale
                    11,  # slg_layer
                    "",  # video_negative_prompt
                    "",  # audio_negative_prompt
                ])
    return examples


def load_i2v_examples():
    """åŠ è½½å›¾åƒåˆ°è§†é¢‘ç¤ºä¾‹"""
    examples = []
    csv_path = "example_prompts/gpt_examples_i2v.csv"
    if os.path.exists(csv_path):
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # è¿”å›æ ¼å¼: [video_text_prompt, image, video_height, video_width, ...]
                examples.append([
                    row['text_prompt'],  # video_text_prompt
                    row['image_path'],  # image
                    512,  # video_height
                    992,  # video_width
                    100,  # video_seed
                    "unipc",  # solver_name
                    50,  # sample_steps
                    5.0,  # shift
                    4.0,  # video_guidance_scale
                    3.0,  # audio_guidance_scale
                    11,  # slg_layer
                    "",  # video_negative_prompt
                    "",  # audio_negative_prompt
                ])
    return examples


def generate_video(
    text_prompt,
    image,
    video_frame_height,
    video_frame_width,
    video_seed,
    solver_name,
    sample_steps,
    shift,
    video_guidance_scale,
    audio_guidance_scale,
    slg_layer,
    video_negative_prompt,
    audio_negative_prompt,
):
    try:
        image_path = None
        if image is not None:
            image_path = image

        generated_video, generated_audio, _ = ovi_engine.generate(
            text_prompt=text_prompt,
            image_path=image_path,
            video_frame_height_width=[video_frame_height, video_frame_width],
            seed=video_seed,
            solver_name=solver_name,
            sample_steps=sample_steps,
            shift=shift,
            video_guidance_scale=video_guidance_scale,
            audio_guidance_scale=audio_guidance_scale,
            slg_layer=slg_layer,
            video_negative_prompt=video_negative_prompt,
            audio_negative_prompt=audio_negative_prompt,
        )

        tmpfile = tempfile.NamedTemporaryFile(suffix=".mp4", delete=False)
        output_path = tmpfile.name
        save_video(output_path, generated_video, generated_audio, fps=24, sample_rate=16000)

        return output_path
    except Exception as e:
        print(f"è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None


def generate_image(text_prompt, image_seed, image_height, image_width):
    if flux_model is None:
        return None
    text_prompt = clean_text(text_prompt)
    print(f"æ­£åœ¨ç”Ÿæˆå›¾åƒï¼Œæç¤ºè¯='{text_prompt}', ç§å­={image_seed}, å°ºå¯¸=({image_height},{image_width})")

    image_h, image_w = scale_hw_to_area_divisible(image_height, image_width, area=1024 * 1024)
    image = flux_model(
        text_prompt,
        height=image_h,
        width=image_w,
        guidance_scale=4.5,
        generator=torch.Generator().manual_seed(int(image_seed))
    ).images[0]

    tmpfile = tempfile.NamedTemporaryFile(suffix=".png", delete=False)
    image.save(tmpfile.name)
    return tmpfile.name


# æ„å»ºç”¨æˆ·ç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("# ğŸ¥ Ovi è”åˆè§†é¢‘ + éŸ³é¢‘ç”Ÿæˆæ¼”ç¤º")
    gr.Markdown(
        """
        ## ğŸ“˜ ä½¿ç”¨è¯´æ˜

        è¯·æŒ‰é¡ºåºå®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š

        1ï¸âƒ£ **è¾“å…¥æ–‡æœ¬æç¤ºè¯** â€” æè¿°æ‚¨æƒ³è¦çš„è§†é¢‘å†…å®¹ã€‚(å¦‚æœå¯ç”¨äº†å›¾åƒç”Ÿæˆï¼Œæ­¤æ–‡æœ¬æç¤ºè¯å°†è¢«å…±äº«ç”¨äºå›¾åƒç”Ÿæˆã€‚)  
        2ï¸âƒ£ **ä¸Šä¼ æˆ–ç”Ÿæˆå›¾åƒ** â€” ä¸Šä¼ ä¸€å¼ å›¾åƒæˆ–ç”Ÿæˆä¸€å¼ ï¼ˆå¦‚æœå¯ç”¨äº†å›¾åƒç”ŸæˆåŠŸèƒ½ï¼‰ã€‚(å¦‚æœæ‚¨æ²¡æœ‰çœ‹åˆ°å›¾åƒç”Ÿæˆé€‰é¡¹ï¼Œè¯·ç¡®ä¿ä½¿ç”¨ `--use_image_gen` å‚æ•°è¿è¡Œè„šæœ¬ã€‚)  
        3ï¸âƒ£ **é…ç½®è§†é¢‘é€‰é¡¹** â€” è®¾ç½®åˆ†è¾¨ç‡ã€ç§å­ã€æ±‚è§£å™¨å’Œå…¶ä»–å‚æ•°ã€‚(å®ƒå°†è‡ªåŠ¨ä½¿ç”¨ä¸Šä¼ /ç”Ÿæˆçš„å›¾åƒä½œä¸ºç¬¬ä¸€å¸§ï¼Œæ— è®ºæ‚¨åœ¨è§†é¢‘ç”Ÿæˆæ—¶å±å¹•ä¸Šæ˜¾ç¤ºå“ªä¸ªã€‚)  
        4ï¸âƒ£ **ç”Ÿæˆè§†é¢‘** â€” ç‚¹å‡»æŒ‰é’®ç”Ÿæˆæ‚¨çš„æœ€ç»ˆè§†é¢‘å’ŒéŸ³é¢‘ã€‚  
        5ï¸âƒ£ **æŸ¥çœ‹ç»“æœ** â€” æ‚¨ç”Ÿæˆçš„è§†é¢‘å°†æ˜¾ç¤ºåœ¨ä¸‹æ–¹ã€‚  

        ---

        ### ğŸ’¡ æç¤º
        1. ä¸ºäº†è·å¾—æœ€ä½³æ•ˆæœï¼Œè¯·ä½¿ç”¨è¯¦ç»†å’Œå…·ä½“çš„æ–‡æœ¬æç¤ºè¯ã€‚  
        2. ç¡®ä¿æ–‡æœ¬æç¤ºè¯æ ¼å¼æ­£ç¡®ï¼Œå³è¦è¯´çš„è¯åº”è¯¥ç”¨ `<S>...<E>` åŒ…è£¹ã€‚å¯ä»¥åœ¨æœ«å°¾æä¾›å¯é€‰çš„éŸ³é¢‘æè¿°ï¼Œç”¨ `<AUDCAP> ... <ENDAUDCAP>` åŒ…è£¹ï¼Œè¯·å‚è€ƒç¤ºä¾‹  
        3. ä¸è¦å› ä¸ºç³Ÿç³•æˆ–å¥‡æ€ªçš„ç»“æœè€Œæ°”é¦ï¼Œæ£€æŸ¥æç¤ºè¯æ ¼å¼å¹¶å°è¯•ä¸åŒçš„ç§å­ã€CFG å€¼å’Œ SLG å±‚ã€‚
        """
    )


    with gr.Row():
        with gr.Column():
            # å›¾åƒéƒ¨åˆ†
            image = gr.Image(type="filepath", label="é¦–å¸§å›¾åƒ (ä¸Šä¼ æˆ–ç”Ÿæˆ)")

            if args.use_image_gen:
                with gr.Accordion("ğŸ–¼ï¸ å›¾åƒç”Ÿæˆé€‰é¡¹", visible=True):
                    image_text_prompt = gr.Textbox(label="å›¾åƒæç¤ºè¯", placeholder="æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„å›¾åƒ...")
                    image_seed = gr.Number(minimum=0, maximum=100000, value=42, label="å›¾åƒç§å­")
                    image_height = gr.Number(minimum=128, maximum=1280, value=720, step=32, label="å›¾åƒé«˜åº¦")
                    image_width = gr.Number(minimum=128, maximum=1280, value=1280, step=32, label="å›¾åƒå®½åº¦")
                    gen_img_btn = gr.Button("ç”Ÿæˆå›¾åƒ ğŸ¨")
            else:
                gen_img_btn = None

            with gr.Accordion("ğŸ¬ è§†é¢‘ç”Ÿæˆé€‰é¡¹", open=True):
                video_text_prompt = gr.Textbox(label="è§†é¢‘æç¤ºè¯", placeholder="æè¿°æ‚¨æƒ³è¦çš„è§†é¢‘...")
                video_height = gr.Number(minimum=128, maximum=1280, value=512, step=32, label="è§†é¢‘é«˜åº¦")
                video_width = gr.Number(minimum=128, maximum=1280, value=992, step=32, label="è§†é¢‘å®½åº¦")

                video_seed = gr.Number(minimum=0, maximum=100000, value=100, label="è§†é¢‘ç§å­")
                solver_name = gr.Dropdown(
                    choices=["unipc", "euler", "dpm++"], value="unipc", label="æ±‚è§£å™¨åç§°"
                )
                sample_steps = gr.Number(
                    value=50,
                    label="é‡‡æ ·æ­¥æ•°",
                    precision=0,
                    minimum=20,
                    maximum=100
                )
                shift = gr.Slider(minimum=0.0, maximum=20.0, value=5.0, step=1.0, label="åç§»é‡")
                video_guidance_scale = gr.Slider(minimum=0.0, maximum=10.0, value=4.0, step=0.5, label="è§†é¢‘å¼•å¯¼æ¯”ä¾‹")
                audio_guidance_scale = gr.Slider(minimum=0.0, maximum=10.0, value=3.0, step=0.5, label="éŸ³é¢‘å¼•å¯¼æ¯”ä¾‹")
                slg_layer = gr.Number(minimum=-1, maximum=30, value=11, step=1, label="SLG å±‚")
                video_negative_prompt = gr.Textbox(label="è§†é¢‘è´Ÿé¢æç¤ºè¯", placeholder="è§†é¢‘ä¸­è¦é¿å…çš„å†…å®¹")
                audio_negative_prompt = gr.Textbox(label="éŸ³é¢‘è´Ÿé¢æç¤ºè¯", placeholder="éŸ³é¢‘ä¸­è¦é¿å…çš„å†…å®¹")

                run_btn = gr.Button("ç”Ÿæˆè§†é¢‘ ğŸš€")

        with gr.Column():
            output_path = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘")

    # æ·»åŠ ç¤ºä¾‹éƒ¨åˆ†
    with gr.Accordion("ğŸ“ ç¤ºä¾‹", open=False):
        gr.Markdown("### æ–‡æœ¬åˆ°è§†é¢‘ (T2AV) ç¤ºä¾‹")
        gr.Markdown("ç‚¹å‡»ä¸‹é¢çš„ç¤ºä¾‹è‡ªåŠ¨å¡«å……å‚æ•°ï¼ˆæ— éœ€é¦–å¸§å›¾åƒï¼‰")
        
        t2v_examples = gr.Examples(
            examples=load_t2v_examples(),
            inputs=[
                video_text_prompt, image, video_height, video_width, video_seed, 
                solver_name, sample_steps, shift, video_guidance_scale, 
                audio_guidance_scale, slg_layer, video_negative_prompt, audio_negative_prompt
            ],
            label="T2AV ç¤ºä¾‹",
            examples_per_page=5
        )
        
        gr.Markdown("### å›¾åƒåˆ°è§†é¢‘ (I2AV) ç¤ºä¾‹")
        gr.Markdown("ç‚¹å‡»ä¸‹é¢çš„ç¤ºä¾‹è‡ªåŠ¨å¡«å……å‚æ•°ï¼ˆåŒ…å«é¦–å¸§å›¾åƒï¼‰")
        
        i2v_examples = gr.Examples(
            examples=load_i2v_examples(),
            inputs=[
                video_text_prompt, image, video_height, video_width, video_seed, 
                solver_name, sample_steps, shift, video_guidance_scale, 
                audio_guidance_scale, slg_layer, video_negative_prompt, audio_negative_prompt
            ],
            label="I2AV ç¤ºä¾‹",
            examples_per_page=5
        )

    if args.use_image_gen and gen_img_btn is not None:
        gen_img_btn.click(
            fn=generate_image,
            inputs=[image_text_prompt, image_seed, image_height, image_width],
            outputs=[image],
        )

    # è¿æ¥è§†é¢‘ç”Ÿæˆ
    run_btn.click(
        fn=generate_video,
        inputs=[
            video_text_prompt, image, video_height, video_width, video_seed, solver_name,
            sample_steps, shift, video_guidance_scale, audio_guidance_scale,
            slg_layer, video_negative_prompt, audio_negative_prompt,
        ],
        outputs=[output_path],
    )

if __name__ == "__main__":
    demo.launch(
        server_name=args.server_name, 
        server_port=args.server_port,
        share=args.share, 
        mcp_server=args.mcp_server,
        inbrowser=True,
    )
