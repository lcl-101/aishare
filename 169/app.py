#!/usr/bin/env python3
"""
OlmOCR Gradio Web UI - å®Œå…¨æœ¬åœ°åŒ–ç‰ˆæœ¬
é›†æˆç¯å¢ƒæ£€æµ‹ã€é…ç½®ç®¡ç†å’Œ Web UI çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
å®Œå…¨é¿å…ä»»ä½• AWS æˆ–å¤–éƒ¨æœåŠ¡è¿æ¥
"""

import gradio as gr
import os
import sys
import subprocess
import shutil
import json
import tempfile
import logging
import re
from pathlib import Path
import time
import requests
from urllib.parse import urlparse
import asyncio
import base64
from io import BytesIO
from PIL import Image
import concurrent.futures
import threading

# å¯¼å…¥ OlmOCR çš„æ ¸å¿ƒç»„ä»¶
try:
    from olmocr.data.renderpdf import render_pdf_to_base64png
    from olmocr.prompts import build_no_anchoring_yaml_prompt
    from olmocr.train.dataloader import FrontMatterParser
    from olmocr.prompts import PageResponse
    from pypdf import PdfReader
    OLMOCR_AVAILABLE = True
except ImportError as e:
    logger = logging.getLogger(__name__)
    logger.warning(f"OlmOCR ç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
    OLMOCR_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å…¨å±€å¤„ç†å™¨å®ä¾‹
global_processor = None

def initialize_global_processor():
    """åˆå§‹åŒ–å…¨å±€å¤„ç†å™¨"""
    global global_processor
    if global_processor is None:
        # æ£€æŸ¥ç¯å¢ƒ
        env_checker = EnvironmentChecker()
        python_ok = env_checker.check_python()
        packages_ok, _ = env_checker.check_packages()
        model_ok, _ = env_checker.check_model()
        
        if python_ok and packages_ok and model_ok:
            try:
                global_processor = OlmOCRProcessor(auto_start_server=False)
                logger.info("âœ… å…¨å±€å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
                return True
            except Exception as e:
                logger.error(f"âŒ å…¨å±€å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                return False
        else:
            logger.warning("âš ï¸ ç¯å¢ƒæ£€æŸ¥æœªé€šè¿‡ï¼Œæ— æ³•åˆå§‹åŒ–å…¨å±€å¤„ç†å™¨")
            return False
    return True

# å†…ç½®é…ç½®
CONFIG = {
    "model": {
        "path": "/workspace/olmocr/checkpoints/olmOCR-7B-0725",
        "name": "olmOCR-7B-0725",
        "type": "local"
    },
    "ui": {
        "title": "OlmOCR æœ¬åœ°æ¨¡å‹ Web UI",
        "description": "åŸºäºæœ¬åœ° olmOCR-7B-0725 æ¨¡å‹çš„æ–‡æ¡£è½¬æ¢å·¥å…· - çœŸå® OCR å¤„ç†",
        "port": 7860,
        "host": "0.0.0.0"
    },
    "processing": {
        "timeout": 600,
        "max_file_size_mb": 100,
        "supported_formats": [".pdf", ".png", ".jpg", ".jpeg", ".tiff", ".bmp"],
        "workspace_base": "/tmp/olmocr_workspace",
        "max_preview_chars": 10000
    },
    "output": {
        "default_format": "markdown",
        "enable_dolma": True,
        "enable_copy": True
    },
    "examples": {
        "sample_url": "https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf",
        "sample_filename": "olmocr-sample.pdf",
        "examples_dir": "/workspace/olmocr/examples",
        "auto_download": True
    },
    "ollama": {
        "base_url": "http://localhost:11434",
        "model": "qwen3:32b",
        "timeout": 120,
        "max_tokens": 4096,
        "temperature": 0.7
    }
}

class EnvironmentChecker:
    """ç¯å¢ƒæ£€æŸ¥ç±»"""
    
    @staticmethod
    def check_python():
        version = sys.version_info
        logger.info(f"Python ç‰ˆæœ¬: {version.major}.{version.minor}.{version.micro}")
        return version.major >= 3 and version.minor >= 8
    
    @staticmethod
    def check_packages():
        required_packages = {
            "gradio": "Web UI æ¡†æ¶",
            "torch": "PyTorch",
            "transformers": "Transformers",
            "PIL": "å›¾åƒå¤„ç†",
        }
        
        missing = []
        for package in required_packages:
            try:
                if package == "PIL":
                    from PIL import Image
                else:
                    __import__(package)
                logger.info(f"âœ… {package}: {required_packages[package]}")
            except ImportError:
                logger.warning(f"âŒ {package}: {required_packages[package]} - æœªå®‰è£…")
                missing.append(package)
        
        return len(missing) == 0, missing
    
    @staticmethod
    def check_gpu():
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0)
                memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logger.info(f"âœ… GPU: {gpu_name}, å†…å­˜: {memory:.1f}GB")
                return True, f"{gpu_name} ({memory:.1f}GB)"
            else:
                logger.warning("âŒ CUDA ä¸å¯ç”¨")
                return False, "CUDA ä¸å¯ç”¨"
        except ImportError:
            logger.warning("âŒ PyTorch æœªå®‰è£…")
            return False, "PyTorch æœªå®‰è£…"
    
    @staticmethod
    def check_model():
        model_path = CONFIG["model"]["path"]
        if not os.path.exists(model_path):
            logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False, f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}"
        
        required_files = ["config.json", "tokenizer.json", "model.safetensors.index.json"]
        missing = []
        
        for file in required_files:
            file_path = os.path.join(model_path, file)
            if not os.path.exists(file_path):
                missing.append(file)
        
        if missing:
            logger.warning(f"ç¼ºå°‘æ¨¡å‹æ–‡ä»¶: {missing}")
            return False, f"ç¼ºå°‘æ–‡ä»¶: {', '.join(missing)}"
        
        # æ£€æŸ¥æ¨¡å‹åˆ†ç‰‡æ–‡ä»¶
        safetensors_files = list(Path(model_path).glob("model-*.safetensors"))
        if not safetensors_files:
            logger.warning("æœªæ‰¾åˆ°æ¨¡å‹åˆ†ç‰‡æ–‡ä»¶")
            return False, "ç¼ºå°‘æ¨¡å‹åˆ†ç‰‡æ–‡ä»¶ (model-*.safetensors)"
        
        # è¯»å–æ¨¡å‹ä¿¡æ¯
        try:
            with open(os.path.join(model_path, "config.json"), 'r') as f:
                config = json.load(f)
            model_type = config.get('model_type', 'æœªçŸ¥')
            vocab_size = config.get('vocab_size', 'æœªçŸ¥')
            model_info = f"{model_type} | è¯æ±‡è¡¨: {vocab_size} | åˆ†ç‰‡: {len(safetensors_files)}ä¸ª"
            logger.info(f"âœ… æ¨¡å‹: {model_info}")
            return True, model_info
        except Exception as e:
            logger.warning(f"è¯»å–æ¨¡å‹é…ç½®å¤±è´¥: {e}")
            return True, f"æ¨¡å‹å­˜åœ¨ä½†é…ç½®è¯»å–å¤±è´¥: {e}"

class ExampleManager:
    """ç¤ºä¾‹æ–‡ä»¶ç®¡ç†å™¨"""
    
    def __init__(self):
        self.examples_dir = CONFIG["examples"]["examples_dir"]
        self.sample_url = CONFIG["examples"]["sample_url"]
        self.sample_filename = CONFIG["examples"]["sample_filename"]
        os.makedirs(self.examples_dir, exist_ok=True)
    
    def download_sample_file(self):
        """ä¸‹è½½ç¤ºä¾‹æ–‡ä»¶"""
        sample_path = os.path.join(self.examples_dir, self.sample_filename)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ£€æŸ¥å¤§å°æ˜¯å¦åˆç†
        if os.path.exists(sample_path):
            file_size = os.path.getsize(sample_path)
            if file_size > 1024:  # å¤§äº1KBè®¤ä¸ºä¸‹è½½æˆåŠŸ
                logger.info(f"âœ… ç¤ºä¾‹æ–‡ä»¶å·²å­˜åœ¨: {sample_path} ({file_size} bytes)")
                return True, sample_path
        
        try:
            logger.info(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ç¤ºä¾‹æ–‡ä»¶: {self.sample_url}")
            
            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(self.sample_url, timeout=30, stream=True)
            response.raise_for_status()
            
            # ä¿å­˜æ–‡ä»¶
            with open(sample_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(sample_path)
            logger.info(f"âœ… ç¤ºä¾‹æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {sample_path} ({file_size} bytes)")
            return True, sample_path
            
        except requests.RequestException as e:
            logger.warning(f"âš ï¸ ç¤ºä¾‹æ–‡ä»¶ä¸‹è½½å¤±è´¥: {e}")
            return False, f"ä¸‹è½½å¤±è´¥: {e}"
        except Exception as e:
            logger.error(f"âŒ ç¤ºä¾‹æ–‡ä»¶å¤„ç†å¼‚å¸¸: {e}")
            return False, f"å¤„ç†å¼‚å¸¸: {e}"
    
    def get_example_files(self):
        """è·å–ç¤ºä¾‹æ–‡ä»¶åˆ—è¡¨"""
        example_files = []
        
        if os.path.exists(self.examples_dir):
            for file in os.listdir(self.examples_dir):
                file_path = os.path.join(self.examples_dir, file)
                if os.path.isfile(file_path):
                    file_ext = Path(file).suffix.lower()
                    if file_ext in CONFIG["processing"]["supported_formats"]:
                        example_files.append(file_path)
        
        return example_files
    
    def prepare_examples(self):
        """å‡†å¤‡ç¤ºä¾‹æ–‡ä»¶"""
        if CONFIG["examples"]["auto_download"]:
            success, result = self.download_sample_file()
            if not success:
                logger.warning(f"ç¤ºä¾‹æ–‡ä»¶å‡†å¤‡å¤±è´¥: {result}")
        
        return self.get_example_files()

class OllamaClient:
    """Ollama å®¢æˆ·ç«¯ï¼Œç”¨äºä¸æœ¬åœ° Ollama æœåŠ¡å™¨é€šä¿¡"""
    
    def __init__(self, auto_start=False):
        self.base_url = CONFIG["ollama"]["base_url"]
        self.model = CONFIG["ollama"]["model"]
        self.timeout = CONFIG["ollama"]["timeout"]
        self.max_tokens = CONFIG["ollama"]["max_tokens"]
        self.temperature = CONFIG["ollama"]["temperature"]
        self.ollama_process = None
        self.model_process = None
        self.is_ollama_ready = False
        self.is_model_ready = False
        self.startup_status = "æœªå¯åŠ¨"
        self.startup_error = None
        
        # å¦‚æœå¯ç”¨è‡ªåŠ¨å¯åŠ¨ï¼Œåœ¨åå°å¯åŠ¨ Ollama
        if auto_start:
            self._start_ollama_background()
    
    def _start_ollama_background(self):
        """åœ¨åå°å¯åŠ¨ Ollama æœåŠ¡å™¨å’Œæ¨¡å‹"""
        def start_ollama_thread():
            try:
                logger.info("ğŸš€ å¼€å§‹åœ¨åå°å¯åŠ¨ Ollama æœåŠ¡å™¨...")
                self.startup_status = "æ­£åœ¨å¯åŠ¨ Ollama..."
                
                # é¦–å…ˆå¯åŠ¨ Ollama æœåŠ¡å™¨
                if self._start_ollama_server():
                    self.startup_status = "æ­£åœ¨åŠ è½½æ¨¡å‹..."
                    logger.info("âœ… Ollama æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼Œå¼€å§‹åŠ è½½æ¨¡å‹...")
                    
                    # ç„¶åå¯åŠ¨æ¨¡å‹
                    if self._start_ollama_model():
                        self.startup_status = "å¯åŠ¨æˆåŠŸ"
                        self.is_ollama_ready = True
                        self.is_model_ready = True
                        logger.info(f"âœ… Ollama å’Œæ¨¡å‹ {self.model} å¯åŠ¨å®Œæˆ")
                    else:
                        self.startup_status = "æ¨¡å‹å¯åŠ¨å¤±è´¥"
                        logger.error("âŒ Ollama æ¨¡å‹å¯åŠ¨å¤±è´¥")
                else:
                    self.startup_status = "Ollama å¯åŠ¨å¤±è´¥"
                    logger.error("âŒ Ollama æœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
                    
            except Exception as e:
                self.startup_status = "å¯åŠ¨å¼‚å¸¸"
                self.startup_error = str(e)
                logger.error(f"âŒ Ollama å¯åŠ¨å¼‚å¸¸: {e}")
        
        # åœ¨æ–°çº¿ç¨‹ä¸­å¯åŠ¨
        startup_thread = threading.Thread(target=start_ollama_thread, daemon=True)
        startup_thread.start()
        logger.info("ğŸ”„ Ollama æ­£åœ¨åå°å¯åŠ¨ä¸­...")
    
    def _start_ollama_server(self):
        """å¯åŠ¨ Ollama æœåŠ¡å™¨"""
        try:
            # æ£€æŸ¥ Ollama æ˜¯å¦å·²ç»åœ¨è¿è¡Œ
            try:
                response = requests.get(f"{self.base_url}/api/tags", timeout=5)
                if response.status_code == 200:
                    logger.info("âœ… Ollama æœåŠ¡å™¨å·²åœ¨è¿è¡Œ")
                    return True
            except requests.ConnectionError:
                logger.info("Ollama æœåŠ¡å™¨æœªè¿è¡Œï¼Œæ­£åœ¨å¯åŠ¨...")
            
            # å¯åŠ¨ Ollama æœåŠ¡å™¨
            cmd = ["ollama", "serve"]
            
            self.ollama_process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=os.environ.copy()
            )
            
            # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
            max_wait = 30  # æœ€å¤šç­‰å¾…30ç§’
            for i in range(max_wait):
                try:
                    response = requests.get(f"{self.base_url}/api/tags", timeout=2)
                    if response.status_code == 200:
                        logger.info(f"âœ… Ollama æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼Œè€—æ—¶: {i+1}ç§’")
                        return True
                except requests.ConnectionError:
                    time.sleep(1)
                    continue
            
            logger.error("âŒ Ollama æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶")
            return False
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨ Ollama æœåŠ¡å™¨å¤±è´¥: {e}")
            self.startup_error = str(e)
            return False
    
    def _start_ollama_model(self):
        """å¯åŠ¨ Ollama æ¨¡å‹"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»å¯ç”¨
            try:
                response = requests.get(f"{self.base_url}/api/tags", timeout=5)
                if response.status_code == 200:
                    models = response.json().get("models", [])
                    available_models = [model["name"] for model in models]
                    if self.model in available_models:
                        logger.info(f"âœ… æ¨¡å‹ {self.model} å·²å¯ç”¨")
                        return True
            except Exception as e:
                logger.warning(f"æ£€æŸ¥æ¨¡å‹çŠ¶æ€å¤±è´¥: {e}")
            
            # å¯åŠ¨æ¨¡å‹
            logger.info(f"æ­£åœ¨å¯åŠ¨æ¨¡å‹ {self.model}...")
            cmd = ["ollama", "run", self.model, "--help"]  # ä½¿ç”¨ --help å¿«é€ŸåŠ è½½æ¨¡å‹
            
            # å…ˆå°è¯•æ‹‰å–æ¨¡å‹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            pull_cmd = ["ollama", "pull", self.model]
            logger.info(f"ç¡®ä¿æ¨¡å‹ {self.model} å·²ä¸‹è½½...")
            
            pull_process = subprocess.run(
                pull_cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            if pull_process.returncode != 0:
                logger.warning(f"æ‹‰å–æ¨¡å‹å¯èƒ½å¤±è´¥: {pull_process.stderr}")
            
            # è¿è¡Œæ¨¡å‹ä»¥ç¡®ä¿åŠ è½½
            run_process = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
            )
            
            if run_process.returncode == 0:
                logger.info(f"âœ… æ¨¡å‹ {self.model} å¯åŠ¨æˆåŠŸ")
                return True
            else:
                logger.error(f"âŒ æ¨¡å‹ {self.model} å¯åŠ¨å¤±è´¥: {run_process.stderr}")
                self.startup_error = run_process.stderr
                return False
                
        except subprocess.TimeoutExpired:
            logger.error(f"âŒ æ¨¡å‹ {self.model} å¯åŠ¨è¶…æ—¶")
            self.startup_error = "æ¨¡å‹å¯åŠ¨è¶…æ—¶"
            return False
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨æ¨¡å‹å¤±è´¥: {e}")
            self.startup_error = str(e)
            return False
    
    def get_ollama_status(self):
        """è·å– Ollama çŠ¶æ€"""
        # å®æ—¶æ£€æŸ¥çŠ¶æ€
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                if self.model in available_models:
                    return {
                        "status": "è¿è¡Œä¸­",
                        "ollama_ready": True,
                        "model_ready": True,
                        "error": None,
                        "model": self.model
                    }
                else:
                    return {
                        "status": f"æ¨¡å‹ {self.model} æœªåŠ è½½",
                        "ollama_ready": True,
                        "model_ready": False,
                        "error": f"è¯·è¿è¡Œ: ollama run {self.model}",
                        "model": self.model
                    }
            else:
                return {
                    "status": f"æœåŠ¡å™¨å“åº”å¼‚å¸¸: {response.status_code}",
                    "ollama_ready": False,
                    "model_ready": False,
                    "error": f"HTTP {response.status_code}",
                    "model": self.model
                }
        except requests.ConnectionError:
            return {
                "status": "æœåŠ¡å™¨æœªè¿è¡Œ",
                "ollama_ready": False,
                "model_ready": False,
                "error": "è¯·è¿è¡Œ: ollama serve",
                "model": self.model
            }
        except Exception as e:
            return {
                "status": "æ£€æŸ¥å¤±è´¥",
                "ollama_ready": False,
                "model_ready": False,
                "error": str(e),
                "model": self.model
            }
        
    def check_ollama_status(self):
        """æ£€æŸ¥ Ollama æœåŠ¡å™¨çŠ¶æ€"""
        # ç›´æ¥è¿›è¡Œå®æ—¶æ£€æŸ¥
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                if self.model in available_models:
                    return True, f"âœ… æ¨¡å‹ {self.model} å¯ç”¨"
                else:
                    return False, f"âŒ æ¨¡å‹ {self.model} ä¸å¯ç”¨ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œ: ollama run {self.model}"
            else:
                return False, f"âŒ Ollama æœåŠ¡å™¨å“åº”å¼‚å¸¸: {response.status_code}"
        except requests.ConnectionError:
            return False, "âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡å™¨ï¼Œè¯·æ‰‹åŠ¨å¯åŠ¨: ollama serve"
        except Exception as e:
            return False, f"âŒ æ£€æŸ¥ Ollama çŠ¶æ€æ—¶å‡ºé”™: {str(e)}"
    
    def generate_response(self, prompt, context=""):
        """ç”Ÿæˆå›å¤"""
        try:
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            if context:
                full_prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{prompt}

è¯·åŸºäºæ–‡æ¡£å†…å®¹ç»™å‡ºå‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""
            else:
                full_prompt = prompt
            
            # å‘é€è¯·æ±‚åˆ° Ollama
            payload = {
                "model": self.model,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "num_predict": self.max_tokens,
                    "temperature": self.temperature
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                return True, result.get("response", "")
            else:
                return False, f"Ollama API é”™è¯¯: {response.status_code} - {response.text}"
                
        except requests.Timeout:
            return False, "è¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•"
        except requests.ConnectionError:
            return False, "æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡å™¨"
        except Exception as e:
            return False, f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"

class OlmOCRProcessor:
    """OlmOCR å¤„ç†å™¨ - æœ¬åœ°åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥é›†æˆ vLLM æœåŠ¡å™¨"""
    
    def __init__(self, auto_start_server=False):
        self.model_path = CONFIG["model"]["path"]
        self.workspace_base = CONFIG["processing"]["workspace_base"]
        self.supported_formats = CONFIG["processing"]["supported_formats"]
        self.server_port = 30024
        self.server_process = None
        self.is_server_ready = False
        self.startup_status = "æœªå¯åŠ¨"
        self.startup_error = None
        self.last_processed_content = ""  # å­˜å‚¨æœ€åå¤„ç†çš„æ–‡æ¡£å†…å®¹
        self.last_processed_filename = ""  # å­˜å‚¨æœ€åå¤„ç†çš„æ–‡ä»¶å
        os.makedirs(self.workspace_base, exist_ok=True)
        
        # å¦‚æœå¯ç”¨è‡ªåŠ¨å¯åŠ¨ï¼Œåœ¨åå°å¯åŠ¨æœåŠ¡å™¨
        if auto_start_server:
            self._start_server_background()
    
    def _start_server_background(self):
        """åœ¨åå°å¯åŠ¨ vLLM æœåŠ¡å™¨"""
        def start_server_thread():
            try:
                logger.info("ğŸš€ å¼€å§‹åœ¨åå°å¯åŠ¨ vLLM æœåŠ¡å™¨...")
                self.startup_status = "æ­£åœ¨å¯åŠ¨..."
                
                # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯å¹¶å¯åŠ¨æœåŠ¡å™¨
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                success = loop.run_until_complete(self.start_vllm_server())
                
                if success:
                    self.startup_status = "å¯åŠ¨æˆåŠŸ"
                    logger.info("âœ… vLLM æœåŠ¡å™¨åå°å¯åŠ¨å®Œæˆ")
                else:
                    self.startup_status = "å¯åŠ¨å¤±è´¥"
                    logger.error("âŒ vLLM æœåŠ¡å™¨åå°å¯åŠ¨å¤±è´¥")
                
                loop.close()
                
            except Exception as e:
                self.startup_status = "å¯åŠ¨å¼‚å¸¸"
                self.startup_error = str(e)
                logger.error(f"âŒ vLLM æœåŠ¡å™¨åå°å¯åŠ¨å¼‚å¸¸: {e}")
        
        # åœ¨æ–°çº¿ç¨‹ä¸­å¯åŠ¨æœåŠ¡å™¨
        startup_thread = threading.Thread(target=start_server_thread, daemon=True)
        startup_thread.start()
        logger.info("ğŸ”„ vLLM æœåŠ¡å™¨æ­£åœ¨åå°å¯åŠ¨ä¸­...")
    
    def get_server_status(self):
        """è·å–æœåŠ¡å™¨çŠ¶æ€"""
        return {
            "status": self.startup_status,
            "ready": self.is_server_ready,
            "error": self.startup_error,
            "port": self.server_port if self.is_server_ready else None
        }
    
    async def start_vllm_server(self):
        """å¯åŠ¨ vLLM æœåŠ¡å™¨"""
        if self.server_process and self.is_server_ready:
            return True
            
        try:
            self.startup_status = "æ­£åœ¨å¯åŠ¨..."
            logger.info(f"å¯åŠ¨ vLLM æœåŠ¡å™¨ï¼Œæ¨¡å‹: {self.model_path}")
            
            cmd = [
                "vllm", "serve", self.model_path,
                "--port", str(self.server_port),
                "--disable-log-requests",
                "--uvicorn-log-level", "warning",
                "--served-model-name", "olmocr",
                "--tensor-parallel-size", "1",
                "--data-parallel-size", "1",
                "--gpu-memory-utilization", "0.6",  # é™ä½åˆ° 0.6 ä»¥é€‚åº”å¯ç”¨å†…å­˜
                "--max-model-len", "8192"  # é™ä½æœ€å¤§é•¿åº¦ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
            ]
            
            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            self.server_process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=os.environ.copy()
            )
            
            # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 5 åˆ†é’Ÿï¼Œå› ä¸ºæ¨¡å‹åŠ è½½å¯èƒ½å¾ˆæ…¢
            timeout = 300  # 5åˆ†é’Ÿè¶…æ—¶
            start_time = time.time()
            
            logger.info("ç­‰å¾… vLLM æœåŠ¡å™¨å¯åŠ¨...")
            self.startup_status = "æ¨¡å‹åŠ è½½ä¸­..."
            
            while time.time() - start_time < timeout:
                try:
                    # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
                    if self.server_process.returncode is not None:
                        stdout, stderr = await self.server_process.communicate()
                        error_msg = f"vLLM è¿›ç¨‹æå‰é€€å‡ºï¼Œè¿”å›ç : {self.server_process.returncode}"
                        logger.error(error_msg)
                        logger.error(f"stdout: {stdout.decode()}")
                        logger.error(f"stderr: {stderr.decode()}")
                        self.startup_status = "å¯åŠ¨å¤±è´¥"
                        self.startup_error = error_msg + f"\nstderr: {stderr.decode()}"
                        return False
                    
                    # å°è¯•è¿æ¥åˆ°æœåŠ¡å™¨
                    try:
                        reader, writer = await asyncio.wait_for(
                            asyncio.open_connection('localhost', self.server_port),
                            timeout=3
                        )
                        
                        # å‘é€ç®€å•çš„å¥åº·æ£€æŸ¥è¯·æ±‚
                        health_request = (
                            f"GET /health HTTP/1.1\r\n"
                            f"Host: localhost:{self.server_port}\r\n"
                            f"Connection: close\r\n\r\n"
                        )
                        
                        writer.write(health_request.encode())
                        await writer.drain()
                        
                        response = await asyncio.wait_for(reader.read(1024), timeout=3)
                        writer.close()
                        await writer.wait_closed()
                        
                        if b"200" in response or b"404" in response:  # 404 ä¹Ÿè¡¨ç¤ºæœåŠ¡å™¨åœ¨è¿è¡Œ
                            self.is_server_ready = True
                            self.startup_status = "å¯åŠ¨æˆåŠŸ"
                            elapsed = time.time() - start_time
                            logger.info(f"âœ… vLLM æœåŠ¡å™¨å·²å¯åŠ¨ï¼Œç«¯å£: {self.server_port}ï¼Œè€—æ—¶: {elapsed:.1f}ç§’")
                            return True
                            
                    except (ConnectionRefusedError, asyncio.TimeoutError, OSError) as e:
                        # æœåŠ¡å™¨è¿˜æ²¡å‡†å¤‡å¥½ï¼Œç»§ç»­ç­‰å¾…
                        elapsed = time.time() - start_time
                        if elapsed % 15 < 2:  # æ¯15ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
                            logger.info(f"ç­‰å¾… vLLM æœåŠ¡å™¨å¯åŠ¨... ({elapsed:.0f}s/{timeout}s)")
                            self.startup_status = f"åŠ è½½ä¸­... ({elapsed:.0f}s/{timeout}s)"
                        await asyncio.sleep(2)
                        continue
                        
                except Exception as e:
                    logger.warning(f"æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€æ—¶å‡ºé”™: {e}")
                    await asyncio.sleep(2)
                    continue
            
            error_msg = "vLLM æœåŠ¡å™¨å¯åŠ¨è¶…æ—¶"
            logger.error(f"âŒ {error_msg}")
            self.startup_status = "å¯åŠ¨è¶…æ—¶"
            self.startup_error = error_msg
            
            # è¶…æ—¶åå°è¯•è·å–è¿›ç¨‹è¾“å‡º
            if self.server_process and self.server_process.returncode is None:
                try:
                    stdout, stderr = await asyncio.wait_for(
                        self.server_process.communicate(), timeout=5
                    )
                    logger.error(f"è¶…æ—¶åçš„ stdout: {stdout.decode()}")
                    logger.error(f"è¶…æ—¶åçš„ stderr: {stderr.decode()}")
                    self.startup_error += f"\nstderr: {stderr.decode()}"
                except asyncio.TimeoutError:
                    logger.error("æ— æ³•è·å–è¿›ç¨‹è¾“å‡º")
                    
            return False
            
        except Exception as e:
            error_msg = f"vLLM æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}"
            logger.error(f"âŒ {error_msg}")
            self.startup_status = "å¯åŠ¨å¼‚å¸¸"
            self.startup_error = str(e)
            return False
    
    def stop_vllm_server(self):
        """åœæ­¢ vLLM æœåŠ¡å™¨"""
        if self.server_process:
            try:
                self.server_process.terminate()
                self.is_server_ready = False
                logger.info("vLLM æœåŠ¡å™¨å·²åœæ­¢")
            except Exception as e:
                logger.warning(f"åœæ­¢ vLLM æœåŠ¡å™¨æ—¶å‡ºé”™: {e}")
    
    async def build_page_query(self, local_pdf_path: str, page: int, target_longest_image_dim: int = 1288) -> dict:
        """æ„å»ºé¡µé¢æŸ¥è¯¢è¯·æ±‚"""
        # æ¸²æŸ“ PDF é¡µé¢ä¸º base64 å›¾åƒ
        image_base64 = await asyncio.to_thread(
            render_pdf_to_base64png, 
            local_pdf_path, 
            page, 
            target_longest_image_dim=target_longest_image_dim
        )
        
        return {
            "model": "olmocr",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}},
                        {"type": "text", "text": build_no_anchoring_yaml_prompt()},
                    ],
                }
            ],
            "max_tokens": 4500,
            "temperature": 0.1,
        }
    
    async def make_request(self, query: dict) -> tuple[int, str]:
        """å‘ vLLM æœåŠ¡å™¨å‘é€è¯·æ±‚"""
        url = f"http://localhost:{self.server_port}/v1/chat/completions"
        
        try:
            # ç®€å•çš„ HTTP POST å®ç°
            json_payload = json.dumps(query)
            
            reader, writer = await asyncio.open_connection('localhost', self.server_port)
            
            request = (
                f"POST /v1/chat/completions HTTP/1.1\r\n"
                f"Host: localhost:{self.server_port}\r\n"
                f"Content-Type: application/json\r\n"
                f"Content-Length: {len(json_payload)}\r\n"
                f"Connection: close\r\n\r\n"
                f"{json_payload}"
            )
            
            writer.write(request.encode())
            await writer.drain()
            
            # è¯»å–å“åº”
            response_data = await reader.read()
            writer.close()
            await writer.wait_closed()
            
            response_text = response_data.decode('utf-8')
            
            # è§£æ HTTP å“åº”
            if "\r\n\r\n" in response_text:
                headers, body = response_text.split("\r\n\r\n", 1)
                status_line = headers.split('\r\n')[0]
                status_code = int(status_line.split()[1])
                return status_code, body
            else:
                return 500, "Invalid response"
                
        except Exception as e:
            logger.error(f"è¯·æ±‚å¤±è´¥: {e}")
            return 500, str(e)
    
    async def process_single_page(self, pdf_path: str, page_num: int) -> dict:
        """å¤„ç†å•ä¸ª PDF é¡µé¢"""
        max_retries = 3
        attempt = 0
        
        while attempt < max_retries:
            try:
                # æ„å»ºæŸ¥è¯¢
                query = await self.build_page_query(pdf_path, page_num)
                
                # å‘é€è¯·æ±‚
                status_code, response_body = await self.make_request(query)
                
                if status_code != 200:
                    raise ValueError(f"HTTPé”™è¯¯: {status_code}")
                
                # è§£æå“åº”
                response_data = json.loads(response_body)
                
                if response_data["choices"][0]["finish_reason"] != "stop":
                    raise ValueError("å“åº”æœªæ­£å¸¸ç»“æŸ")
                
                # æå–å†…å®¹
                model_response = response_data["choices"][0]["message"]["content"]
                
                # è§£æå‰ç½®å…ƒæ•°æ®å’Œæ–‡æœ¬
                parser = FrontMatterParser(front_matter_class=PageResponse)
                front_matter, text = parser._extract_front_matter_and_text(model_response)
                page_response = parser._parse_front_matter(front_matter, text)
                
                return {
                    "page_num": page_num,
                    "success": True,
                    "content": text,
                    "front_matter": front_matter,
                    "page_response": page_response,
                    "tokens": {
                        "input": response_data["usage"].get("prompt_tokens", 0),
                        "output": response_data["usage"].get("completion_tokens", 0)
                    }
                }
                
            except Exception as e:
                attempt += 1
                logger.warning(f"é¡µé¢ {page_num} å¤„ç†å¤±è´¥ (å°è¯• {attempt}/{max_retries}): {e}")
                if attempt >= max_retries:
                    return {
                        "page_num": page_num,
                        "success": False,
                        "error": str(e),
                        "content": "",
                        "tokens": {"input": 0, "output": 0}
                    }
                await asyncio.sleep(1)
    
    async def process_pdf_async(self, pdf_path: str, output_format: str = "markdown") -> tuple[str, str]:
        """å¼‚æ­¥å¤„ç† PDF æ–‡ä»¶"""
        try:
            # ç¡®ä¿æœåŠ¡å™¨å·²å¯åŠ¨
            if not await self.start_vllm_server():
                return "âŒ vLLM æœåŠ¡å™¨å¯åŠ¨å¤±è´¥", ""
            
            # è·å– PDF é¡µæ•°
            reader = PdfReader(pdf_path)
            total_pages = len(reader.pages)
            
            logger.info(f"å¼€å§‹å¤„ç† PDF: {os.path.basename(pdf_path)} ({total_pages} é¡µ)")
            
            # å¤„ç†æ‰€æœ‰é¡µé¢
            tasks = []
            for page_num in range(total_pages):
                task = self.process_single_page(pdf_path, page_num)
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰é¡µé¢å¤„ç†å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æ”¶é›†ç»“æœ
            successful_pages = []
            failed_pages = []
            total_tokens = {"input": 0, "output": 0}
            
            for result in results:
                if isinstance(result, Exception):
                    failed_pages.append(f"å¼‚å¸¸: {result}")
                elif result["success"]:
                    successful_pages.append(result)
                    total_tokens["input"] += result["tokens"]["input"]
                    total_tokens["output"] += result["tokens"]["output"]
                else:
                    failed_pages.append(f"é¡µé¢ {result['page_num']}: {result['error']}")
            
            # ç”Ÿæˆè¾“å‡º
            if output_format == "markdown":
                content = self._format_markdown_output(successful_pages, failed_pages, total_tokens)
            else:
                content = self._format_dolma_output(successful_pages, failed_pages, total_tokens, pdf_path)
            
            status = f"âœ… å¤„ç†å®Œæˆ: {len(successful_pages)}/{total_pages} é¡µæˆåŠŸ"
            if failed_pages:
                status += f", {len(failed_pages)} é¡µå¤±è´¥"
            
            return status, content
            
        except Exception as e:
            logger.error(f"PDF å¤„ç†å¼‚å¸¸: {e}")
            return f"âŒ å¤„ç†å¤±è´¥: {str(e)}", ""
    
    def _format_markdown_output(self, successful_pages: list, failed_pages: list, total_tokens: dict) -> str:
        """æ ¼å¼åŒ– Markdown è¾“å‡º"""
        content = f"""# PDF OCR å¤„ç†ç»“æœ

**å¤„ç†æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
**æˆåŠŸé¡µé¢**: {len(successful_pages)}
**å¤±è´¥é¡µé¢**: {len(failed_pages)}
**ä½¿ç”¨ tokens**: è¾“å…¥ {total_tokens['input']}, è¾“å‡º {total_tokens['output']}

---

"""
        
        # æŒ‰é¡µé¢é¡ºåºæ’åº
        successful_pages.sort(key=lambda x: x["page_num"])
        
        for page_result in successful_pages:
            content += f"## é¡µé¢ {page_result['page_num'] + 1}\n\n"
            content += page_result["content"]
            content += "\n\n---\n\n"
        
        if failed_pages:
            content += "## å¤„ç†å¤±è´¥çš„é¡µé¢\n\n"
            for error in failed_pages:
                content += f"- {error}\n"
        
        return content
    
    def _format_dolma_output(self, successful_pages: list, failed_pages: list, total_tokens: dict, pdf_path: str) -> str:
        """æ ¼å¼åŒ– Dolma è¾“å‡º"""
        results = []
        
        # æŒ‰é¡µé¢é¡ºåºæ’åº
        successful_pages.sort(key=lambda x: x["page_num"])
        
        for page_result in successful_pages:
            dolma_record = {
                "id": f"{Path(pdf_path).stem}_page_{page_result['page_num']}",
                "text": page_result["content"],
                "source": "OlmOCR-Local",
                "added": time.strftime('%Y-%m-%d %H:%M:%S'),
                "metadata": {
                    "page_number": page_result["page_num"] + 1,
                    "tokens": page_result["tokens"],
                    "pdf_file": os.path.basename(pdf_path)
                }
            }
            results.append(dolma_record)
        
        # æ·»åŠ å¤±è´¥é¡µé¢ä¿¡æ¯
        if failed_pages:
            error_record = {
                "id": f"{Path(pdf_path).stem}_errors",
                "text": "",
                "source": "OlmOCR-Local-Errors",
                "added": time.strftime('%Y-%m-%d %H:%M:%S'),
                "metadata": {
                    "failed_pages": failed_pages,
                    "pdf_file": os.path.basename(pdf_path)
                }
            }
            results.append(error_record)
        
        # æ ¼å¼åŒ–ä¸º JSON Lines
        return '\n'.join(json.dumps(record, ensure_ascii=False) for record in results)
    
    def process_file(self, file, output_format="markdown"):
        """å¤„ç†å•ä¸ªæ–‡ä»¶ - åŒæ­¥æ¥å£åŒ…è£…"""
        if file is None:
            return "âŒ è¯·ä¸Šä¼ æ–‡ä»¶", ""
        
        # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
        if not self.is_server_ready:
            status_info = self.get_server_status()
            if status_info["status"] == "å¯åŠ¨å¤±è´¥" or status_info["status"] == "å¯åŠ¨å¼‚å¸¸":
                return f"âŒ vLLM æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {status_info.get('error', 'æœªçŸ¥é”™è¯¯')}", ""
            else:
                return f"â³ vLLM æœåŠ¡å™¨å°šæœªå°±ç»ªï¼Œå½“å‰çŠ¶æ€: {status_info['status']}", ""
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        file_ext = Path(file.name).suffix.lower()
        if file_ext not in self.supported_formats:
            return f"âŒ ä¸æ”¯æŒçš„æ ¼å¼: {file_ext}", ""
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size_mb = os.path.getsize(file.name) / (1024 * 1024)
        if file_size_mb > CONFIG["processing"]["max_file_size_mb"]:
            return f"âŒ æ–‡ä»¶è¿‡å¤§: {file_size_mb:.1f}MB (é™åˆ¶: {CONFIG['processing']['max_file_size_mb']}MB)", ""
        
        try:
            # åˆ›å»ºä¸´æ—¶å·¥ä½œç©ºé—´
            session_id = f"session_{int(time.time())}"
            workspace = os.path.join(self.workspace_base, session_id)
            os.makedirs(workspace, exist_ok=True)
            
            # å¤åˆ¶æ–‡ä»¶åˆ°å·¥ä½œç©ºé—´
            filename = os.path.basename(file.name)
            local_file = os.path.join(workspace, filename)
            shutil.copy2(file.name, local_file)
            
            logger.info(f"å¼€å§‹å¤„ç†: {filename} ({file_size_mb:.1f}MB)")
            
            # è¿è¡Œå¼‚æ­¥å¤„ç†
            try:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯
                try:
                    current_loop = asyncio.get_running_loop()
                    logger.info("æ£€æµ‹åˆ°è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡")
                    
                    # åœ¨çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
                    result_holder = {"status": None, "content": None, "error": None}
                    
                    def run_async_in_thread():
                        try:
                            # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            
                            status, content = new_loop.run_until_complete(
                                self.process_pdf_async(local_file, output_format)
                            )
                            
                            new_loop.close()
                            result_holder["status"] = status
                            result_holder["content"] = content
                            
                        except Exception as e:
                            result_holder["error"] = str(e)
                    
                    # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œ
                    thread = threading.Thread(target=run_async_in_thread)
                    thread.start()
                    thread.join(timeout=CONFIG["processing"]["timeout"])
                    
                    if thread.is_alive():
                        return "â° å¤„ç†è¶…æ—¶", ""
                    
                    if result_holder["error"]:
                        raise Exception(result_holder["error"])
                    
                    status = result_holder["status"]
                    content = result_holder["content"]
                    
                except RuntimeError:
                    # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥è¿è¡Œ
                    logger.info("æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥è¿è¡Œå¼‚æ­¥ä»»åŠ¡")
                    status, content = asyncio.run(
                        self.process_pdf_async(local_file, output_format)
                    )
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                shutil.rmtree(workspace, ignore_errors=True)
                
                # å­˜å‚¨å¤„ç†ç»“æœä¾›èŠå¤©ä½¿ç”¨
                if status and content and "âœ… å¤„ç†å®Œæˆ" in status:
                    self.last_processed_content = content
                    self.last_processed_filename = filename
                    logger.info(f"âœ… å·²ä¿å­˜æ–‡æ¡£å†…å®¹ä¾›èŠå¤©ä½¿ç”¨: {filename} (é•¿åº¦: {len(content)} å­—ç¬¦)")
                else:
                    logger.warning(f"âš ï¸ æ–‡æ¡£å†…å®¹æœªä¿å­˜ - çŠ¶æ€: {status}, å†…å®¹é•¿åº¦: {len(content) if content else 0}")
                
                return status, content
                
            except Exception as e:
                logger.error(f"å¼‚æ­¥å¤„ç†å¤±è´¥: {e}")
                # å›é€€åˆ°ç®€å•å¤„ç†
                return self._create_fallback_result(file, output_format, workspace, filename, str(e))
            
        except Exception as e:
            logger.error(f"å¤„ç†å¼‚å¸¸: {e}")
            return f"âŒ å¤„ç†å¤±è´¥: {str(e)}", ""
        finally:
            # ç¡®ä¿æ¸…ç†
            if 'workspace' in locals() and os.path.exists(workspace):
                shutil.rmtree(workspace, ignore_errors=True)
    
    def _create_fallback_result(self, file, output_format, workspace, filename, error_msg):
        """åˆ›å»ºé”™è¯¯å›é€€ç»“æœ"""
        try:
            if output_format == "markdown":
                mock_content = f"""# OlmOCR å¤„ç†å¤±è´¥

**æ–‡ä»¶å**: {filename}
**å¤„ç†æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
**çŠ¶æ€**: å¤„ç†å¤±è´¥

## é”™è¯¯ä¿¡æ¯
```
{error_msg}
```

## è§£å†³å»ºè®®

1. **æ£€æŸ¥ GPU èµ„æº**:
   ```bash
   nvidia-smi
   ```

2. **éªŒè¯æ¨¡å‹æ–‡ä»¶**:
   ```bash
   ls -la {CONFIG['model']['path']}
   ```

3. **æ‰‹åŠ¨æµ‹è¯• vLLM æœåŠ¡å™¨**:
   ```bash
   vllm serve {CONFIG['model']['path']} --port 30024
   ```

## æ–‡ä»¶ä¿¡æ¯
- **æ–‡ä»¶**: {filename}
- **å¤§å°**: {os.path.getsize(file.name) / (1024*1024):.1f} MB
- **æ ¼å¼**: {Path(filename).suffix.upper()}

è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯èƒ½æ˜¯ vLLM æœåŠ¡å™¨å¯åŠ¨å¤±è´¥ã€‚
"""
                return f"âŒ å¤„ç†å¤±è´¥: {filename}", mock_content
            else:
                mock_dolma = {
                    "id": f"error_{int(time.time())}",
                    "error": error_msg,
                    "file": filename,
                    "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
                    "suggestions": "æ£€æŸ¥ GPU çŠ¶æ€å’Œ vLLM æœåŠ¡å™¨"
                }
                return f"âŒ å¤„ç†å¤±è´¥: {filename}", json.dumps(mock_dolma, indent=2, ensure_ascii=False)
        except Exception as e:
            return f"âŒ é”™è¯¯å¤„ç†å¤±è´¥: {str(e)}", ""
    
    def get_document_content(self):
        """è·å–æœ€åå¤„ç†çš„æ–‡æ¡£å†…å®¹"""
        return self.last_processed_content, self.last_processed_filename

def get_global_processor():
    """è·å–å…¨å±€å¤„ç†å™¨å®ä¾‹"""
    global global_processor
    if global_processor is None:
        initialize_global_processor()
    return global_processor

def test_document_content():
    """æµ‹è¯•æ–‡æ¡£å†…å®¹"""
    global global_processor
    if global_processor is None:
        print("ğŸ”§ å…¨å±€å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
        if not initialize_global_processor():
            print("âŒ å…¨å±€å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥")
            return None, None
    
    if global_processor:
        content, filename = global_processor.get_document_content()
        print(f"ğŸ“„ å…¨å±€å¤„ç†å™¨æ–‡æ¡£çŠ¶æ€:")
        print(f"  æ–‡ä»¶å: {filename}")
        print(f"  å†…å®¹é•¿åº¦: {len(content) if content else 0}")
        if content:
            print(f"  å†…å®¹é¢„è§ˆ: {content[:200]}...")
        else:
            print("  âš ï¸ æ²¡æœ‰æ–‡æ¡£å†…å®¹")
        return content, filename
    else:
        print("âŒ å…¨å±€å¤„ç†å™¨æœªåˆå§‹åŒ–")
        return None, None

def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # ç¯å¢ƒæ£€æŸ¥
    env_checker = EnvironmentChecker()
    
    # æ£€æŸ¥ç»“æœ
    python_ok = env_checker.check_python()
    packages_ok, missing_packages = env_checker.check_packages()
    gpu_ok, gpu_info = env_checker.check_gpu()
    model_ok, model_info = env_checker.check_model()
    
    # ç¤ºä¾‹æ–‡ä»¶ç®¡ç†å™¨
    example_manager = ExampleManager()
    example_files = example_manager.prepare_examples()
    
    # åˆ›å»º Ollama å®¢æˆ·ç«¯
    ollama_client = OllamaClient(auto_start=False)  # ç¦ç”¨è‡ªåŠ¨å¯åŠ¨
    ollama_ok, ollama_info = ollama_client.check_ollama_status()
    
    # åˆ›å»ºå¤„ç†å™¨
    global global_processor
    processor = None
    if python_ok and packages_ok and model_ok:
        try:
            # å¯ç”¨è‡ªåŠ¨å¯åŠ¨æœåŠ¡å™¨
            processor = OlmOCRProcessor(auto_start_server=True)
            global_processor = processor  # è®¾ç½®å…¨å±€å¤„ç†å™¨
            logger.info("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼Œå·²åˆå§‹åŒ–å¤„ç†å™¨å¹¶å¼€å§‹å¯åŠ¨ vLLM æœåŠ¡å™¨")
        except Exception as e:
            logger.error(f"å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # è‡ªå®šä¹‰ CSS
    css = """
    .model-info { 
        background: linear-gradient(135deg, #e3f2fd, #bbdefb);
        padding: 15px;
        border-radius: 10px;
        border-left: 4px solid #2196f3;
        margin: 10px 0;
    }
    .env-check {
        font-family: monospace;
        background: #f8f9fa;
        padding: 10px;
        border-radius: 5px;
        border: 1px solid #dee2e6;
    }
    .local-badge {
        background: linear-gradient(135deg, #4caf50, #45a049);
        color: white;
        padding: 5px 10px;
        border-radius: 15px;
        font-weight: bold;
        margin: 5px;
    }
    .examples-section {
        background: #f8f9fa;
        padding: 10px;
        border-radius: 8px;
        border: 1px solid #dee2e6;
        margin: 10px 0;
    }
    .server-status, #vllm-status, #ollama-status {
        background: #f0f9ff !important;
        padding: 15px !important;
        border-radius: 8px !important;
        border-left: 4px solid #0ea5e9 !important;
        margin: 10px 0 !important;
        font-family: monospace !important;
        color: #1f2937 !important;
        border: 1px solid #e0e7ff !important;
    }
    .chat-section {
        background: #f8fdf9;
        padding: 15px;
        border-radius: 10px;
        border-left: 4px solid #22c55e;
        margin: 10px 0;
    }
    .chat-message {
        background: #ffffff;
        padding: 10px;
        border-radius: 8px;
        margin: 5px 0;
        border: 1px solid #e5e7eb;
    }
    .user-message {
        background: #dbeafe;
        border-left: 3px solid #3b82f6;
    }
    .assistant-message {
        background: #f0fdf4;
        border-left: 3px solid #22c55e;
    }
    """
    
    with gr.Blocks(
        title=CONFIG["ui"]["title"],
        theme=gr.themes.Soft(),
        css=css
    ) as demo:
        
        # æ ‡é¢˜
        gr.HTML(f"""
        <div style="text-align: center; padding: 20px;">
            <h1>ğŸ” {CONFIG["ui"]["title"]}</h1>
            <p style="font-size: 18px; color: #666;">
                {CONFIG["ui"]["description"]}
            </p>
            <div class="local-badge">
                ğŸ”’ å®Œå…¨æœ¬åœ°åŒ– | ğŸš« é›¶ç½‘ç»œä¾èµ– | ğŸ›¡ï¸ éšç§ä¿æŠ¤
            </div>
        </div>
        """)
        
        # ç¯å¢ƒçŠ¶æ€æ£€æŸ¥
        with gr.Accordion("ğŸ› ï¸ ç¯å¢ƒçŠ¶æ€æ£€æŸ¥", open=True):
            env_status_text = f"""
**ç³»ç»Ÿç¯å¢ƒ:**
- {'âœ…' if python_ok else 'âŒ'} **Python 3.8+**: {sys.version.split()[0]}
- {'âœ…' if packages_ok else 'âŒ'} **ä¾èµ–åŒ…**: {'å…¨éƒ¨å·²å®‰è£…' if packages_ok else f'ç¼ºå°‘: {", ".join(missing_packages)}'}
- {'âœ…' if gpu_ok else 'âš ï¸'} **GPU**: {gpu_info}
- {'âœ…' if model_ok else 'âŒ'} **æ¨¡å‹**: {model_info}
- {'âœ…' if ollama_ok else 'âš ï¸'} **Ollama**: {ollama_info}

**æœ¬åœ°åŒ–ç‰¹æ€§:**
- ğŸ”’ **å®Œå…¨ç¦»çº¿**: æ— ä»»ä½•å¤–éƒ¨ç½‘ç»œè¿æ¥
- ğŸš« **AWS ç¦ç”¨**: å®Œå…¨ç»•è¿‡äº‘æœåŠ¡ä¾èµ–
- ğŸ›¡ï¸ **éšç§ä¿æŠ¤**: æ‰€æœ‰æ•°æ®æœ¬åœ°å¤„ç†

**çŠ¶æ€**: {'ğŸŸ¢ å°±ç»ª' if all([python_ok, packages_ok, model_ok]) else 'ğŸ”´ éœ€è¦ä¿®å¤'}
            """
            gr.Markdown(env_status_text)
        
        # vLLM æœåŠ¡å™¨çŠ¶æ€
        if processor:
            with gr.Accordion("ğŸš€ vLLM æœåŠ¡å™¨çŠ¶æ€", open=True):
                # è·å–åˆå§‹çŠ¶æ€
                status_info = processor.get_server_status()
                if status_info["ready"]:
                    vllm_status_text = f"""
**vLLM æœåŠ¡å™¨:**
- **çŠ¶æ€**: âœ… {status_info['status']}
- **ç«¯å£**: {status_info['port']}
- **æ¨¡å‹**: olmOCR-7B-0725
- **å°±ç»ª**: âœ… å¯ä»¥å¼€å§‹å¤„ç†æ–‡æ¡£
"""
                elif status_info["status"] == "å¯åŠ¨å¤±è´¥" or status_info["status"] == "å¯åŠ¨å¼‚å¸¸":
                    error_detail = status_info.get("error", "æœªçŸ¥é”™è¯¯")
                    vllm_status_text = f"""
**vLLM æœåŠ¡å™¨:**
- **çŠ¶æ€**: âŒ {status_info['status']}
- **é”™è¯¯**: {error_detail}
- **å»ºè®®**: æ£€æŸ¥ GPU å†…å­˜å’Œæ¨¡å‹æ–‡ä»¶
"""
                else:
                    vllm_status_text = f"""
**vLLM æœåŠ¡å™¨:**
- **çŠ¶æ€**: ğŸ”„ {status_info['status']}
- **æç¤º**: é¦–æ¬¡å¯åŠ¨éœ€è¦åŠ è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…
- **é¢„è®¡**: å¤§çº¦éœ€è¦ 2-5 åˆ†é’Ÿ
"""
                gr.Markdown(vllm_status_text)
        
        # ä¸»ç•Œé¢
        if processor:
            # OCR å¤„ç†ç•Œé¢
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“ æ–‡ä»¶å¤„ç†")
                    
                    # ç¤ºä¾‹æ–‡ä»¶é€‰æ‹©å™¨
                    if example_files:
                        with gr.Group():
                            gr.Markdown("#### ğŸ“‹ ç¤ºä¾‹æ–‡ä»¶")
                            example_selector = gr.Dropdown(
                                choices=[(os.path.basename(f), f) for f in example_files],
                                label="é€‰æ‹©ç¤ºä¾‹æ–‡ä»¶",
                                value=example_files[0] if example_files else None,
                                elem_classes=["examples-section"]
                            )
                            
                            def load_example_file(selected_file):
                                if selected_file:
                                    return gr.File(value=selected_file)
                                return gr.File(value=None)
                    else:
                        example_selector = None
                        gr.Markdown("âš ï¸ ç¤ºä¾‹æ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
                    
                    gr.Markdown("#### ğŸ“ æˆ–ä¸Šä¼ è‡ªå·±çš„æ–‡ä»¶")
                    file_input = gr.File(
                        label="é€‰æ‹©æ–‡æ¡£æ–‡ä»¶",
                        file_types=CONFIG["processing"]["supported_formats"],
                        height=120
                    )
                    
                    # ç¤ºä¾‹æ–‡ä»¶åŠ è½½äº‹ä»¶
                    if example_selector:
                        example_selector.change(
                            fn=lambda x: x,
                            inputs=[example_selector],
                            outputs=[file_input]
                        )
                    
                    output_format = gr.Radio(
                        choices=["markdown", "dolma"],
                        value=CONFIG["output"]["default_format"],
                        label="è¾“å‡ºæ ¼å¼"
                    )
                    
                    with gr.Row():
                        process_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
                        refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°ç¤ºä¾‹", variant="secondary")
                    
                    # é…ç½®ä¿¡æ¯
                    gr.Markdown(f"""
                    ### âš™ï¸ é…ç½®ä¿¡æ¯
                    - **å¤„ç†æ¨¡å¼**: å®Œå…¨æœ¬åœ°åŒ–
                    - **è¶…æ—¶æ—¶é—´**: {CONFIG['processing']['timeout']}ç§’
                    - **æ–‡ä»¶å¤§å°é™åˆ¶**: {CONFIG['processing']['max_file_size_mb']}MB
                    - **å·¥ä½œç©ºé—´**: {CONFIG['processing']['workspace_base']}
                    - **æ”¯æŒæ ¼å¼**: {', '.join(CONFIG['processing']['supported_formats'])}
                    - **ç¤ºä¾‹æ–‡ä»¶**: {len(example_files)} ä¸ªå¯ç”¨
                    """)
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“Š å¤„ç†ç»“æœ")
                    
                    status_output = gr.Textbox(
                        label="å¤„ç†çŠ¶æ€",
                        interactive=False,
                        max_lines=3
                    )
                    
                    content_output = gr.Textbox(
                        label="è½¬æ¢å†…å®¹",
                        interactive=False,
                        max_lines=25,
                        show_copy_button=CONFIG["output"]["enable_copy"],
                        placeholder="è½¬æ¢ç»“æœå°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                    )
                    
                    # æ™ºèƒ½é—®ç­”åŠŸèƒ½
                    gr.Markdown("### ğŸ’¬ æ™ºèƒ½é—®ç­”")
                    
                    # æ£€æŸ¥ Ollama çŠ¶æ€
                    def check_current_ollama_status():
                        status_info = ollama_client.get_ollama_status()
                        return status_info["ollama_ready"] and status_info["model_ready"]
                    
                    if check_current_ollama_status():
                        gr.Markdown("*åŸºäºä¸Šæ–¹å¤„ç†çš„æ–‡æ¡£å†…å®¹è¿›è¡Œæ™ºèƒ½é—®ç­”*")
                        
                        # èŠå¤©å†å²
                        chatbot = gr.Chatbot(
                            label="å¯¹è¯å†å²",
                            height=300,
                            type="messages",
                            elem_classes=["chat-section"]
                        )
                        
                        # è¾“å…¥åŒºåŸŸ
                        with gr.Row():
                            msg_input = gr.Textbox(
                                label="",
                                placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                                scale=4
                            )
                            send_btn = gr.Button("å‘é€", variant="primary", scale=1)
                        
                        # æ§åˆ¶æŒ‰é’®
                        with gr.Row():
                            clear_chat_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary")
                            refresh_doc_btn = gr.Button("ğŸ”„ åˆ·æ–°æ–‡æ¡£", variant="secondary")
                        
                        gr.Markdown("""
                        **ğŸ’¡ ä½¿ç”¨æç¤º**
                        - å…ˆå¤„ç†æ–‡æ¡£ï¼Œç„¶ååŸºäºæ–‡æ¡£å†…å®¹æé—®
                        - æ”¯æŒå¤šè½®å¯¹è¯å’Œä¸Šä¸‹æ–‡å…³è”
                        
                        **ğŸ¯ é—®é¢˜ç¤ºä¾‹**
                        - "è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
                        - "æ–‡æ¡£ä¸­æåˆ°äº†å“ªäº›å…³é”®æ•°æ®ï¼Ÿ"
                        - "è¯·æ€»ç»“æ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹"
                        """)
                    else:
                        # è·å–å½“å‰çŠ¶æ€ä¿¡æ¯
                        status_info = ollama_client.get_ollama_status()
                        gr.HTML(f"""
                        <div style="text-align: center; padding: 20px; background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 10px;">
                            <h4>âš ï¸ Ollama æœåŠ¡æœªå°±ç»ª</h4>
                            <p><strong>å½“å‰çŠ¶æ€</strong>: {status_info['status']}</p>
                            <p><strong>å»ºè®®æ“ä½œ</strong>: {status_info.get('error', 'æ£€æŸ¥ Ollama çŠ¶æ€')}</p>
                        </div>
                        """)
            
            # OCR å¤„ç†äº‹ä»¶ç»‘å®š
            def process_with_notification(file, format):
                """å¤„ç†æ–‡ä»¶å¹¶æ·»åŠ èŠå¤©æç¤º"""
                status, content = processor.process_file(file, format)
                
                # å¦‚æœå¤„ç†æˆåŠŸä¸”èŠå¤©åŠŸèƒ½å¯ç”¨ï¼Œæ·»åŠ æç¤º
                if status and content and "âœ… å¤„ç†å®Œæˆ" in status and check_current_ollama_status():
                    status += "\n\nğŸ’¬ æ–‡æ¡£å·²å°±ç»ªï¼Œå¯ä»¥åœ¨ä¸‹æ–¹æ™ºèƒ½é—®ç­”åŒºåŸŸè¿›è¡Œæé—®ï¼"
                
                return status, content
            
            process_btn.click(
                fn=process_with_notification,
                inputs=[file_input, output_format],
                outputs=[status_output, content_output]
            )
            
            def clear_all():
                return None, "", "", None if example_selector else None
            
            clear_btn.click(
                fn=clear_all,
                outputs=[file_input, status_output, content_output] + ([example_selector] if example_selector else [])
            )
            
            # åˆ·æ–°ç¤ºä¾‹æ–‡ä»¶
            if example_selector:
                def refresh_examples():
                    new_files = example_manager.prepare_examples()
                    choices = [(os.path.basename(f), f) for f in new_files]
                    return gr.Dropdown(choices=choices, value=new_files[0] if new_files else None)
                
                refresh_btn.click(
                    fn=refresh_examples,
                    outputs=[example_selector]
                )
            
            # æ™ºèƒ½é—®ç­”åŠŸèƒ½äº‹ä»¶ç»‘å®š
            def check_current_ollama_status():
                status_info = ollama_client.get_ollama_status()
                return status_info["ollama_ready"] and status_info["model_ready"]
            
            if check_current_ollama_status():
                def respond_to_message(message, history):
                    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›å›å¤"""
                    if not message.strip():
                        return history, ""
                    
                    # æ£€æŸ¥ Ollama æ˜¯å¦å°±ç»ª
                    status_info = ollama_client.get_ollama_status()
                    if not (status_info["ollama_ready"] and status_info["model_ready"]):
                        bot_message = f"æŠ±æ­‰ï¼ŒOllama æœåŠ¡å°šæœªå°±ç»ªã€‚å½“å‰çŠ¶æ€: {status_info['status']}"
                        new_history = history.copy() if history else []
                        new_history.append({"role": "user", "content": message})
                        new_history.append({"role": "assistant", "content": bot_message})
                        return new_history, ""
                    
                    # è·å–æ–‡æ¡£å†…å®¹
                    global global_processor
                    if not global_processor:
                        bot_message = "æŠ±æ­‰ï¼Œå¤„ç†å™¨æœªåˆå§‹åŒ–ã€‚è¯·é‡å¯åº”ç”¨ã€‚"
                        new_history = history.copy() if history else []
                        new_history.append({"role": "user", "content": message})
                        new_history.append({"role": "assistant", "content": bot_message})
                        return new_history, ""
                    
                    content, filename = global_processor.get_document_content()
                    
                    # è°ƒè¯•ä¿¡æ¯
                    logger.info(f"ğŸ” èŠå¤©è°ƒè¯• - æ–‡æ¡£å†…å®¹é•¿åº¦: {len(content) if content else 0}")
                    logger.info(f"ğŸ” èŠå¤©è°ƒè¯• - æ–‡ä»¶å: {filename}")
                    logger.info(f"ğŸ” èŠå¤©è°ƒè¯• - ç”¨æˆ·é—®é¢˜: {message}")
                    
                    if not content:
                        bot_message = "æŠ±æ­‰ï¼Œå½“å‰æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£å†…å®¹ã€‚è¯·å…ˆå¤„ç†æ–‡æ¡£åå†è¿›è¡Œé—®ç­”ã€‚"
                        logger.warning("âš ï¸ èŠå¤©å¤±è´¥ï¼šæ²¡æœ‰æ–‡æ¡£å†…å®¹")
                        new_history = history.copy() if history else []
                        new_history.append({"role": "user", "content": message})
                        new_history.append({"role": "assistant", "content": bot_message})
                        return new_history, ""
                    
                    # ä½¿ç”¨ Ollama ç”Ÿæˆå›å¤
                    logger.info("ğŸš€ å¼€å§‹è°ƒç”¨ Ollama ç”Ÿæˆå›å¤...")
                    success, response = ollama_client.generate_response(message, content)
                    logger.info(f"ğŸ“ Ollama å›å¤ç»“æœ - æˆåŠŸ: {success}, å“åº”é•¿åº¦: {len(response) if response else 0}")
                    
                    if success:
                        # æ¸…ç† AI æ€è€ƒæ ‡è®°
                        cleaned_response = response
                        if "<think>" in cleaned_response and "</think>" in cleaned_response:
                            # ç§»é™¤ <think>...</think> æ ‡è®°åŠå…¶å†…å®¹
                            cleaned_response = re.sub(r'<think>.*?</think>\s*', '', cleaned_response, flags=re.DOTALL)
                        
                        bot_message = cleaned_response.strip()
                        logger.info(f"âœ… ç”ŸæˆæˆåŠŸï¼Œæ¸…ç†åå›å¤é•¿åº¦: {len(bot_message)}")
                        logger.info(f"âœ… æ¸…ç†åå›å¤é¢„è§ˆ: {bot_message[:100]}...")
                    else:
                        bot_message = f"æŠ±æ­‰ï¼Œç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯ï¼š{response}"
                        logger.error(f"âŒ ç”Ÿæˆå¤±è´¥: {response}")
                    
                    # æ„å»ºæ–°çš„å†å²è®°å½•
                    new_history = history.copy() if history else []
                    new_history.append({"role": "user", "content": message})
                    new_history.append({"role": "assistant", "content": bot_message})
                    
                    logger.info(f"ğŸ“‹ è¿”å›å†å²è®°å½•é•¿åº¦: {len(new_history)}")
                    logger.info(f"ğŸ“‹ æœ€åä¸€æ¡æ¶ˆæ¯: {new_history[-1] if new_history else 'None'}")
                    
                    return new_history, ""
                
                def clear_chat():
                    """æ¸…ç©ºèŠå¤©å†å²"""
                    logger.info("ğŸ—‘ï¸ æ¸…ç©ºèŠå¤©å†å²")
                    return []
                
                # ç»‘å®šèŠå¤©äº‹ä»¶
                try:
                    send_btn.click(
                        fn=respond_to_message,
                        inputs=[msg_input, chatbot],
                        outputs=[chatbot, msg_input]
                    )
                    
                    msg_input.submit(
                        fn=respond_to_message,
                        inputs=[msg_input, chatbot],
                        outputs=[chatbot, msg_input]
                    )
                    
                    clear_chat_btn.click(
                        fn=clear_chat,
                        outputs=[chatbot]
                    )
                    
                    # åˆå§‹åŒ–æ—¶ä¸éœ€è¦æ˜¾ç¤ºæ–‡æ¡£ä¿¡æ¯
                except NameError:
                    # å¦‚æœèŠå¤©ç»„ä»¶æœªåˆ›å»ºï¼Œåˆ™è·³è¿‡ç»‘å®š
                    pass
            
        else:
            gr.HTML("""
            <div style="text-align: center; padding: 40px; background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 10px;">
                <h3>âš ï¸ ç¯å¢ƒé…ç½®ä¸å®Œæ•´</h3>
                <p>è¯·è§£å†³ä¸Šè¿°ç¯å¢ƒé—®é¢˜åé‡å¯åº”ç”¨</p>
            </div>
            """)
        
        # å¸®åŠ©ä¿¡æ¯
        with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown(f"""
            ### ğŸ”§ ç¯å¢ƒä¿®å¤
            ```bash
            # å¦‚æœç¼ºå°‘ä¾èµ–åŒ…
            pip install gradio torch transformers pillow opencv-python requests
            
            # å¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´
            # è¯·ç¡®ä¿ {CONFIG['model']['path']} ç›®å½•åŒ…å«å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶
            ```
            
            ### ğŸ¯ ä½¿ç”¨æ­¥éª¤
            1. ç¡®ä¿ç¯å¢ƒçŠ¶æ€ä¸º ğŸŸ¢ å°±ç»ª
            2. **OCR å¤„ç†**: é€‰æ‹©ç¤ºä¾‹æ–‡ä»¶æˆ–ä¸Šä¼ æ–‡æ¡£è¿›è¡Œ OCR å¤„ç†
            3. **æ™ºèƒ½é—®ç­”**: åˆ‡æ¢åˆ°é—®ç­”é¡µé¢ï¼ŒåŸºäºå¤„ç†ç»“æœè¿›è¡Œé—®ç­”
            4. é€‰æ‹©è¾“å‡ºæ ¼å¼ (Markdown æ¨è)
            5. ç‚¹å‡»å¼€å§‹å¤„ç†å¹¶ç­‰å¾…å®Œæˆ
            
            ### ğŸ’¬ æ™ºèƒ½é—®ç­”åŠŸèƒ½
            - **æ–‡æ¡£é—®ç­”**: åŸºäº OCR å¤„ç†ç»“æœè¿›è¡Œæ™ºèƒ½é—®ç­”
            - **æœ¬åœ° AI**: ä½¿ç”¨ Ollama + Qwen3:32B æ¨¡å‹
            - **ä¸Šä¸‹æ–‡ç†è§£**: AI ä¼šåŸºäºæ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
            - **å¤šè½®å¯¹è¯**: æ”¯æŒè¿ç»­å¯¹è¯å’Œä¸Šä¸‹æ–‡å…³è”
            
            #### Ollama è®¾ç½®
            ```bash
            # æ‰‹åŠ¨å®‰è£… Ollamaï¼š
            curl -fsSL https://ollama.ai/install.sh | sh
            
            # æ‰‹åŠ¨ä¸‹è½½ Qwen3:32B æ¨¡å‹
            ollama pull qwen3:32b
            
            # æ‰‹åŠ¨å¯åŠ¨ Ollama æœåŠ¡
            ollama serve
            
            # åœ¨å¦ä¸€ä¸ªç»ˆç«¯ä¸­è¿è¡Œæ¨¡å‹
            ollama run qwen3:32b
            ```
            
            #### Ollama åŠŸèƒ½ç‰¹æ€§
            - **æ‰‹åŠ¨æ§åˆ¶**: éœ€è¦æ‰‹åŠ¨å¯åŠ¨ Ollama æœåŠ¡å™¨å’Œæ¨¡å‹
            - **çŠ¶æ€ç›‘æ§**: å®æ—¶æ˜¾ç¤º Ollama å’Œæ¨¡å‹çš„è¿è¡ŒçŠ¶æ€
            - **æ™ºèƒ½åˆ‡æ¢**: æ ¹æ®æœåŠ¡çŠ¶æ€è‡ªåŠ¨å¯ç”¨/ç¦ç”¨èŠå¤©åŠŸèƒ½
            - **ç‹¬ç«‹è¿è¡Œ**: Ollama ä¸åº”ç”¨ç‹¬ç«‹ï¼Œå¯åœ¨éœ€è¦æ—¶å•ç‹¬å¯åŠ¨
            
            ### ğŸ“‹ ç¤ºä¾‹æ–‡ä»¶åŠŸèƒ½
            - **è‡ªåŠ¨ä¸‹è½½**: é¦–æ¬¡å¯åŠ¨æ—¶è‡ªåŠ¨ä¸‹è½½å®˜æ–¹ç¤ºä¾‹ PDF
            - **å¿«é€Ÿä½“éªŒ**: æ— éœ€å‡†å¤‡æ–‡ä»¶å³å¯æµ‹è¯• OCR åŠŸèƒ½
            - **ç¤ºä¾‹è·¯å¾„**: `{CONFIG['examples']['examples_dir']}`
            - **åˆ·æ–°åŠŸèƒ½**: å¯é‡æ–°ä¸‹è½½æˆ–æ·»åŠ æ›´å¤šç¤ºä¾‹æ–‡ä»¶
            
            ### ğŸ“ è¾“å‡ºæ ¼å¼
            - **Markdown**: ç»“æ„åŒ–æ–‡æœ¬ï¼Œé€‚åˆé˜…è¯»å’Œç¼–è¾‘
            - **Dolma**: JSON Lines æ ¼å¼ï¼Œé€‚åˆæ•°æ®å¤„ç†
            
            ### ğŸ”’ æœ¬åœ°åŒ–ç‰¹æ€§
            - **å®Œå…¨ç¦»çº¿**: æ‰€æœ‰å¤„ç†åœ¨æœ¬åœ°å®Œæˆï¼Œæ— éœ€ç½‘ç»œè¿æ¥
            - **éšç§ä¿æŠ¤**: æ–‡æ¡£ä¸ä¼šä¸Šä¼ åˆ°ä»»ä½•å¤–éƒ¨æœåŠ¡
            - **AWS ç¦ç”¨**: å®Œå…¨ç»•è¿‡ AWS å’Œäº‘æœåŠ¡ä¾èµ–
            - **ä»£ç†æ— å…³**: ä¸å—ç½‘ç»œä»£ç†è®¾ç½®å½±å“
            
            ### ğŸš€ å‘½ä»¤è¡Œä½¿ç”¨ï¼ˆæœ¬åœ°åŒ–ç‰ˆæœ¬ï¼‰
            ```bash
            # å®Œå…¨æœ¬åœ°åŒ–å¤„ç†ï¼Œæ— å¤–éƒ¨ä¾èµ–
            cd /workspace/olmocr
            python app.py
            # ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—® http://localhost:7860
            
            # ä½¿ç”¨ä¸‹è½½çš„ç¤ºä¾‹æ–‡ä»¶è¿›è¡Œå‘½ä»¤è¡Œæµ‹è¯•
            python -m olmocr.pipeline ./localworkspace --markdown --pdfs {CONFIG['examples']['examples_dir']}/{CONFIG['examples']['sample_filename']}
            ```
            
            ### âš¡ æ€§èƒ½æç¤º
            - å»ºè®®ä½¿ç”¨ GPU ä»¥è·å¾—æœ€ä½³æ€§èƒ½
            - å¤§æ–‡ä»¶å¤„ç†éœ€è¦æ›´å¤šæ—¶é—´å’Œå†…å­˜
            - æ¸…æ™°ã€é«˜åˆ†è¾¨ç‡çš„æ–‡æ¡£æ•ˆæœæ›´å¥½
            - é¦–æ¬¡ä½¿ç”¨æ—¶æ¨¡å‹åŠ è½½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…
            
            ### ğŸ› ï¸ æŠ€æœ¯ä¼˜åŠ¿
            - **é›¶ä¾èµ–**: ä¸éœ€è¦ä»»ä½•å¤–éƒ¨æœåŠ¡æˆ–ç½‘ç»œè¿æ¥
            - **é«˜å®‰å…¨**: æ–‡æ¡£å¤„ç†å®Œå…¨åœ¨æœ¬åœ°ç¯å¢ƒä¸­è¿›è¡Œ
            - **æ˜“éƒ¨ç½²**: å•ä¸€æ–‡ä»¶åŒ…å«æ‰€æœ‰åŠŸèƒ½
            - **é«˜æ€§èƒ½**: ç›´æ¥ä½¿ç”¨æœ¬åœ° GPU è¿›è¡Œæ¨ç†
            - **ç¤ºä¾‹é›†æˆ**: å†…ç½®ç¤ºä¾‹æ–‡ä»¶ï¼Œå¼€ç®±å³ç”¨
            - **æ™ºèƒ½é—®ç­”**: é›†æˆ Ollama æä¾›æ–‡æ¡£é—®ç­”èƒ½åŠ›
            - **å¤šæ¨¡æ€**: æ”¯æŒ OCR + æ–‡æœ¬ç†è§£çš„å®Œæ•´æµç¨‹
            """)
    
    return demo

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ OlmOCR Web UI (å®Œå…¨æœ¬åœ°åŒ–ç‰ˆæœ¬)...")
    print(f"ğŸ“ è®¿é—®åœ°å€: http://{CONFIG['ui']['host']}:{CONFIG['ui']['port']}")
    print(f"ğŸ¤– æ¨¡å‹è·¯å¾„: {CONFIG['model']['path']}")
    print("ğŸ”’ ç‰¹æ€§: å®Œå…¨æœ¬åœ°åŒ– | é›¶ç½‘ç»œä¾èµ– | éšç§ä¿æŠ¤")
    print("")
    print("âš¡ å¯åŠ¨æµç¨‹:")
    print("1. ğŸ“‹ ç¯å¢ƒæ£€æŸ¥...")
    print("2. ğŸ”„ å¯åŠ¨ Gradio WebUI...")
    print("3. ğŸš€ åå°å¯åŠ¨ vLLM æœåŠ¡å™¨...")
    print("4. ğŸ“‚ ä¸‹è½½ç¤ºä¾‹æ–‡ä»¶...")
    print("5. âœ… å®Œæˆï¼å¯ä»¥å¼€å§‹ä½¿ç”¨")
    print("")
    print("ğŸ’¡ æç¤º: vLLM æœåŠ¡å™¨é¦–æ¬¡å¯åŠ¨éœ€è¦åŠ è½½æ¨¡å‹ï¼Œå¤§çº¦éœ€è¦ 2-5 åˆ†é’Ÿ")
    print("ğŸ’¡ æç¤º: å¦‚éœ€ä½¿ç”¨æ™ºèƒ½é—®ç­”åŠŸèƒ½ï¼Œè¯·æ‰‹åŠ¨å¯åŠ¨ Ollama: ollama serve")
    print("ğŸ’¡ æç¤º: ç„¶åè¿è¡Œæ¨¡å‹: ollama run qwen3:32b")
    print("ğŸ’¡ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ä¸Šè¿°åœ°å€å¹¶ç­‰å¾… vLLM æœåŠ¡çŠ¶æ€å˜ä¸º 'å°±ç»ª'")
    
    demo = create_ui()
    demo.launch(
        server_name=CONFIG["ui"]["host"],
        server_port=CONFIG["ui"]["port"],
        share=False,
        debug=True
    )
