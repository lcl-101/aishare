#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
SoulX-FlashTalk Gradio Web åº”ç”¨
åŸºäºå®˜æ–¹æ¨ç†è„šæœ¬æ”¹ç¼–ï¼Œå®ç° Web åŒ–æ¼”ç¤º
"""

import os
import sys
import time
import tempfile
import subprocess
from datetime import datetime
from collections import deque

import numpy as np
import torch
import librosa
import imageio
import gradio as gr
from loguru import logger

# å¯¼å…¥ FlashTalk æ¨ç†æ¨¡å—
from flash_talk.inference import (
    get_pipeline, 
    get_base_data, 
    get_audio_embedding, 
    run_pipeline, 
    infer_params
)

# ==================== å…¨å±€é…ç½® ====================
CKPT_DIR = "checkpoints/SoulX-FlashTalk-14B"
WAV2VEC_DIR = "checkpoints/chinese-wav2vec2-base"
CPU_OFFLOAD = False  # H20 æœ‰ 141GB æ˜¾å­˜ï¼Œæ— éœ€ CPU offload

# æ¨ç†å‚æ•°
SAMPLE_RATE = infer_params['sample_rate']
TGT_FPS = infer_params['tgt_fps']
CACHED_AUDIO_DURATION = infer_params['cached_audio_duration']
FRAME_NUM = infer_params['frame_num']
MOTION_FRAMES_NUM = infer_params['motion_frames_num']
SLICE_LEN = FRAME_NUM - MOTION_FRAMES_NUM

# å…¨å±€ Pipelineï¼ˆå¯åŠ¨æ—¶åŠ è½½ï¼‰
pipeline = None

# ==================== ç¤ºä¾‹æ•°æ® ====================
# æ¯ä¸ªç¤ºä¾‹åŒ…å«ï¼š(å›¾ç‰‡è·¯å¾„, éŸ³é¢‘è·¯å¾„, æç¤ºè¯)
EXAMPLES = [
    {
        "name": "ç¤ºä¾‹ 1",
        "image": "examples/man.png",
        "audio": "examples/cantonese_16k.wav",
        "prompt": "A person is talking. Only the foreground characters are moving, the background remains static."
    },
    {
        "name": "ç¤ºä¾‹ 2",
        "image": "examples/man.png",
        "audio": "examples/cantonese_16k.wav",
        "prompt": "A young woman is speaking passionately. Only the foreground characters are moving, the background remains static."
    },
    {
        "name": "ç¤ºä¾‹ 3",
        "image": "examples/man.png",
        "audio": "examples/cantonese_16k.wav",
        "prompt": "A man in a suit is giving a speech. Only the foreground characters are moving, the background remains static."
    },
    {
        "name": "ç¤ºä¾‹ 4",
        "image": "examples/man.png",
        "audio": "examples/cantonese_16k.wav",
        "prompt": "An elderly person is telling a story with expressive gestures. Only the foreground characters are moving, the background remains static."
    },
]

# ==================== æ¨¡å‹åŠ è½½ ====================
def load_model():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global pipeline
    logger.info("æ­£åœ¨åŠ è½½ SoulX-FlashTalk æ¨¡å‹...")
    logger.info(f"æ¨¡å‹è·¯å¾„: {CKPT_DIR}")
    logger.info(f"Wav2Vec è·¯å¾„: {WAV2VEC_DIR}")
    
    start_time = time.time()
    
    # å• GPU æ¨¡å¼
    world_size = 1
    pipeline = get_pipeline(
        world_size=world_size, 
        ckpt_dir=CKPT_DIR, 
        wav2vec_dir=WAV2VEC_DIR, 
        cpu_offload=CPU_OFFLOAD
    )
    
    elapsed_time = time.time() - start_time
    logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼è€—æ—¶: {elapsed_time:.2f} ç§’")
    
    return pipeline

# ==================== è§†é¢‘ä¿å­˜ ====================
def save_video(frames_list, video_path, audio_path, fps):
    """ä¿å­˜è§†é¢‘å¹¶åˆå¹¶éŸ³é¢‘"""
    temp_video_path = video_path.replace('.mp4', '_temp.mp4')
    
    with imageio.get_writer(
        temp_video_path, 
        format='mp4', 
        mode='I',
        fps=fps, 
        codec='h264', 
        ffmpeg_params=['-bf', '0']
    ) as writer:
        for frames in frames_list:
            frames_np = frames.numpy().astype(np.uint8)
            for i in range(frames_np.shape[0]):
                frame = frames_np[i, :, :, :]
                writer.append_data(frame)
    
    # åˆå¹¶è§†é¢‘å’ŒéŸ³é¢‘
    cmd = [
        'ffmpeg', '-y',
        '-i', temp_video_path, 
        '-i', audio_path, 
        '-c:v', 'copy',
        '-c:a', 'aac',
        '-shortest', 
        video_path
    ]
    subprocess.run(cmd, capture_output=True)
    
    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    if os.path.exists(temp_video_path):
        os.remove(temp_video_path)
    
    return video_path

# ==================== ç”Ÿæˆè§†é¢‘ ====================
def generate_video(
    input_image,
    audio_file,
    prompt,
    seed,
    audio_encode_mode,
    progress=gr.Progress()
):
    """ç”Ÿæˆè¯´è¯è§†é¢‘"""
    global pipeline
    
    if pipeline is None:
        return None, "âŒ é”™è¯¯ï¼šæ¨¡å‹æœªåŠ è½½ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•"
    
    if input_image is None:
        return None, "âŒ é”™è¯¯ï¼šè¯·ä¸Šä¼ ä¸€å¼ äººç‰©å›¾ç‰‡"
    
    if audio_file is None:
        return None, "âŒ é”™è¯¯ï¼šè¯·ä¸Šä¼ ä¸€æ®µéŸ³é¢‘æ–‡ä»¶"
    
    try:
        progress(0.1, desc="æ­£åœ¨å‡†å¤‡æ•°æ®...")
        
        # ä¿å­˜ä¸Šä¼ çš„å›¾ç‰‡
        temp_image_path = tempfile.mktemp(suffix=".png")
        input_image.save(temp_image_path)
        
        # å‡†å¤‡åŸºç¡€æ•°æ®
        base_seed = seed if seed >= 0 else 9999
        get_base_data(
            pipeline, 
            input_prompt=prompt, 
            cond_image=temp_image_path, 
            base_seed=base_seed
        )
        
        progress(0.2, desc="æ­£åœ¨åŠ è½½éŸ³é¢‘...")
        
        # åŠ è½½éŸ³é¢‘
        human_speech_array_all, _ = librosa.load(
            audio_file, 
            sr=SAMPLE_RATE, 
            mono=True
        )
        
        # è®¡ç®—æ¯ä¸ª slice å¯¹åº”çš„éŸ³é¢‘é‡‡æ ·ç‚¹æ•°
        human_speech_array_slice_len = SLICE_LEN * SAMPLE_RATE // TGT_FPS
        
        # åœ¨éŸ³é¢‘æœ«å°¾æ·»åŠ é™éŸ³ï¼Œç¡®ä¿æœ€åä¸€æ®µéŸ³é¢‘ä¸è¢«æˆªæ–­
        remainder = len(human_speech_array_all) % human_speech_array_slice_len
        if remainder > 0:
            # éœ€è¦è¡¥é½çš„é™éŸ³é•¿åº¦
            padding_len = human_speech_array_slice_len - remainder
            # æ·»åŠ é™éŸ³ï¼ˆé›¶å€¼ï¼‰
            human_speech_array_all = np.concatenate([
                human_speech_array_all, 
                np.zeros(padding_len, dtype=human_speech_array_all.dtype)
            ])
            logger.info(f"éŸ³é¢‘æœ«å°¾æ·»åŠ  {padding_len / SAMPLE_RATE:.2f} ç§’é™éŸ³ä»¥è¡¥é½")
        
        generated_list = []
        
        progress(0.3, desc="æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
        
        if audio_encode_mode == "once":
            # ä¸€æ¬¡æ€§ç¼–ç éŸ³é¢‘
            audio_embedding_all = get_audio_embedding(pipeline, human_speech_array_all)
            num_chunks = (audio_embedding_all.shape[1] - FRAME_NUM) // SLICE_LEN
            audio_embedding_chunks_list = [
                audio_embedding_all[:, i * SLICE_LEN: i * SLICE_LEN + FRAME_NUM].contiguous() 
                for i in range(num_chunks)
            ]
            
            for chunk_idx, audio_embedding_chunk in enumerate(audio_embedding_chunks_list):
                progress_val = 0.3 + 0.6 * (chunk_idx + 1) / len(audio_embedding_chunks_list)
                progress(progress_val, desc=f"æ­£åœ¨ç”Ÿæˆç¬¬ {chunk_idx + 1}/{len(audio_embedding_chunks_list)} æ®µè§†é¢‘...")
                
                start_time = time.time()
                video = run_pipeline(pipeline, audio_embedding_chunk)
                elapsed = time.time() - start_time
                
                logger.info(f"ç”Ÿæˆç¬¬ {chunk_idx + 1} æ®µè§†é¢‘å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}s")
                generated_list.append(video.cpu())
                
        else:  # stream æ¨¡å¼
            cached_audio_length_sum = SAMPLE_RATE * CACHED_AUDIO_DURATION
            audio_end_idx = CACHED_AUDIO_DURATION * TGT_FPS
            audio_start_idx = audio_end_idx - FRAME_NUM
            
            audio_dq = deque([0.0] * cached_audio_length_sum, maxlen=cached_audio_length_sum)
            
            # ä½¿ç”¨å‰é¢å·²è®¡ç®—çš„ slice é•¿åº¦
            num_slices = len(human_speech_array_all) // human_speech_array_slice_len
            human_speech_array_slices = human_speech_array_all[
                :num_slices * human_speech_array_slice_len
            ].reshape(-1, human_speech_array_slice_len)
            
            for chunk_idx, human_speech_array in enumerate(human_speech_array_slices):
                progress_val = 0.3 + 0.6 * (chunk_idx + 1) / len(human_speech_array_slices)
                progress(progress_val, desc=f"æ­£åœ¨ç”Ÿæˆç¬¬ {chunk_idx + 1}/{len(human_speech_array_slices)} æ®µè§†é¢‘...")
                
                # æµå¼ç¼–ç éŸ³é¢‘
                audio_dq.extend(human_speech_array.tolist())
                audio_array = np.array(audio_dq)
                audio_embedding = get_audio_embedding(
                    pipeline, audio_array, audio_start_idx, audio_end_idx
                )
                
                start_time = time.time()
                video = run_pipeline(pipeline, audio_embedding)
                elapsed = time.time() - start_time
                
                logger.info(f"ç”Ÿæˆç¬¬ {chunk_idx + 1} æ®µè§†é¢‘å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}s")
                generated_list.append(video.cpu())
        
        progress(0.95, desc="æ­£åœ¨ä¿å­˜è§†é¢‘...")
        
        # ä¿å­˜è§†é¢‘
        output_dir = "sample_results"
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(output_dir, f"result_{timestamp}.mp4")
        
        save_video(generated_list, output_path, audio_file, fps=TGT_FPS)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_image_path):
            os.remove(temp_image_path)
        
        progress(1.0, desc="å®Œæˆ!")
        
        return output_path, f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼ä¿å­˜è‡³: {output_path}"
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆè§†é¢‘æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"

# ==================== Gradio ç•Œé¢ ====================
def create_ui():
    """åˆ›å»º Gradio Web ç•Œé¢"""
    
    # è‡ªå®šä¹‰ CSS
    custom_css = """
    .youtube-banner {
        background: linear-gradient(135deg, #ff0000 0%, #cc0000 100%);
        padding: 15px 20px;
        border-radius: 10px;
        margin-bottom: 20px;
        text-align: center;
    }
    .youtube-banner a {
        color: white !important;
        font-size: 18px;
        font-weight: bold;
        text-decoration: none;
    }
    .youtube-banner a:hover {
        text-decoration: underline;
    }
    .title-text {
        text-align: center;
        margin-bottom: 10px;
    }
    """
    
    with gr.Blocks(
        title="SoulX-FlashTalk - å®æ—¶éŸ³é¢‘é©±åŠ¨æ•°å­—äºº",
        css=custom_css,
        theme=gr.themes.Soft()
    ) as demo:
        
        # YouTube é¢‘é“ä¿¡æ¯
        gr.HTML("""
        <div class="youtube-banner">
            <a href="https://www.youtube.com/@rongyi-ai" target="_blank">
                ğŸ“º AI æŠ€æœ¯åˆ†äº«é¢‘é“ | YouTube: https://www.youtube.com/@rongyi-ai
            </a>
        </div>
        """)
        
        # æ ‡é¢˜
        gr.Markdown("""
        # ğŸ¬ SoulX-FlashTalk: å®æ—¶éŸ³é¢‘é©±åŠ¨æ•°å­—äººè§†é¢‘ç”Ÿæˆ
        
        ä¸Šä¼ ä¸€å¼ äººç‰©å›¾ç‰‡å’Œä¸€æ®µéŸ³é¢‘ï¼Œå³å¯ç”Ÿæˆé€¼çœŸçš„è¯´è¯è§†é¢‘ã€‚
        
        ---
        """)
        
        with gr.Row():
            # å·¦ä¾§ï¼šè¾“å…¥åŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¥ è¾“å…¥è®¾ç½®")
                
                input_image = gr.Image(
                    label="äººç‰©å›¾ç‰‡",
                    type="pil",
                    height=300,
                )
                gr.Markdown("<small>ğŸ’¡ æç¤ºï¼šå»ºè®®ä¸Šä¼ ç«–ç‰ˆäººç‰©ç…§ç‰‡ï¼ˆæ¯”ä¾‹çº¦ 9:16ï¼‰ï¼Œæ¨¡å‹è¾“å‡ºå›ºå®šä¸º 448Ã—768</small>")
                
                audio_file = gr.Audio(
                    label="éŸ³é¢‘æ–‡ä»¶",
                    type="filepath",
                )
                
                prompt = gr.Textbox(
                    label="æç¤ºè¯",
                    placeholder="è¯·è¾“å…¥æè¿°è§†é¢‘çš„æç¤ºè¯...",
                    value=EXAMPLES[0]["prompt"],
                    lines=3,
                )
                
                gr.Markdown("#### ç¤ºä¾‹ï¼ˆç‚¹å‡»åŠ è½½å›¾ç‰‡ã€éŸ³é¢‘å’Œæç¤ºè¯ï¼‰")
                example_btns = []
                for i, example in enumerate(EXAMPLES):
                    btn = gr.Button(
                        example["name"],
                        size="sm",
                        variant="secondary"
                    )
                    example_btns.append((btn, example))
                
                with gr.Accordion("âš™ï¸ é«˜çº§è®¾ç½®", open=False):
                    seed = gr.Slider(
                        label="éšæœºç§å­",
                        minimum=-1,
                        maximum=99999,
                        value=9999,
                        step=1,
                        info="-1 è¡¨ç¤ºä½¿ç”¨é»˜è®¤ç§å­ (9999)"
                    )
                    
                    audio_encode_mode = gr.Radio(
                        label="éŸ³é¢‘ç¼–ç æ¨¡å¼",
                        choices=["stream", "once"],
                        value="stream",
                        info="stream: æµå¼ç¼–ç ï¼ˆæ¨èï¼‰ï¼›once: ä¸€æ¬¡æ€§ç¼–ç "
                    )
                
                generate_btn = gr.Button(
                    "ğŸš€ å¼€å§‹ç”Ÿæˆ",
                    variant="primary",
                    size="lg"
                )
            
            # å³ä¾§ï¼šè¾“å‡ºåŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¤ ç”Ÿæˆç»“æœ")
                
                output_video = gr.Video(
                    label="ç”Ÿæˆçš„è§†é¢‘",
                    height=400,
                )
                
                status_text = gr.Textbox(
                    label="çŠ¶æ€ä¿¡æ¯",
                    interactive=False,
                    lines=2,
                )
        
        # ä½¿ç”¨è¯´æ˜
        gr.Markdown("""
        ---
        ### ğŸ“– ä½¿ç”¨è¯´æ˜
        
        1. **ä¸Šä¼ å›¾ç‰‡**ï¼šé€‰æ‹©ä¸€å¼ æ­£é¢äººç‰©ç…§ç‰‡ï¼ˆå»ºè®®æ¸…æ™°ã€å…‰çº¿å‡åŒ€ï¼‰
        2. **ä¸Šä¼ éŸ³é¢‘**ï¼šé€‰æ‹©ä¸€æ®µéŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒ WAVã€MP3 ç­‰æ ¼å¼ï¼Œæ¨è 16kHz é‡‡æ ·ç‡ï¼‰
        3. **è¾“å…¥æç¤ºè¯**ï¼šæè¿°è§†é¢‘ä¸­äººç‰©çš„çŠ¶æ€ï¼ˆå¯ä½¿ç”¨ç¤ºä¾‹æç¤ºè¯ï¼‰
        4. **ç‚¹å‡»ç”Ÿæˆ**ï¼šç­‰å¾…è§†é¢‘ç”Ÿæˆå®Œæˆ
        
        ### âš ï¸ æ³¨æ„äº‹é¡¹
        
        - é¦–æ¬¡ç”Ÿæˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´è¿›è¡Œæ¨¡å‹é¢„çƒ­
        - ç”Ÿæˆæ—¶é—´ä¸éŸ³é¢‘é•¿åº¦ç›¸å…³
        - å»ºè®®ä½¿ç”¨é«˜è´¨é‡çš„äººç‰©æ­£é¢ç…§ç‰‡
        
        ---
        <div style="text-align: center; color: #666;">
            åŸºäº <a href="https://github.com/Soul-AILab/SoulX-FlashTalk" target="_blank">SoulX-FlashTalk</a> å¼€æºé¡¹ç›® | 
            æ¨¡å‹ç”± Soul AI Lab æä¾›
        </div>
        """)
        
        # åŠ è½½ç¤ºä¾‹çš„å‡½æ•°
        def load_example(example):
            from PIL import Image
            img = Image.open(example["image"])
            return img, example["audio"], example["prompt"]
        
        # ç»‘å®šç¤ºä¾‹æŒ‰é’®äº‹ä»¶
        for btn, example in example_btns:
            btn.click(
                fn=lambda e=example: load_example(e),
                inputs=[],
                outputs=[input_image, audio_file, prompt]
            )
        
        # ç»‘å®šç”ŸæˆæŒ‰é’®äº‹ä»¶
        generate_btn.click(
            fn=generate_video,
            inputs=[
                input_image,
                audio_file,
                prompt,
                seed,
                audio_encode_mode,
            ],
            outputs=[output_video, status_text]
        )
        
        # æ·»åŠ ç¤ºä¾‹
        gr.Examples(
            examples=[
                [
                    "examples/man.png",
                    "examples/cantonese_16k.wav",
                    "A person is talking. Only the foreground characters are moving, the background remains static.",
                    9999,
                    "stream"
                ],
            ],
            inputs=[
                input_image,
                audio_file,
                prompt,
                seed,
                audio_encode_mode,
            ],
            outputs=[output_video, status_text],
            fn=generate_video,
            cache_examples=False,
            label="ğŸ“Œ å¿«é€Ÿç¤ºä¾‹"
        )
    
    return demo


# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == "__main__":
    # å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
    logger.info("=" * 50)
    logger.info("SoulX-FlashTalk Web åº”ç”¨å¯åŠ¨ä¸­...")
    logger.info("=" * 50)
    
    # åŠ è½½æ¨¡å‹
    load_model()
    
    # åˆ›å»ºå¹¶å¯åŠ¨ Gradio åº”ç”¨
    demo = create_ui()
    
    demo.queue(max_size=5)
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
    )
