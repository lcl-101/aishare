# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
"""
Gradio Web UI for Wan-Move: Motion-Controllable Image-to-Video Generation
"""
import os
import sys
import logging
import tempfile
import json
from datetime import datetime

import numpy as np
import torch
import gradio as gr
from PIL import Image, ImageDraw
from scipy.interpolate import interp1d

import wan
from wan.configs import MAX_AREA_CONFIGS, SIZE_CONFIGS, WAN_CONFIGS
from wan.utils.utils import cache_video
from wan.modules.trajectory import draw_tracks_on_video
import torchvision.transforms.functional as TF

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    handlers=[logging.StreamHandler(stream=sys.stdout)]
)

# Global model instance
wan_move_model = None
CHECKPOINT_DIR = "./checkpoints/Wan-Move-14B-480P"


def load_model():
    """Load the Wan-Move model."""
    global wan_move_model
    
    if wan_move_model is not None:
        return wan_move_model
    
    logging.info("Loading Wan-Move model...")
    cfg = WAN_CONFIGS['wan-move-i2v']
    
    wan_move_model = wan.WanMove(
        config=cfg,
        checkpoint_dir=CHECKPOINT_DIR,
        device_id=0,
        rank=0,
        t5_fsdp=False,
        dit_fsdp=False,
        use_usp=False,
        t5_cpu=False,
    )
    
    logging.info("Wan-Move model loaded successfully!")
    return wan_move_model


def interpolate_trajectory(points, num_frames):
    """
    å°†ç”¨æˆ·ç»˜åˆ¶çš„è½¨è¿¹ç‚¹æ’å€¼åˆ°æŒ‡å®šå¸§æ•°ã€‚
    
    Args:
        points: ç”¨æˆ·ç»˜åˆ¶çš„ç‚¹åˆ—è¡¨ [(x1, y1), (x2, y2), ...]
        num_frames: ç›®æ ‡å¸§æ•°
    
    Returns:
        interpolated: æ’å€¼åçš„è½¨è¿¹ [num_frames, 2]
    """
    if len(points) < 2:
        # å¦‚æœåªæœ‰ä¸€ä¸ªç‚¹ï¼Œå¤åˆ¶åˆ°æ‰€æœ‰å¸§
        return np.array([points[0]] * num_frames)
    
    points = np.array(points)
    
    # è®¡ç®—ç´¯ç§¯è·ç¦»ä½œä¸ºå‚æ•°
    distances = np.zeros(len(points))
    for i in range(1, len(points)):
        distances[i] = distances[i-1] + np.linalg.norm(points[i] - points[i-1])
    
    # å½’ä¸€åŒ–è·ç¦»åˆ° [0, 1]
    if distances[-1] > 0:
        distances = distances / distances[-1]
    else:
        distances = np.linspace(0, 1, len(points))
    
    # åˆ›å»ºæ’å€¼å‡½æ•°
    interp_x = interp1d(distances, points[:, 0], kind='linear', fill_value='extrapolate')
    interp_y = interp1d(distances, points[:, 1], kind='linear', fill_value='extrapolate')
    
    # åœ¨å‡åŒ€åˆ†å¸ƒçš„å‚æ•°ä¸Šæ’å€¼
    t = np.linspace(0, 1, num_frames)
    interpolated = np.stack([interp_x(t), interp_y(t)], axis=1)
    
    return interpolated


def create_trajectory_from_drawing(drawing_data, image_size, num_frames=81):
    """
    ä»ç»˜å›¾æ•°æ®åˆ›å»ºè½¨è¿¹å’Œå¯è§æ€§æ•°ç»„ã€‚
    
    Args:
        drawing_data: Gradio ImageEditor è¿”å›çš„æ•°æ®
        image_size: åŸå§‹å›¾ç‰‡å°ºå¯¸ (width, height)
        num_frames: è§†é¢‘å¸§æ•°
    
    Returns:
        track: è½¨è¿¹æ•°ç»„ [1, num_frames, num_points, 2]
        visibility: å¯è§æ€§æ•°ç»„ [1, num_frames, num_points]
    """
    if drawing_data is None:
        return None, None
    
    # ä» composite å›¾å±‚æå–è½¨è¿¹
    # Gradio ImageEditor è¿”å›çš„æ•°æ®ç»“æ„
    if isinstance(drawing_data, dict):
        # æ–°ç‰ˆ Gradio æ ¼å¼
        if 'composite' in drawing_data:
            composite = drawing_data['composite']
        else:
            composite = drawing_data.get('image', None)
        
        layers = drawing_data.get('layers', [])
    else:
        return None, None
    
    all_trajectories = []
    
    # ä» layers ä¸­æå–ç»˜åˆ¶çš„è·¯å¾„
    for layer in layers:
        if isinstance(layer, np.ndarray):
            # åˆ†æå›¾å±‚æ‰¾åˆ°ç»˜åˆ¶çš„è½¨è¿¹
            points = extract_points_from_layer(layer)
            if points and len(points) >= 2:
                all_trajectories.append(points)
    
    if not all_trajectories:
        return None, None
    
    # ä¸ºæ¯æ¡è½¨è¿¹åˆ›å»ºæ’å€¼
    num_points = len(all_trajectories)
    track = np.zeros((1, num_frames, num_points, 2), dtype=np.float32)
    visibility = np.ones((1, num_frames, num_points), dtype=bool)
    
    for i, points in enumerate(all_trajectories):
        interpolated = interpolate_trajectory(points, num_frames)
        track[0, :, i, :] = interpolated
    
    return track, visibility


def extract_points_from_layer(layer):
    """
    ä»ç»˜å›¾å›¾å±‚ä¸­æå–è½¨è¿¹ç‚¹ã€‚
    é€šè¿‡åˆ†æéé€æ˜åƒç´ æ¥æ‰¾åˆ°ç»˜åˆ¶çš„è·¯å¾„ã€‚
    """
    if layer is None or len(layer.shape) < 3:
        return []
    
    # è·å– alpha é€šé“æˆ–éé›¶åƒç´ 
    if layer.shape[2] == 4:
        alpha = layer[:, :, 3]
    else:
        # RGB å›¾å±‚ï¼Œæ‰¾éé»‘è‰²åƒç´ 
        alpha = np.any(layer > 10, axis=2).astype(np.uint8) * 255
    
    # æ‰¾åˆ°æ‰€æœ‰éé€æ˜åƒç´ çš„åæ ‡
    y_coords, x_coords = np.where(alpha > 128)
    
    if len(x_coords) == 0:
        return []
    
    # ä½¿ç”¨è¿é€šæ€§åˆ†ææ¥æ’åºç‚¹ï¼Œå½¢æˆè·¯å¾„
    points = list(zip(x_coords, y_coords))
    
    if len(points) < 2:
        return points
    
    # ç®€å•çš„æ’åºï¼šæŒ‰ç…§ä»å·¦åˆ°å³æˆ–ä»ä¸Šåˆ°ä¸‹çš„é¡ºåº
    # æ›´å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨æœ€è¿‘é‚»ç®—æ³•
    sorted_points = sort_points_by_path(points)
    
    # ä¸‹é‡‡æ ·ä»¥å‡å°‘ç‚¹æ•°
    if len(sorted_points) > 100:
        indices = np.linspace(0, len(sorted_points)-1, 100, dtype=int)
        sorted_points = [sorted_points[i] for i in indices]
    
    return sorted_points


def sort_points_by_path(points):
    """
    ä½¿ç”¨æœ€è¿‘é‚»ç®—æ³•å°†æ•£ä¹±çš„ç‚¹æ’åºæˆè·¯å¾„ã€‚
    """
    if len(points) <= 2:
        return points
    
    points = list(points)
    sorted_points = [points.pop(0)]
    
    while points:
        last = sorted_points[-1]
        # æ‰¾æœ€è¿‘çš„ç‚¹
        min_dist = float('inf')
        min_idx = 0
        for i, p in enumerate(points):
            dist = (p[0] - last[0])**2 + (p[1] - last[1])**2
            if dist < min_dist:
                min_dist = dist
                min_idx = i
        sorted_points.append(points.pop(min_idx))
    
    return sorted_points


def process_drawing_input(image_with_drawing, num_frames=81):
    """
    å¤„ç†å¸¦ç»˜å›¾çš„å›¾ç‰‡è¾“å…¥ï¼Œè¿”å›è½¨è¿¹æ•°æ®ã€‚
    """
    if image_with_drawing is None:
        return None, None, None
    
    if isinstance(image_with_drawing, dict):
        # ImageEditor æ ¼å¼
        background = image_with_drawing.get('background', None)
        layers = image_with_drawing.get('layers', [])
        composite = image_with_drawing.get('composite', None)
        
        if background is not None:
            if isinstance(background, np.ndarray):
                original_image = Image.fromarray(background)
            else:
                original_image = background
        elif composite is not None:
            if isinstance(composite, np.ndarray):
                original_image = Image.fromarray(composite)
            else:
                original_image = composite
        else:
            return None, None, None
        
        image_size = original_image.size if hasattr(original_image, 'size') else (original_image.shape[1], original_image.shape[0])
        
        # æå–è½¨è¿¹
        track, visibility = create_trajectory_from_drawing(
            image_with_drawing, image_size, num_frames
        )
        
        return original_image, track, visibility
    
    return None, None, None


def generate_video_from_drawing(
    image_with_drawing,
    prompt,
    size,
    frame_num,
    sample_steps,
    sample_shift,
    guide_scale,
    seed,
    offload_model,
    vis_track,
    progress=gr.Progress(track_tqdm=True)
):
    """Generate video from image with drawn trajectory."""
    
    if image_with_drawing is None:
        gr.Warning("è¯·ä¸Šä¼ å›¾ç‰‡å¹¶ç»˜åˆ¶è½¨è¿¹!")
        return None, None
    
    if not prompt.strip():
        gr.Warning("è¯·è¾“å…¥æç¤ºè¯!")
        return None, None
    
    try:
        # å¤„ç†ç»˜å›¾è¾“å…¥
        img, track, visibility = process_drawing_input(image_with_drawing, frame_num)
        
        if img is None:
            gr.Warning("æ— æ³•è¯»å–å›¾ç‰‡!")
            return None, None
        
        if track is None or visibility is None:
            gr.Warning("è¯·åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶è½¨è¿¹! ä½¿ç”¨ç”»ç¬”å·¥å…·ç”»å‡ºç‰©ä½“è¿åŠ¨çš„è·¯å¾„ã€‚")
            return None, None
        
        logging.info(f"Track shape from drawing: {track.shape}")
        logging.info(f"Visibility shape: {visibility.shape}")
        
        # Load model
        model = load_model()
        cfg = WAN_CONFIGS['wan-move-i2v']
        
        # Ensure img is PIL Image with RGB (3 channels, no alpha)
        if isinstance(img, np.ndarray):
            # Handle RGBA -> RGB conversion
            if img.shape[-1] == 4:
                img = img[:, :, :3]
            img = Image.fromarray(img).convert("RGB")
        elif isinstance(img, Image.Image):
            img = img.convert("RGB")
        else:
            img = Image.fromarray(np.array(img)).convert("RGB")
        
        # ç”Ÿæˆè½¨è¿¹é¢„è§ˆ
        preview_img = visualize_trajectory_on_image(img, track, visibility)
        
        video_path = _generate_video_core(
            model, cfg, img, track, visibility,
            prompt, size, frame_num, sample_steps, sample_shift,
            guide_scale, seed, offload_model, vis_track
        )
        
        return video_path, preview_img
        
    except Exception as e:
        logging.exception(f"Error generating video: {e}")
        gr.Error(f"ç”Ÿæˆè§†é¢‘æ—¶å‡ºé”™: {str(e)}")
        return None, None


def visualize_trajectory_on_image(img, track, visibility):
    """åœ¨å›¾ç‰‡ä¸Šå¯è§†åŒ–è½¨è¿¹ã€‚"""
    img_copy = img.copy()
    draw = ImageDraw.Draw(img_copy)
    
    # track shape: [1, frames, num_points, 2]
    num_points = track.shape[2]
    num_frames = track.shape[1]
    
    colors = [
        (255, 0, 0), (0, 255, 0), (0, 0, 255),
        (255, 255, 0), (255, 0, 255), (0, 255, 255),
        (255, 128, 0), (128, 0, 255)
    ]
    
    for p in range(num_points):
        color = colors[p % len(colors)]
        points = []
        for f in range(num_frames):
            if visibility[0, f, p]:
                x, y = track[0, f, p, 0], track[0, f, p, 1]
                points.append((x, y))
        
        # ç»˜åˆ¶è½¨è¿¹çº¿
        if len(points) >= 2:
            draw.line(points, fill=color, width=3)
        
        # ç»˜åˆ¶èµ·ç‚¹å’Œç»ˆç‚¹
        if points:
            # èµ·ç‚¹ - åœ†å½¢
            start = points[0]
            draw.ellipse([start[0]-6, start[1]-6, start[0]+6, start[1]+6], 
                        fill=(0, 255, 0), outline=(255, 255, 255))
            # ç»ˆç‚¹ - æ–¹å½¢
            end = points[-1]
            draw.rectangle([end[0]-6, end[1]-6, end[0]+6, end[1]+6], 
                          fill=(255, 0, 0), outline=(255, 255, 255))
    
    return img_copy


def _generate_video_core(
    model, cfg, img, track, track_visibility,
    prompt, size, frame_num, sample_steps, sample_shift,
    guide_scale, seed, offload_model, vis_track
):
    """Core video generation logic."""
    
    # Get target size from config
    target_h, target_w = SIZE_CONFIGS[size]
    original_w, original_h = img.size
    
    # Resize image to target size
    if (original_w, original_h) != (target_w, target_h):
        logging.info(f"Resizing image from {original_w}x{original_h} to {target_w}x{target_h}")
        
        # Calculate scale factors
        scale_x = target_w / original_w
        scale_y = target_h / original_h
        
        # Resize image
        img = img.resize((target_w, target_h), Image.LANCZOS)
        
        # Scale track coordinates
        track = track.copy()
        track[:, :, :, 0] = track[:, :, :, 0] * scale_x  # x coordinates
        track[:, :, :, 1] = track[:, :, :, 1] * scale_y  # y coordinates
        
        logging.info(f"Scaled track coordinates by ({scale_x:.3f}, {scale_y:.3f})")
    
    logging.info(f"Input prompt: {prompt}")
    logging.info(f"Track shape: {track.shape}")
    logging.info(f"Track visibility shape: {track_visibility.shape}")
    logging.info(f"Image size: {img.size}")
    
    # Set seed
    if seed < 0:
        seed = torch.randint(0, 2**31, (1,)).item()
    
    logging.info(f"Using seed: {seed}")
    
    # Generate video
    video = model.generate(
        input_prompt=prompt,
        img=img,
        track=track,
        track_visibility=track_visibility,
        max_area=MAX_AREA_CONFIGS[size],
        frame_num=frame_num,
        shift=sample_shift,
        sample_solver='unipc',
        sampling_steps=sample_steps,
        guide_scale=guide_scale,
        seed=seed,
        offload_model=offload_model,
        eval_bench=True
    )
    
    # Save video
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = "outputs"
    os.makedirs(output_dir, exist_ok=True)
    
    if vis_track:
        # Create track visualization video
        device = torch.device("cuda:0")
        first_frame_repeat = torch.as_tensor(np.array(img)).permute(2, 0, 1).unsqueeze(0).unsqueeze(1).repeat(1, frame_num, 1, 1, 1)
        track_video = draw_tracks_on_video(
            first_frame_repeat, 
            torch.from_numpy(track) if isinstance(track, np.ndarray) else track, 
            torch.from_numpy(track_visibility) if isinstance(track_visibility, np.ndarray) else track_visibility
        )
        track_video = torch.stack([TF.to_tensor(frame) for frame in track_video], dim=0).permute(1, 0, 2, 3).mul(2).sub(1).to(device)
        
        save_file = os.path.join(output_dir, f"wan_move_{timestamp}_with_track.mp4")
        cache_video(
            tensor=torch.stack([track_video, video]),
            save_file=save_file,
            fps=cfg.sample_fps,
            nrow=1,
            normalize=True,
            value_range=(-1, 1)
        )
    else:
        save_file = os.path.join(output_dir, f"wan_move_{timestamp}.mp4")
        cache_video(
            tensor=video[None],
            save_file=save_file,
            fps=cfg.sample_fps,
            nrow=1,
            normalize=True,
            value_range=(-1, 1)
        )
    
    logging.info(f"Video saved to: {save_file}")
    return save_file


# Create Gradio interface
def create_ui():
    with gr.Blocks(
        title="Wan-Move: Motion-Controllable Image-to-Video Generation",
        theme=gr.themes.Soft()
    ) as demo:
        gr.Markdown("""
        # ğŸ¬ Wan-Move: Motion-Controllable Image-to-Video Generation
        
        é€šè¿‡è½¨è¿¹ç‚¹æ§åˆ¶è§†é¢‘ä¸­ç‰©ä½“çš„è¿åŠ¨æ–¹å‘å’Œè·¯å¾„ï¼Œå°†é™æ€å›¾åƒè½¬åŒ–ä¸ºåŠ¨æ€è§†é¢‘ã€‚
        
        ### ä½¿ç”¨è¯´æ˜
        1. ä¸Šä¼ ä¸€å¼ å›¾ç‰‡
        2. ä½¿ç”¨ç”»ç¬”åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶ç‰©ä½“è¿åŠ¨è½¨è¿¹ï¼ˆä»èµ·ç‚¹ç”»åˆ°ç»ˆç‚¹ï¼‰
        3. è¾“å…¥æè¿°è§†é¢‘å†…å®¹çš„æç¤ºè¯
        4. ç‚¹å‡»"ç”Ÿæˆè§†é¢‘"
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¥ è¾“å…¥")
                
                image_editor = gr.ImageEditor(
                    label="ä¸Šä¼ å›¾ç‰‡å¹¶ç»˜åˆ¶è½¨è¿¹",
                    type="numpy",
                    height=400,
                    brush=gr.Brush(colors=["#FF0000", "#00FF00", "#0000FF"], default_size=5),
                    eraser=gr.Eraser(default_size=10),
                    layers=True,
                )
                
                gr.Markdown("""
                ğŸ’¡ **ç»˜åˆ¶æç¤º**:
                - ç”¨ç”»ç¬”ç”»å‡ºç‰©ä½“è¿åŠ¨çš„è·¯å¾„
                - ä»èµ·ç‚¹ç”»åˆ°ç»ˆç‚¹ï¼Œçº¿æ¡æ–¹å‘å°±æ˜¯è¿åŠ¨æ–¹å‘
                - å¯ä»¥ç”»å¤šæ¡è½¨è¿¹æ§åˆ¶å¤šä¸ªç‰©ä½“
                - ä¸åŒé¢œè‰²ä»£è¡¨ä¸åŒè½¨è¿¹
                """)
                
                prompt_input = gr.Textbox(
                    label="æç¤ºè¯ (Prompt)",
                    placeholder="æè¿°è§†é¢‘å†…å®¹...",
                    lines=3
                )
                
                with gr.Accordion("âš™ï¸ é«˜çº§è®¾ç½®", open=False):
                    size_dropdown = gr.Dropdown(
                        label="è§†é¢‘å°ºå¯¸",
                        choices=["480*832", "832*480"],
                        value="480*832"
                    )
                    
                    frame_num_slider = gr.Slider(
                        label="å¸§æ•°",
                        minimum=17, maximum=81, step=4, value=81,
                        info="å¸§æ•°åº”ä¸º 4n+1 çš„å½¢å¼"
                    )
                    
                    sample_steps_slider = gr.Slider(
                        label="é‡‡æ ·æ­¥æ•°",
                        minimum=10, maximum=50, step=1, value=40
                    )
                    
                    sample_shift_slider = gr.Slider(
                        label="é‡‡æ ·åç§»",
                        minimum=1.0, maximum=10.0, step=0.5, value=3.0
                    )
                    
                    guide_scale_slider = gr.Slider(
                        label="å¼•å¯¼å¼ºåº¦",
                        minimum=1.0, maximum=15.0, step=0.5, value=5.0
                    )
                    
                    seed_input = gr.Number(label="éšæœºç§å­", value=-1, precision=0, info="-1 è¡¨ç¤ºéšæœºç§å­")
                    offload_checkbox = gr.Checkbox(label="æ¨¡å‹å¸è½½", value=True, info="å¯ç”¨ä»¥å‡å°‘ GPU æ˜¾å­˜ä½¿ç”¨")
                    vis_track_checkbox = gr.Checkbox(label="å¯è§†åŒ–è½¨è¿¹", value=False, info="åœ¨è¾“å‡ºä¸­æ˜¾ç¤ºè½¨è¿¹å¯è§†åŒ–")
                
                generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
            
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¤ è¾“å‡º")
                
                trajectory_preview = gr.Image(
                    label="è½¨è¿¹é¢„è§ˆ",
                    height=200
                )
                
                video_output = gr.Video(
                    label="ç”Ÿæˆçš„è§†é¢‘",
                    height=350
                )
        
        generate_btn.click(
            fn=generate_video_from_drawing,
            inputs=[
                image_editor, prompt_input, size_dropdown, frame_num_slider,
                sample_steps_slider, sample_shift_slider, guide_scale_slider,
                seed_input, offload_checkbox, vis_track_checkbox
            ],
            outputs=[video_output, trajectory_preview]
        )
        
        gr.Markdown("""
        ---
        **Wan-Move** - ç”±é˜¿é‡Œå·´å·´ Wan Team å¼€å‘
        """)
    
    return demo


if __name__ == "__main__":
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
