import gradio as gr
import asyncio
import sys
import os
import re

# ç¦ç”¨ vllm v1 å¼•æ“
os.environ['VLLM_USE_V1'] = '0'

# æ·»åŠ æœ¬åœ°æ¨¡å—è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'DeepSeek-OCR-master/DeepSeek-OCR-vllm'))

from vllm import AsyncLLMEngine, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.model_executor.models.registry import ModelRegistry
from deepseek_ocr import DeepseekOCRForCausalLM
from process.ngram_norepeat import NoRepeatNGramLogitsProcessor
from process.image_process import DeepseekOCRProcessor
# from config import MODEL_PATH, CROP_MODE  # ä¸ä½¿ç”¨ config.py çš„ MODEL_PATH
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import time

# æœ¬åœ°æ¨¡å‹é…ç½®
MODEL_PATH = 'checkpoints/DeepSeek-OCR'
CROP_MODE = True  # Gundam æ¨¡å¼ï¼šåŠ¨æ€åˆ†è¾¨ç‡

# æ³¨å†Œæ¨¡å‹
ModelRegistry.register_model("DeepseekOCRForCausalLM", DeepseekOCRForCausalLM)

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å®ä¾‹
engine = None

async def initialize_model():
    """åˆå§‹åŒ–æ¨¡å‹"""
    global engine
    if engine is None:
        print("æ­£åœ¨åŠ è½½ DeepSeek-OCR æ¨¡å‹...")
        engine_args = AsyncEngineArgs(
            model=MODEL_PATH,
            hf_overrides={"architectures": ["DeepseekOCRForCausalLM"]},
            block_size=256,
            max_model_len=8192,
            enforce_eager=False,
            trust_remote_code=True,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.75,
        )
        engine = AsyncLLMEngine.from_engine_args(engine_args)
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return engine

def re_match(text):
    """æå–æ–‡æœ¬ä¸­çš„åæ ‡æ ‡è®°"""
    pattern = r'(<\|ref\|>(.*?)<\|/ref\|><\|det\|>(.*?)<\|/det\|>)'
    matches = re.findall(pattern, text, re.DOTALL)
    
    matches_image = []
    matches_other = []
    for a_match in matches:
        if '<|ref|>image<|/ref|>' in a_match[0]:
            matches_image.append(a_match)
        else:
            matches_other.append(a_match)
    return matches, matches_image, matches_other

def extract_coordinates_and_label(ref_text, image_width, image_height):
    """ä»æ ‡è®°ä¸­æå–åæ ‡å’Œæ ‡ç­¾"""
    try:
        label_type = ref_text[1]
        cor_list = eval(ref_text[2])
    except Exception as e:
        print(f"æå–åæ ‡é”™è¯¯: {e}")
        return None
    return (label_type, cor_list)

def draw_bounding_boxes(image, refs):
    """åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†"""
    image_width, image_height = image.size
    img_draw = image.copy()
    draw = ImageDraw.Draw(img_draw)
    
    overlay = Image.new('RGBA', img_draw.size, (0, 0, 0, 0))
    draw2 = ImageDraw.Draw(overlay)
    
    font = ImageFont.load_default()
    
    for i, ref in enumerate(refs):
        try:
            result = extract_coordinates_and_label(ref, image_width, image_height)
            if result:
                label_type, points_list = result
                
                # éšæœºé¢œè‰²
                color = (np.random.randint(0, 200), np.random.randint(0, 200), np.random.randint(0, 255))
                color_a = color + (20,)
                
                for points in points_list:
                    x1, y1, x2, y2 = points
                    
                    # è½¬æ¢åæ ‡
                    x1 = int(x1 / 999 * image_width)
                    y1 = int(y1 / 999 * image_height)
                    x2 = int(x2 / 999 * image_width)
                    y2 = int(y2 / 999 * image_height)
                    
                    try:
                        # ç»˜åˆ¶è¾¹ç•Œæ¡†
                        if label_type == 'title':
                            draw.rectangle([x1, y1, x2, y2], outline=color, width=4)
                            draw2.rectangle([x1, y1, x2, y2], fill=color_a, outline=(0, 0, 0, 0), width=1)
                        else:
                            draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
                            draw2.rectangle([x1, y1, x2, y2], fill=color_a, outline=(0, 0, 0, 0), width=1)
                        
                        # ç»˜åˆ¶æ ‡ç­¾
                        text_x = x1
                        text_y = max(0, y1 - 15)
                        
                        text_bbox = draw.textbbox((0, 0), label_type, font=font)
                        text_width = text_bbox[2] - text_bbox[0]
                        text_height = text_bbox[3] - text_bbox[1]
                        draw.rectangle([text_x, text_y, text_x + text_width, text_y + text_height],
                                     fill=(255, 255, 255, 200))
                        
                        draw.text((text_x, text_y), label_type, font=font, fill=color)
                    except Exception as e:
                        print(f"ç»˜åˆ¶æ¡†é”™è¯¯: {e}")
                        pass
        except Exception as e:
            print(f"å¤„ç†æ ‡è®°é”™è¯¯: {e}")
            continue
    
    img_draw.paste(overlay, (0, 0), overlay)
    return img_draw

def process_image_with_refs(image, ref_texts):
    """å¤„ç†å›¾ç‰‡å¹¶æ·»åŠ æ ‡è®°"""
    result_image = draw_bounding_boxes(image, ref_texts)
    return result_image


async def ocr_image_async(image, temperature, max_tokens, ngram_size, window_size, prompt_text):
    """
    å¯¹ä¸Šä¼ çš„å›¾ç‰‡è¿›è¡Œ OCR è¯†åˆ«ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
    """
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        llm = await initialize_model()
        
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦ä¸Šä¼ 
        if image is None:
            return "âŒ è¯·å…ˆä¸Šä¼ å›¾ç‰‡", None
        
        # è½¬æ¢ä¸º RGB æ ¼å¼
        if isinstance(image, str):
            image = Image.open(image).convert("RGB")
        else:
            image = image.convert("RGB")
        
        # ä¿å­˜åŸå›¾ç”¨äºåç»­ç»˜åˆ¶
        original_image = image.copy()
        
        # ä½¿ç”¨ DeepseekOCRProcessor å¤„ç†å›¾ç‰‡
        image_features = DeepseekOCRProcessor().tokenize_with_images(
            images=[image], 
            bos=True, 
            eos=True, 
            cropping=CROP_MODE
        )
        
        # è®¾ç½® logits processors
        logits_processors = [
            NoRepeatNGramLogitsProcessor(
                ngram_size=ngram_size, 
                window_size=window_size, 
                whitelist_token_ids={128821, 128822}
            )
        ]
        
        # è®¾ç½®é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=max_tokens,
            logits_processors=logits_processors,
            skip_special_tokens=False,
        )
        
        request_id = f"request-{int(time.time())}"
        
        # å‡†å¤‡è¯·æ±‚
        request = {
            "prompt": prompt_text,
            "multi_modal_data": {"image": image_features}
        }
        
        # æ‰§è¡Œ OCR
        print(f"æ­£åœ¨æ‰§è¡Œ OCR è¯†åˆ«... prompt: {prompt_text}")
        full_text = ""
        async for request_output in llm.generate(
            request, sampling_params, request_id
        ):
            if request_output.outputs:
                full_text = request_output.outputs[0].text
        
        print("OCR è¯†åˆ«å®Œæˆï¼")
        
        # å¦‚æœåŒ…å« <|grounding|> æ ‡è®°ï¼Œåˆ™ç»˜åˆ¶è¾¹ç•Œæ¡†
        annotated_image = None
        if '<|grounding|>' in prompt_text or '<|ref|>' in full_text:
            try:
                matches_ref, matches_images, matches_other = re_match(full_text)
                if matches_ref:
                    print(f"æ£€æµ‹åˆ° {len(matches_ref)} ä¸ªæ ‡è®°åŒºåŸŸï¼Œæ­£åœ¨ç»˜åˆ¶è¾¹ç•Œæ¡†...")
                    annotated_image = process_image_with_refs(original_image, matches_ref)
            except Exception as e:
                print(f"ç»˜åˆ¶è¾¹ç•Œæ¡†æ—¶å‡ºé”™: {e}")
        
        return full_text, annotated_image
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ é”™è¯¯: {str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg, None

def ocr_image(image, temperature, max_tokens, ngram_size, window_size, prompt_text):
    """
    å¯¹ä¸Šä¼ çš„å›¾ç‰‡è¿›è¡Œ OCR è¯†åˆ«ï¼ˆåŒæ­¥åŒ…è£…ï¼‰
    """
    return asyncio.run(ocr_image_async(image, temperature, max_tokens, ngram_size, window_size, prompt_text))


# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="DeepSeek-OCR WebUI", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ” DeepSeek-OCR WebUI
        
        åŸºäº DeepSeek-OCR æ¨¡å‹çš„å…‰å­¦å­—ç¬¦è¯†åˆ« (OCR) Web ç•Œé¢
        """
    )
    
    # å•å¼ å›¾ç‰‡ OCR
    with gr.Row():
        # å·¦ä¾§ï¼šè¾“å…¥åŒºåŸŸ
        with gr.Column(scale=1):
            single_image = gr.Image(
                type="pil",
                label="ä¸Šä¼ å›¾ç‰‡",
                height=450
            )
            
            prompt_single = gr.Textbox(
                label="æç¤ºè¯ (Prompt)",
                value="<image>\n<|grounding|>Convert the document to markdown.",
                lines=2,
                placeholder="è¾“å…¥æç¤ºè¯ï¼Œä¾‹å¦‚: <image>\n<|grounding|>OCR this image."
            )
            
            with gr.Accordion("âš™ï¸ é«˜çº§å‚æ•°", open=False):
                temperature_single = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.0,
                    step=0.1,
                    label="Temperature (æ¸©åº¦)"
                )
                max_tokens_single = gr.Slider(
                    minimum=512,
                    maximum=16384,
                    value=8192,
                    step=512,
                    label="æœ€å¤§ Token æ•°"
                )
                ngram_size_single = gr.Slider(
                    minimum=10,
                    maximum=50,
                    value=30,
                    step=5,
                    label="N-gram å¤§å°"
                )
                window_size_single = gr.Slider(
                    minimum=30,
                    maximum=150,
                    value=90,
                    step=10,
                    label="çª—å£å¤§å°"
                )
            
            ocr_button_single = gr.Button("ğŸš€ å¼€å§‹è¯†åˆ«", variant="primary", size="lg")
        
        # å³ä¾§ï¼šè¾“å‡ºåŒºåŸŸ
        with gr.Column(scale=1):
            output_single = gr.Textbox(
                label="è¯†åˆ«ç»“æœ",
                lines=22,
                max_lines=30,
                show_copy_button=True
            )
    
    # ä¸‹æ–¹ï¼šæ ‡æ³¨å›¾ç‰‡ï¼ˆå¯æŠ˜å ï¼‰
    with gr.Row():
        with gr.Column():
            with gr.Accordion("ğŸ–¼ï¸ æ ‡æ³¨å›¾ç‰‡ (å¦‚æœ‰)", open=True):
                annotated_image_single = gr.Image(
                    type="pil",
                    label="",
                    height=500,
                    show_label=False
                )
    
    # æ·»åŠ å¸¸ç”¨æç¤ºè¯ç¤ºä¾‹
    with gr.Row():
        gr.Examples(
            examples=[
                ["<image>\n<|grounding|>Convert the document to markdown."],
                ["<image>\n<|grounding|>OCR this image."],
                ["<image>\nFree OCR."],
                ["<image>\nParse the figure."],
                ["<image>\nDescribe this image in detail."],
            ],
            inputs=prompt_single,
            label="ğŸ“ å¸¸ç”¨æç¤ºè¯ç¤ºä¾‹"
        )
    
    ocr_button_single.click(
        fn=ocr_image,
        inputs=[
            single_image,
            temperature_single,
            max_tokens_single,
            ngram_size_single,
            window_size_single,
            prompt_single
        ],
        outputs=[output_single, annotated_image_single]
    )
    
    gr.Markdown(
        """
        ---
        ### ğŸ“– ä½¿ç”¨è¯´æ˜
        
        1. **ä¸Šä¼ å›¾ç‰‡** - æ”¯æŒ PNGã€JPGã€JPEGã€BMP ç­‰æ ¼å¼
        2. **é€‰æ‹©æç¤ºè¯** - å¯ä»¥ä½¿ç”¨é¢„è®¾çš„æç¤ºè¯æˆ–è‡ªå®šä¹‰
        3. **è°ƒæ•´å‚æ•°** - å±•å¼€"é«˜çº§å‚æ•°"å¯è°ƒæ•´è¯†åˆ«å‚æ•°
        4. **å¼€å§‹è¯†åˆ«** - ç‚¹å‡»"å¼€å§‹è¯†åˆ«"æŒ‰é’®è¿›è¡Œ OCR
        5. **æŸ¥çœ‹ç»“æœ** - æ–‡æœ¬ç»“æœæ˜¾ç¤ºåœ¨å³ä¾§ï¼Œæ ‡æ³¨å›¾ç‰‡æ˜¾ç¤ºåœ¨ä¸‹æ–¹
        
        ### ğŸ¯ æ”¯æŒçš„æ¨¡å¼ (Support Modes)
        
        å½“å‰å¼€æºæ¨¡å‹æ”¯æŒä»¥ä¸‹æ¨¡å¼ï¼ˆå¯åœ¨ `config.py` ä¸­é…ç½®ï¼‰:
        
        **åŸç”Ÿåˆ†è¾¨ç‡ (Native resolution):**
        - **Tiny**: 512Ã—512 (64 vision tokens) âœ…
        - **Small**: 640Ã—640 (100 vision tokens) âœ…
        - **Base**: 1024Ã—1024 (256 vision tokens) âœ…
        - **Large**: 1280Ã—1280 (400 vision tokens) âœ…
        
        **åŠ¨æ€åˆ†è¾¨ç‡ (Dynamic resolution):**
        - **Gundam**: nÃ—640Ã—640 + 1Ã—1024Ã—1024 âœ… (é»˜è®¤é…ç½®)
        
        ### ğŸ’¡ æç¤ºè¯ç¤ºä¾‹ (Prompt Examples)
        
        - `<image>\n<|grounding|>Convert the document to markdown.` - å°†æ–‡æ¡£è½¬æ¢ä¸º Markdownï¼ˆå¸¦ä½ç½®æ ‡æ³¨ï¼‰
        - `<image>\n<|grounding|>OCR this image.` - å¸¦å¸ƒå±€çš„ OCRï¼ˆå…¶ä»–å›¾ç‰‡ï¼Œå¸¦ä½ç½®æ ‡æ³¨ï¼‰
        - `<image>\nFree OCR.` - è‡ªç”± OCRï¼Œä¸å¸¦å¸ƒå±€ä¿¡æ¯
        - `<image>\nParse the figure.` - è§£ææ–‡æ¡£ä¸­çš„å›¾è¡¨
        - `<image>\nDescribe this image in detail.` - è¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹
        - `<image>\nLocate <|ref|>å…ˆå¤©ä¸‹ä¹‹å¿§è€Œå¿§<|/ref|> in the image.` - å®šä½æŒ‡å®šæ–‡æœ¬åœ¨å›¾ç‰‡ä¸­çš„ä½ç½®
        
        ### âš ï¸ æ³¨æ„äº‹é¡¹
        
        - é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…
        - å¸¦ `<|grounding|>` æ ‡è®°çš„æç¤ºè¯ä¼šåŒ…å«ä½ç½®ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆå¸¦çº¢æ¡†æ ‡æ³¨çš„å›¾ç‰‡
        - ä½¿ç”¨ `<|ref|>...<|/ref|>` å¯ä»¥å®šä½ç‰¹å®šæ–‡æœ¬åœ¨å›¾ç‰‡ä¸­çš„ä½ç½®
        - æ ‡æ³¨å›¾ç‰‡ä¼šç”¨ä¸åŒé¢œè‰²çš„è¾¹æ¡†æ ‡è®°ä¸åŒç±»å‹çš„åŒºåŸŸï¼ˆæ ‡é¢˜ç”¨ç²—æ¡†ï¼Œå…¶ä»–ç”¨ç»†æ¡†ï¼‰
        """
    )

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ å¯åŠ¨ DeepSeek-OCR WebUI")
    print("=" * 60)
    
    # é¢„åŠ è½½æ¨¡å‹
    print("\nâ³ æ­£åœ¨é¢„åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
    asyncio.run(initialize_model())
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n")
    
    # å¯åŠ¨ Gradio åº”ç”¨
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,        # ç«¯å£å·
        share=False,             # æ˜¯å¦åˆ›å»ºå…¬å…±é“¾æ¥
        show_error=True          # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
    )
