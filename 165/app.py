import torch
import math
import numpy as np
import gradio as gr
import os

# ÁÆÄÂåñÁöÑÂØºÂÖ•ÔºåÈÅøÂÖçÂú®Ê≤°ÊúâÊ®°ÂûãÊó∂Âá∫Èîô
try:
    import decord
    from decord import VideoReader, cpu
    from PIL import Image
    from moviepy.editor import VideoFileClip
    from pydub import AudioSegment
    import librosa
    import tempfile
    from transformers import ARCHunyuanVideoProcessor, ARCHunyuanVideoForConditionalGeneration
    DEPS_AVAILABLE = True
except ImportError as e:
    print(f"ÈÉ®ÂàÜ‰æùËµñÊú™ÂÆâË£Ö: {e}")
    DEPS_AVAILABLE = False

# ÂÖ®Â±ÄÂèòÈáèÂ≠òÂÇ®Ê®°ÂûãÂíåÂ§ÑÁêÜÂô®
model = None
processor = None
processor = None


def initialize_model():
    """ÂàùÂßãÂåñÊ®°Âûã"""
    global model, processor
    
    if not DEPS_AVAILABLE:
        print("‚ùå ‰æùËµñÊú™ÂÆâË£ÖÔºåË∑≥ËøáÊ®°ÂûãÂä†ËΩΩ")
        return False
    
    print("üöÄ Ê≠£Âú®ÂàùÂßãÂåñ ARC Hunyuan Video Analysis Â∫îÁî®")
    
    try:
        model_path = "checkpoints/ARC-Hunyuan-Video-7B"
        
        # Ê£ÄÊü•Ë∑ØÂæÑÊòØÂê¶Â≠òÂú®
        if not os.path.exists(model_path):
            print(f"‚ùå Ê®°ÂûãË∑ØÂæÑ‰∏çÂ≠òÂú®: {model_path}")
            return False
        
        # Ê£ÄÊü• CUDA
        if not torch.cuda.is_available():
            print("‚ùå CUDA ‰∏çÂèØÁî®ÔºåËØ∑Ê£ÄÊü•GPUÁéØÂ¢É")
            return False
        
        print(f"‚úÖ CUDA ÂèØÁî®ÔºåGPU: {torch.cuda.get_device_name(0)}")
        
        # Âä†ËΩΩÂ§ÑÁêÜÂô®
        print("üìù Ê≠£Âú®Âä†ËΩΩÂ§ÑÁêÜÂô®...")
        processor = ARCHunyuanVideoProcessor.from_pretrained(model_path)
        print("‚úÖ Â§ÑÁêÜÂô®Âä†ËΩΩÊàêÂäü")
        
        # Âä†ËΩΩÊ®°Âûã
        print("ü§ñ Ê≠£Âú®Âä†ËΩΩÊ®°Âûã...")
        model = ARCHunyuanVideoForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
        )
        model = model.eval().to("cuda")
        print("‚úÖ Ê®°ÂûãÂä†ËΩΩÊàêÂäü")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Ê®°ÂûãÂàùÂßãÂåñÂ§±Ë¥•: {str(e)}")
        return False


def calculate_frame_indices(vlen: int, fps: float, duration: float) -> list:
    frames_per_second = fps

    if duration <= 150:
        interval = 1
        intervals = [
            (int(i * interval * frames_per_second), int((i + 1) * interval * frames_per_second))
            for i in range(math.ceil(duration))
        ]
        sample_fps = 1
    else:
        num_segments = 150
        segment_duration = duration / num_segments
        intervals = [
            (int(i * segment_duration * frames_per_second), int((i + 1) * segment_duration * frames_per_second))
            for i in range(num_segments)
        ]
        sample_fps = 1 / segment_duration

    frame_indices = []
    for start, end in intervals:
        if end > vlen:
            end = vlen
        frame_indices.append((start + end) // 2)

    return frame_indices, sample_fps


def load_video_frames(video_path: str):
    video_reader = VideoReader(video_path, ctx=cpu(0), num_threads=4)
    vlen = len(video_reader)
    input_fps = video_reader.get_avg_fps()
    duration = vlen / input_fps

    frame_indices, sample_fps = calculate_frame_indices(vlen, input_fps, duration)

    return [Image.fromarray(video_reader[idx].asnumpy()) for idx in frame_indices], sample_fps


def cut_audio_with_librosa(audio_path, max_num_frame=150, segment_sec=2, max_total_sec=300, sr=16000):
    audio, _ = librosa.load(audio_path, sr=sr)
    total_samples = len(audio)
    total_sec = total_samples / sr

    if total_sec <= max_total_sec:
        return audio, sr

    segment_length = total_samples // max_num_frame
    segment_samples = int(segment_sec * sr)
    segments = []
    for i in range(max_num_frame):
        start = i * segment_length
        end = min(start + segment_samples, total_samples)
        segments.append(audio[start:end])
    new_audio = np.concatenate(segments)
    return new_audio, sr


def pad_audio(audio: np.ndarray, sr: int) -> np.ndarray:
    if len(audio.shape) == 2:
        audio = audio[:, 0]
    if len(audio) < sr:
        sil = np.zeros(sr - len(audio), dtype=float)
        audio = np.concatenate((audio, sil), axis=0)
    return audio


def load_audio(video_path, audio_path):
    if audio_path is None:
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=True) as temp_audio:
            audio_path = temp_audio.name
            video = VideoFileClip(video_path)
            try:
                video.audio.write_audiofile(audio_path, logger=None)
                audio, sr = cut_audio_with_librosa(
                    audio_path, max_num_frame=150, segment_sec=2, max_total_sec=300, sr=16000
                )
            except:
                duration = min(math.ceil(video.duration), 300)
                silent_audio = AudioSegment.silent(duration=duration * 1000)
                silent_audio.export(audio_path, format="mp3")
                audio, sr = librosa.load(audio_path, sr=16000)
    else:
        audio, sr = cut_audio_with_librosa(audio_path, max_num_frame=150, segment_sec=2, max_total_sec=300, sr=16000)

    audio = pad_audio(audio, sr)
    duration = math.ceil(len(audio) / sr)

    return audio, sr, duration


def build_prompt(question: str, num_frames: int, task: str = "summary"):
    video_prefix = "<image>" * num_frames

    if task == "MCQ":
        return f"<|startoftext|>{video_prefix}\n{question}\nOutput the thinking process in <think> </think> and final answer (only option index) in <answer> </answer> tags, i.e., <think> reasoning process here </think><answer> answer here </answer>.<sep>"
    elif task == "Grounding":
        return f"<|startoftext|>{video_prefix}\n{question}\nOutput the thinking process in <think> </think> and final answer (only time range) in <answer> </answer> tags, i.e., <think> reasoning process here </think><answer> answer here </answer>.<sep>"
    else:  # QA„ÄÅsummary„ÄÅsegment
        return f"<|startoftext|>{video_prefix}\n{question}\nOutput the thinking process in <think> </think> and final answer in <answer> </answer> tags, i.e., <think> reasoning process here </think><answer> answer here </answer>.<sep>"


def prepare_inputs(question: str, video_path: str, audio_path: str = None, task: str = "summary"):
    video_frames, sample_fps = load_video_frames(video_path)
    audio, sr, duration = load_audio(video_path, audio_path)

    # To solve mismatched duration between video and audio
    video_duration = int(len(video_frames) / sample_fps)
    audio_duration = duration

    # Truncate video frames to match audio duration
    # The audio duration will be truncated in the model
    duration = min(video_duration, audio_duration)
    if duration <= 150:
        video_frames = video_frames[: int(duration * sample_fps)]

    prompt = build_prompt(question, len(video_frames), task)

    video_inputs = {
        "video": video_frames,
        "video_metadata": {
            "fps": sample_fps,
        },
    }

    audio_inputs = {
        "audio": audio,
        "sampling_rate": sr,
        "duration": duration,
    }

    return prompt, video_inputs, audio_inputs


def inference(model, processor, question: str, video_path: str, audio_path: str = None, task: str = "summary"):
    try:
        prompt, video_inputs, audio_inputs = prepare_inputs(question, video_path, audio_path, task)
        inputs = processor(
            text=prompt,
            **video_inputs,
            **audio_inputs,
            return_tensors="pt",
        )

        inputs = inputs.to("cuda", dtype=torch.bfloat16)
        outputs = model.generate(**inputs, max_new_tokens=1024, do_sample=False)
        output_text = processor.decode(outputs[0], skip_special_tokens=True)

        return output_text
    except Exception as e:
        return f"Êé®ÁêÜËøáÁ®ã‰∏≠Âá∫Áé∞ÈîôËØØ: {str(e)}"


def get_sample_data():
    """Ëé∑ÂèñÁ§∫‰æãÊï∞ÊçÆÔºåÁî®‰∫éÊºîÁ§∫ÂíåÊµãËØï"""
    examples_data = []
    
    # First video test cases - ‰ΩøÁî®ÁªùÂØπË∑ØÂæÑ
    video_path = os.path.abspath("examples/demo1.mp4")
    
    # Summary task
    examples_data.append([
        video_path, None, 
        "ËØ•ËßÜÈ¢ëÊ†áÈ¢ò‰∏∫ÁôΩÈáëÊû™È±ºÂØøÂè∏ÁöÑÈô∑Èò±\nÊèèËø∞ËßÜÈ¢ëÂÜÖÂÆπ.", 
        "summary"
    ])
    
    # Grounding task
    examples_data.append([
        video_path, None,
        "Êàë‰ª¨‰ΩïÊó∂ËÉΩÁúãÂà∞‰∏Ä‰∏™Á©øÂà∂ÊúçÁöÑÁî∑‰∫∫Á´ôÂú®ËèäËä±Èó®Ââç?",
        "Grounding"
    ])
    
    # QA task
    examples_data.append([
        video_path, None,
        "Ëøô‰∏™ËßÜÈ¢ëÊúâ‰ªÄ‰πàÁ¨ëÁÇπÔºü",
        "QA"
    ])
    
    # MCQ task
    examples_data.append([
        video_path, None,
        "ËßÜÈ¢ë‰∏≠ÊúÄÂêéËÄÅÊùøÊèê‰æõ‰∫Ü‰ªÄ‰πàÁªôÈ°æÂÆ¢‰Ωú‰∏∫Ëµ†ÂìÅÔºü\nA.Á∫∏Â∞øË£§\nB.ÂØøÂè∏\nC.Áé∞Èáë\nD.Èù¢Â∑æÁ∫∏",
        "MCQ"
    ])
    
    # Second video test cases - ‰ΩøÁî®ÁªùÂØπË∑ØÂæÑ
    video_path = os.path.abspath("examples/demo3.mov")
    examples_data.append([
        video_path, None,
        "ËØ∑ÊåâÊó∂Èó¥È°∫Â∫èÁªôÂá∫ËßÜÈ¢ëÁöÑÁ´†ËäÇÊëòË¶ÅÂíåÂØπÂ∫îÊó∂Èó¥ÁÇπ",
        "segment"
    ])
    
    # Third video test cases - ‰ΩøÁî®ÁªùÂØπË∑ØÂæÑ 
    video_path = os.path.abspath("examples/demo2.mp4")
    
    # Summary task
    examples_data.append([
        video_path, None,
        "The title of the video is\nDescribe the video content.",
        "summary"
    ])
    
    # Grounding task
    examples_data.append([
        video_path, None,
        "When will we be able to see the man in the video eat the pork cutlet in the restaurant?",
        "Grounding"
    ])
    
    # QA task
    examples_data.append([
        video_path, None,
        "Why is the man dissatisfied with the pork cutlet he cooked himself at home?",
        "QA"
    ])
    
    # Multi-granularity caption task
    examples_data.append([
        video_path, None,
        "Localize video chapters with temporal boundaries and the corresponding sentence description.",
        "segment"
    ])
    
    return examples_data


def create_demo():
    """ÂàõÂª∫ Gradio ÁïåÈù¢"""
    
    # Ê£ÄÊü•Ê®°ÂûãÊòØÂê¶Â∑≤Âä†ËΩΩ
    global model, processor
    model_status = "‚úÖ Â∑≤Â∞±Áª™" if (model is not None and processor is not None) else "‚ùå Êú™Âä†ËΩΩ"
    
    with gr.Blocks(title="ARC Hunyuan Video Analysis", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            f"""
            # üé• ARC Hunyuan Video Analysis
            
            Âü∫‰∫é ARC-Hunyuan-Video-7B Ê®°ÂûãÁöÑËßÜÈ¢ëÁêÜËß£ÂíåÂàÜÊûêÂ∑•ÂÖ∑
            
            **Ê®°ÂûãÁä∂ÊÄÅ**: {model_status}
            
            ## ‰ΩøÁî®ËØ¥Êòé
            1. ÁÇπÂáª‰∏ãÈù¢ÁöÑÁ§∫‰æãËá™Âä®Â°´ÂÖÖË°®Âçï
            2. ÊàñÊâãÂä®‰∏ä‰º†ËßÜÈ¢ëÊñá‰ª∂Âπ∂ËæìÂÖ•ÈóÆÈ¢ò
            3. ÁÇπÂáª"ÂàÜÊûêËßÜÈ¢ë"ÂºÄÂßãÂ§ÑÁêÜ
            """
        )
        
        with gr.Row():
            with gr.Column():
                # Êñá‰ª∂‰∏ä‰º†
                video_input = gr.File(
                    label="üìπ ‰∏ä‰º†ËßÜÈ¢ëÊñá‰ª∂",
                    file_types=[".mp4", ".mov", ".avi", ".mkv", ".webm"],
                    type="filepath"
                )
                
                # ËßÜÈ¢ëÈ¢ÑËßà
                video_preview = gr.Video(
                    label="üìΩÔ∏è ËßÜÈ¢ëÈ¢ÑËßà",
                    visible=False
                )
                
                audio_input = gr.File(
                    label="üéµ ‰∏ä‰º†Èü≥È¢ëÊñá‰ª∂ÔºàÂèØÈÄâÔºâ",
                    file_types=[".mp3", ".wav", ".aac", ".m4a"],
                    type="filepath"
                )
                
                # Èü≥È¢ëÈ¢ÑËßà
                audio_preview = gr.Audio(
                    label="üéß Èü≥È¢ëÈ¢ÑËßà",
                    visible=False
                )
                
                # ÈóÆÈ¢òËæìÂÖ•
                question_input = gr.Textbox(
                    label="‚ùì ËØ∑ËæìÂÖ•ÊÇ®ÁöÑÈóÆÈ¢ò",
                    placeholder="‰æãÂ¶ÇÔºöÊèèËø∞Ëøô‰∏™ËßÜÈ¢ëÁöÑÂÜÖÂÆπ",
                    lines=3
                )
                
                # ‰ªªÂä°Á±ªÂûãÈÄâÊã©
                task_input = gr.Dropdown(
                    label="üìã ÈÄâÊã©‰ªªÂä°Á±ªÂûã",
                    choices=["summary", "QA", "Grounding", "MCQ", "segment"],
                    value="summary",
                    info="summary: ËßÜÈ¢ëÊëòË¶Å, QA: ÈóÆÁ≠î, Grounding: Êó∂Èó¥ÂÆö‰Ωç, MCQ: ÈÄâÊã©È¢ò, segment: Á´†ËäÇÂàÜÊûê"
                )
                
                # ÂàÜÊûêÊåâÈíÆ
                analyze_btn = gr.Button("üîç ÂàÜÊûêËßÜÈ¢ë", variant="secondary", size="lg")
            
            with gr.Column():
                # ÁªìÊûúÊòæÁ§∫
                output = gr.Textbox(
                    label="üìä ÂàÜÊûêÁªìÊûú",
                    lines=20,
                    max_lines=30,
                    interactive=False,
                    placeholder="ÂàÜÊûêÁªìÊûúÂ∞ÜÂú®ËøôÈáåÊòæÁ§∫..."
                )
        
        # Á§∫‰æãÈÉ®ÂàÜ
        gr.Markdown("## üìù ÂÜÖÁΩÆÁ§∫‰æã")
        
        try:
            examples_data = get_sample_data()
            print(f"üîß [Ë∞ÉËØï] ÂáÜÂ§áÂàõÂª∫ Examples ÁªÑ‰ª∂ÔºåÊÄªÂÖ± {len(examples_data)} ‰∏™Á§∫‰æã")
            
            # Ê£ÄÊü•Á§∫‰æãÊñá‰ª∂ÊòØÂê¶Â≠òÂú®
            for i, (video_path, audio_path, question, task) in enumerate(examples_data):
                file_exists = os.path.exists(video_path) if video_path else False
                print(f"üîß [Ë∞ÉËØï] Á§∫‰æã {i+1}: {os.path.basename(video_path) if video_path else 'None'} | {task} | Â≠òÂú®: {file_exists}")
                print(f"ÔøΩ [Ë∞ÉËØï] ÂÆåÊï¥Ë∑ØÂæÑ: {video_path}")
            
            # ‰ΩøÁî® Gradio Ê†áÂáÜ Examples Êéß‰ª∂
            examples = gr.Examples(
                examples=examples_data,
                inputs=[video_input, audio_input, question_input, task_input],
                label="üìã ÁÇπÂáª‰∏ãÈù¢ÁöÑÁ§∫‰æãË°åËá™Âä®Â°´ÂÖÖË°®Âçï"
            )
            
            print("‚úÖ [Ë∞ÉËØï] Examples ÁªÑ‰ª∂ÂàõÂª∫ÊàêÂäü")
            
            # ‰ΩøÁî®ËØ¥Êòé
            gr.Markdown("""
            üí° **‰ΩøÁî®ËØ¥Êòé**: 
            1. **ÁÇπÂáªÁ§∫‰æãË°å**: Áõ¥Êé•ÁÇπÂáª‰∏äÈù¢Ë°®Ê†º‰∏≠ÁöÑ‰ªªÊÑè‰∏ÄË°åÔºå‰ºöËá™Âä®Â°´ÂÖÖÊâÄÊúâË°®ÂçïÂ≠óÊÆµ
            2. **ÂàÜÊûêËßÜÈ¢ë**: Â°´ÂÖÖÂêéÁõ¥Êé•ÁÇπÂáª"üîç ÂàÜÊûêËßÜÈ¢ë"ÊåâÈíÆÂºÄÂßãÂàÜÊûê
            
            üìÅ **Á§∫‰æãËßÜÈ¢ë**:
            - `demo1.mp4`: ÁôΩÈáëÊû™È±ºÂØøÂè∏Áõ∏ÂÖ≥ËßÜÈ¢ëÔºàÊó•ËØ≠Ôºâ
            - `demo2.mp4`: Áå™ÊéíÂà∂‰ΩúÁõ∏ÂÖ≥ËßÜÈ¢ëÔºàËã±ËØ≠Ôºâ  
            - `demo3.mov`: Á´†ËäÇÂàÜÊûêÁ§∫‰æãËßÜÈ¢ë
            """)
                        
        except Exception as e:
            print(f"‚ö†Ô∏è Âä†ËΩΩÁ§∫‰æãÊï∞ÊçÆÂ§±Ë¥•: {e}")
            import traceback
            traceback.print_exc()
            gr.Markdown("‚ùå Á§∫‰æãÊï∞ÊçÆÂä†ËΩΩÂ§±Ë¥•ÔºåËØ∑Ê£ÄÊü• examples ÁõÆÂΩï")
        
        # ÂÆåÊï¥ÁöÑËßÜÈ¢ëÂàÜÊûêÂ§ÑÁêÜÂáΩÊï∞
        def process_video(video_file, audio_file, question, task):
            global model, processor
            
            if video_file is None:
                return "ËØ∑‰∏ä‰º†ËßÜÈ¢ëÊñá‰ª∂Êàñ‰ΩøÁî®Á§∫‰æãÔºÅ"
            if not question.strip():
                return "ËØ∑ËæìÂÖ•ÈóÆÈ¢òÔºÅ"
            if model is None or processor is None:
                return "‚ùå Ê®°ÂûãÊú™Âä†ËΩΩÔºåËØ∑ÈáçÂêØÂ∫îÁî®ÔºÅ"
            
            try:
                # Ëé∑ÂèñÊñá‰ª∂Ë∑ØÂæÑ
                video_path = video_file if isinstance(video_file, str) else video_file.name
                audio_path = None
                if audio_file:
                    audio_path = audio_file if isinstance(audio_file, str) else audio_file.name
                
                # ÊâßË°åÊé®ÁêÜ
                result = inference(model, processor, question, video_path, audio_path, task)
                
                return f"‚úÖ ËßÜÈ¢ëÂàÜÊûêÂÆåÊàêÔºÅ\n\nüìπ ËßÜÈ¢ë: {os.path.basename(video_path)}\n‚ùì ÈóÆÈ¢ò: {question}\nüìã ‰ªªÂä°: {task}\n\nüìä ÂàÜÊûêÁªìÊûú:\n{result}"
                
            except Exception as e:
                error_msg = f"‚ùå Â§ÑÁêÜËßÜÈ¢ëÊó∂Âá∫Áé∞ÈîôËØØ: {str(e)}"
                return error_msg
        
        # ËßÜÈ¢ëÈ¢ÑËßàÊõ¥Êñ∞ÂáΩÊï∞
        def update_video_preview(video_file):
            if video_file is not None:
                return gr.Video(value=video_file, visible=True)
            else:
                return gr.Video(visible=False)
        
        # Èü≥È¢ëÈ¢ÑËßàÊõ¥Êñ∞ÂáΩÊï∞
        def update_audio_preview(audio_file):
            if audio_file is not None:
                return gr.Audio(value=audio_file, visible=True)
            else:
                return gr.Audio(visible=False)
        
        # ‰∫ã‰ª∂ÁªëÂÆö
        analyze_btn.click(
            fn=process_video,
            inputs=[video_input, audio_input, question_input, task_input],
            outputs=output
        )
        
        # Êñá‰ª∂‰∏ä‰º†Êó∂Êõ¥Êñ∞È¢ÑËßà
        video_input.change(
            fn=update_video_preview,
            inputs=video_input,
            outputs=video_preview
        )
        
        audio_input.change(
            fn=update_audio_preview,
            inputs=audio_input,
            outputs=audio_preview
        )
        
        # Ê∏ÖÁ©∫ÊåâÈíÆ
        with gr.Row():
            clear_btn = gr.Button("üóëÔ∏è Ê∏ÖÁ©∫ÊâÄÊúâ", variant="stop")
            clear_btn.click(
                lambda: ("", "", None, None, "summary"),
                outputs=[question_input, output, video_input, audio_input, task_input]
            )
    
    return demo


if __name__ == "__main__":
    print("üöÄ ÂêØÂä® ARC Hunyuan Video Analysis...")
    
    # ÂàùÂßãÂåñÊ®°Âûã
    initialize_model()
    
    # ÂàõÂª∫Âπ∂ÂêØÂä® Gradio Â∫îÁî®
    demo = create_demo()
    
    # ÂêØÂä®ÊúçÂä°Âô®
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True,
        show_error=True
    )
