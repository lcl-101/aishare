import torch
import gradio as gr
from transformers import AutoModel, AutoTokenizer
from utils import load_image, split_model
import os
import tempfile
from PIL import Image

class SkyworkWebUI:
    def __init__(self, model_path='checkpoints/Skywork-R1V3-38B'):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            device_map = split_model(self.model_path)
            self.model = AutoModel.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16,
                load_in_8bit=False,
                low_cpu_mem_usage=True,
                use_flash_attn=True,
                trust_remote_code=True,
                device_map=device_map
            ).eval()
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, 
                trust_remote_code=True, 
                use_fast=False
            )
            return "æ¨¡å‹åŠ è½½æˆåŠŸï¼"
        except Exception as e:
            return f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    
    def process_images(self, image_files):
        """å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶"""
        if not image_files:
            return None, []
        
        print(f"Debug: image_files type: {type(image_files)}")
        print(f"Debug: image_files content: {image_files}")
        
        # å¤„ç†Galleryç»„ä»¶è¿”å›çš„å›¾ç‰‡åˆ—è¡¨
        pixel_values_list = []
        image_paths = []
        
        # Galleryç»„ä»¶è¿”å›çš„æ ¼å¼å¯èƒ½æ˜¯ä¸åŒç±»å‹çš„å¯¹è±¡
        if isinstance(image_files, list):
            for img_item in image_files:
                print(f"Debug: processing item type: {type(img_item)}, content: {img_item}")
                if isinstance(img_item, str):
                    # ç›´æ¥æ˜¯æ–‡ä»¶è·¯å¾„
                    image_paths.append(img_item)
                elif hasattr(img_item, 'name'):
                    # æ–‡ä»¶å¯¹è±¡ï¼Œè·å–è·¯å¾„
                    image_paths.append(img_item.name)
                elif isinstance(img_item, tuple) and len(img_item) > 0:
                    # å…ƒç»„æ ¼å¼ï¼Œé€šå¸¸ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯è·¯å¾„
                    image_paths.append(img_item[0] if isinstance(img_item[0], str) else str(img_item[0]))
                else:
                    # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    image_paths.append(str(img_item))
        else:
            # å•ä¸ªå›¾ç‰‡
            print(f"Debug: processing single item type: {type(image_files)}, content: {image_files}")
            if isinstance(image_files, str):
                image_paths = [image_files]
            elif hasattr(image_files, 'name'):
                image_paths = [image_files.name]
            elif isinstance(image_files, tuple) and len(image_files) > 0:
                image_paths = [image_files[0] if isinstance(image_files[0], str) else str(image_files[0])]
            else:
                image_paths = [str(image_files)]
        
        # è¿‡æ»¤æ‰ç©ºè·¯å¾„
        image_paths = [path for path in image_paths if path and path.strip() and path != 'None']
        
        print(f"Debug: final image_paths: {image_paths}")
        
        if not image_paths:
            return None, []
        
        # åŠ è½½å›¾ç‰‡
        pixel_values = [load_image(img_path, max_num=12).to(torch.bfloat16).cuda() for img_path in image_paths]
        
        if len(pixel_values) > 1:
            num_patches_list = [img.size(0) for img in pixel_values]
            pixel_values = torch.cat(pixel_values, dim=0)
        else:
            pixel_values = pixel_values[0]
            num_patches_list = None
            
        return pixel_values, num_patches_list
    
    def chat(self, images, question, max_tokens=64000, temperature=0.6, top_p=0.95, repetition_penalty=1.05):
        """ä¸æ¨¡å‹å¯¹è¯"""
        if not question.strip():
            return "è¯·è¾“å…¥é—®é¢˜"
        
        if not images:
            return "è¯·ä¸Šä¼ è‡³å°‘ä¸€å¼ å›¾ç‰‡"
        
        if self.model is None or self.tokenizer is None:
            return "æ¨¡å‹å°šæœªåŠ è½½ï¼Œè¯·ç¨å€™å†è¯•"
        
        try:
            # å¤„ç†å›¾ç‰‡
            pixel_values, num_patches_list = self.process_images(images)
            
            if pixel_values is None:
                return "å›¾ç‰‡å¤„ç†å¤±è´¥"
            
            # è®¡ç®—å®é™…çš„å›¾ç‰‡æ•°é‡
            actual_image_count = len(images) if isinstance(images, list) else 1
            
            # æ„å»ºpromptï¼Œæ˜ç¡®è¦æ±‚ä¸­æ–‡å›ç­”
            prompt = "<image>\n" * actual_image_count + f"è¯·ç”¨ä¸­æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}"
            
            # ç”Ÿæˆé…ç½®
            generation_config = dict(
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            # ç”Ÿæˆå›ç­”
            response = self.model.chat(
                self.tokenizer, 
                pixel_values, 
                prompt, 
                generation_config, 
                num_patches_list=num_patches_list
            )
            
            return response
            
        except Exception as e:
            return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"

# åˆ›å»ºWebUIå®ä¾‹
webui = SkyworkWebUI()

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(title="Skywork-R1V3 WebUI", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸš€ Skywork-R1V3 å¤šæ¨¡æ€å¤§æ¨¡å‹ WebUI")
        gr.Markdown("ä¸Šä¼ å›¾ç‰‡å¹¶æé—®ï¼Œè®©AIä¸ºæ‚¨è§£ç­”ï¼")
        
        with gr.Row():
            with gr.Column(scale=1):
                # å›¾ç‰‡ä¸Šä¼ å’Œé¢„è§ˆï¼ˆåˆå¹¶ä¸ºä¸€ä¸ªç»„ä»¶ï¼‰
                images_input = gr.Gallery(
                    label="ä¸Šä¼ å›¾ç‰‡ ğŸ“· (æ”¯æŒå¤šå¼ å›¾ç‰‡ï¼Œç‚¹å‡»+å·ä¸Šä¼ )",
                    show_label=True,
                    columns=3,
                    rows=2,
                    height=400,
                    allow_preview=True,
                    show_share_button=False,
                    show_download_button=False,
                    interactive=True,
                    type="filepath"
                )
                
                # å‚æ•°è®¾ç½®
                with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                    max_tokens = gr.Slider(
                        minimum=100,
                        maximum=64000,
                        value=64000,
                        step=100,
                        label="æœ€å¤§ç”Ÿæˆtokens"
                    )
                    temperature = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=0.6,
                        step=0.1,
                        label="æ¸©åº¦ (Temperature)"
                    )
                    top_p = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=0.95,
                        step=0.05,
                        label="Top-p"
                    )
                    repetition_penalty = gr.Slider(
                        minimum=1.0,
                        maximum=2.0,
                        value=1.05,
                        step=0.05,
                        label="é‡å¤æƒ©ç½š"
                    )
            
            with gr.Column(scale=1):
                # é—®é¢˜è¾“å…¥
                question_input = gr.Textbox(
                    label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ ğŸ’¬",
                    placeholder="ä¾‹å¦‚ï¼šè¿™å¼ å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿè¯·è¯¦ç»†æè¿°ã€‚",
                    lines=3
                )
                
                # æäº¤æŒ‰é’®
                submit_btn = gr.Button("ğŸ¤– ç”Ÿæˆå›ç­”", variant="primary", size="lg")
                
                # å›ç­”æ˜¾ç¤º
                answer_output = gr.Textbox(
                    label="AIå›ç­” ğŸ¯",
                    lines=15,
                    max_lines=30,
                    show_copy_button=True
                )
                
                # æ¸…é™¤æŒ‰é’®
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤", variant="secondary")
        
        # ç¤ºä¾‹
        gr.Examples(
            examples=[
                [None, "è¯·ç”¨ä¸­æ–‡è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹"],
                [None, "è¯·ç”¨ä¸­æ–‡å›ç­”ï¼šå›¾ç‰‡ä¸­æœ‰å“ªäº›ç‰©ä½“ï¼Ÿ"],
                [None, "è¯·ç”¨ä¸­æ–‡åˆ†æå›¾ç‰‡ä¸­çš„åœºæ™¯å’Œç¯å¢ƒ"],
                [None, "è¯·ç”¨ä¸­æ–‡è¯†åˆ«å¹¶æè¿°å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹"],
            ],
            inputs=[images_input, question_input],
            label="ğŸ’¡ ç¤ºä¾‹é—®é¢˜"
        )
        
        # äº‹ä»¶å¤„ç†
        def process_chat(images, question, max_tokens, temperature, top_p, repetition_penalty):
            if not images:
                return "è¯·å…ˆä¸Šä¼ å›¾ç‰‡"
            return webui.chat(images, question, max_tokens, temperature, top_p, repetition_penalty)
        
        def clear_all():
            return None, "", ""
        
        # ç»‘å®šäº‹ä»¶
        submit_btn.click(
            fn=process_chat,
            inputs=[images_input, question_input, max_tokens, temperature, top_p, repetition_penalty],
            outputs=[answer_output]
        )
        
        clear_btn.click(
            fn=clear_all,
            inputs=[],
            outputs=[images_input, question_input, answer_output]
        )
        
        # å›è½¦æäº¤
        question_input.submit(
            fn=process_chat,
            inputs=[images_input, question_input, max_tokens, temperature, top_p, repetition_penalty],
            outputs=[answer_output]
        )
    
    return demo

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ Skywork-R1V3 WebUI...")
    print("ğŸ“ æ¨¡å‹è·¯å¾„:", webui.model_path)
    
    demo = create_interface()
    
    # å¯åŠ¨ç•Œé¢
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
    )

if __name__ == "__main__":
    main()
