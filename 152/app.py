import gradio as gr
from vllm import LLM, SamplingParams
import gc
import torch
import time

class SeedXTranslator:
    def __init__(self):
        self.models = {}
        self.model_paths = {
            "Seed-X-Instruct-7B": "checkpoints/Seed-X-Instruct-7B",
            "Seed-X-PPO-7B": "checkpoints/Seed-X-PPO-7B"
        }
        self.model_descriptions = {
            "Seed-X-Instruct-7B": "æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ (Instruction-tuned)",
            "Seed-X-PPO-7B": "PPOå¼ºåŒ–å­¦ä¹ æ¨¡å‹ (PPO Reinforcement Learning)"
        }
        self.loading_status = "ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ..."
        self.ready = False
        
        # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ‰€æœ‰æ¨¡å‹
        self.load_all_models()
    
    def clear_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def load_all_models(self):
        """å¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹åŠ è½½ Seed-X æ¨¡å‹...")
        self.loading_status = "â³ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™..."
        
        try:
            # ä¸ºåŒæ—¶åŠ è½½ä¸¤ä¸ªæ¨¡å‹ä¼˜åŒ–å†…å­˜é…ç½®
            memory_per_model = 0.35  # æ¯ä¸ªæ¨¡å‹ä½¿ç”¨35%æ˜¾å­˜ï¼Œæ€»å…±70%ï¼Œç•™30%ç¼“å†²
            
            for i, (model_name, model_path) in enumerate(self.model_paths.items()):
                print(f"ğŸ“¦ [{i+1}/{len(self.model_paths)}] åŠ è½½æ¨¡å‹: {model_name}")
                self.loading_status = f"â³ æ­£åœ¨åŠ è½½ {model_name}... ({i+1}/{len(self.model_paths)})"
                
                # åœ¨åŠ è½½ç¬¬äºŒä¸ªæ¨¡å‹å‰æ¸…ç†å†…å­˜
                if i > 0:
                    print("ğŸ§¹ æ¸…ç†GPUå†…å­˜...")
                    self.clear_memory()
                
                model = LLM(
                    model=model_path,
                    max_num_seqs=64,  # è¿›ä¸€æ­¥é™ä½å¹¶å‘æ•°
                    tensor_parallel_size=1,
                    enable_prefix_caching=True,
                    gpu_memory_utilization=memory_per_model,  # ä¿å®ˆçš„å†…å­˜ä½¿ç”¨
                    trust_remote_code=True,
                    enforce_eager=True,  # ç¦ç”¨CUDAå›¾ä»¥èŠ‚çœå†…å­˜
                    disable_log_stats=True  # ç¦ç”¨ç»Ÿè®¡æ—¥å¿—
                )
                
                self.models[model_name] = model
                print(f"âœ… {model_name} åŠ è½½å®Œæˆ")
            
            self.loading_status = f"âœ… æ‰€æœ‰æ¨¡å‹å·²å°±ç»ªï¼å…±åŠ è½½ {len(self.models)} ä¸ªæ¨¡å‹"
            self.ready = True
            print("ğŸ‰ æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼Œç³»ç»Ÿå°±ç»ªï¼")
            
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            self.loading_status = error_msg
            print(error_msg)
            self.ready = False
    
    def translate_single(self, text, model_name, task_type, temperature=0, max_tokens=512):
        """å•ä¸ªæ¨¡å‹ç¿»è¯‘"""
        try:
            if not self.ready:
                return "â³ ç³»ç»Ÿæ­£åœ¨åˆå§‹åŒ–ï¼Œè¯·ç¨å€™..."
                
            if model_name not in self.models:
                return f"âŒ æ¨¡å‹ {model_name} æœªåŠ è½½"
            
            # æ„å»ºæç¤ºè¯
            if task_type == "ç®€å•ç¿»è¯‘":
                prompt = f"Translate the following English sentence into Chinese:\n{text} <zh>"
            elif task_type == "è¯¦ç»†è§£é‡Šç¿»è¯‘":
                prompt = f"Translate the following English sentence into Chinese and explain it in detail:\n{text} <zh>"
            elif task_type == "æŠ€æœ¯æœ¯è¯­ç¿»è¯‘":
                prompt = f"Translate the following technical English sentence into Chinese:\n{text} <zh>"
            elif task_type == "è¯—æ„è¡¨è¾¾ç¿»è¯‘":
                prompt = f"Translate the following poetic English sentence into Chinese, preserving the artistic beauty:\n{text} <zh>"
            else:
                prompt = f"Translate the following English sentence into Chinese:\n{text} <zh>"
            
            # è®¾ç½®ç”Ÿæˆå‚æ•°
            decoding_params = SamplingParams(
                temperature=temperature,
                max_tokens=max_tokens,
                skip_special_tokens=True,
                top_p=0.9,
                frequency_penalty=0.1
            )
            
            # ç”Ÿæˆç¿»è¯‘
            model = self.models[model_name]
            results = model.generate([prompt], decoding_params)
            response = results[0].outputs[0].text.strip()
            
            return response
            
        except Exception as e:
            return f"âŒ ç¿»è¯‘å¤±è´¥: {str(e)}"
    
    def compare_models(self, text, task_type, temperature=0, max_tokens=512, progress=gr.Progress()):
        """æ¯”è¾ƒæ‰€æœ‰æ¨¡å‹çš„ç¿»è¯‘ç»“æœ"""
        if not text.strip():
            return "è¯·è¾“å…¥è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬"
        
        if not self.ready:
            return "â³ ç³»ç»Ÿæ­£åœ¨åˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™ç‰‡åˆ»..."
        
        results = {}
        model_names = list(self.model_paths.keys())
        total_models = len(model_names)
        
        # å¹¶è¡Œç¿»è¯‘æ‰€æœ‰æ¨¡å‹
        for i, model_name in enumerate(model_names):
            progress((i + 1) / total_models, desc=f"ç¿»è¯‘ä¸­... ({model_name})")
            result = self.translate_single(text, model_name, task_type, temperature, max_tokens)
            results[model_name] = result
        
        # æ ¼å¼åŒ–è¾“å‡º
        output = f"ğŸ“ **åŸæ–‡**: {text}\n"
        output += f"ğŸ¯ **ä»»åŠ¡ç±»å‹**: {task_type}\n"
        output += f"âš™ï¸ **å‚æ•°**: Temperature={temperature}, Max_tokens={max_tokens}\n"
        output += "=" * 80 + "\n\n"
        
        for model_name, result in results.items():
            output += f"## ğŸ¤– {model_name}\n"
            output += f"**æè¿°**: {self.model_descriptions[model_name]}\n\n"
            output += f"**ç¿»è¯‘ç»“æœ**:\n```\n{result}\n```\n\n"
            output += "-" * 60 + "\n\n"
        
        return output
    
    def get_system_status(self):
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        if self.ready:
            return f"ğŸŸ¢ ç³»ç»Ÿå°±ç»ª | å·²åŠ è½½ {len(self.models)} ä¸ªæ¨¡å‹"
        else:
            return self.loading_status

# åˆå§‹åŒ–ç¿»è¯‘å™¨
translator = SeedXTranslator()

# åˆ›å»º Gradio ç•Œé¢
def create_interface():
    with gr.Blocks(title="Seed-X æ¨¡å‹ç¿»è¯‘å¯¹æ¯”ç³»ç»Ÿ", theme=gr.themes.Soft()) as iface:
        gr.Markdown("""
        # ğŸ¤– Seed-X æ¨¡å‹ç¿»è¯‘å¯¹æ¯”ç³»ç»Ÿ
        
        åŸºäº Seed-X-Instruct-7B å’Œ Seed-X-PPO-7B çš„å®æ—¶ç¿»è¯‘å¯¹æ¯”ç³»ç»Ÿ
        
        ## ï¿½ ç‰¹ç‚¹ï¼š
        - **é¢„åŠ è½½æ¨¡å‹**: å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼Œç¿»è¯‘å“åº”æ›´å¿«
        - **å®æ—¶å¯¹æ¯”**: åŒæ—¶ä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹ç¿»è¯‘ï¼Œç›´è§‚æ¯”è¾ƒæ•ˆæœå·®å¼‚
        - **å¤šç§ä»»åŠ¡**: æ”¯æŒç®€å•ç¿»è¯‘ã€è¯¦ç»†è§£é‡Šã€æŠ€æœ¯æœ¯è¯­ã€è¯—æ„è¡¨è¾¾ç­‰
        - **å‚æ•°è°ƒèŠ‚**: å¯è°ƒèŠ‚æ¸©åº¦å’Œè¾“å‡ºé•¿åº¦ç­‰ç”Ÿæˆå‚æ•°
        """)
        
        # ç³»ç»ŸçŠ¶æ€æ˜¾ç¤º
        with gr.Row():
            system_status = gr.Textbox(
                label="ğŸ”§ ç³»ç»ŸçŠ¶æ€",
                value=translator.get_system_status(),
                interactive=False,
                show_copy_button=False
            )
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ ç¿»è¯‘å‚æ•°")
                
                task_type = gr.Dropdown(
                    choices=["ç®€å•ç¿»è¯‘", "è¯¦ç»†è§£é‡Šç¿»è¯‘", "æŠ€æœ¯æœ¯è¯­ç¿»è¯‘", "è¯—æ„è¡¨è¾¾ç¿»è¯‘"],
                    value="ç®€å•ç¿»è¯‘",
                    label="ä»»åŠ¡ç±»å‹",
                    info="é€‰æ‹©ç¿»è¯‘ä»»åŠ¡çš„ç±»å‹"
                )
                
                temperature = gr.Slider(
                    minimum=0,
                    maximum=1,
                    value=0,
                    step=0.1,
                    label="Temperature",
                    info="æ§åˆ¶ç¿»è¯‘çš„éšæœºæ€§ï¼Œ0=æœ€ç¡®å®šï¼Œ1=æœ€éšæœº"
                )
                
                max_tokens = gr.Slider(
                    minimum=64,
                    maximum=1024,
                    value=512,
                    step=64,
                    label="æœ€å¤§è¾“å‡ºé•¿åº¦",
                    info="é™åˆ¶ç¿»è¯‘ç»“æœçš„æœ€å¤§é•¿åº¦"
                )
                
                gr.Markdown("### ğŸ“Š æ¨¡å‹ä¿¡æ¯")
                gr.Markdown("""
                **Seed-X-Instruct-7B**: æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼Œæ“…é•¿éµå¾ªç¿»è¯‘æŒ‡ä»¤
                
                **Seed-X-PPO-7B**: PPOå¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œæä¾›æ›´è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹
                """)
            
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ“ ç¿»è¯‘å¯¹æ¯”")
                
                input_text = gr.Textbox(
                    label="è¾“å…¥è‹±æ–‡æ–‡æœ¬",
                    placeholder="è¯·è¾“å…¥è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬ï¼Œä¾‹å¦‚ï¼šMay the force be with you",
                    lines=4
                )
                
                with gr.Row():
                    translate_btn = gr.Button("ğŸš€ å¼€å§‹ç¿»è¯‘å¯¹æ¯”", variant="primary", size="lg")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
                
                output_text = gr.Markdown(
                    value="å‡†å¤‡å°±ç»ªï¼Œè¯·è¾“å…¥æ–‡æœ¬å¼€å§‹ç¿»è¯‘...",
                    show_copy_button=True
                )
        
        # é¢„è®¾ç¤ºä¾‹
        gr.Markdown("### ğŸ’¡ å¿«é€Ÿæµ‹è¯•ç¤ºä¾‹")
        with gr.Row():
            examples_data = [
                ("May the force be with you", "ç®€å•ç¿»è¯‘"),
                ("Machine learning algorithms can process vast amounts of data", "æŠ€æœ¯æœ¯è¯­ç¿»è¯‘"), 
                ("The stars shine brightest in the darkest night", "è¯—æ„è¡¨è¾¾ç¿»è¯‘"),
                ("Life is like a box of chocolates", "è¯—æ„è¡¨è¾¾ç¿»è¯‘"),
                ("Artificial intelligence is transforming our world", "æŠ€æœ¯æœ¯è¯­ç¿»è¯‘"),
                ("Please explain the meaning behind this quote", "è¯¦ç»†è§£é‡Šç¿»è¯‘")
            ]
            
            # åˆ›å»ºç¤ºä¾‹æŒ‰é’®
            for i in range(0, len(examples_data), 2):
                with gr.Row():
                    for j in range(2):
                        if i + j < len(examples_data):
                            text, task = examples_data[i + j]
                            btn = gr.Button(f"ğŸ“ {text[:35]}...", size="sm")
                            btn.click(
                                fn=lambda t=text, ta=task: (t, ta),
                                outputs=[input_text, task_type]
                            )
        
        # ç»‘å®šäº‹ä»¶
        translate_btn.click(
            fn=translator.compare_models,
            inputs=[input_text, task_type, temperature, max_tokens],
            outputs=[output_text]
        )
        
        clear_btn.click(
            fn=lambda: ("", "å‡†å¤‡å°±ç»ªï¼Œè¯·è¾“å…¥æ–‡æœ¬å¼€å§‹ç¿»è¯‘..."),
            outputs=[input_text, output_text]
        )
        
        # å®šæœŸæ›´æ–°ç³»ç»ŸçŠ¶æ€
        def update_status():
            return translator.get_system_status()
        
        # åœ¨é¡µé¢åŠ è½½æ—¶æ›´æ–°çŠ¶æ€
        iface.load(fn=update_status, outputs=[system_status])
    
    return iface

# å¯åŠ¨ç•Œé¢
if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ Seed-X ç¿»è¯‘å¯¹æ¯”ç³»ç»Ÿ...")
    print("ï¿½ æ­£åœ¨é¢„åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
    
    # åˆ›å»ºç•Œé¢ï¼ˆæ­¤æ—¶ä¼šè‡ªåŠ¨åŠ è½½æ¨¡å‹ï¼‰
    demo = create_interface()
    
    print("ğŸŒ å¯åŠ¨ Web æœåŠ¡...")
    print("ğŸ“ è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®æ˜¾ç¤ºçš„åœ°å€")
    
    # å¯åŠ¨æœåŠ¡
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,       # é»˜è®¤ç«¯å£
        share=False,            # ä¸åˆ›å»ºå…¬å¼€é“¾æ¥
        show_error=True,        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        quiet=False             # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    )
