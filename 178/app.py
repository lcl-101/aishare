# -*- coding: utf-8 -*-
# ==============================================================================
# arxive: https://arxiv.org/abs/2507.03905
# GitHUb: https://github.com/antgroup/echomimic_v3
# Project Page: https://antgroup.github.io/ai/echomimic_v3/
# ==============================================================================

import os
import math
import datetime
from functools import partial
import glob

import numpy as np
import torch
from PIL import Image
from omegaconf import OmegaConf
from transformers import AutoTokenizer, Wav2Vec2Model, Wav2Vec2Processor
from moviepy import VideoFileClip, AudioFileClip
import librosa

# Custom modules
from diffusers import FlowMatchEulerDiscreteScheduler

from src.dist import set_multi_gpus_devices
from src.wan_vae import AutoencoderKLWan
from src.wan_image_encoder import  CLIPModel
from src.wan_text_encoder import  WanT5EncoderModel
from src.wan_transformer3d_audio import WanTransformerAudioMask3DModel
from src.pipeline_wan_fun_inpaint_audio import WanFunInpaintAudioPipeline

from src.utils import (
    filter_kwargs,
    get_image_to_video_latent3,
    save_videos_grid,
)
from src.fm_solvers import FlowDPMSolverMultistepScheduler
from src.fm_solvers_unipc import FlowUniPCMultistepScheduler
from src.cache_utils import get_teacache_coefficients

from src.face_detect import get_mask_coord

import argparse
import gradio as gr
import random
import gc

parser = argparse.ArgumentParser() 
parser.add_argument("--server_name", type=str, default="0.0.0.0", help="IPåœ°å€ï¼Œå±€åŸŸç½‘è®¿é—®æ”¹ä¸º0.0.0.0")
parser.add_argument("--server_port", type=int, default=7860, help="ä½¿ç”¨ç«¯å£")
parser.add_argument("--share", action="store_true", help="æ˜¯å¦å¯ç”¨gradioå…±äº«")
parser.add_argument("--mcp_server", action="store_true", help="æ˜¯å¦å¯ç”¨mcpæœåŠ¡")
args = parser.parse_args()


# --------------------- Configuration ---------------------
class Config:
    def __init__(self):
        # General settings
        self.ulysses_degree = 1
        self.ring_degree = 1
        self.fsdp_dit = False

        # Pipeline parameters
        self.num_skip_start_steps = 5
        self.teacache_offload = False  # æ”¹ä¸ºä¸å®˜æ–¹ä¸€è‡´
        self.cfg_skip_ratio = 0
        self.enable_riflex = False
        self.riflex_k = 6

        # Paths
        self.config_path = "config/config.yaml"
        self.model_name = "models/Wan2.1-Fun-V1.1-1.3B-InP"
        self.transformer_path = "models/transformer/diffusion_pytorch_model.safetensors"

        # Sampler and audio settings
        self.sampler_name = "Flow_DPM++"
        self.audio_scale = 1.0
        self.enable_teacache = False  # æ”¹ä¸ºä¸å®˜æ–¹ä¸€è‡´
        self.teacache_threshold = 0.1
        self.shift = 5.0
        self.use_un_ip_mask = False

        self.partial_video_length = 49
        self.overlap_video_length = 8
        self.neg_scale = 1.5
        self.neg_steps = 2
        self.guidance_scale = 4.0  # æ”¹ä¸ºä¸å®˜æ–¹ä¸€è‡´ 4.0 ~ 6.0
        self.audio_guidance_scale = 2.5 #2.0 ~ 3.0
        self.use_dynamic_cfg = True
        self.use_dynamic_acfg = True
        self.seed = 43
        self.num_inference_steps = 20
        self.lora_weight = 1.0

        # Model settings
        self.weight_dtype = torch.bfloat16
        self.sample_size = [768, 768]
        self.fps = 25

        # Test data paths
        self.wav2vec_model_dir = "models/wav2vec2-base-960h"


# --------------------- Helper Functions ---------------------
def load_wav2vec_models(wav2vec_model_dir):
    """Load Wav2Vec models for audio feature extraction."""
    processor = Wav2Vec2Processor.from_pretrained(wav2vec_model_dir)
    # ä¸å®˜æ–¹ä¿æŒä¸€è‡´ï¼Œä¸ç§»åŠ¨åˆ°GPU
    model = Wav2Vec2Model.from_pretrained(wav2vec_model_dir).eval()
    model.requires_grad_(False)
    return processor, model


def extract_audio_features(audio_path, processor, model):
    """Extract audio features using Wav2Vec."""
    sr = 16000
    audio_segment, sample_rate = librosa.load(audio_path, sr=sr)
    input_values = processor(audio_segment, sampling_rate=sample_rate, return_tensors="pt").input_values
    # ä¸å®˜æ–¹ä¿æŒä¸€è‡´ï¼Œæ ¹æ®æ¨¡å‹è®¾å¤‡åŠ¨æ€å¤„ç†
    if hasattr(model, 'device'):
        input_values = input_values.to(model.device)
    features = model(input_values).last_hidden_state
    return features.squeeze(0)


def get_sample_size(image, default_size):
    """Calculate the sample size based on the input image dimensions."""
    width, height = image.size
    original_area = width * height
    default_area = default_size[0] * default_size[1]

    if default_area < original_area:
        ratio = math.sqrt(original_area / default_area)
        width = width / ratio // 16 * 16
        height = height / ratio // 16 * 16
    else:
        width = width // 16 * 16
        height = height // 16 * 16

    return int(height), int(width)


def get_ip_mask(coords):
    y1, y2, x1, x2, h, w = coords
    Y, X = torch.meshgrid(torch.arange(h), torch.arange(w), indexing='ij')
    mask = (Y.unsqueeze(-1) >= y1) & (Y.unsqueeze(-1) < y2) & (X.unsqueeze(-1) >= x1) & (X.unsqueeze(-1) < x2)
    
    mask = mask.reshape(-1)
    return mask.float()


def get_examples():
    """è·å–æ¼”ç¤ºæ ·ä¾‹ï¼ŒåŸºäº datasets ç›®å½•ä¸­çš„æ–‡ä»¶"""
    examples = []
    datasets_path = "datasets/echomimicv3_demos"
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(datasets_path):
        print(f"Warning: Demo datasets directory not found: {datasets_path}")
        return examples
    
    try:
        # è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ‰©å±•å
        audio_extensions = ["*.WAV", "*.wav"]
        audio_files = []
        for ext in audio_extensions:
            audio_files.extend(glob.glob(os.path.join(datasets_path, "audios", ext)))
        
        # è¿‡æ»¤æ‰ééŸ³é¢‘æ–‡ä»¶ï¼Œç¡®ä¿åªå¤„ç†çœŸæ­£çš„éŸ³é¢‘æ–‡ä»¶
        valid_audio_files = []
        for audio_file in audio_files:
            filename = os.path.basename(audio_file)
            # æ’é™¤ä¸€äº›æ˜æ˜¾ä¸æ˜¯éŸ³é¢‘çš„æ–‡ä»¶
            if not filename.lower().endswith(('.wav', '.WAV')):
                continue
            # ç¡®ä¿æ–‡ä»¶ç¡®å®å­˜åœ¨ä¸”ä¸æ˜¯ç›®å½•
            if os.path.isfile(audio_file):
                valid_audio_files.append(audio_file)
        
        # åŠ è½½æ‰€æœ‰æœ‰æ•ˆçš„ç¤ºä¾‹
        audio_files = sorted(set(valid_audio_files))
        print(f"Found {len(audio_files)} audio files for examples")
        
        for audio_file in audio_files:
            # æå–æ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰
            base_name = os.path.splitext(os.path.basename(audio_file))[0]
            
            # æŸ¥æ‰¾å¯¹åº”çš„å›¾ç‰‡æ–‡ä»¶ï¼Œä½¿ç”¨å®˜æ–¹çš„æ‰©å±•åé¡ºåº
            img_extensions = ['png', 'jpeg', 'jpg']  # ä¸å®˜æ–¹ä¿æŒä¸€è‡´
            img_file = None
            for ext in img_extensions:
                potential_img = os.path.join(datasets_path, "imgs", f"{base_name}.{ext}")
                if os.path.exists(potential_img):
                    img_file = potential_img
                    break
            
            # æŸ¥æ‰¾å¯¹åº”çš„ prompt æ–‡ä»¶
            prompt_file = os.path.join(datasets_path, "prompts", f"{base_name}.txt")
            prompt_text = ""
            if os.path.exists(prompt_file):
                try:
                    with open(prompt_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        # é™åˆ¶æç¤ºè¯é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                        if len(content) > 500:
                            content = content[:500] + "..."
                        prompt_text = content
                except Exception as e:
                    print(f"Warning: Failed to read prompt file {prompt_file}: {e}")
                    prompt_text = ""
            
            # å¦‚æœæ‰¾åˆ°äº†å¯¹åº”çš„å›¾ç‰‡å’ŒéŸ³é¢‘æ–‡ä»¶ï¼Œæ·»åŠ åˆ°ç¤ºä¾‹ä¸­
            if img_file and os.path.exists(img_file):
                examples.append([
                    img_file,  # image
                    audio_file,  # audio
                    prompt_text,  # prompt
                    "Gesture is bad. Gesture is unclear. Strange and twisted hands. Bad hands. Bad fingers. Unclear and blurry hands. æ‰‹éƒ¨å¿«é€Ÿæ‘†åŠ¨, æ‰‹æŒ‡é¢‘ç¹æŠ½æ, å¤¸å¼ æ‰‹åŠ¿, é‡å¤æœºæ¢°æ€§åŠ¨ä½œ.",  # negative_prompt
                    -1  # seed_param
                ])
                print(f"Added example: {base_name}")
            else:
                print(f"Skipped {base_name}: missing image file")
                
    except Exception as e:
        print(f"Error loading demo examples: {e}")
        return []
    
    return examples


# Initialize configuration
config = Config()

# Set up multi-GPU devices
device = set_multi_gpus_devices(config.ulysses_degree, config.ring_degree)

# Load configuration file
cfg = OmegaConf.load(config.config_path)

# Load models
transformer = WanTransformerAudioMask3DModel.from_pretrained(
    os.path.join(config.model_name, cfg['transformer_additional_kwargs'].get('transformer_subpath', 'transformer')),
    transformer_additional_kwargs=OmegaConf.to_container(cfg['transformer_additional_kwargs']),
    torch_dtype=config.weight_dtype,
)
if config.transformer_path is not None:
    if config.transformer_path.endswith("safetensors"):
        from safetensors.torch import load_file, safe_open
        state_dict = load_file(config.transformer_path)
    else:
        state_dict = torch.load(config.transformer_path)
    state_dict = state_dict["state_dict"] if "state_dict" in state_dict else state_dict
    missing, unexpected = transformer.load_state_dict(state_dict, strict=False)
    print(f"Missing keys: {len(missing)}, Unexpected keys: {len(unexpected)}")
vae = AutoencoderKLWan.from_pretrained(
    os.path.join(config.model_name, cfg['vae_kwargs'].get('vae_subpath', 'vae')),
    additional_kwargs=OmegaConf.to_container(cfg['vae_kwargs']),
).to(config.weight_dtype)

tokenizer = AutoTokenizer.from_pretrained(
    os.path.join(config.model_name, cfg['text_encoder_kwargs'].get('tokenizer_subpath', 'tokenizer')),
)

text_encoder = WanT5EncoderModel.from_pretrained(
    os.path.join(config.model_name, cfg['text_encoder_kwargs'].get('text_encoder_subpath', 'text_encoder')),
    additional_kwargs=OmegaConf.to_container(cfg['text_encoder_kwargs']),
    torch_dtype=config.weight_dtype,
).eval()

clip_image_encoder = CLIPModel.from_pretrained(
    os.path.join(config.model_name, cfg['image_encoder_kwargs'].get('image_encoder_subpath', 'image_encoder')),
).to(config.weight_dtype).eval()

# Load scheduler
scheduler_cls = {
    "Flow": FlowMatchEulerDiscreteScheduler,
    "Flow_Unipc": FlowUniPCMultistepScheduler,
    "Flow_DPM++": FlowDPMSolverMultistepScheduler,
}[config.sampler_name]
scheduler = scheduler_cls(**filter_kwargs(scheduler_cls, OmegaConf.to_container(cfg['scheduler_kwargs'])))

# Create pipeline
pipeline = WanFunInpaintAudioPipeline(
    transformer=transformer,
    vae=vae,
    tokenizer=tokenizer,
    text_encoder=text_encoder,
    scheduler=scheduler,
    clip_image_encoder=clip_image_encoder,
)
pipeline.to(device=device)

# Enable TeaCache if required
if config.enable_teacache:
    coefficients = get_teacache_coefficients(config.model_name)
    pipeline.transformer.enable_teacache(
        coefficients, config.num_inference_steps, config.teacache_threshold,
        num_skip_start_steps=config.num_skip_start_steps, offload=config.teacache_offload
    )# Load Wav2Vec models
print("Loading Wav2Vec models...")
wav2vec_processor, wav2vec_model = load_wav2vec_models(config.wav2vec_model_dir)

# é¢„å…ˆç”Ÿæˆç¤ºä¾‹åˆ—è¡¨ï¼Œé¿å…åœ¨ç•Œé¢ä¸­é‡å¤è°ƒç”¨
print("Loading demo examples...")
DEMO_EXAMPLES = get_examples()
print(f"âœ… Loaded {len(DEMO_EXAMPLES)} demo examples")

if len(DEMO_EXAMPLES) == 0:
    print("âš ï¸  No demo examples found. Please check the datasets directory structure.")


def generate(
    image,
    audio,
    prompt,
    negative_prompt,
    seed_param,
    # æ·»åŠ é«˜çº§å‚æ•°
    audio_guidance_scale,
    guidance_scale,
    num_inference_steps,
    teacache_threshold,
    partial_video_length,
    enable_teacache,
    progress=gr.Progress()
):
    # è¾“å…¥éªŒè¯
    if image is None:
        raise gr.Error("è¯·ä¸Šä¼ å‚è€ƒå›¾ç‰‡ï¼")
    if audio is None:
        raise gr.Error("è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼")
    
    progress(0, desc="åˆå§‹åŒ–...")
    
    # åŠ¨æ€é…ç½®TeaCache
    if enable_teacache and not config.enable_teacache:
        # å¦‚æœç”¨æˆ·å¯ç”¨äº†TeaCacheä½†é»˜è®¤é…ç½®å…³é—­ï¼Œéœ€è¦åŠ¨æ€å¯ç”¨
        coefficients = get_teacache_coefficients(config.model_name)
        pipeline.transformer.enable_teacache(
            coefficients, int(num_inference_steps), float(teacache_threshold),
            num_skip_start_steps=config.num_skip_start_steps, offload=config.teacache_offload
        )
    elif not enable_teacache and config.enable_teacache:
        # å¦‚æœç”¨æˆ·å…³é—­äº†TeaCacheä½†é»˜è®¤é…ç½®å¯ç”¨ï¼Œéœ€è¦åŠ¨æ€å…³é—­
        if hasattr(pipeline.transformer, 'disable_teacache'):
            pipeline.transformer.disable_teacache()
    
    if seed_param<0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    save_path = "outputs"
    os.makedirs(save_path, exist_ok=True)

    progress(0.1, desc="è®¾ç½®éšæœºç§å­...")
    # Process test cases
    generator = torch.Generator(device=device).manual_seed(seed)

    progress(0.2, desc="å¤„ç†å‚è€ƒå›¾ç‰‡...")
    # Load reference image and prompt
    ref_img = Image.open(image).convert("RGB")

    y1, y2, x1, x2, h_, w_ = get_mask_coord(image)

    progress(0.3, desc="æå–éŸ³é¢‘ç‰¹å¾...")
    # Extract audio features - éªŒè¯éŸ³é¢‘æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(audio):
        # å°è¯•ä¸åŒçš„æ‰©å±•åï¼Œä¸å®˜æ–¹é€»è¾‘ä¸€è‡´
        audio_alt = audio.replace("WAV", "wav") if "WAV" in audio else audio.replace("wav", "WAV")
        if os.path.exists(audio_alt):
            audio = audio_alt
        else:
            raise gr.Error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio}")
    
    audio_clip = AudioFileClip(audio)
    audio_features = extract_audio_features(audio, wav2vec_processor, wav2vec_model)
    audio_embeds = audio_features.unsqueeze(0).to(device=device, dtype=config.weight_dtype)

    progress(0.4, desc="è®¡ç®—è§†é¢‘å‚æ•°...")
    # Calculate video length and latent frames
    video_length = int(audio_clip.duration * config.fps)
    video_length = (
        int((video_length - 1) // vae.config.temporal_compression_ratio * vae.config.temporal_compression_ratio) + 1
        if video_length != 1 else 1
    )
    latent_frames = (video_length - 1) // vae.config.temporal_compression_ratio + 1
    
    print(f"è§†é¢‘æ€»é•¿åº¦: {video_length} å¸§ ({audio_clip.duration:.2f}ç§’)")

    if config.enable_riflex:
        pipeline.transformer.enable_riflex(k = config.riflex_k, L_test = latent_frames)

    progress(0.5, desc="å‡†å¤‡å›¾åƒå¤„ç†...")
    # Adjust sample size and create IP mask
    sample_height, sample_width = get_sample_size(ref_img, config.sample_size)
    downratio = math.sqrt(sample_height * sample_width / h_ / w_)
    coords = (
        y1 * downratio // 16, y2 * downratio // 16,
        x1 * downratio // 16, x2 * downratio // 16,
        sample_height // 16, sample_width // 16,
    )
    ip_mask = get_ip_mask(coords).unsqueeze(0)
    ip_mask = torch.cat([ip_mask]*3).to(device=device, dtype=config.weight_dtype)

    # æ¢å¤ä½¿ç”¨å®˜æ–¹é…ç½®
    # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„ç‰‡æ®µé•¿åº¦
    user_partial_video_length = int(partial_video_length)
    partial_video_length_adjusted = int((user_partial_video_length - 1) // vae.config.temporal_compression_ratio * vae.config.temporal_compression_ratio) + 1 if video_length != 1 else 1
    latent_frames = (partial_video_length_adjusted - 1) // vae.config.temporal_compression_ratio + 1

    # get clip image
    _, _, clip_image = get_image_to_video_latent3(ref_img, None, video_length=partial_video_length_adjusted, sample_size=[sample_height, sample_width])

    progress(0.6, desc="å¼€å§‹ç”Ÿæˆè§†é¢‘...")
    # Generate video in chunks
    init_frames = 0
    last_frames = init_frames + partial_video_length_adjusted
    new_sample = None
    
    current_chunk = 0

    while init_frames < video_length:
        current_chunk += 1
        # åŸºäºå‰©ä½™å¸§æ•°åŠ¨æ€è®¡ç®—è¿›åº¦
        progress_ratio = min(0.9, 0.6 + 0.3 * (init_frames / video_length))
        remaining_frames = video_length - init_frames
        progress(progress_ratio, desc=f"å¤„ç†è§†é¢‘ç‰‡æ®µ {current_chunk} (å‰©ä½™çº¦ {remaining_frames/config.fps:.1f}ç§’)...")
        
        if last_frames >= video_length:
            partial_video_length_adjusted = video_length - init_frames
            partial_video_length_adjusted = (
                int((partial_video_length_adjusted - 1) // vae.config.temporal_compression_ratio * vae.config.temporal_compression_ratio) + 1
                if video_length != 1 else 1
            )
            latent_frames = (partial_video_length_adjusted - 1) // vae.config.temporal_compression_ratio + 1

            if partial_video_length_adjusted <= 0:
                break

        input_video, input_video_mask, _ = get_image_to_video_latent3(
            ref_img, None, video_length=partial_video_length_adjusted, sample_size=[sample_height, sample_width]
        )

        partial_audio_embeds = audio_embeds[:, init_frames * 2 : (init_frames + partial_video_length_adjusted) * 2]

        sample = pipeline(
            prompt,
            num_frames            = partial_video_length_adjusted,
            negative_prompt       = negative_prompt,
            audio_embeds          = partial_audio_embeds,
            audio_scale           = config.audio_scale,
            ip_mask               = ip_mask,
            use_un_ip_mask        = config.use_un_ip_mask,
            height                = sample_height,
            width                 = sample_width,
            generator             = generator,
            neg_scale             = config.neg_scale,
            neg_steps             = config.neg_steps,
            use_dynamic_cfg       = config.use_dynamic_cfg,
            use_dynamic_acfg      = config.use_dynamic_acfg,
            guidance_scale        = float(guidance_scale),
            audio_guidance_scale  = float(audio_guidance_scale),
            num_inference_steps   = int(num_inference_steps),
            video                 = input_video,
            mask_video            = input_video_mask,
            clip_image            = clip_image,
            cfg_skip_ratio        = config.cfg_skip_ratio,
            shift                 = config.shift,
        ).videos
        
        if init_frames != 0:
            mix_ratio = torch.from_numpy(
                np.array([float(i) / float(config.overlap_video_length) for i in range(config.overlap_video_length)], np.float32)
            ).unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)
            new_sample[:, :, -config.overlap_video_length:] = (
                new_sample[:, :, -config.overlap_video_length:] * (1 - mix_ratio) +
                sample[:, :, :config.overlap_video_length] * mix_ratio
            )
            new_sample = torch.cat([new_sample, sample[:, :, config.overlap_video_length:]], dim=2)
            sample = new_sample
        else:
            new_sample = sample

        if last_frames >= video_length:
            break

        ref_img = [
            Image.fromarray(
                (sample[0, :, i].transpose(0, 1).transpose(1, 2) * 255).numpy().astype(np.uint8)
            ) for i in range(-config.overlap_video_length, 0)
        ]

        init_frames += partial_video_length_adjusted - config.overlap_video_length
        last_frames = init_frames + partial_video_length_adjusted

    progress(0.9, desc="ä¿å­˜è§†é¢‘æ–‡ä»¶...")
    # Save generated video
    video_path = os.path.join(save_path, f"{timestamp}.mp4")
    video_audio_path = os.path.join(save_path, f"{timestamp}_audio.mp4")

    print ("final sample shape:",sample.shape)
    video_length = sample.shape[2]
    print ("final length:",video_length)

    save_videos_grid(sample[:, :, :video_length], video_path, fps=config.fps)

    progress(0.95, desc="åˆæˆéŸ³é¢‘...")
    video_clip = VideoFileClip(video_path)
    audio_clip = audio_clip.subclipped(0, video_length / config.fps)
    video_clip = video_clip.with_audio(audio_clip)
    video_clip.write_videofile(video_audio_path, codec="libx264", audio_codec="aac", threads=2)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œä¸å®˜æ–¹é€»è¾‘ä¿æŒä¸€è‡´
    try:
        import shutil
        if os.path.exists(video_path):
            os.remove(video_path)  # ä½¿ç”¨æ›´å®‰å…¨çš„åˆ é™¤æ–¹å¼æ›¿ä»£ os.system
    except Exception as e:
        print(f"Warning: Failed to clean temporary file {video_path}: {e}")

    progress(1.0, desc="å®Œæˆï¼")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()

    return video_audio_path, seed


with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
            <div style="text-align: center; padding: 20px;">
                <h1 style="font-size: 36px; color: #2c3e50; margin-bottom: 10px;">ğŸ­ EchoMimicV3</h1>
                <p style="font-size: 18px; color: #7f8c8d;">Audio-Driven Portrait Video Generation with Examples</p>
            </div>
            """)
    
    with gr.Row(equal_height=True):
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“ è¾“å…¥è®¾ç½®")
            
            with gr.Group():
                image = gr.Image(
                    label="ğŸ“· å‚è€ƒå›¾ç‰‡", 
                    type="filepath", 
                    height=300,
                    show_label=True
                )
                audio = gr.Audio(
                    label="ğŸµ éŸ³é¢‘æ–‡ä»¶", 
                    type="filepath",
                    show_label=True
                )
            
            with gr.Group():
                prompt = gr.Textbox(
                    label="âœ¨ æç¤ºè¯", 
                    value="",
                    placeholder="è¯·è¾“å…¥æè¿°è§†é¢‘å†…å®¹çš„æç¤ºè¯...",
                    lines=3
                )
                negative_prompt = gr.Textbox(
                    label="ğŸš« è´Ÿé¢æç¤ºè¯", 
                    value="Gesture is bad. Gesture is unclear. Strange and twisted hands. Bad hands. Bad fingers. Unclear and blurry hands. æ‰‹éƒ¨å¿«é€Ÿæ‘†åŠ¨, æ‰‹æŒ‡é¢‘ç¹æŠ½æ, å¤¸å¼ æ‰‹åŠ¿, é‡å¤æœºæ¢°æ€§åŠ¨ä½œ.",
                    lines=3
                )
                seed_param = gr.Number(
                    label="ğŸ² éšæœºç§å­ (-1ä¸ºéšæœº)", 
                    value=-1,
                    precision=0
                )
            
            # é«˜çº§å‚æ•°æ§åˆ¶
            with gr.Accordion("ğŸ”§ é«˜çº§å‚æ•°è®¾ç½®", open=False):
                gr.Markdown("""
                **å‚æ•°è¯´æ˜ï¼š**
                - **Audio CFG**: éŸ³é¢‘å¼•å¯¼å¼ºåº¦ï¼Œ2-3æœ€ä½³ã€‚å¢åŠ å€¼æé«˜å”‡åŒæ­¥ï¼Œå‡å°‘å€¼æé«˜è§†è§‰è´¨é‡
                - **Text CFG**: æ–‡æœ¬å¼•å¯¼å¼ºåº¦ï¼Œ3-6æœ€ä½³ã€‚å¢åŠ å€¼æé«˜æç¤ºè¯è·Ÿéšï¼Œå‡å°‘å€¼æé«˜è§†è§‰è´¨é‡  
                - **é‡‡æ ·æ­¥æ•°**: äººå¤´å¯¹è¯5æ­¥ï¼Œå…¨èº«å¯¹è¯15-25æ­¥
                - **TeaCache**: ä¼˜åŒ–èŒƒå›´0-0.1ï¼Œå¯åŠ é€Ÿæ¨ç†
                - **è§†é¢‘ç‰‡æ®µé•¿åº¦**: è¾ƒå°å€¼å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼Œé•¿è§†é¢‘æ¨è81ã€65æˆ–æ›´å°
                """)
                
                with gr.Row():
                    audio_guidance_scale = gr.Slider(
                        minimum=1.0, maximum=5.0, value=2.5, step=0.1,
                        label="ğŸµ Audio CFG (éŸ³é¢‘å¼•å¯¼å¼ºåº¦)",
                        info="æ¨èèŒƒå›´: 2.0-3.0"
                    )
                    guidance_scale = gr.Slider(
                        minimum=2.0, maximum=8.0, value=4.0, step=0.5,
                        label="ğŸ“ Text CFG (æ–‡æœ¬å¼•å¯¼å¼ºåº¦)", 
                        info="æ¨èèŒƒå›´: 3.0-6.0"
                    )
                
                with gr.Row():
                    num_inference_steps = gr.Slider(
                        minimum=5, maximum=50, value=20, step=5,
                        label="ğŸ”„ é‡‡æ ·æ­¥æ•°",
                        info="äººå¤´å¯¹è¯:5æ­¥, å…¨èº«å¯¹è¯:15-25æ­¥"
                    )
                    partial_video_length = gr.Slider(
                        minimum=25, maximum=97, value=49, step=8,
                        label="ğŸ“¹ è§†é¢‘ç‰‡æ®µé•¿åº¦",
                        info="è¾ƒå°å€¼å‡å°‘æ˜¾å­˜ä½¿ç”¨"
                    )
                
                with gr.Row():
                    enable_teacache = gr.Checkbox(
                        value=False,
                        label="âš¡ å¯ç”¨ TeaCache åŠ é€Ÿ",
                        info="å¯æå‡æ¨ç†é€Ÿåº¦"
                    )
                    teacache_threshold = gr.Slider(
                        minimum=0.0, maximum=0.3, value=0.1, step=0.01,
                        label="ğŸ¯ TeaCache é˜ˆå€¼",
                        info="æ¨èèŒƒå›´: 0.0-0.1"
                    )
            
            gr.Markdown("""
            **ğŸ’¡ æç¤ºï¼š**
            - éŸ³é¢‘è¶Šé•¿ï¼Œç”Ÿæˆæ—¶é—´è¶Šä¹…ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆ†ç‰‡æ®µå¤„ç†
            - å»ºè®®éŸ³é¢‘æ—¶é•¿æ§åˆ¶åœ¨ 10-30 ç§’å†…ä»¥è·å¾—æœ€ä½³ä½“éªŒ
            - ç”Ÿæˆè¿‡ç¨‹ä¸­æ˜¾ç¤ºçš„"ç‰‡æ®µ"æ˜¯æŒ‡é•¿è§†é¢‘çš„åˆ†å—å¤„ç†ï¼Œä¸æ˜¯å¤šä¸ªè§†é¢‘
            """)
            
            generate_button = gr.Button(
                "ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘", 
                variant='primary',
                size='lg'
            )
            
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“º ç”Ÿæˆç»“æœ")
            
            with gr.Group():
                video_output = gr.Video(
                    label="ğŸ¥ ç”Ÿæˆçš„è§†é¢‘", 
                    interactive=False,
                    height=400
                )
                seed_output = gr.Textbox(
                    label="ğŸ”¢ ä½¿ç”¨çš„ç§å­å€¼",
                    interactive=False
                )
    
    # ç¤ºä¾‹åŒºåŸŸæ”¾åœ¨åº•éƒ¨ï¼Œå æ®å…¨å®½
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ­ æ¼”ç¤ºæ ·ä¾‹")
            gr.Markdown("ç‚¹å‡»ä¸‹æ–¹ä»»æ„æ ·ä¾‹å¿«é€Ÿå¡«å……è¾“å…¥å­—æ®µï¼Œç„¶åç‚¹å‡»ç”ŸæˆæŒ‰é’®å¼€å§‹å¤„ç†")
            
            if DEMO_EXAMPLES:
                gr.Markdown(f"**ğŸ“Š å…±åŠ è½½ {len(DEMO_EXAMPLES)} ä¸ªæ¼”ç¤ºæ ·ä¾‹**")
                example_component = gr.Examples(
                    examples=DEMO_EXAMPLES,
                    inputs=[image, audio, prompt, negative_prompt, seed_param],
                    label=None,
                    examples_per_page=10  # æ¯é¡µæ˜¾ç¤º10ä¸ªç¤ºä¾‹
                )
            else:
                gr.Markdown("*âŒ æš‚æ— å¯ç”¨çš„æ¼”ç¤ºæ ·ä¾‹*")

    gr.on(
        triggers=[generate_button.click],
        fn=generate,
        inputs=[
            image,
            audio,
            prompt,
            negative_prompt,
            seed_param,
            audio_guidance_scale,
            guidance_scale,
            num_inference_steps,
            teacache_threshold,
            partial_video_length,
            enable_teacache,
        ],
        outputs=[video_output, seed_output]
    )
        

if __name__ == "__main__": 
    demo.launch(
        server_name=args.server_name, 
        server_port=args.server_port,
        share=args.share, 
        mcp_server=args.mcp_server,
        inbrowser=True,
    )
