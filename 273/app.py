"""
Molmo2 Gradio Web Application

A multi-tab web interface for interacting with Molmo2-8B model supporting:
- General Video QA
- Pointing Video QA
- Tracking Video QA
- Multi-image QA
- Multi-Image Point QA
"""

import os
import re
import torch
import gradio as gr
import requests
from PIL import Image, ImageDraw, ImageFont
from io import BytesIO
from pathlib import Path
from typing import Optional
from dataclasses import dataclass

from transformers import AutoProcessor, AutoModelForImageTextToText

# ============================================================================
# Constants and Configuration
# ============================================================================

MODEL_PATH = "./checkpoints/Molmo2-8B"
EXAMPLES_DIR = "./examples"

# Example URLs to download
EXAMPLE_URLS = {
    "videos": {
        "penguins": "https://storage.googleapis.com/oe-training-public/demo_videos/many_penguins.mp4",
        "basketball": "https://storage.googleapis.com/oe-training-public/demo_videos/arena_basketball.mp4",
    },
    "images": {
        "dog": "https://picsum.photos/id/237/536/354",
        "cherry_blossom": "https://vllm-public-assets.s3.us-west-2.amazonaws.com/vision_model_images/cherry_blossom.jpg",
        "boat1": "https://storage.googleapis.com/oe-training-public/demo_images/boat1.jpeg",
        "boat2": "https://storage.googleapis.com/oe-training-public/demo_images/boat2.jpeg",
    }
}

# ============================================================================
# Regex Patterns for Point Extraction
# ============================================================================

COORD_REGEX = re.compile(r"<(?:points|tracks).*? coords=\"([0-9\t:;, .]+)\"/?>")
FRAME_REGEX = re.compile(r"(?:^|\t|:|,|;)([0-9\.]+) ([0-9\. ]+)")
POINTS_REGEX = re.compile(r"([0-9]+) ([0-9]{3,4}) ([0-9]{3,4})")


# ============================================================================
# Example Download Functions
# ============================================================================

def download_examples():
    """Download example files at startup."""
    os.makedirs(EXAMPLES_DIR, exist_ok=True)
    os.makedirs(os.path.join(EXAMPLES_DIR, "videos"), exist_ok=True)
    os.makedirs(os.path.join(EXAMPLES_DIR, "images"), exist_ok=True)
    
    downloaded_files = {"videos": {}, "images": {}}
    
    # Download videos
    for name, url in EXAMPLE_URLS["videos"].items():
        save_path = os.path.join(EXAMPLES_DIR, "videos", f"{name}.mp4")
        if not os.path.exists(save_path):
            print(f"Downloading {name} video...")
            try:
                response = requests.get(url, stream=True)
                response.raise_for_status()
                with open(save_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                print(f"  Saved to {save_path}")
            except Exception as e:
                print(f"  Error downloading {name}: {e}")
                continue
        downloaded_files["videos"][name] = save_path
    
    # Download images
    for name, url in EXAMPLE_URLS["images"].items():
        ext = ".jpg" if "jpg" in url or "jpeg" in url else ".png"
        save_path = os.path.join(EXAMPLES_DIR, "images", f"{name}{ext}")
        if not os.path.exists(save_path):
            print(f"Downloading {name} image...")
            try:
                response = requests.get(url, stream=True)
                response.raise_for_status()
                with open(save_path, "wb") as f:
                    f.write(response.content)
                print(f"  Saved to {save_path}")
            except Exception as e:
                print(f"  Error downloading {name}: {e}")
                continue
        downloaded_files["images"][name] = save_path
    
    return downloaded_files


# ============================================================================
# Point Extraction Functions
# ============================================================================

def _points_from_num_str(text, image_w, image_h, extract_ids=False):
    """Extract points from number string format."""
    for points in POINTS_REGEX.finditer(text):
        ix, x, y = points.group(1), points.group(2), points.group(3)
        # Our points format assumes coordinates are scaled by 1000
        x, y = float(x) / 1000 * image_w, float(y) / 1000 * image_h
        if 0 <= x <= image_w and 0 <= y <= image_h:
            yield ix, x, y


def extract_video_points(text, image_w, image_h, extract_ids=False):
    """
    Extract video pointing coordinates as a flattened list of (t, x, y) triplets from model output text.
    """
    all_points = []
    for coord in COORD_REGEX.finditer(text):
        for point_grp in FRAME_REGEX.finditer(coord.group(1)):
            frame_id = float(point_grp.group(1))
            w, h = (image_w, image_h)
            for idx, x, y in _points_from_num_str(point_grp.group(2), w, h):
                if extract_ids:
                    all_points.append((frame_id, idx, x, y))
                else:
                    all_points.append((frame_id, x, y))
    return all_points


def extract_multi_image_points(text, image_w, image_h, extract_ids=False):
    """
    ä»æ¨¡å‹è¾“å‡ºæ–‡æœ¬ä¸­æå–å¤šå›¾æŒ‡å‘åæ ‡ã€‚
    
    æ ¼å¼ç¤ºä¾‹: <points coords="1 1 098 629 2 162 629...;2 22 142 418...">boats</points>
    - åˆ†å· ; åˆ†éš”ä¸åŒå›¾ç‰‡
    - æ¯ä¸ªå›¾ç‰‡ç»„çš„ç¬¬ä¸€ä¸ªæ•°å­—æ˜¯å›¾ç‰‡ç´¢å¼•ï¼ˆ1æˆ–2ï¼‰
    - åç»­æ˜¯ "ç‚¹ç´¢å¼• Xåæ ‡ Yåæ ‡" çš„é‡å¤
    """
    all_points = []
    
    # åˆ¤æ–­æ˜¯å¦æœ‰å¤šä¸ªä¸åŒå°ºå¯¸çš„å›¾ç‰‡
    if isinstance(image_w, (list, tuple)) and isinstance(image_h, (list, tuple)):
        assert len(image_w) == len(image_h)
        multi_size = True
    else:
        multi_size = False
        image_w = [image_w]
        image_h = [image_h]
    
    # æå– coords å±æ€§å†…å®¹
    for coord_match in COORD_REGEX.finditer(text):
        coords_str = coord_match.group(1)
        
        # æŒ‰åˆ†å·åˆ†å‰²ä¸åŒå›¾ç‰‡çš„ç‚¹
        image_groups = coords_str.split(';')
        
        for group in image_groups:
            group = group.strip()
            if not group:
                continue
            
            # è§£ææ•°å­—åºåˆ—
            numbers = re.findall(r'[0-9]+', group)
            if len(numbers) < 4:  # è‡³å°‘éœ€è¦: å›¾ç‰‡ç´¢å¼•, ç‚¹ç´¢å¼•, x, y
                continue
            
            # ç¬¬ä¸€ä¸ªæ•°å­—æ˜¯å›¾ç‰‡ç´¢å¼•
            frame_id = int(numbers[0])
            
            # è·å–å¯¹åº”å›¾ç‰‡çš„å°ºå¯¸
            img_idx = frame_id - 1  # è½¬ä¸º 0-indexed
            if img_idx < 0 or img_idx >= len(image_w):
                img_idx = 0  # å›é€€åˆ°ç¬¬ä¸€å¼ å›¾
            w, h = image_w[img_idx], image_h[img_idx]
            
            # å‰©ä½™æ•°å­—æ¯3ä¸ªä¸€ç»„: ç‚¹ç´¢å¼•, x, y
            remaining = numbers[1:]
            for i in range(0, len(remaining) - 2, 3):
                try:
                    point_idx = int(remaining[i])
                    x = float(remaining[i + 1]) / 1000 * w
                    y = float(remaining[i + 2]) / 1000 * h
                    
                    if 0 <= x <= w and 0 <= y <= h:
                        if extract_ids:
                            all_points.append((frame_id, point_idx, x, y))
                        else:
                            all_points.append((frame_id, x, y))
                except (ValueError, IndexError):
                    continue
    
    return all_points


def get_video_dimensions(video_path: str) -> tuple[int, int]:
    """Get video dimensions using decord or fallback."""
    try:
        import decord
        vr = decord.VideoReader(video_path, ctx=decord.cpu(0))
        # Get first frame to determine dimensions
        frame = vr[0].asnumpy()
        height, width = frame.shape[:2]
        return width, height
    except Exception:
        pass
    
    try:
        import cv2
        cap = cv2.VideoCapture(video_path)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        cap.release()
        if width > 0 and height > 0:
            return width, height
    except Exception:
        pass
    
    # Default fallback
    return 1920, 1080


def extract_video_frames_with_points(video_path: str, points: list, max_frames: int = 4) -> list[Image.Image]:
    """
    ä»è§†é¢‘ä¸­æå–åŒ…å«æ ‡è®°ç‚¹çš„å¸§ï¼Œå¹¶åœ¨å¸§ä¸Šç»˜åˆ¶ç‚¹ã€‚
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        points: ç‚¹åˆ—è¡¨ï¼Œæ ¼å¼ä¸º (timestamp, x, y) æˆ– (timestamp, idx, x, y)
        max_frames: æœ€å¤šè¿”å›å¤šå°‘å¸§
    
    Returns:
        æ ‡æ³¨åçš„ PIL Image åˆ—è¡¨
    """
    if not points:
        return []
    
    # æŒ‰æ—¶é—´æˆ³åˆ†ç»„ç‚¹
    from collections import defaultdict
    points_by_time = defaultdict(list)
    for point in points:
        if len(point) >= 3:
            timestamp = float(point[0])
            if len(point) == 3:
                points_by_time[timestamp].append((point[1], point[2]))
            elif len(point) == 4:
                points_by_time[timestamp].append((point[2], point[3]))
    
    # é€‰æ‹©è¦æ˜¾ç¤ºçš„æ—¶é—´æˆ³ï¼ˆæœ€å¤š max_frames ä¸ªï¼Œå‡åŒ€åˆ†å¸ƒï¼‰
    all_timestamps = sorted(points_by_time.keys())
    if len(all_timestamps) <= max_frames:
        selected_timestamps = all_timestamps
    else:
        # å‡åŒ€é€‰æ‹©
        indices = [int(i * (len(all_timestamps) - 1) / (max_frames - 1)) for i in range(max_frames)]
        selected_timestamps = [all_timestamps[i] for i in indices]
    
    annotated_frames = []
    
    try:
        import decord
        vr = decord.VideoReader(video_path, ctx=decord.cpu(0))
        fps = vr.get_avg_fps()
        
        for timestamp in selected_timestamps:
            # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºå¸§ç´¢å¼•
            frame_idx = int(timestamp * fps)
            frame_idx = min(frame_idx, len(vr) - 1)
            frame_idx = max(frame_idx, 0)
            
            # æå–å¸§
            frame = vr[frame_idx].asnumpy()
            frame_image = Image.fromarray(frame)
            
            # åœ¨å¸§ä¸Šç»˜åˆ¶ç‚¹
            pts = points_by_time[timestamp]
            if pts:
                frame_image = draw_points_on_image(frame_image, pts)
            
            # æ·»åŠ æ—¶é—´æˆ³æ ‡ç­¾
            draw = ImageDraw.Draw(frame_image)
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 24)
            except:
                font = ImageFont.load_default()
            
            label = f"t={timestamp:.1f}s"
            # ç»˜åˆ¶èƒŒæ™¯
            draw.rectangle([(5, 5), (150, 35)], fill=(0, 0, 0, 180))
            draw.text((10, 8), label, fill=(255, 255, 255), font=font)
            
            annotated_frames.append(frame_image)
            
    except Exception as e:
        print(f"[ERROR] æå–è§†é¢‘å¸§å¤±è´¥: {e}")
    
    return annotated_frames


def format_points_output(points: list) -> str:
    """Format points list into readable string."""
    if not points:
        return "No points detected."
    
    lines = []
    for i, point in enumerate(points[:20]):  # Limit to first 20 points
        if len(point) == 3:
            lines.append(f"Point {i+1}: Frame/Image {point[0]:.1f}, X={point[1]:.2f}, Y={point[2]:.2f}")
        elif len(point) == 4:
            lines.append(f"Point {i+1}: Frame/Image {point[0]:.1f}, ID={point[1]}, X={point[2]:.2f}, Y={point[3]:.2f}")
    
    if len(points) > 20:
        lines.append(f"... and {len(points) - 20} more points")
    
    return "\n".join(lines)


# ============================================================================
# Visualization Functions
# ============================================================================

# Color palette for different points/objects
COLOR_PALETTE = [
    (255, 0, 0),      # Red
    (0, 255, 0),      # Green
    (0, 0, 255),      # Blue
    (255, 255, 0),    # Yellow
    (255, 0, 255),    # Magenta
    (0, 255, 255),    # Cyan
    (255, 128, 0),    # Orange
    (128, 0, 255),    # Purple
    (0, 255, 128),    # Spring Green
    (255, 0, 128),    # Rose
    (128, 255, 0),    # Lime
    (0, 128, 255),    # Sky Blue
]


def draw_points_on_image(
    image: Image.Image,
    points: list[tuple],
    point_radius: int = None,
    show_labels: bool = True,
    label_offset: int = None,
) -> Image.Image:
    """
    Draw points on an image with labels.
    
    Args:
        image: PIL Image to draw on
        points: List of (x, y) or (idx, x, y) tuples
        point_radius: Radius of the point circle (auto-scaled if None)
        show_labels: Whether to show point number labels
        label_offset: Offset for label text from point
    
    Returns:
        New PIL Image with points drawn
    """
    # Make a copy to avoid modifying original
    annotated = image.copy()
    draw = ImageDraw.Draw(annotated)
    
    # Auto-scale point size based on image dimensions
    img_size = max(image.width, image.height)
    if point_radius is None:
        point_radius = max(12, img_size // 100)  # è‡³å°‘12åƒç´ ï¼Œæˆ–å›¾ç‰‡å°ºå¯¸çš„1%
    if label_offset is None:
        label_offset = point_radius + 5
    
    # æ ¹æ®å›¾ç‰‡å¤§å°é€‰æ‹©å­—ä½“å¤§å°
    font_size = max(16, img_size // 60)
    
    # Try to get a font, fall back to default if not available
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", font_size)
    except:
        try:
            font = ImageFont.truetype("/usr/share/fonts/dejavu/DejaVuSans-Bold.ttf", font_size)
        except:
            font = ImageFont.load_default()
    
    for i, point in enumerate(points):
        # Handle different point formats
        if len(point) == 2:
            x, y = point
        elif len(point) == 3:
            _, x, y = point  # (frame_id, x, y)
        elif len(point) == 4:
            _, _, x, y = point  # (frame_id, idx, x, y)
        else:
            continue
        
        # Get color from palette (cycle if more points than colors)
        color = COLOR_PALETTE[i % len(COLOR_PALETTE)]
        
        # Draw filled circle
        draw.ellipse(
            [(x - point_radius, y - point_radius),
             (x + point_radius, y + point_radius)],
            fill=color,
            outline=(255, 255, 255),
            width=2
        )
        
        # Draw label
        if show_labels:
            label = str(i + 1)
            # Draw text with outline for better visibility
            for dx, dy in [(-1, -1), (-1, 1), (1, -1), (1, 1)]:
                draw.text(
                    (x + label_offset + dx, y - label_offset + dy),
                    label,
                    fill=(0, 0, 0),
                    font=font
                )
            draw.text(
                (x + label_offset, y - label_offset),
                label,
                fill=color,
                font=font
            )
    
    return annotated


def draw_points_on_multi_images(
    images: list[Image.Image],
    points: list[tuple],
) -> list[Image.Image]:
    """
    Draw points on multiple images based on frame/image index.
    
    Args:
        images: List of PIL Images
        points: List of (frame_id, x, y) or (frame_id, idx, x, y) tuples
                frame_id is 1-indexed
    
    Returns:
        List of annotated PIL Images
    """
    # Group points by image index
    points_per_image = {i: [] for i in range(len(images))}
    
    for point in points:
        if len(point) >= 3:
            frame_id = int(point[0])  # 1-indexed
            image_idx = frame_id - 1   # Convert to 0-indexed
            if 0 <= image_idx < len(images):
                if len(point) == 3:
                    points_per_image[image_idx].append((point[1], point[2]))
                elif len(point) == 4:
                    points_per_image[image_idx].append((point[2], point[3]))
    
    # Debug
    for i, pts in points_per_image.items():
        print(f"[DEBUG] å›¾ç‰‡ {i+1} æœ‰ {len(pts)} ä¸ªç‚¹")
        if pts:
            print(f"[DEBUG]   å‰3ä¸ª: {pts[:3]}")
    
    # Draw points on each image
    annotated_images = []
    for i, img in enumerate(images):
        if points_per_image[i]:
            annotated = draw_points_on_image(img, points_per_image[i])  # è‡ªåŠ¨è°ƒæ•´å¤§å°
        else:
            annotated = img.copy()
        annotated_images.append(annotated)
    
    return annotated_images


# ============================================================================
# Model Loading
# ============================================================================

# Global model and processor (loaded once)
model = None
processor = None


def load_model():
    """Load the Molmo2 model and processor."""
    global model, processor
    
    if processor is None:
        print("Loading processor...")
        processor = AutoProcessor.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
        )
        print("Processor loaded.")
    
    if model is None:
        print("Loading model...")
        model = AutoModelForImageTextToText.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto",
        )
        print("Model loaded.")
    
    return model, processor


# ============================================================================
# Inference Functions
# ============================================================================

def general_video_qa(video_path: str, question: str, max_tokens: int = 2048) -> str:
    """
    General Video QA: Answer questions about a video.
    """
    if not video_path:
        return "Please provide a video."
    if not question:
        return "Please provide a question."
    
    model, processor = load_model()
    
    messages = [
        {
            "role": "user",
            "content": [
                dict(type="text", text=question),
                dict(type="video", video=video_path),
            ],
        }
    ]
    
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
        return_dict=True,
    )
    
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.inference_mode():
        generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)
    
    generated_tokens = generated_ids[0, inputs['input_ids'].size(1):]
    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    return generated_text


def pointing_video_qa(video_path: str, question: str, max_tokens: int = 2048) -> tuple[str, str, list]:
    """
    Pointing Video QA: Point to objects in a video.
    
    Returns:
        Tuple of (model_response, points_text, annotated_frames)
    """
    if not video_path:
        return "è¯·ä¸Šä¼ è§†é¢‘", "", []
    if not question:
        return "è¯·è¾“å…¥é—®é¢˜", "", []
    
    model, processor = load_model()
    
    messages = [
        {
            "role": "user",
            "content": [
                dict(type="text", text=question),
                dict(type="video", video=video_path),
            ],
        }
    ]
    
    # Use apply_chat_template which handles video processing internally
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
        return_dict=True,
    )
    
    # Get video dimensions from the processed metadata
    video_metadata = inputs.pop("video_metadata", None)
    if video_metadata and len(video_metadata) > 0:
        width = getattr(video_metadata[0], "width", None) or 1920
        height = getattr(video_metadata[0], "height", None) or 1080
    else:
        # Fallback: try to get dimensions from video file
        width, height = get_video_dimensions(video_path)
    
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.inference_mode():
        generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)
    
    generated_tokens = generated_ids[0, inputs['input_ids'].size(1):]
    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # Extract points
    points = extract_video_points(generated_text, image_w=width, image_h=height)
    points_str = format_points_output(points)
    
    # æå–å¹¶æ ‡æ³¨è§†é¢‘å¸§
    annotated_frames = extract_video_frames_with_points(video_path, points, max_frames=4)
    
    return generated_text, points_str, annotated_frames


def tracking_video_qa(video_path: str, question: str, max_tokens: int = 2048) -> tuple[str, str, list]:
    """
    Tracking Video QA: Track objects in a video.
    
    Returns:
        Tuple of (model_response, points_text, annotated_frames)
    """
    if not video_path:
        return "è¯·ä¸Šä¼ è§†é¢‘", "", []
    if not question:
        return "è¯·è¾“å…¥è¿½è¸ªæŒ‡ä»¤", "", []
    
    model, processor = load_model()
    
    messages = [
        {
            "role": "user",
            "content": [
                dict(type="text", text=question),
                dict(type="video", video=video_path),
            ],
        }
    ]
    
    # Use apply_chat_template which handles video processing internally
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
        return_dict=True,
    )
    
    # Get video dimensions from the processed metadata
    video_metadata = inputs.pop("video_metadata", None)
    if video_metadata and len(video_metadata) > 0:
        width = getattr(video_metadata[0], "width", None) or 1920
        height = getattr(video_metadata[0], "height", None) or 1080
    else:
        # Fallback: try to get dimensions from video file
        width, height = get_video_dimensions(video_path)
    
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.inference_mode():
        generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)
    
    generated_tokens = generated_ids[0, inputs['input_ids'].size(1):]
    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # Extract tracking points
    points = extract_video_points(generated_text, image_w=width, image_h=height)
    points_str = format_points_output(points)
    
    # æå–å¹¶æ ‡æ³¨è§†é¢‘å¸§
    annotated_frames = extract_video_frames_with_points(video_path, points, max_frames=4)
    
    return generated_text, points_str, annotated_frames


def multi_image_qa(images: list, question: str, max_tokens: int = 448) -> str:
    """
    Multi-image QA: Answer questions about multiple images.
    """
    if not images or len(images) == 0:
        return "Please provide at least one image."
    if not question:
        return "Please provide a question."
    
    model, processor = load_model()
    
    # Build message content
    content = [dict(type="text", text=question)]
    
    for img in images:
        if img is not None:
            if isinstance(img, str):
                img = Image.open(img)
            content.append(dict(type="image", image=img))
    
    messages = [{"role": "user", "content": content}]
    
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
        return_dict=True,
    )
    
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.inference_mode():
        generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)
    
    generated_tokens = generated_ids[0, inputs['input_ids'].size(1):]
    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    return generated_text


def multi_image_point_qa(images: list, question: str, max_tokens: int = 2048) -> tuple[str, str, list]:
    """
    Multi-Image Point QA: Point to objects across multiple images.
    
    Returns:
        Tuple of (model_response, points_text, annotated_images)
    """
    if not images or len(images) == 0:
        return "Please provide at least one image.", "", []
    if not question:
        return "Please provide a question.", "", []
    
    model, processor = load_model()
    
    # Build message content and collect image dimensions
    content = [dict(type="text", text=question)]
    image_widths = []
    image_heights = []
    pil_images = []
    
    for img in images:
        if img is not None:
            # ç¡®ä¿æ˜¯ PIL Image
            if isinstance(img, str):
                img = Image.open(img)
            elif not isinstance(img, Image.Image):
                # å¯èƒ½æ˜¯ numpy æ•°ç»„ï¼Œè½¬æ¢ä¸º PIL
                import numpy as np
                if isinstance(img, np.ndarray):
                    img = Image.fromarray(img)
            
            pil_images.append(img.copy())  # Make copies
            image_widths.append(img.width)
            image_heights.append(img.height)
            content.append(dict(type="image", image=img))
    
    if len(pil_images) == 0:
        return "No valid images provided.", "", []
    
    messages = [{"role": "user", "content": content}]
    
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
        return_dict=True,
    )
    
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.inference_mode():
        generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)
    
    generated_tokens = generated_ids[0, inputs['input_ids'].size(1):]
    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # Extract points
    points = extract_multi_image_points(generated_text, image_widths, image_heights)
    points_str = format_points_output(points)
    
    # Debug: æ‰“å°ç‚¹ä¿¡æ¯
    print(f"[DEBUG] æå–åˆ° {len(points)} ä¸ªç‚¹")
    print(f"[DEBUG] å›¾ç‰‡æ•°é‡: {len(pil_images)}, å°ºå¯¸: {list(zip(image_widths, image_heights))}")
    if points:
        print(f"[DEBUG] å‰3ä¸ªç‚¹: {points[:3]}")
    
    # Draw points on images
    annotated_images = draw_points_on_multi_images(pil_images, points)
    
    return generated_text, points_str, annotated_images


# ============================================================================
# Gradio Interface
# ============================================================================

def create_interface():
    """Create the Gradio interface with multiple tabs."""
    
    # Download examples at startup
    print("Downloading example files...")
    example_files = download_examples()
    print("Examples ready.")
    
    # Get example file paths
    penguin_video = example_files["videos"].get("penguins", "")
    basketball_video = example_files["videos"].get("basketball", "")
    dog_image = example_files["images"].get("dog", "")
    cherry_image = example_files["images"].get("cherry_blossom", "")
    boat1_image = example_files["images"].get("boat1", "")
    boat2_image = example_files["images"].get("boat2", "")
    
    with gr.Blocks(title="Molmo2-8B æ¼”ç¤º", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¬ Molmo2-8B å¤šæ¨¡æ€æ¼”ç¤º
        
        æœ¬æ¼”ç¤ºå±•ç¤º Molmo2-8B æ¨¡å‹çš„å¤šç§èƒ½åŠ›ï¼š
        - **è§†é¢‘é—®ç­”** - å¯¹è§†é¢‘å†…å®¹æé—®
        - **è§†é¢‘ç›®æ ‡æŒ‡å‘** - åœ¨è§†é¢‘ä¸­æŒ‡å‡ºç‰¹å®šç‰©ä½“
        - **è§†é¢‘ç›®æ ‡è¿½è¸ª** - è¿½è¸ªè§†é¢‘ä¸­çš„ç‰©ä½“
        - **å¤šå›¾é—®ç­”** - å¯¹å¤šå¼ å›¾ç‰‡è¿›è¡Œå¯¹æ¯”å’Œé—®ç­”
        - **å¤šå›¾ç›®æ ‡æŒ‡å‘** - åœ¨å¤šå¼ å›¾ç‰‡ä¸­æŒ‡å‡ºç‰¹å®šç‰©ä½“
        
        é€‰æ‹©ä¸‹æ–¹æ ‡ç­¾é¡µä½“éªŒä¸åŒåŠŸèƒ½ï¼
        """)
        
        with gr.Tabs():
            # ================================================================
            # Tab 1: General Video QA
            # ================================================================
            with gr.TabItem("ğŸ¥ è§†é¢‘é—®ç­”"):
                gr.Markdown("""
                ### é€šç”¨è§†é¢‘é—®ç­”
                ä¸Šä¼ è§†é¢‘å¹¶å¯¹å…¶å†…å®¹è¿›è¡Œæé—®ã€‚
                """)
                
                with gr.Row():
                    with gr.Column():
                        video_input_1 = gr.Video(label="ä¸Šä¼ è§†é¢‘")
                        question_1 = gr.Textbox(
                            label="é—®é¢˜",
                            placeholder="è§†é¢‘ä¸­å‡ºç°äº†ä»€ä¹ˆåŠ¨ç‰©ï¼Ÿ",
                            lines=2
                        )
                        max_tokens_1 = gr.Slider(
                            minimum=64, maximum=4096, value=2048, step=64,
                            label="æœ€å¤§è¾“å‡ºé•¿åº¦"
                        )
                        submit_btn_1 = gr.Button("æäº¤", variant="primary")
                    
                    with gr.Column():
                        output_1 = gr.Textbox(label="å›ç­”", lines=10)
                
                gr.Examples(
                    examples=[
                        [penguin_video, "è§†é¢‘ä¸­å‡ºç°äº†ä»€ä¹ˆåŠ¨ç‰©ï¼Ÿ"],
                        [basketball_video, "è§†é¢‘ä¸­åœ¨è¿›è¡Œä»€ä¹ˆè¿åŠ¨ï¼Ÿ"],
                    ] if penguin_video else [],
                    inputs=[video_input_1, question_1],
                    label="ç¤ºä¾‹"
                )
                
                submit_btn_1.click(
                    fn=general_video_qa,
                    inputs=[video_input_1, question_1, max_tokens_1],
                    outputs=[output_1]
                )
            
            # ================================================================
            # Tab 2: Pointing Video QA
            # ================================================================
            with gr.TabItem("ğŸ‘† è§†é¢‘ç›®æ ‡æŒ‡å‘"):
                gr.Markdown("""
                ### è§†é¢‘ç›®æ ‡æŒ‡å‘
                ä¸Šä¼ è§†é¢‘å¹¶è®©æ¨¡å‹æŒ‡å‡ºç‰¹å®šç‰©ä½“çš„ä½ç½®ã€‚
                æ¨¡å‹ä¼šè¿”å›ç‰©ä½“åœ¨å„å¸§ä¸­çš„åæ ‡ã€‚
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        video_input_2 = gr.Video(label="ä¸Šä¼ è§†é¢‘")
                        question_2 = gr.Textbox(
                            label="æŒ‡å‘æŒ‡ä»¤",
                            placeholder="æŒ‡å‡ºä¼é¹…çš„ä½ç½®",
                            lines=2
                        )
                        max_tokens_2 = gr.Slider(
                            minimum=64, maximum=4096, value=2048, step=64,
                            label="æœ€å¤§è¾“å‡ºé•¿åº¦"
                        )
                        submit_btn_2 = gr.Button("æäº¤", variant="primary")
                    
                    with gr.Column(scale=2):
                        output_2a = gr.Textbox(label="æ¨¡å‹è¾“å‡º", lines=3)
                        output_2b = gr.Textbox(label="æå–çš„åæ ‡ç‚¹ (å¸§, X, Y)", lines=6)
                        gr.Markdown("### ğŸ“Œ æ ‡æ³¨åçš„å…³é”®å¸§")
                        with gr.Row():
                            output_frame_2a = gr.Image(label="å¸§ 1", type="pil")
                            output_frame_2b = gr.Image(label="å¸§ 2", type="pil")
                        with gr.Row():
                            output_frame_2c = gr.Image(label="å¸§ 3", type="pil")
                            output_frame_2d = gr.Image(label="å¸§ 4", type="pil")
                
                def pointing_video_qa_wrapper(video, question, max_tokens):
                    response, points_str, frames = pointing_video_qa(video, question, max_tokens)
                    # å¡«å……åˆ°4å¸§
                    while len(frames) < 4:
                        frames.append(None)
                    return response, points_str, frames[0], frames[1], frames[2], frames[3]
                
                gr.Examples(
                    examples=[
                        [penguin_video, "Point to the penguins."],
                    ] if penguin_video else [],
                    inputs=[video_input_2, question_2],
                    label="ç¤ºä¾‹"
                )
                
                submit_btn_2.click(
                    fn=pointing_video_qa_wrapper,
                    inputs=[video_input_2, question_2, max_tokens_2],
                    outputs=[output_2a, output_2b, output_frame_2a, output_frame_2b, output_frame_2c, output_frame_2d]
                )
            
            # ================================================================
            # Tab 3: Tracking Video QA
            # ================================================================
            with gr.TabItem("ğŸ¯ è§†é¢‘ç›®æ ‡è¿½è¸ª"):
                gr.Markdown("""
                ### è§†é¢‘ç›®æ ‡è¿½è¸ª
                ä¸Šä¼ è§†é¢‘å¹¶è®©æ¨¡å‹è¿½è¸ªç‰¹å®šç‰©ä½“ã€‚
                æ¨¡å‹ä¼šè¿”å›ç‰©ä½“åœ¨å„å¸§ä¸­çš„è¿½è¸ªåæ ‡ã€‚
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        video_input_3 = gr.Video(label="ä¸Šä¼ è§†é¢‘")
                        question_3 = gr.Textbox(
                            label="è¿½è¸ªæŒ‡ä»¤",
                            placeholder="è¿½è¸ªæ­£åœ¨æ‰£ç¯®çš„çƒå‘˜",
                            lines=2
                        )
                        max_tokens_3 = gr.Slider(
                            minimum=64, maximum=4096, value=2048, step=64,
                            label="æœ€å¤§è¾“å‡ºé•¿åº¦"
                        )
                        submit_btn_3 = gr.Button("æäº¤", variant="primary")
                    
                    with gr.Column(scale=2):
                        output_3a = gr.Textbox(label="æ¨¡å‹è¾“å‡º", lines=3)
                        output_3b = gr.Textbox(label="è¿½è¸ªåæ ‡ç‚¹ (å¸§, X, Y)", lines=6)
                        gr.Markdown("### ğŸ“Œ æ ‡æ³¨åçš„å…³é”®å¸§")
                        with gr.Row():
                            output_frame_3a = gr.Image(label="å¸§ 1", type="pil")
                            output_frame_3b = gr.Image(label="å¸§ 2", type="pil")
                        with gr.Row():
                            output_frame_3c = gr.Image(label="å¸§ 3", type="pil")
                            output_frame_3d = gr.Image(label="å¸§ 4", type="pil")
                
                def tracking_video_qa_wrapper(video, question, max_tokens):
                    response, points_str, frames = tracking_video_qa(video, question, max_tokens)
                    # å¡«å……åˆ°4å¸§
                    while len(frames) < 4:
                        frames.append(None)
                    return response, points_str, frames[0], frames[1], frames[2], frames[3]
                
                gr.Examples(
                    examples=[
                        [basketball_video, "Track the player who is dunking"],
                    ] if basketball_video else [],
                    inputs=[video_input_3, question_3],
                    label="ç¤ºä¾‹"
                )
                
                submit_btn_3.click(
                    fn=tracking_video_qa_wrapper,
                    inputs=[video_input_3, question_3, max_tokens_3],
                    outputs=[output_3a, output_3b, output_frame_3a, output_frame_3b, output_frame_3c, output_frame_3d]
                )
            
            # ================================================================
            # Tab 4: Multi-Image QA
            # ================================================================
            with gr.TabItem("ğŸ–¼ï¸ å›¾ç‰‡é—®ç­”"):
                gr.Markdown("""
                ### å›¾ç‰‡é—®ç­”
                ä¸Šä¼ å›¾ç‰‡å¹¶å¯¹å…¶å†…å®¹è¿›è¡Œæé—®ã€‚
                """)
                
                with gr.Row():
                    with gr.Column():
                        image_input_4a = gr.Image(label="å›¾ç‰‡ 1", type="pil")
                        image_input_4b = gr.Image(label="å›¾ç‰‡ 2", type="pil")
                        question_4 = gr.Textbox(
                            label="é—®é¢˜",
                            placeholder="å¯¹æ¯”è¿™ä¸¤å¼ å›¾ç‰‡",
                            lines=2
                        )
                        max_tokens_4 = gr.Slider(
                            minimum=64, maximum=2048, value=448, step=64,
                            label="æœ€å¤§è¾“å‡ºé•¿åº¦"
                        )
                        submit_btn_4 = gr.Button("æäº¤", variant="primary")
                    
                    with gr.Column():
                        output_4 = gr.Textbox(label="å›ç­”", lines=15)
                
                def multi_image_qa_wrapper(img1, img2, question, max_tokens):
                    images = [img for img in [img1, img2] if img is not None]
                    return multi_image_qa(images, question, max_tokens)
                
                gr.Examples(
                    examples=[
                        [dog_image, None, "Describe this image."],
                        [cherry_image, None, "What do you see in this image?"],
                    ] if dog_image else [],
                    inputs=[image_input_4a, image_input_4b, question_4],
                    label="ç¤ºä¾‹"
                )
                
                submit_btn_4.click(
                    fn=multi_image_qa_wrapper,
                    inputs=[image_input_4a, image_input_4b, question_4, max_tokens_4],
                    outputs=[output_4]
                )
            
            # ================================================================
            # Tab 5: Multi-Image Point QA
            # ================================================================
            with gr.TabItem("ğŸ“ å›¾ç‰‡æ ‡è®°"):
                gr.Markdown("""
                ### å›¾ç‰‡æ ‡è®°
                ä¸Šä¼ å›¾ç‰‡å¹¶è®©æ¨¡å‹æŒ‡å‡ºç‰¹å®šç‰©ä½“çš„ä½ç½®ã€‚
                æ ‡è®°ç‚¹ä¼šç›´æ¥æ˜¾ç¤ºåœ¨å›¾ç‰‡ä¸Šã€‚
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        image_input_5a = gr.Image(label="å›¾ç‰‡ 1", type="pil")
                        image_input_5b = gr.Image(label="å›¾ç‰‡ 2ï¼ˆå¯é€‰ï¼‰", type="pil")
                        question_5 = gr.Textbox(
                            label="æŒ‡å‘æŒ‡ä»¤",
                            placeholder="Point to the boats",
                            lines=2
                        )
                        max_tokens_5 = gr.Slider(
                            minimum=64, maximum=4096, value=2048, step=64,
                            label="æœ€å¤§è¾“å‡ºé•¿åº¦"
                        )
                        submit_btn_5 = gr.Button("æäº¤", variant="primary")
                    
                    with gr.Column(scale=2):
                        output_5a = gr.Textbox(label="æ¨¡å‹è¾“å‡º", lines=3)
                        output_5b = gr.Textbox(label="æå–çš„åæ ‡ç‚¹ (å›¾ç‰‡, X, Y)", lines=6)
                        gr.Markdown("### ğŸ“Œ æ ‡æ³¨åçš„å›¾ç‰‡")
                        with gr.Row():
                            output_img_5a = gr.Image(label="å›¾ç‰‡ 1 - æ ‡æ³¨", type="pil")
                            output_img_5b = gr.Image(label="å›¾ç‰‡ 2 - æ ‡æ³¨", type="pil")
                
                def multi_image_point_qa_wrapper(img1, img2, question, max_tokens):
                    images = [img for img in [img1, img2] if img is not None]
                    response, points_str, annotated_images = multi_image_point_qa(images, question, max_tokens)
                    
                    # Prepare output images (handle cases with 1 or 2 images)
                    out_img1 = annotated_images[0] if len(annotated_images) > 0 else None
                    out_img2 = annotated_images[1] if len(annotated_images) > 1 else None
                    
                    return response, points_str, out_img1, out_img2
                
                gr.Examples(
                    examples=[
                        [boat1_image, None, "Point to the boats"],
                        [dog_image, None, "Point to the dog's eyes"],
                        [cherry_image, None, "Point to the flowers"],
                    ] if boat1_image else [],
                    inputs=[image_input_5a, image_input_5b, question_5],
                    label="ç¤ºä¾‹"
                )
                
                submit_btn_5.click(
                    fn=multi_image_point_qa_wrapper,
                    inputs=[image_input_5a, image_input_5b, question_5, max_tokens_5],
                    outputs=[output_5a, output_5b, output_img_5a, output_img_5b]
                )
        
        gr.Markdown("""
        ---
        **æ³¨æ„ï¼š** æ¨¡å‹ä¼šåœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åŠ è½½ï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚åç»­æŸ¥è¯¢ä¼šæ›´å¿«ã€‚
        
        **æ¨¡å‹ï¼š** Allen AI çš„ Molmo2-8B
        """)
    
    return demo


# ============================================================================
# Main Entry Point
# ============================================================================

if __name__ == "__main__":
    # Pre-load model (optional, can be loaded lazily)
    print("Initializing Molmo2 Demo...")
    
    demo = create_interface()
    
    # Launch the Gradio app
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
    )
