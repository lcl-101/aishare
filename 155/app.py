import gradio as gr
import torch
import llava
from peft import PeftModel
import os
import copy
import ollama

# ---------------------------------
# SINGLE-TURN MODEL SETUP
# ---------------------------------
MODEL_BASE_SINGLE = "checkpoints/audio-flamingo-3"
MODEL_BASE_THINK = os.path.join(MODEL_BASE_SINGLE, 'stage35')

# model_single = llava.load(MODEL_BASE_SINGLE, model_base=None, devices=[0])
model_single = llava.load(MODEL_BASE_SINGLE, model_base=None)
model_single = model_single.to("cuda")
model_single_copy = copy.deepcopy(model_single)

generation_config_single = model_single.default_generation_config

model_think = PeftModel.from_pretrained(
        model_single,
        MODEL_BASE_THINK,
        device_map="auto",
        torch_dtype=torch.float16,
        )

# ---------------------------------
# MULTI-TURN MODEL SETUP
# ---------------------------------
MODEL_BASE_MULTI = "checkpoints/audio-flamingo-chat"
# model_multi = llava.load(MODEL_BASE_MULTI, model_base=None, devices=[0])
model_multi = llava.load(MODEL_BASE_MULTI, model_base=None)
model_multi = model_multi.to("cuda")
generation_config_multi = model_multi.default_generation_config


# ---------------------------------
# TRANSLATION FUNCTION
# ---------------------------------
def translate_to_chinese(text):
    try:
        response = ollama.chat(
            model='qwen3:32b',
            messages=[{
                'role': 'user',
                'content': f'è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šï¼š\n\n{text}\n\n/no_think'
            }]
        )
        return response['message']['content']
    except Exception as e:
        return f"ç¿»è¯‘é”™è¯¯: {str(e)}"

# ---------------------------------
# SINGLE-TURN INFERENCE FUNCTION
# ---------------------------------
def single_turn_infer(audio_file, prompt_text):
    try:
        sound = llava.Sound(audio_file)
        # ç§»é™¤æç¤ºè¯ä¸­çš„ <sound> æ ‡è®°ä»¥é¿å…è­¦å‘Š
        clean_prompt = prompt_text.replace('<sound>', '').strip()
        response = model_single_copy.generate_content([sound, clean_prompt], generation_config=generation_config_single)
        translation = translate_to_chinese(response)
        return response, translation
    except Exception as e:
        error_msg = f"âŒ é”™è¯¯: {str(e)}"
        return error_msg, error_msg

def think_infer(audio_file, prompt_text):
    try:
        sound = llava.Sound(audio_file)
        # ç§»é™¤æç¤ºè¯ä¸­çš„ <sound> æ ‡è®°ä»¥é¿å…è­¦å‘Š
        clean_prompt = prompt_text.replace('<sound>', '').strip()
        response = model_think.generate_content([sound, clean_prompt], generation_config=generation_config_single)
        translation = translate_to_chinese(response)
        return response, translation
    except Exception as e:
        error_msg = f"âŒ é”™è¯¯: {str(e)}"
        return error_msg, error_msg

# ---------------------------------
# MULTI-TURN INFERENCE FUNCTION
# ---------------------------------
def multi_turn_chat(user_input, audio_file, history, current_audio):
    try:
        if audio_file is not None:
            current_audio = audio_file  # Update state if a new file is uploaded

        if current_audio is None:
            return history + [("ç³»ç»Ÿ", "âŒ è¯·åœ¨èŠå¤©å‰å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ã€‚")], history, current_audio

        sound = llava.Sound(current_audio)
        # ç§»é™¤æç¤ºè¯ä¸­çš„ <sound> æ ‡è®°ä»¥é¿å…è­¦å‘Š
        clean_prompt = user_input.replace('<sound>', '').strip()

        response = model_multi.generate_content([sound, clean_prompt], generation_config=generation_config_multi)

        history.append((user_input, response))
        return history, history, current_audio
    except Exception as e:
        history.append((user_input, f"âŒ é”™è¯¯: {str(e)}"))
        return history, history, current_audio

def speech_prompt_infer(audio_prompt_file):
    try:
        sound = llava.Sound(audio_prompt_file)
        # å¯¹äºè¯­éŸ³æç¤ºï¼Œæˆ‘ä»¬åªä¼ é€’éŸ³é¢‘ï¼Œä¸éœ€è¦æ–‡æœ¬æç¤ºè¯
        response = model_multi.generate_content([sound], generation_config=generation_config_multi)
        translation = translate_to_chinese(response)
        return response, translation
    except Exception as e:
        error_msg = f"âŒ é”™è¯¯: {str(e)}"
        return error_msg, error_msg

# ---------------------------------
# INTERFACE
# ---------------------------------
with gr.Blocks(css="""
.gradio-container { 
    max-width: 100% !important; 
    width: 100% !important;
    margin: 0 !important; 
    padding: 0 !important;
}
#component-0, .gr-block.gr-box { 
    width: 100% !important; 
}
.gr-block.gr-box, .gr-column, .gr-row {
    padding: 0 !important;
    margin: 0 !important;
}
""") as demo:

    with gr.Column():
        gr.HTML("""
<div align="center">
  <img src="https://raw.githubusercontent.com/NVIDIA/audio-flamingo/audio_flamingo_3/static/logo-no-bg.png" alt="Audio Flamingo 3 Logo" width="120" style="margin-bottom: 10px;">
  <h2><strong>éŸ³é¢‘ç«çƒˆé¸Ÿ 3</strong></h2>
  <p><em>æ¨è¿›éŸ³é¢‘æ™ºèƒ½ä¸å®Œå…¨å¼€æºçš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹</em></p>
</div>

<div align="center" style="margin-top: 10px;">
  <a href="https://arxiv.org/abs/2507.08128">
    <img src="https://img.shields.io/badge/arXiv-2503.03983-AD1C18" alt="arXiv" style="display:inline;">
  </a>
  <a href="https://research.nvidia.com/labs/adlr/AF3/">
    <img src="https://img.shields.io/badge/Demo%20page-228B22" alt="Demo Page" style="display:inline;">
  </a>
  <a href="https://github.com/NVIDIA/audio-flamingo">
    <img src="https://img.shields.io/badge/Github-Audio_Flamingo_3-9C276A" alt="GitHub" style="display:inline;">
  </a>
  <a href="https://github.com/NVIDIA/audio-flamingo/stargazers">
    <img src="https://img.shields.io/github/stars/NVIDIA/audio-flamingo.svg?style=social" alt="GitHub Stars" style="display:inline;">
  </a>
</div>
<div align="center" style="display: flex; justify-content: center; margin-top: 10px; flex-wrap: wrap; gap: 5px;">
  <a href="https://huggingface.co/nvidia/audio-flamingo-3">
    <img src="https://img.shields.io/badge/ğŸ¤—-Checkpoints-ED5A22.svg">
  </a>
  <a href="https://huggingface.co/nvidia/audio-flamingo-3-chat">
    <img src="https://img.shields.io/badge/ğŸ¤—-Checkpoints_(Chat)-ED5A22.svg">
  </a>
</div>
<div align="center" style="display: flex; justify-content: center; margin-top: 10px; flex-wrap: wrap; gap: 5px;">
  <a href="https://huggingface.co/datasets/nvidia/AudioSkills">
    <img src="https://img.shields.io/badge/ğŸ¤—-Dataset:_AudioSkills--XL-ED5A22.svg">
  </a>
  <a href="https://huggingface.co/datasets/nvidia/LongAudio">
    <img src="https://img.shields.io/badge/ğŸ¤—-Dataset:_LongAudio--XL-ED5A22.svg">
  </a>
  <a href="https://huggingface.co/datasets/nvidia/AF-Chat">
    <img src="https://img.shields.io/badge/ğŸ¤—-Dataset:_AF--Chat-ED5A22.svg">
  </a>
  <a href="https://huggingface.co/datasets/nvidia/AF-Think">
    <img src="https://img.shields.io/badge/ğŸ¤—-Dataset:_AF--Think-ED5A22.svg">
  </a>
</div>
""")
    # gr.Markdown("#### NVIDIA (2025)")

    with gr.Tabs():
        # ---------------- SINGLE-TURN ----------------
        with gr.Tab("ğŸ¯ å•è½®æ¨ç†"):
            with gr.Row():
                with gr.Column():
                    audio_input_single = gr.Audio(type="filepath", label="ä¸Šä¼ éŸ³é¢‘")
                    prompt_input_single = gr.Textbox(label="æç¤ºè¯", placeholder="è¯·å¯¹éŸ³é¢‘æå‡ºé—®é¢˜...", lines=8)
                    btn_single = gr.Button("ç”Ÿæˆå›ç­”")

                    gr.Examples(
                        examples=[
                            ["static/emergent/audio1.wav", "ç‹—å«å£°å’ŒéŸ³ä¹ä¹‹é—´æœ‰ä»€ä¹ˆä»¤äººæƒŠè®¶çš„å…³ç³»ï¼Ÿ"],
                            ["static/audio/audio2.wav", "è¯·è¯¦ç»†æè¿°è¿™æ®µéŸ³é¢‘ã€‚"],
                            ["static/speech/audio3.wav", "è¯·è½¬å½•ä½ å¬åˆ°çš„ä»»ä½•è¯­éŸ³ã€‚"],
                        ],
                        inputs=[audio_input_single, prompt_input_single],
                        label="ğŸ§ª è¯•è¯•è¿™äº›ä¾‹å­"
                    )

                with gr.Column():
                    output_single = gr.Textbox(label="æ¨¡å‹å›ç­”", lines=10)
                    translated_output_single = gr.Textbox(label="ä¸­æ–‡ç¿»è¯‘", lines=5)

            btn_single.click(fn=single_turn_infer, inputs=[audio_input_single, prompt_input_single], outputs=[output_single, translated_output_single])

        # ---------------- THINK / LONG ----------------
        with gr.Tab("ğŸ¤” æ€è€ƒ / é•¿éŸ³é¢‘"):
            with gr.Row():
                with gr.Column():
                    audio_input_think = gr.Audio(type="filepath", label="ä¸Šä¼ éŸ³é¢‘")
                    prompt_input_think = gr.Textbox(label="æç¤ºè¯", placeholder="è¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œè¯·åœ¨æç¤ºè¯ä¸­æ·»åŠ ï¼š'\nè¯·æ€è€ƒå¹¶æ¨ç†è¾“å…¥çš„éŸ³é¢‘å†…å®¹ï¼Œç„¶åå†å›ç­”ã€‚'", lines=8)
                    btn_think = gr.Button("ç”Ÿæˆå›ç­”")

                    gr.Examples(
                        examples=[
                            ["static/think/audio1.wav", "éŸ³é¢‘ä¸­çš„ä¸¤ä¸ªäººåœ¨åšä»€ä¹ˆï¼Ÿè¯·ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©æ­£ç¡®ç­”æ¡ˆï¼š\n(A) ä¸€ä¸ªäººåœ¨æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è®¾å¤‡\n(B) ä¸¤ä¸ªäººåœ¨è®¨è®ºå¦‚ä½•ä½¿ç”¨è®¾å¤‡\n(C) ä¸¤ä¸ªäººåœ¨æ‹†å¸è®¾å¤‡\n(D) ä¸€ä¸ªäººåœ¨æ•™å¦ä¸€ä¸ªäººå¦‚ä½•ä½¿ç”¨è®¾å¤‡\n"],
                            ["static/think/audio2.wav", "è§†é¢‘ä¸­çš„èˆ¹åªæ˜¯åœ¨é è¿‘è¿˜æ˜¯è¿œç¦»ï¼Ÿè¯·ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©æ­£ç¡®ç­”æ¡ˆï¼š\n(A) é è¿‘\n(B) è¿œç¦»\n"],
                        ],
                        inputs=[audio_input_think, prompt_input_think],
                        label="ğŸ§ª è¯•è¯•è¿™äº›ä¾‹å­"
                    )

                with gr.Column():
                    output_think = gr.Textbox(label="æ¨¡å‹å›ç­”", lines=10)
                    translated_output_think = gr.Textbox(label="ä¸­æ–‡ç¿»è¯‘", lines=5)

            btn_think.click(fn=think_infer, inputs=[audio_input_think, prompt_input_think], outputs=[output_think, translated_output_think])

        # ---------------- MULTI-TURN CHAT ----------------
        with gr.Tab("ğŸ’¬ å¤šè½®å¯¹è¯"):
            chatbot = gr.Chatbot(label="éŸ³é¢‘èŠå¤©æœºå™¨äºº")
            audio_input_multi = gr.Audio(type="filepath", label="ä¸Šä¼ æˆ–æ›¿æ¢éŸ³é¢‘å†…å®¹")
            user_input_multi = gr.Textbox(label="æ‚¨çš„æ¶ˆæ¯", placeholder="å¯¹éŸ³é¢‘æå‡ºé—®é¢˜...", lines=8)
            btn_multi = gr.Button("å‘é€")
            history_state = gr.State([])           # Chat history
            current_audio_state = gr.State(None)   # Most recent audio file path

            btn_multi.click(
                fn=multi_turn_chat,
                inputs=[user_input_multi, audio_input_multi, history_state, current_audio_state],
                outputs=[chatbot, history_state, current_audio_state]
            )
            gr.Examples(
                examples=[
                    ["static/chat/audio1.mp3", "è¿™é¦–æ›²å­æ„Ÿè§‰éå¸¸å¹³é™å’Œå†…çœã€‚æ˜¯ä»€ä¹ˆå…ƒç´ è®©å®ƒæ„Ÿè§‰å¦‚æ­¤å¹³é™å’Œå†¥æƒ³ï¼Ÿ"],
                    ["static/chat/audio2.mp3", "æ¢ä¸ªé£æ ¼ï¼Œè¿™é¦–éå¸¸æœ‰æ´»åŠ›ä¸”å¯Œæœ‰åˆæˆæ„Ÿã€‚å¦‚æœæˆ‘æƒ³æŠŠé‚£é¦–å¹³é™çš„æ°‘è°£é‡æ–°æ··éŸ³æˆæ¥è¿‘è¿™ç§é£æ ¼ï¼Œä½ æœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ"],
                ],
                inputs=[audio_input_multi, user_input_multi],
                label="ğŸ§ª è¯•è¯•è¿™äº›ä¾‹å­"
            )

        # ---------------- SPEECH PROMPT ----------------
        with gr.Tab("ğŸ—£ï¸ è¯­éŸ³æç¤º"):
            gr.Markdown("ä½¿ç”¨æ‚¨çš„**è¯­éŸ³**ä¸æ¨¡å‹å¯¹è¯ã€‚")

            with gr.Row():
                with gr.Column():
                    speech_input = gr.Audio(type="filepath", label="è¯´è¯æˆ–ä¸Šä¼ éŸ³é¢‘")
                    btn_speech = gr.Button("æäº¤")
                    gr.Examples(
                        examples=[
                            ["static/voice/voice_0.mp3"],
                            ["static/voice/voice_1.mp3"],
                            ["static/voice/voice_2.mp3"],
                        ],
                        inputs=speech_input,
                        label="ğŸ§ª è¯•è¯•è¿™äº›ä¾‹å­"
                    )
                with gr.Column():
                    response_box = gr.Textbox(label="æ¨¡å‹å›ç­”", lines=10)
                    translated_response_box = gr.Textbox(label="ä¸­æ–‡ç¿»è¯‘", lines=5)

            btn_speech.click(fn=speech_prompt_infer, inputs=speech_input, outputs=[response_box, translated_response_box])

        # ---------------- ABOUT ----------------
        with gr.Tab("ğŸ“„ å…³äº"):
            gr.Markdown("""
### ğŸ“š æ¦‚è¿°

**éŸ³é¢‘ç«çƒˆé¸Ÿ 3** æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„æœ€å…ˆè¿›ï¼ˆSOTAï¼‰å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œåœ¨è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹çš„æ¨ç†å’Œç†è§£æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚AF3 å¼•å…¥äº†ï¼š

(i) AF-Whisperï¼Œä¸€ä¸ªç»Ÿä¸€çš„éŸ³é¢‘ç¼–ç å™¨ï¼Œä½¿ç”¨æ–°é¢–çš„ç­–ç•¥è¿›è¡Œè¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¸‰ç§æ¨¡æ€çš„è”åˆè¡¨ç¤ºå­¦ä¹ ï¼›

(ii) çµæ´»çš„æŒ‰éœ€æ€è€ƒï¼Œå…è®¸æ¨¡å‹åœ¨å›ç­”å‰è¿›è¡Œé“¾å¼æ€ç»´æ¨ç†ï¼›

(iii) å¤šè½®ã€å¤šéŸ³é¢‘å¯¹è¯ï¼›

(iv) é•¿éŸ³é¢‘ç†è§£å’Œæ¨ç†ï¼ˆåŒ…æ‹¬è¯­éŸ³ï¼‰æœ€é•¿å¯è¾¾10åˆ†é’Ÿï¼›ä»¥åŠ

(v) è¯­éŸ³åˆ°è¯­éŸ³äº¤äº’ã€‚

ä¸ºäº†å®ç°è¿™äº›åŠŸèƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†å‡ ä¸ªä½¿ç”¨æ–°é¢–ç­–ç•¥ç­–åˆ’çš„å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬AudioSkills-XLã€LongAudio-XLã€AF-Thinkå’ŒAF-Chatï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„äº”é˜¶æ®µè¯¾ç¨‹å¼è®­ç»ƒç­–ç•¥è®­ç»ƒAF3ã€‚ä»…åŸºäºå¼€æºéŸ³é¢‘æ•°æ®è®­ç»ƒï¼ŒAF3åœ¨20å¤šä¸ªï¼ˆé•¿ï¼‰éŸ³é¢‘ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„SOTAç»“æœï¼Œè¶…è¶Šäº†åœ¨æ›´å¤§æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¼€æ”¾æƒé‡å’Œé—­æºæ¨¡å‹ã€‚

**ä¸»è¦ç‰¹æ€§ï¼š**

ğŸ’¡ éŸ³é¢‘ç«çƒˆé¸Ÿ3å…·æœ‰å¼ºå¤§çš„éŸ³é¢‘ã€éŸ³ä¹å’Œè¯­éŸ³ç†è§£èƒ½åŠ›ã€‚

ğŸ’¡ éŸ³é¢‘ç«çƒˆé¸Ÿ3æ”¯æŒæŒ‰éœ€æ€è€ƒè¿›è¡Œé“¾å¼æ€ç»´æ¨ç†ã€‚

ğŸ’¡ éŸ³é¢‘ç«çƒˆé¸Ÿ3æ”¯æŒé•¿è¾¾10åˆ†é’Ÿçš„é•¿éŸ³é¢‘å’Œè¯­éŸ³ç†è§£ã€‚

ğŸ’¡ éŸ³é¢‘ç«çƒˆé¸Ÿ3å¯ä»¥åœ¨å¤æ‚ä¸Šä¸‹æ–‡ä¸­ä¸ç”¨æˆ·è¿›è¡Œå¤šè½®ã€å¤šéŸ³é¢‘èŠå¤©ã€‚

ğŸ’¡ éŸ³é¢‘ç«çƒˆé¸Ÿ3å…·æœ‰è¯­éŸ³åˆ°è¯­éŸ³å¯¹è¯èƒ½åŠ›ã€‚


""")

    gr.Markdown("Â© 2025 NVIDIA | ä½¿ç”¨ â¤ï¸ å’Œ Gradio + PyTorch æ„å»º")


# -----------------------
# Launch App
# -----------------------
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0", 
        server_port=7860,
        debug=True,
        inbrowser=False
    )
