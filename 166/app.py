#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UniPic Gradio Web UI
A comprehensive web interface for image generation, editing, and understanding tasks.
"""

import os
import sys
import json
import numpy as np
import torch
import gradio as gr
from PIL import Image
from einops import rearrange
from mmengine.config import Config
from src.builder import BUILDER
from src.datasets.utils import crop2square
import tempfile
from datetime import datetime

# Set PYTHONPATH
if "./" not in sys.path:
    sys.path.append("./")

# Configuration paths
CONFIG_PATH = "configs/models/qwen2_5_1_5b_kl16_mar_h.py"
CHECKPOINT_PATH = "checkpoint/pytorch_model.bin"
DEFAULT_IMAGE_SIZE = 1024


class UniPicApp:
    """UniPic application wrapper for all tasks."""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.image_token_idx = None
        self.cfg = None
        self.load_model()
    
    def load_model(self):
        """Load the UniPic model."""
        print("Loading UniPic model...")
        
        # Load configuration
        self.cfg = Config.fromfile(CONFIG_PATH)
        
        # Build and load model
        self.model = BUILDER.build(self.cfg.model).eval().cuda().to(torch.bfloat16)
        
        # Load checkpoint
        if os.path.exists(CHECKPOINT_PATH):
            state_dict = torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=False)
            missing, unexpected = self.model.load_state_dict(state_dict, strict=False)
            print(f"Model loaded. Missing: {len(missing)}, Unexpected: {len(unexpected)}")
        else:
            print(f"Warning: Checkpoint not found at {CHECKPOINT_PATH}")
        
        # Add special tokens
        special_tokens_dict = {'additional_special_tokens': ["<image>"]}
        self.model.tokenizer.add_special_tokens(special_tokens_dict)
        self.image_token_idx = self.model.tokenizer.encode("<image>", add_special_tokens=False)[-1]
        
        print("Model loaded successfully!")
    
    def preprocess_image_with_dtype(self, image: Image.Image, image_size: int, dtype: torch.dtype = torch.float32) -> torch.Tensor:
        """Process PIL image to normalized tensor [1,C,H,W] - matches original implementation."""
        img = crop2square(image)
        img = img.resize((image_size, image_size))
        arr = np.asarray(img).astype(np.float32) / 255.0
        arr = 2 * arr - 1
        tensor = torch.from_numpy(arr).to(dtype=dtype)
        return rearrange(tensor, "h w c -> 1 c h w")
    
    def preprocess_image(self, image: Image.Image, image_size: int = None) -> torch.Tensor:
        """Preprocess PIL image to normalized tensor."""
        if image_size is None:
            image_size = DEFAULT_IMAGE_SIZE
            
        img = crop2square(image)
        img = img.resize((image_size, image_size))
        arr = np.asarray(img).astype(np.float32) / 255.0
        arr = 2 * arr - 1
        tensor = torch.from_numpy(arr).to(dtype=self.model.dtype)
        return rearrange(tensor, "h w c -> 1 c h w")
    
    def expand2square(self, pil_img, background_color=(127, 127, 127)):
        """Expand image to square."""
        width, height = pil_img.size
        if width == height:
            return pil_img
        elif width > height:
            result = Image.new(pil_img.mode, (width, width), background_color)
            result.paste(pil_img, (0, (width - height) // 2))
            return result
        else:
            result = Image.new(pil_img.mode, (height, height), background_color)
            result.paste(pil_img, ((height - width) // 2, 0))
            return result
    
    def text_to_image(self, prompt, cfg=3.0, temperature=1.0, num_iter=50, 
                     grid_size=2, image_size=1024, cfg_prompt="Generate an image."):
        """Generate image from text prompt."""
        try:
            # Prepare prompt
            full_prompt = f"Generate an image: {prompt}"
            
            # Prepare text conditions
            class_info = self.model.prepare_text_conditions(full_prompt, cfg_prompt)
            input_ids = class_info['input_ids']
            attention_mask = class_info['attention_mask']
            
            # Handle CFG
            if cfg == 1.0:
                input_ids = input_ids[:1]
                attention_mask = attention_mask[:1]
            
            # Repeat for batch
            bsz = grid_size ** 2
            if cfg != 1.0:
                input_ids = torch.cat([
                    input_ids[:1].expand(bsz, -1),
                    input_ids[1:].expand(bsz, -1),
                ])
                attention_mask = torch.cat([
                    attention_mask[:1].expand(bsz, -1),
                    attention_mask[1:].expand(bsz, -1),
                ])
            else:
                input_ids = input_ids.expand(bsz, -1)
                attention_mask = attention_mask.expand(bsz, -1)
            
            # Sample
            m = n = image_size // 16
            
            with torch.no_grad():
                samples = self.model.sample(
                    input_ids=input_ids, 
                    attention_mask=attention_mask,
                    num_iter=num_iter, 
                    cfg=cfg, 
                    cfg_schedule="constant",
                    temperature=temperature, 
                    progress=True, 
                    image_shape=(m, n)
                )
            
            # Convert to image
            samples = rearrange(samples, '(m n) c h w -> (m h) (n w) c', m=grid_size, n=grid_size)
            samples = torch.clamp(127.5 * samples + 128.0, 0, 255).to("cpu", dtype=torch.uint8).numpy()
            
            result_image = Image.fromarray(samples)
            return result_image, "å›¾åƒç”ŸæˆæˆåŠŸï¼"
            
        except Exception as e:
            return None, f"å›¾åƒç”Ÿæˆå‡ºé”™: {str(e)}"
    
    def image_to_text(self, image, prompt="Describe the image in detail.", image_size=1024):
        """Generate text description from image."""
        try:
            if image is None:
                return "è¯·å…ˆä¸Šä¼ å›¾ç‰‡ã€‚"
            
            # Preprocess image
            image = image.convert('RGB')
            image = self.expand2square(image)
            image = image.resize(size=(image_size, image_size))
            image_tensor = torch.from_numpy(np.array(image)).to(dtype=self.model.dtype, device=self.model.device)
            image_tensor = rearrange(image_tensor, 'h w c -> c h w')[None]
            image_tensor = 2 * (image_tensor / 255) - 1
            
            # Prepare prompt
            if hasattr(self.cfg.model, 'prompt_template'):
                full_prompt = self.cfg.model.prompt_template['INSTRUCTION'].format(
                    input="<image>\n" + prompt
                )
            else:
                full_prompt = f"<image>\n{prompt}"
            
            # Replace image token
            image_length = (image_size // 16) ** 2 + 64
            full_prompt = full_prompt.replace('<image>', '<image>' * image_length)
            
            # Tokenize
            input_ids = self.model.tokenizer.encode(
                full_prompt, add_special_tokens=True, return_tensors='pt'
            ).cuda()
            
            # Extract visual features
            with torch.no_grad():
                _, z_enc = self.model.extract_visual_feature(self.model.encode(image_tensor))
            
            # Prepare embeddings
            inputs_embeds = z_enc.new_zeros(*input_ids.shape, self.model.llm.config.hidden_size)
            inputs_embeds[input_ids == self.image_token_idx] = z_enc.flatten(0, 1)
            inputs_embeds[input_ids != self.image_token_idx] = self.model.llm.get_input_embeddings()(
                input_ids[input_ids != self.image_token_idx]
            )
            
            # Generate
            with torch.no_grad():
                output = self.model.llm.generate(
                    inputs_embeds=inputs_embeds,
                    use_cache=True,
                    do_sample=False,
                    max_new_tokens=1024,
                    eos_token_id=self.model.tokenizer.eos_token_id,
                    pad_token_id=self.model.tokenizer.pad_token_id 
                    if self.model.tokenizer.pad_token_id is not None 
                    else self.model.tokenizer.eos_token_id
                )
            
            result = self.model.tokenizer.decode(output[0], skip_special_tokens=True)
            
            # Extract only the generated part
            if full_prompt.replace('<image>' * image_length, '') in result:
                result = result.split(full_prompt.replace('<image>' * image_length, ''))[-1].strip()
            
            return result
            
        except Exception as e:
            return f"å›¾åƒå¤„ç†å‡ºé”™: {str(e)}"
    
    def edit_image(self, source_image, prompt, num_iter=50, cfg=3.0, 
                  cfg_prompt="Repeat this image.", temperature=0.85, image_size=1024):
        """Edit image based on text prompt - follows original EditInferencer implementation exactly."""
        try:
            if source_image is None:
                return None, "è¯·å…ˆä¸Šä¼ å›¾ç‰‡ã€‚"
            
            grid_size = 1  # Force single image for web UI
            
            # 1) Preprocess source image - exactly like original
            img_tensor = self.preprocess_image_with_dtype(
                source_image, 
                image_size,
                dtype=self.model.dtype
            ).to(self.model.device)
            
            # 2) Encode image and extract features
            with torch.no_grad():
                x_enc = self.model.encode(img_tensor)
                x_con, z_enc = self.model.extract_visual_feature(x_enc)
            
            # 3) Prepare text prompts
            m = n = image_size // 16
            image_length = m * n + 64
            
            if hasattr(self.cfg.model, 'prompt_template'):
                prompt_str = self.cfg.model.prompt_template['INSTRUCTION'].format(
                    input="<image>\n" + prompt.strip()
                )
                cfg_prompt_str = self.cfg.model.prompt_template['INSTRUCTION'].format(
                    input="<image>\n" + cfg_prompt.strip()
                )
            else:
                prompt_str = f"<image>\n{prompt.strip()}"
                cfg_prompt_str = f"<image>\n{cfg_prompt.strip()}"
            
            # Replace <image> token with multiple tokens
            prompt_str = prompt_str.replace('<image>', '<image>' * image_length)
            cfg_prompt_str = cfg_prompt_str.replace('<image>', '<image>' * image_length)
            
            # 4) Tokenize and prepare inputs
            input_ids = self.model.tokenizer.encode(
                prompt_str, add_special_tokens=True, return_tensors='pt')[0].cuda()
            
            if cfg != 1.0:
                null_input_ids = self.model.tokenizer.encode(
                    cfg_prompt_str, add_special_tokens=True, return_tensors='pt')[0].cuda()
                
                # Import pad_sequence locally to match original
                from torch.nn.utils.rnn import pad_sequence
                attention_mask = pad_sequence(
                    [torch.ones_like(input_ids), torch.ones_like(null_input_ids)],
                    batch_first=True, padding_value=0
                ).to(torch.bool)
                input_ids = pad_sequence(
                    [input_ids, null_input_ids],
                    batch_first=True, padding_value=self.model.tokenizer.eos_token_id
                )
            else:
                input_ids = input_ids[None]
                attention_mask = torch.ones_like(input_ids).to(torch.bool)
            
            # 5) Prepare embeddings
            if cfg != 1.0:
                z_enc = torch.cat([z_enc, z_enc], dim=0)
                x_con = torch.cat([x_con, x_con], dim=0)
            
            inputs_embeds = z_enc.new_zeros(*input_ids.shape, self.model.llm.config.hidden_size)
            inputs_embeds[input_ids == self.image_token_idx] = z_enc.flatten(0, 1)
            inputs_embeds[input_ids != self.image_token_idx] = self.model.llm.get_input_embeddings()(
                input_ids[input_ids != self.image_token_idx]
            )
            
            # 6) Repeat for grid sampling
            bsz = grid_size ** 2
            x_con = torch.cat([x_con] * bsz)
            if cfg != 1.0:
                inputs_embeds = torch.cat([
                    inputs_embeds[:1].expand(bsz, -1, -1),
                    inputs_embeds[1:].expand(bsz, -1, -1),
                ])
                attention_mask = torch.cat([
                    attention_mask[:1].expand(bsz, -1),
                    attention_mask[1:].expand(bsz, -1),
                ])
            else:
                inputs_embeds = inputs_embeds.expand(bsz, -1, -1)
                attention_mask = attention_mask.expand(bsz, -1)
            
            # 7) Sampling - exactly like original
            samples = self.model.sample(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                num_iter=num_iter,
                cfg=cfg,
                cfg_schedule="constant",  # Use constant instead of cfg_schedule parameter
                temperature=temperature,
                progress=False,  # Set to False for web UI
                image_shape=(m, n),
                x_con=x_con
            )
            
            # 8) Convert to PIL Image - exactly like original
            samples = rearrange(samples, '(m n) c h w -> (m h) (n w) c', m=grid_size, n=grid_size)
            samples = torch.clamp(127.5 * samples + 128.0, 0, 255)
            out = samples.to("cpu", torch.uint8).numpy()
            result_image = Image.fromarray(out)
            
            return result_image, "å›¾åƒç¼–è¾‘æˆåŠŸï¼"
            
        except Exception as e:
            import traceback
            error_msg = f"Error editing image: {str(e)}\n{traceback.format_exc()}"
            print(error_msg)  # For debugging
            return None, f"å›¾åƒç¼–è¾‘å‡ºé”™: {str(e)}"


# Initialize the app (will be done in main)
app = None

# Define Gradio interface
def create_interface():
    """Create the Gradio interface."""
    
    global app  # Use the global app instance
    
    # Example prompts for text-to-image
    t2i_examples = [
        "A glossy-coated golden retriever stands on the park lawn beside a life-sized penguin statue.",
        "Digital portrait of a girl with rainbow hair.",
        "A majestic mountain landscape with snow-capped peaks and a crystal clear lake.",
        "A futuristic cityscape at night with flying cars and neon lights.",
        "A beautiful butterfly garden with colorful flowers and gentle sunlight.",
        "An ancient castle on a hill surrounded by misty forests."
    ]
    
    # Example prompts for image editing
    edit_examples = [
        "Replace the stars with the candle.",
        "Change the background to a beach scene.",
        "Add some flowers in the foreground.",
        "Make the sky more dramatic with storm clouds.",
        "Replace the grass with snow.",
        "Add a rainbow in the sky."
    ]
    
    # Example prompts for image-to-text
    i2t_examples = [
        "Describe the image in detail.",
        "What objects can you see in this image?",
        "What is the mood or atmosphere of this image?",
        "Describe the colors and lighting in this image.",
        "What activities are happening in this image?",
        "Describe the setting and environment."
    ]
    
    with gr.Blocks(title="UniPic ç½‘é¡µç•Œé¢", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ¨ UniPic ç½‘é¡µç•Œé¢")
        gr.Markdown("ç”¨äºå›¾åƒç”Ÿæˆã€ç¼–è¾‘å’Œç†è§£ä»»åŠ¡çš„ç»¼åˆç•Œé¢")
        gr.Markdown("ğŸ’¡ **æç¤º**: ç‚¹å‡»ç¤ºä¾‹æŒ‰é’®å¿«é€Ÿå¼€å§‹ï¼Œæˆ–è€…ä¸Šä¼ ä½ è‡ªå·±çš„å›¾ç‰‡å’Œæç¤ºè¯ã€‚")
        
        with gr.Tabs():
            # Text-to-Image Tab
            with gr.Tab("ğŸ–¼ï¸ æ–‡æœ¬ç”Ÿæˆå›¾åƒ"):
                gr.Markdown("### æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒ")
                gr.Markdown("è¾“å…¥æ–‡æœ¬æè¿°ï¼Œç”¨AIç”Ÿæˆç²¾ç¾å›¾åƒã€‚")
                
                with gr.Row():
                    with gr.Column():
                        t2i_prompt = gr.Textbox(
                            label="æç¤ºè¯",
                            placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„å›¾åƒæè¿°...",
                            lines=3,
                            value=t2i_examples[0]
                        )
                        
                        # Example buttons for quick prompts
                        with gr.Row():
                            gr.Markdown("**å¿«é€Ÿç¤ºä¾‹:**")
                        
                        example_buttons_t2i = []
                        for i in range(0, len(t2i_examples), 3):
                            with gr.Row():
                                for j in range(3):
                                    if i + j < len(t2i_examples):
                                        btn = gr.Button(f"ç¤ºä¾‹ {i+j+1}", size="sm")
                                        example_buttons_t2i.append((btn, t2i_examples[i+j]))
                        
                        with gr.Row():
                            t2i_cfg = gr.Slider(1.0, 10.0, value=3.0, label="CFG æ§åˆ¶å¼ºåº¦")
                            t2i_temperature = gr.Slider(0.1, 2.0, value=1.0, label="éšæœºæ€§")
                        
                        with gr.Row():
                            t2i_num_iter = gr.Slider(10, 100, value=50, step=1, label="è¿­ä»£æ¬¡æ•°")
                            t2i_grid_size = gr.Slider(1, 4, value=2, step=1, label="ç”Ÿæˆæ•°é‡")
                        
                        t2i_image_size = gr.Dropdown(
                            choices=[512, 768, 1024], 
                            value=1024, 
                            label="å›¾åƒå°ºå¯¸"
                        )
                        
                        t2i_generate_btn = gr.Button("ğŸš€ ç”Ÿæˆå›¾åƒ", variant="primary", size="lg")
                    
                    with gr.Column():
                        t2i_output = gr.Image(label="ç”Ÿæˆçš„å›¾åƒ", type="pil")
                        t2i_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
                
                # Connect example buttons
                for btn, example_text in example_buttons_t2i:
                    btn.click(fn=lambda x=example_text: x, outputs=t2i_prompt)
                
                t2i_generate_btn.click(
                    fn=app.text_to_image,
                    inputs=[t2i_prompt, t2i_cfg, t2i_temperature, t2i_num_iter, 
                           t2i_grid_size, t2i_image_size],
                    outputs=[t2i_output, t2i_status]
                )
            
            # Image Editing Tab
            with gr.Tab("âœ‚ï¸ å›¾åƒç¼–è¾‘"):
                gr.Markdown("### æ ¹æ®æ–‡æœ¬æç¤ºç¼–è¾‘å›¾åƒ")
                gr.Markdown("ä¸Šä¼ å›¾åƒå¹¶æè¿°æ‚¨æƒ³è¦çš„ä¿®æ”¹ã€‚")
                
                with gr.Row():
                    with gr.Column():
                        edit_image_input = gr.Image(label="æºå›¾åƒ", type="pil")
                        
                        # Sample image button
                        sample_btn = gr.Button("ğŸ“ åŠ è½½ç¤ºä¾‹å›¾åƒ", size="sm")
                        
                        edit_prompt = gr.Textbox(
                            label="ç¼–è¾‘æç¤ºè¯",
                            placeholder="æè¿°æ‚¨æƒ³è¦å¦‚ä½•ç¼–è¾‘å›¾åƒ...",
                            lines=2,
                            value=edit_examples[0]
                        )
                        
                        # Example buttons for editing prompts
                        with gr.Row():
                            gr.Markdown("**å¿«é€Ÿç¤ºä¾‹:**")
                        
                        example_buttons_edit = []
                        for i in range(0, len(edit_examples), 2):
                            with gr.Row():
                                for j in range(2):
                                    if i + j < len(edit_examples):
                                        btn = gr.Button(f"ç¼–è¾‘ {i+j+1}", size="sm")
                                        example_buttons_edit.append((btn, edit_examples[i+j]))
                        
                        with gr.Row():
                            edit_cfg = gr.Slider(1.0, 10.0, value=3.0, label="CFG æ§åˆ¶å¼ºåº¦")
                            edit_temperature = gr.Slider(0.1, 2.0, value=0.85, label="éšæœºæ€§")
                        
                        edit_num_iter = gr.Slider(10, 100, value=50, step=1, label="è¿­ä»£æ¬¡æ•°")
                        edit_image_size = gr.Dropdown(
                            choices=[512, 768, 1024], 
                            value=1024, 
                            label="å›¾åƒå°ºå¯¸"
                        )
                        
                        edit_btn = gr.Button("âœ¨ ç¼–è¾‘å›¾åƒ", variant="primary", size="lg")
                    
                    with gr.Column():
                        edit_output = gr.Image(label="ç¼–è¾‘åçš„å›¾åƒ", type="pil")
                        edit_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
                
                # Load sample image function
                def load_sample_image():
                    try:
                        from PIL import Image
                        return Image.open("data/sample.png")
                    except:
                        return None
                
                sample_btn.click(fn=load_sample_image, outputs=edit_image_input)
                
                # Connect example buttons
                for btn, example_text in example_buttons_edit:
                    btn.click(fn=lambda x=example_text: x, outputs=edit_prompt)
                
                edit_btn.click(
                    fn=app.edit_image,
                    inputs=[edit_image_input, edit_prompt, edit_num_iter, edit_cfg, 
                           gr.Textbox(value="Repeat this image.", visible=False), 
                           edit_temperature, edit_image_size],
                    outputs=[edit_output, edit_status]
                )
            
            # Image-to-Text Tab
            with gr.Tab("ğŸ“– å›¾åƒç†è§£"):
                gr.Markdown("### ä»å›¾åƒç”Ÿæˆæ–‡æœ¬æè¿°")
                gr.Markdown("ä¸Šä¼ å›¾åƒå¹¶è¯¢é—®ç›¸å…³é—®é¢˜æˆ–è·å–è¯¦ç»†æè¿°ã€‚")
                
                with gr.Row():
                    with gr.Column():
                        i2t_image_input = gr.Image(label="è¾“å…¥å›¾åƒ", type="pil")
                        
                        # Sample image button for i2t
                        sample_btn_i2t = gr.Button("ğŸ“ åŠ è½½ç¤ºä¾‹å›¾åƒ", size="sm")
                        
                        i2t_prompt = gr.Textbox(
                            label="é—®é¢˜",
                            value=i2t_examples[0],
                            lines=2
                        )
                        
                        # Example buttons for i2t prompts
                        with gr.Row():
                            gr.Markdown("**å¿«é€Ÿç¤ºä¾‹:**")
                        
                        example_buttons_i2t = []
                        for i in range(0, len(i2t_examples), 2):
                            with gr.Row():
                                for j in range(2):
                                    if i + j < len(i2t_examples):
                                        btn = gr.Button(f"é—®é¢˜ {i+j+1}", size="sm")
                                        example_buttons_i2t.append((btn, i2t_examples[i+j]))
                        
                        i2t_image_size = gr.Dropdown(
                            choices=[512, 768, 1024], 
                            value=1024, 
                            label="å›¾åƒå°ºå¯¸"
                        )
                        i2t_btn = gr.Button("ğŸ” ç”Ÿæˆæè¿°", variant="primary", size="lg")
                    
                    with gr.Column():
                        i2t_output = gr.Textbox(label="ç”Ÿæˆçš„æè¿°", lines=10)
                
                sample_btn_i2t.click(fn=load_sample_image, outputs=i2t_image_input)
                
                # Connect example buttons
                for btn, example_text in example_buttons_i2t:
                    btn.click(fn=lambda x=example_text: x, outputs=i2t_prompt)
                
                i2t_btn.click(
                    fn=app.image_to_text,
                    inputs=[i2t_image_input, i2t_prompt, i2t_image_size],
                    outputs=i2t_output
                )
            
            # Batch Processing Tab
            with gr.Tab("ğŸ”„ æ‰¹é‡å¤„ç†"):
                gr.Markdown("### æ‰¹é‡æ–‡æœ¬ç”Ÿæˆå›¾åƒ")
                gr.Markdown("ä»æç¤ºè¯åˆ—è¡¨ç”Ÿæˆå¤šå¼ å›¾åƒã€‚")
                
                with gr.Row():
                    with gr.Column():
                        batch_prompts = gr.Textbox(
                            label="æç¤ºè¯ (æ¯è¡Œä¸€ä¸ª)",
                            placeholder="è¾“å…¥å¤šä¸ªæç¤ºè¯ï¼Œæ¯è¡Œä¸€ä¸ª...",
                            lines=5,
                            value="\n".join(t2i_examples[:4])
                        )
                        
                        # Load example prompts button
                        load_examples_btn = gr.Button("ğŸ“ åŠ è½½ç¤ºä¾‹æç¤ºè¯", size="sm")
                        
                        with gr.Row():
                            batch_cfg = gr.Slider(1.0, 10.0, value=3.0, label="CFG æ§åˆ¶å¼ºåº¦")
                            batch_temperature = gr.Slider(0.1, 2.0, value=1.0, label="éšæœºæ€§")
                        
                        batch_num_iter = gr.Slider(10, 100, value=50, step=1, label="è¿­ä»£æ¬¡æ•°")
                        batch_image_size = gr.Dropdown(
                            choices=[512, 768, 1024], 
                            value=1024, 
                            label="å›¾åƒå°ºå¯¸"
                        )
                        
                        batch_btn = gr.Button("ğŸš€ æ‰¹é‡ç”Ÿæˆ", variant="primary", size="lg")
                    
                    with gr.Column():
                        batch_gallery = gr.Gallery(label="ç”Ÿæˆçš„å›¾åƒ", columns=2, rows=2)
                        batch_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
                
                def load_example_prompts():
                    return "\n".join(t2i_examples)
                
                load_examples_btn.click(fn=load_example_prompts, outputs=batch_prompts)
                
                def batch_text_to_image(prompts_text, cfg, temperature, num_iter, image_size):
                    """Generate multiple images from prompts."""
                    try:
                        prompts = [p.strip() for p in prompts_text.split('\n') if p.strip()]
                        if not prompts:
                            return [], "è¯·è‡³å°‘è¾“å…¥ä¸€ä¸ªæç¤ºè¯ã€‚"
                        
                        images = []
                        for i, prompt in enumerate(prompts):
                            image, status = app.text_to_image(
                                prompt, cfg, temperature, num_iter, 1, image_size
                            )
                            if image:
                                images.append(image)
                        
                        return images, f"æˆåŠŸç”Ÿæˆ {len(images)} å¼ å›¾åƒï¼"
                    except Exception as e:
                        return [], f"æ‰¹é‡å¤„ç†å‡ºé”™: {str(e)}"
                
                batch_btn.click(
                    fn=batch_text_to_image,
                    inputs=[batch_prompts, batch_cfg, batch_temperature, batch_num_iter, batch_image_size],
                    outputs=[batch_gallery, batch_status]
                )
        
        # Footer
        gr.Markdown("---")
        gr.Markdown("### ğŸ“š ä½¿ç”¨è¯´æ˜")
        with gr.Row():
            with gr.Column():
                gr.Markdown("""
                **æ–‡æœ¬ç”Ÿæˆå›¾åƒ:**
                - ä½¿ç”¨æè¿°æ€§æç¤ºè¯è·å¾—æ›´å¥½æ•ˆæœ
                - æ›´é«˜çš„CFGå€¼ = æ›´ä¸¥æ ¼éµå¾ªæç¤ºè¯
                - æ›´å¤šè¿­ä»£æ¬¡æ•° = æ›´é«˜è´¨é‡ (ä½†æ›´æ…¢)
                """)
            with gr.Column():
                gr.Markdown("""
                **å›¾åƒç¼–è¾‘:**
                - ä¸Šä¼ æ¸…æ™°çš„æºå›¾åƒ
                - æ˜ç¡®æè¿°æ‰€éœ€çš„ä¿®æ”¹
                - å°è¯•ä¸åŒçš„éšæœºæ€§å€¼è·å¾—å¤šæ ·æ€§
                """)
            with gr.Column():
                gr.Markdown("""
                **å›¾åƒç†è§£:**
                - é—®å…·ä½“é—®é¢˜è·å¾—é’ˆå¯¹æ€§å›ç­”
                - æ›´é«˜åˆ†è¾¨ç‡ = æ£€æµ‹æ›´å¤šç»†èŠ‚
                - å°è¯•ä¸åŒçš„é—®é¢˜æ–¹å¼
                """)
        
        gr.Markdown("---")
        gr.Markdown("åŸºäº SkyworkAI çš„ UniPic æ„å»º | ç”± Gradio æä¾›æ”¯æŒ")
    
    return interface


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ UniPic Gradio ç½‘é¡µç•Œé¢...")
    
    # Check if gradio is available
    try:
        import gradio as gr
        print("âœ… Gradio å¯¼å…¥æˆåŠŸ")
    except ImportError:
        print("âŒ æœªæ‰¾åˆ° Gradioï¼Œæ­£åœ¨å®‰è£…...")
        import os
        os.system("pip install gradio>=4.0.0")
        import gradio as gr
        print("âœ… Gradio å®‰è£…å¹¶å¯¼å…¥æˆåŠŸ")
    
    # Initialize the app (this will load the model)
    print("ğŸ”„ åˆå§‹åŒ– UniPic æ¨¡å‹...")
    app = UniPicApp()
    print("âœ… UniPic æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # Create and launch the interface
    print("ğŸ¨ åˆ›å»º Gradio ç•Œé¢...")
    interface = create_interface()
    print("âœ… ç•Œé¢åˆ›å»ºæˆåŠŸ")
    
    print("")
    print("ğŸŒŸ UniPic ç½‘é¡µç•Œé¢å·²å°±ç»ªï¼")
    print("ğŸ“± åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://localhost:7860")
    print("â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
    print("")
    print("ğŸ“‹ å¯ç”¨åŠŸèƒ½:")
    print("   ğŸ–¼ï¸  æ–‡æœ¬ç”Ÿæˆå›¾åƒ")
    print("   âœ‚ï¸  å›¾åƒç¼–è¾‘")
    print("   ğŸ“– å›¾åƒç†è§£")
    print("   ğŸ”„ æ‰¹é‡å¤„ç†")
    print("")
    
    # Launch with optimal settings
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=False,
        quiet=False,
        show_error=True,
        favicon_path=None,
        ssl_verify=False
    )
