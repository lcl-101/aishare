import gradio as gr
import numpy as np
import random
import os
import time
import torch
import math
import torch.nn as nn
from PIL import Image
from diffusers import DiffusionPipeline, FlowMatchEulerDiscreteScheduler
from diffusers.models import QwenImageTransformer2DModel
from safetensors.torch import safe_open

# é…ç½®å‚æ•°
MAX_SEED = np.iinfo(np.int32).max
MAX_IMAGE_SIZE = 1440

# å…¨å±€å˜é‡å­˜å‚¨ç®¡é“
base_pipe = None
lora_pipe = None

# ç¤ºä¾‹æç¤ºè¯
EXAMPLE_PROMPTS = [
    'A coffee shop entrance features a chalkboard sign reading "Qwen Coffee ğŸ˜Š $2 per cup," with a neon light beside it displaying "é€šä¹‰åƒé—®". Next to it hangs a poster showing a beautiful Chinese woman, and beneath the poster is written "Ï€â‰ˆ3.1415926-53589793-23846264-33832795-02384197".',
    "ä¸€ä¸ªå’–å•¡åº—é—¨å£æœ‰ä¸€ä¸ªé»‘æ¿ï¼Œä¸Šé¢å†™ç€é€šä¹‰åƒé—®å’–å•¡ï¼Œ2ç¾å…ƒä¸€æ¯ï¼Œæ—è¾¹æœ‰ä¸ªéœ“è™¹ç¯ï¼Œå†™ç€é˜¿é‡Œå·´å·´ï¼Œæ—è¾¹æœ‰ä¸ªæµ·æŠ¥ï¼Œæµ·æŠ¥ä¸Šé¢æ˜¯ä¸€ä¸ªä¸­å›½ç¾å¥³ï¼Œæµ·æŠ¥ä¸‹æ–¹å†™ç€qwen newbee",
    "ä¸€å¹…ç²¾è‡´ç»†è…»çš„å·¥ç¬”ç”»ï¼Œç”»é¢ä¸­å¿ƒæ˜¯ä¸€æ ªè“¬å‹ƒç”Ÿé•¿çš„çº¢è‰²ç‰¡ä¸¹ï¼ŒèŠ±æœµç¹èŒ‚ï¼Œæ—¢æœ‰ç››å¼€çš„ç¡•å¤§èŠ±ç“£ï¼Œä¹Ÿæœ‰å«è‹å¾…æ”¾çš„èŠ±è•¾ï¼Œå±‚æ¬¡ä¸°å¯Œï¼Œè‰²å½©è‰³ä¸½è€Œä¸å¤±å…¸é›…ã€‚ç‰¡ä¸¹æå¶èˆ’å±•ï¼Œå¶ç‰‡æµ“ç»¿é¥±æ»¡ï¼Œè„‰ç»œæ¸…æ™°å¯è§ï¼Œä¸çº¢èŠ±ç›¸æ˜ æˆè¶£ã€‚ä¸€åªè“ç´«è‰²è´è¶ä»¿ä½›è¢«ç”»ä¸­èŠ±æœµå¸å¼•ï¼Œåœé©»åœ¨ç”»é¢ä¸­å¤®çš„ä¸€æœµç››å¼€ç‰¡ä¸¹ä¸Šï¼Œæµè¿å¿˜è¿”ï¼Œè¶ç¿¼è½»å±•ï¼Œç»†èŠ‚é€¼çœŸï¼Œä»¿ä½›éšæ—¶ä¼šéšé£é£èˆã€‚æ•´å¹…ç”»ä½œç¬”è§¦å·¥æ•´ä¸¥è°¨ï¼Œè‰²å½©æµ“éƒé²œæ˜ï¼Œå±•ç°å‡ºä¸­å›½ä¼ ç»Ÿå·¥ç¬”ç”»çš„ç²¾å¦™ä¸ç¥éŸµï¼Œç”»é¢å……æ»¡ç”Ÿæœºä¸çµåŠ¨ä¹‹æ„Ÿã€‚",
    "A capybara wearing a suit holding a sign that reads Hello World",
    'ä¸€ä¸ªä¼šè®®å®¤ï¼Œå¢™ä¸Šå†™ç€"3.14159265-358979-32384626-4338327950"ï¼Œä¸€ä¸ªå°é™€èºåœ¨æ¡Œä¸Šè½¬åŠ¨',
    'A young girl wearing school uniform stands in a classroom, writing on a chalkboard. The text "Introducing Qwen-Image, a foundational image generation model that excels in complex text rendering and precise image editing" appears in neat white chalk at the center of the blackboard. Soft natural light filters through windows, casting gentle shadows. The scene is rendered in a realistic photography style with fine details, shallow depth of field, and warm tones. The girl\'s focused expression and chalk dust in the air add dynamism. Background elements include desks and educational posters, subtly blurred to emphasize the central action. Ultra-detailed 32K resolution, DSLR-quality, soft bokeh effect, documentary-style composition',
    "Realistic still life photography style: A single, fresh apple resting on a clean, soft-textured surface. The apple is slightly off-center, softly backlit to highlight its natural gloss and subtle color gradientsâ€”deep crimson red blending into light golden hues. Fine details such as small blemishes, dew drops, and a few light highlights enhance its lifelike appearance. A shallow depth of field gently blurs the neutral background, drawing full attention to the apple. Hyper-detailed 8K resolution, studio lighting, photorealistic render, emphasizing texture and form."
]

def build_lora_names(key, lora_down_key, lora_up_key, is_native_weight):
    """æ„å»ºLoRAæƒé‡åç§°"""
    base = "diffusion_model." if is_native_weight else ""
    lora_down = base + key.replace(".weight", lora_down_key)
    lora_up = base + key.replace(".weight", lora_up_key)
    lora_alpha = base + key.replace(".weight", ".alpha")
    return lora_down, lora_up, lora_alpha

def load_and_merge_lora_weight(
    model: nn.Module,
    lora_state_dict: dict,
    lora_down_key: str = ".lora_down.weight",
    lora_up_key: str = ".lora_up.weight",
):
    """åŠ è½½å¹¶åˆå¹¶LoRAæƒé‡"""
    is_native_weight = any("diffusion_model." in key for key in lora_state_dict)
    for key, value in model.named_parameters():
        lora_down_name, lora_up_name, lora_alpha_name = build_lora_names(
            key, lora_down_key, lora_up_key, is_native_weight
        )
        if lora_down_name in lora_state_dict:
            lora_down = lora_state_dict[lora_down_name]
            lora_up = lora_state_dict[lora_up_name]
            lora_alpha = float(lora_state_dict[lora_alpha_name])
            rank = lora_down.shape[0]
            scaling_factor = lora_alpha / rank
            assert lora_up.dtype == torch.float32
            assert lora_down.dtype == torch.float32
            delta_W = scaling_factor * torch.matmul(lora_up, lora_down)
            value.data = (value.data + delta_W).type_as(value.data)
    return model

def load_and_merge_lora_weight_from_safetensors(
    model: nn.Module,
    lora_weight_path: str,
    lora_down_key: str = ".lora_down.weight",
    lora_up_key: str = ".lora_up.weight",
):
    """ä»SafeTensorsæ–‡ä»¶åŠ è½½LoRAæƒé‡"""
    lora_state_dict = {}
    with safe_open(lora_weight_path, framework="pt", device="cpu") as f:
        for key in f.keys():
            lora_state_dict[key] = f.get_tensor(key)
    model = load_and_merge_lora_weight(
        model, lora_state_dict, lora_down_key, lora_up_key
    )
    return model

def load_base_model():
    """åŠ è½½åŸºç¡€æ¨¡å‹"""
    global base_pipe
    if base_pipe is None:
        model_name = "checkpoints"
        
        if torch.cuda.is_available():
            torch_dtype = torch.bfloat16
            device = "cuda"
        else:
            torch_dtype = torch.float32
            device = "cpu"
        
        base_pipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch_dtype)
        base_pipe = base_pipe.to(device)
        print(f"åŸºç¡€æ¨¡å‹å·²åŠ è½½åˆ° {device}")
    return base_pipe

def load_lora_model():
    """åŠ è½½LoRAæ¨¡å‹"""
    global lora_pipe
    if lora_pipe is None:
        model_name = "checkpoints"
        lora_path = "checkpoints/lora/Qwen-Image-Lightning-8steps-V1.0.safetensors"
        
        if torch.cuda.is_available():
            torch_dtype = torch.bfloat16
            device = "cuda"
        else:
            torch_dtype = torch.float32
            device = "cpu"
        
        if os.path.exists(lora_path):
            # åŠ è½½LoRAæ¨¡å‹
            model = QwenImageTransformer2DModel.from_pretrained(
                model_name, subfolder="transformer", torch_dtype=torch_dtype
            )
            model = load_and_merge_lora_weight_from_safetensors(model, lora_path)
            
            # è®¾ç½®è°ƒåº¦å™¨é…ç½®
            scheduler_config = {
                "base_image_seq_len": 256,
                "base_shift": math.log(3),
                "invert_sigmas": False,
                "max_image_seq_len": 8192,
                "max_shift": math.log(3),
                "num_train_timesteps": 1000,
                "shift": 1.0,
                "shift_terminal": None,
                "stochastic_sampling": False,
                "time_shift_type": "exponential",
                "use_beta_sigmas": False,
                "use_dynamic_shifting": True,
                "use_exponential_sigmas": False,
                "use_karras_sigmas": False,
            }
            scheduler = FlowMatchEulerDiscreteScheduler.from_config(scheduler_config)
            lora_pipe = DiffusionPipeline.from_pretrained(
                model_name, transformer=model, scheduler=scheduler, torch_dtype=torch_dtype
            )
            lora_pipe = lora_pipe.to(device)
            print(f"LoRAæ¨¡å‹å·²åŠ è½½åˆ° {device}")
        else:
            print(f"LoRAæ–‡ä»¶ {lora_path} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹")
            lora_pipe = load_base_model()
    return lora_pipe

def detect_language(text):
    """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
    chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
    return "zh" if chinese_chars > 0 else "en"

def generate_single_image(pipe, prompt, width, height, num_steps, cfg_scale, seed):
    """ç”Ÿæˆå•å¼ å›¾ç‰‡"""
    # æ£€æµ‹è¯­è¨€å¹¶æ·»åŠ é­”æ³•è¯
    lang = detect_language(prompt)
    positive_magic = {
        "en": "Ultra HD, 4K, cinematic composition. ",
        "zh": "è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾ã€‚"
    }
    
    full_prompt = positive_magic[lang] + prompt
    negative_prompt = " "  # æ¨èçš„ç©ºè´Ÿé¢æç¤ºè¯
    
    generator = torch.Generator(device=pipe.device).manual_seed(seed)
    
    image = pipe(
        prompt=full_prompt,
        negative_prompt=negative_prompt,
        width=width,
        height=height,
        num_inference_steps=num_steps,
        true_cfg_scale=cfg_scale,
        generator=generator
    ).images[0]
    
    return image

def generate_comparison(prompt, width, height, seed, progress=gr.Progress()):
    """ç”Ÿæˆå¯¹æ¯”å›¾ç‰‡"""
    if not prompt:
        return None, None, "è¯·è¾“å…¥æç¤ºè¯"
    
    if seed == -1:
        seed = random.randint(0, MAX_SEED)
    
    progress(0.1, desc="æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½æ¨¡å‹
    base_model = load_base_model()
    lora_model = load_lora_model()
    
    try:
        # ç”ŸæˆåŸºç¡€æ¨¡å‹å›¾ç‰‡
        progress(0.3, desc="æ­£åœ¨ç”ŸæˆåŸºç¡€æ¨¡å‹å›¾ç‰‡...")
        base_start_time = time.time()
        
        base_image = generate_single_image(
            base_model, prompt, width, height, 
            num_steps=50, cfg_scale=4.0, seed=seed
        )
        
        base_end_time = time.time()
        base_generation_time = base_end_time - base_start_time
        
        # ç”ŸæˆLoRAæ¨¡å‹å›¾ç‰‡
        progress(0.7, desc="æ­£åœ¨ç”ŸæˆLoRAæ¨¡å‹å›¾ç‰‡...")
        lora_start_time = time.time()
        
        lora_image = generate_single_image(
            lora_model, prompt, width, height,
            num_steps=8, cfg_scale=1.0, seed=seed
        )
        
        lora_end_time = time.time()
        lora_generation_time = lora_end_time - lora_start_time
        
        progress(1.0, desc="ç”Ÿæˆå®Œæˆ!")
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = int(time.time())
        base_filename = f"base_{timestamp}_{seed}.png"
        lora_filename = f"lora_{timestamp}_{seed}.png"
        base_image.save(base_filename)
        lora_image.save(lora_filename)
        
        # è®¡ç®—é€Ÿåº¦æå‡
        speed_improvement = base_generation_time / lora_generation_time if lora_generation_time > 0 else 0
        
        info = f"""ç”Ÿæˆå®Œæˆï¼
ç§å­å€¼: {seed}
å°ºå¯¸: {width}x{height}

ğŸ“Š æ€§èƒ½å¯¹æ¯”:
åŸºç¡€æ¨¡å‹ (50æ­¥, CFG 4.0): {base_generation_time:.2f}ç§’
LoRAæ¨¡å‹ (8æ­¥, CFG 1.0): {lora_generation_time:.2f}ç§’
é€Ÿåº¦æå‡: {speed_improvement:.1f}x

ğŸ’¾ ä¿å­˜æ–‡ä»¶:
åŸºç¡€æ¨¡å‹: {base_filename}
LoRAæ¨¡å‹: {lora_filename}
        """
        
        return base_image, lora_image, info
        
    except Exception as e:
        return None, None, f"ç”Ÿæˆå¤±è´¥: {str(e)}"

def randomize_seed():
    """éšæœºç§å­ç”Ÿæˆ"""
    return random.randint(0, MAX_SEED)

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    with gr.Blocks(title="Qwen-Image å¯¹æ¯”ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as demo:
        gr.HTML("""
        <div style="text-align: center; margin-bottom: 20px;">
            <h1 style="color: #2E86AB; font-size: 2.5em;">ğŸ¨ Qwen-Image å¯¹æ¯”ç”Ÿæˆå™¨</h1>
            <p style="font-size: 1.2em; color: #555;">åŸºç¡€æ¨¡å‹ vs LoRA Lightning æ¨¡å‹å¯¹æ¯”</p>
            <p style="color: #777;">â±ï¸ é€ä¸ªç”Ÿæˆå›¾ç‰‡ï¼Œç²¾ç¡®æµ‹é‡ç”Ÿæˆæ—¶é—´</p>
            <p style="color: #888; font-size: 0.9em;">å·¦ä¾§ï¼šåŸºç¡€æ¨¡å‹ (50æ­¥, CFG 4.0) | å³ä¾§ï¼šLoRAæ¨¡å‹ (8æ­¥, CFG 1.0)</p>
            <p style="color: #666; font-size: 0.8em;">ğŸš€ LoRA Lightningæ¨¡å‹èƒ½æ˜¾è‘—æå‡ç”Ÿæˆé€Ÿåº¦</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                prompt_input = gr.Textbox(
                    label="æç¤ºè¯ (Prompt)",
                    placeholder="æè¿°ä½ æƒ³è¦ç”Ÿæˆçš„å›¾ç‰‡...",
                    lines=4,
                    value=""
                )
                
                # ç¤ºä¾‹æç¤ºè¯
                with gr.Row():
                    gr.Markdown("**ç¤ºä¾‹æç¤ºè¯:**")
                example_dropdown = gr.Dropdown(
                    choices=[prompt[:100] + "..." if len(prompt) > 100 else prompt for prompt in EXAMPLE_PROMPTS],
                    label="é€‰æ‹©ç¤ºä¾‹",
                    interactive=True
                )
                
                with gr.Row():
                    seed_input = gr.Number(
                        label="ç§å­å€¼ (Seed)",
                        value=42,
                        minimum=-1,
                        maximum=MAX_SEED
                    )
                    random_seed_btn = gr.Button("ğŸ² éšæœº", size="sm")
                
                with gr.Row():
                    width_input = gr.Slider(
                        label="å®½åº¦",
                        minimum=512,
                        maximum=MAX_IMAGE_SIZE,
                        value=1328,
                        step=64
                    )
                    height_input = gr.Slider(
                        label="é«˜åº¦",
                        minimum=512,
                        maximum=MAX_IMAGE_SIZE,
                        value=1328,
                        step=64
                    )
                
                # å¿«é€Ÿå°ºå¯¸é¢„è®¾
                with gr.Row():
                    gr.Markdown("**å¿«é€Ÿå°ºå¯¸é¢„è®¾:**")
                with gr.Row():
                    square_btn = gr.Button("â¬œ 1:1", size="sm")
                    landscape_btn = gr.Button("ğŸ“º 16:9", size="sm")
                    portrait_btn = gr.Button("ğŸ“± 9:16", size="sm")
                    photo_btn = gr.Button("ğŸ“· 4:3", size="sm")
                
                generate_btn = gr.Button("ğŸ¨ ç”Ÿæˆå¯¹æ¯”å›¾ç‰‡", variant="primary", size="lg")
                
                # æ˜¾ç¤ºç”Ÿæˆä¿¡æ¯å’Œæ€§èƒ½å¯¹æ¯”
                output_info = gr.Textbox(
                    label="â±ï¸ ç”Ÿæˆä¿¡æ¯ä¸æ€§èƒ½å¯¹æ¯”", 
                    interactive=False, 
                    lines=10,
                    placeholder="ç”Ÿæˆä¿¡æ¯å°†åœ¨è¿™é‡Œæ˜¾ç¤ºï¼ŒåŒ…æ‹¬è¯¦ç»†çš„æ—¶é—´å¯¹æ¯”..."
                )
        
        # å¯¹æ¯”ç»“æœæ˜¾ç¤º
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### åŸºç¡€æ¨¡å‹ (50æ­¥, CFG 4.0)")
                base_output = gr.Image(label="åŸºç¡€æ¨¡å‹ç»“æœ", type="pil")
            
            with gr.Column(scale=1):
                gr.Markdown("### LoRA Lightningæ¨¡å‹ (8æ­¥, CFG 1.0)")
                lora_output = gr.Image(label="LoRAæ¨¡å‹ç»“æœ", type="pil")
        
        # äº‹ä»¶ç»‘å®š
        def load_example(choice):
            if choice:
                # æ‰¾åˆ°å®Œæ•´çš„æç¤ºè¯
                for prompt in EXAMPLE_PROMPTS:
                    if choice.startswith(prompt[:100]):
                        return prompt
            return ""
        
        example_dropdown.change(
            fn=load_example,
            inputs=[example_dropdown],
            outputs=[prompt_input]
        )
        
        random_seed_btn.click(fn=randomize_seed, outputs=seed_input)
        
        # å°ºå¯¸é¢„è®¾æŒ‰é’®
        square_btn.click(fn=lambda: (1328, 1328), outputs=[width_input, height_input])
        landscape_btn.click(fn=lambda: (1408, 928), outputs=[width_input, height_input])  # ä¿®æ­£ä¸º1408ä»¥ç¬¦åˆé™åˆ¶
        portrait_btn.click(fn=lambda: (928, 1408), outputs=[width_input, height_input])  # ä¿®æ­£ä¸º1408ä»¥ç¬¦åˆé™åˆ¶
        photo_btn.click(fn=lambda: (1408, 1056), outputs=[width_input, height_input])  # ä¿®æ­£å°ºå¯¸
        
        generate_btn.click(
            fn=generate_comparison,
            inputs=[prompt_input, width_input, height_input, seed_input],
            outputs=[base_output, lora_output, output_info]
        )
        
        # æ·»åŠ CSSæ ·å¼
        demo.css = """
        .gradio-container {
            max-width: 1600px !important;
        }
        .gr-button {
            transition: all 0.3s ease;
        }
        .gr-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        .gr-tab {
            font-weight: 600;
        }
        """
    
    return demo

if __name__ == "__main__":
    # æ£€æŸ¥LoRAæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    lora_path = "checkpoints/lora/Qwen-Image-Lightning-8steps-V1.0.safetensors"
    if not os.path.exists(lora_path):
        print(f"è­¦å‘Š: LoRAæ–‡ä»¶ {lora_path} ä¸å­˜åœ¨")
        print("è¯·ç¡®ä¿å·²ä¸‹è½½LoRAæƒé‡æ–‡ä»¶ï¼Œæˆ–è€…ç¨‹åºå°†ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œå¯¹æ¯”")
    
    # é¢„åŠ è½½æ¨¡å‹
    print("æ­£åœ¨é¢„åŠ è½½Qwen-Imageæ¨¡å‹...")
    load_base_model()
    load_lora_model()
    print("æ¨¡å‹åŠ è½½å®Œæˆ!")
    
    # å¯åŠ¨ç•Œé¢
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    )
