"""
M4-7: æ ¡å¯¹å¢å¼ºç‰ˆ (æœ€ç»ˆå®Œå…¨ä½“)

æœ¬è„šæœ¬æ˜¯æ•´ä¸ª Module çš„æœ€ç»ˆå½¢æ€ï¼Œé‡‡ç”¨äº†â€œä»£ç é¢„å¤„ç† + LLMæ ¡å¯¹â€çš„ç»ˆææ–¹æ¡ˆï¼š
1.  **ä»£ç é¢„å¤„ç†**: ç¡®å®šæ€§åœ°åœ¨æ¯ä¸ªè¯­éŸ³ç‰‡æ®µåæ·»åŠ é€—å·ï¼Œç”Ÿæˆä¸€ä»½â€œæ ‡ç‚¹è‰ç¨¿â€ã€‚
2.  **LLMæ ¡å¯¹**: LLM çš„ä»»åŠ¡è¢«ç®€åŒ–ä¸ºâ€œæ ¡å¯¹å’Œä¿®æ­£â€è¿™ä»½è‰ç¨¿ï¼Œå°†éƒ¨åˆ†é€—å·æ ¹æ®è¯­å¢ƒæ™ºèƒ½åœ°å‡çº§ä¸ºå¥å·ã€é—®å·ç­‰ã€‚
è¿™æ˜¯ä¸€ç§æå…¶é«˜æ•ˆå’Œå¯é çš„ AI å·¥ç¨‹å®è·µã€‚

æœ€ç»ˆæµç¨‹:
1. è½¬å½• -> 2. é¢„å¤„ç† -> 3. ä»£ç ç”Ÿæˆæ ‡ç‚¹è‰ç¨¿ -> 4. LLM æ ¡å¯¹ä¿®æ­£ -> 5. æ‘˜è¦æå– -> 6. ä¿å­˜
"""
import os
import whisper
import datetime
import torch
import re
import ollama

# --- 1. æ–‡æœ¬å¤„ç†å‡½æ•° ---
# (apply_replacements å’Œ add_spaces_around_english æ— å˜åŒ–)
def apply_replacements(text, replacement_map):
    for old_word, new_word in replacement_map.items():
        text = text.replace(old_word, new_word)
    return text

def add_spaces_around_english(text):
    pattern1 = re.compile(r'([\u4e00-\u9fa5])([a-zA-Z0-9]+)')
    pattern2 = re.compile(r'([a-zA-Z0-9]+)([\u4e00-\u9fa5])')
    text = pattern1.sub(r'\1 \2', text)
    text = pattern2.sub(r'\1 \2', text)
    return text

# <--- é‡ç‚¹ï¼šå…¨æ–°çš„ LLM æ ¡å¯¹å‡½æ•° ---
def correct_punctuation_with_llm(text_with_commas, llm_model_name):
    """
    æ¥æ”¶ä¸€ä»½ç”±é€—å·é¢„å¤„ç†è¿‡çš„â€œæ ‡ç‚¹è‰ç¨¿â€ï¼Œè®© LLM è¿›è¡Œæ ¡å¯¹å’Œä¿®æ­£ã€‚
    """
    if not text_with_commas.strip():
        return ""

    prompt = f"""# è§’è‰²
ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä¸­æ–‡æ€»ç¼–å®¡ï¼Œæ“…é•¿ç²¾ä¿®æ–‡ç¨¿ã€‚

# ä»»åŠ¡
ä½ æ”¶åˆ°äº†ä¸€ä»½ç”±åˆçº§åŠ©ç†å¤„ç†è¿‡çš„æ–‡ç¨¿ã€‚åŠ©ç†å·²åœ¨æ¯ä¸ªè‡ªç„¶çš„è¯­éŸ³åœé¡¿å¤„æ’å…¥äº†ã€é€—å·ã€‘ï¼Œå½¢æˆäº†ä¸€ä»½â€œæ ‡ç‚¹è‰ç¨¿â€ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ã€å®¡æ ¡å¹¶ä¿®æ­£ã€‘è¿™ä»½è‰ç¨¿ï¼Œå°†ä¸€äº›ä¸æ°å½“çš„é€—å·ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡è¯­å¢ƒå’Œå¥å­å®Œæ•´æ€§ï¼Œä¿®æ­£ä¸ºæ›´åˆé€‚çš„ã€å¥å·ã€‘æˆ–ã€é—®å·ã€‘ç­‰ã€‚

# æŒ‡å¯¼åŸåˆ™
1.  **ä¿®æ­£ä¸ºä¸»**: ä½ çš„ä¸»è¦å·¥ä½œæ˜¯åˆ¤æ–­å“ªäº›é€—å·åº”è¯¥è¢«â€œå‡çº§â€ä¸ºå¥å·ã€‚
2.  **ä¿ç•™ä¸ºè¾…**: å¦‚æœä¸€ä¸ªé€—å·ä½äºä¸€ä¸ªé•¿å¥çš„ä¸­é—´ï¼Œç”¨äºè‡ªç„¶çš„åœé¡¿ï¼Œé‚£ä¹ˆå®ƒåº”è¯¥è¢«ä¿ç•™ã€‚
3.  **å¿ å®å†…å®¹**: ç»å¯¹ä¸å…è®¸å¢åˆ æˆ–æ”¹åŠ¨åŸæ–‡çš„ä»»ä½•å­—è¯ã€‚

# å­¦ä¹ ç¤ºä¾‹ (Examples)
*   **ç¤ºä¾‹ 1 (éœ€è¦ä¿®æ­£)**
    *   **æ ‡ç‚¹è‰ç¨¿**: å¤§å®¶å¥½æˆ‘æ˜¯å°æ˜,ä»Šå¤©æˆ‘ä»¬æ¥èŠèŠäººå·¥æ™ºèƒ½,
    *   **æœŸæœ›è¾“å‡º**: å¤§å®¶å¥½,æˆ‘æ˜¯å°æ˜ã€‚ä»Šå¤©æˆ‘ä»¬æ¥èŠèŠäººå·¥æ™ºèƒ½ã€‚

*   **ç¤ºä¾‹ 2 (éƒ¨åˆ†ä¿®æ­£ï¼Œéƒ¨åˆ†ä¿ç•™)**
    *   **æ ‡ç‚¹è‰ç¨¿**: è¿™ä¸ªé¡¹ç›®åŸºäºPythonè¯­è¨€,ä½¿ç”¨äº†å¤šç§å…ˆè¿›çš„ç®—æ³•æ¨¡å‹,æ•ˆæœéå¸¸å‡ºè‰²,
    *   **æœŸæœ›è¾“å‡º**: è¿™ä¸ªé¡¹ç›®åŸºäºPythonè¯­è¨€,ä½¿ç”¨äº†å¤šç§å…ˆè¿›çš„ç®—æ³•æ¨¡å‹,æ•ˆæœéå¸¸å‡ºè‰²ã€‚


# å¾…æ ¡å¯¹çš„æ ‡ç‚¹è‰ç¨¿
---
{text_with_commas}
---

# è¾“å‡ºè¦æ±‚
è¯·ç›´æ¥è¾“å‡ºç»è¿‡ä½ æœ€ç»ˆå®¡æ ¡å’Œä¿®æ­£åçš„å®Œç¾æ–‡ç¨¿ã€‚
/no_think
"""

    try:
        print("   ğŸ”„ æ­£åœ¨å‘é€ [æ ¡å¯¹] è¯·æ±‚åˆ° Ollama...")
        response = ollama.chat(
            model=llm_model_name, messages=[{'role': 'user', 'content': prompt}],
            options={'temperature': 0.4} # æ ¡å¯¹ä»»åŠ¡éœ€è¦ä¸€å®šçš„ä¸Šä¸‹æ–‡ç†è§£ï¼Œä½†ä¸èƒ½å¤ªéšæ„
        )
        corrected_text = response['message']['content'].strip()
        print("   âœ… Ollama [æ ¡å¯¹] å®Œæˆã€‚")
        return corrected_text
    except Exception as e:
        print(f"âŒ è°ƒç”¨ Ollama API [æ ¡å¯¹] æ—¶å‡ºé”™: {e}")
        # å‡ºé”™æ—¶ï¼Œè¿”å›è‡³å°‘ä¿è¯æ–­å¥çš„è‰ç¨¿
        return text_with_commas

# (extract_summary_with_llm å’Œ save_text_file æ— å˜åŒ–)
def extract_summary_with_llm(full_text, llm_model_name):
    # ... (æ— å˜åŒ–)
    if not full_text.strip(): return "åŸæ–‡ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦ã€‚"
    prompt = f"""# è§’è‰²
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ†æå¸ˆå’Œæ‘˜è¦ä¸“å®¶ã€‚
# ä»»åŠ¡
ä½ çš„ä»»åŠ¡æ˜¯ä¸ºä»¥ä¸‹å®Œæ•´çš„æ–‡ç¨¿ï¼Œæå–æ ¸å¿ƒè¦ç‚¹ï¼Œå¹¶ç”Ÿæˆä¸€æ®µç®€æ´ã€æµç•…ã€é‡ç‚¹çªå‡ºçš„æ‘˜è¦ã€‚
# æŒ‡å¯¼åŸåˆ™
1. **é•¿åº¦æ§åˆ¶**: æ‘˜è¦çš„é•¿åº¦åº”ä¿æŒåœ¨150åˆ°300å­—ä¹‹é—´ï¼Œç²¾å‡†æ¦‚æ‹¬ï¼Œé¿å…å†—é•¿ã€‚
2. **å†…å®¹è¦†ç›–**: æ‘˜è¦åº”è¦†ç›–æ–‡ç¨¿çš„ä¸»è¦è¯é¢˜ã€å…³é”®æ¦‚å¿µå’Œæœ€ç»ˆç»“è®ºã€‚
3. **æ ¼å¼çµæ´»**: ä½ å¯ä»¥ä½¿ç”¨æ— åºåˆ—è¡¨ï¼ˆä½¿ç”¨-æˆ–*ï¼‰æ¥å‘ˆç°è¦ç‚¹ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨é€šé¡ºçš„æ®µè½å½¢å¼ã€‚
4. **ä¿æŒä¸­ç«‹**: æ‘˜è¦åº”å®¢è§‚åæ˜ åŸæ–‡å†…å®¹ï¼Œä¸æ·»åŠ ä¸ªäººè§‚ç‚¹ã€‚
# å¾…å¤„ç†æ–‡ç¨¿
---
{full_text}
---
# è¾“å‡ºè¦æ±‚
è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„å‰ç¼€ï¼Œå¦‚â€œè¿™æ˜¯æ‘˜è¦ï¼šâ€ã€‚
/no_think
"""
    try:
        print("   ğŸ”„ æ­£åœ¨å‘é€ [æ‘˜è¦] è¯·æ±‚åˆ° Ollama...")
        response = ollama.chat(
            model=llm_model_name, messages=[{'role': 'user', 'content': prompt}],
            options={'temperature': 0.3}
        )
        return response['message']['content'].strip()
    except Exception as e:
        return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}"

def save_text_file(text, output_path):
    # ... (æ— å˜åŒ–)
    with open(output_path, 'w', encoding='utf-8') as txt_file:
        txt_file.write(text)
    print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜: {os.path.abspath(output_path)}")

# --- 2. ä¸»æµç¨‹å‡½æ•° ---
def run_transcription_pipeline(model_name, media_path, language, replacement_map, llm_model_name):
    """æ‰§è¡Œå®Œæ•´çš„è½¬å½•ã€åå¤„ç†å’Œæ‘˜è¦æå–æµç¨‹ (v7.0 - æ ¡å¯¹å¢å¼ºç‰ˆ)ã€‚"""
    print("ğŸš€ å¼€å§‹ Whisper è¯­éŸ³è½¬å½•ä¸æ™ºèƒ½åå¤„ç†æµç¨‹")
    print("=" * 70)
    
    total_start_time = datetime.datetime.now()
    # ... (å‰é¢æ­¥éª¤çš„åˆå§‹åŒ–æ— å˜åŒ–)
    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else '.'
    output_dir = script_dir
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æ­¥éª¤ 1 & 2: åŠ è½½æ¨¡å‹ä¸è½¬å½• (æ— å˜åŒ–)
    # ... (ä»£ç çœç•¥ï¼Œä¸å‰ä¸€ç‰ˆç›¸åŒ)
    step1_start = datetime.datetime.now()
    print(f"ğŸ”§ æ­¥éª¤ 1: æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ '{model_name}'...")
    model = whisper.load_model(model_name, device=device)
    step1_end = datetime.datetime.now(); step1_duration = step1_end - step1_start
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆã€‚è€—æ—¶: {step1_duration}\n")

    step2_start = datetime.datetime.now()
    print(f"ğŸ™ï¸ æ­¥éª¤ 2: æ­£åœ¨è½¬å½•éŸ³é¢‘æ–‡ä»¶...")
    result = model.transcribe(media_path, language=language, verbose=False)
    original_segments = result['segments']
    original_text_display = result['text'].strip()
    step2_end = datetime.datetime.now(); step2_duration = step2_end - step2_start
    print(f"âœ… éŸ³é¢‘è½¬å½•å®Œæˆã€‚è€—æ—¶: {step2_duration}")
    print("\nğŸ“ åŸå§‹è½¬å½•æ–‡æœ¬ (æ— æ ‡ç‚¹æ‹¼æ¥):\n" + "-"*50 + f"\n{original_text_display}\n" + "-"*50 + "\n")


    # æ­¥éª¤ 3 & 4: é¢„å¤„ç† (æ— å˜åŒ–)
    # ... (ä»£ç çœç•¥ï¼Œä¸å‰ä¸€ç‰ˆç›¸åŒ)
    step3_4_start = datetime.datetime.now()
    print("ğŸ”„ æ­¥éª¤ 3 & 4: æ­£åœ¨è¿›è¡Œçƒ­è¯æ›¿æ¢ä¸ä¸­è‹±æ–‡ç©ºæ ¼å¤„ç†...")
    processed_segments = []
    for segment in original_segments:
        new_segment = segment.copy()
        text = apply_replacements(new_segment['text'], replacement_map)
        text = add_spaces_around_english(text)
        new_segment['text'] = text
        processed_segments.append(new_segment)
    step3_4_end = datetime.datetime.now(); step3_4_duration = step3_4_end - step3_4_start
    print(f"âœ… é¢„å¤„ç†å®Œæˆã€‚è€—æ—¶: {step3_4_duration}\n")


    # <--- é‡ç‚¹ä¿®æ”¹ï¼šå…¨æ–°çš„æ­¥éª¤5 ---
    # æ­¥éª¤ 5.1: ä»£ç ç”Ÿæˆæ ‡ç‚¹è‰ç¨¿
    step5_start = datetime.datetime.now()
    print("âœï¸  æ­¥éª¤ 5.1: æ­£åœ¨ç”±ä»£ç ç”Ÿæˆâ€œæ ‡ç‚¹è‰ç¨¿â€...")
    texts_to_join = [s['text'].strip() for s in processed_segments if s['text'].strip()]
    text_with_commas = ",".join(texts_to_join) + "," # åœ¨æœ«å°¾ä¹ŸåŠ ä¸Šé€—å·ï¼Œè®©LLMå†³å®šæœ€åç”¨ä»€ä¹ˆæ ‡ç‚¹
    print(text_with_commas)
    print("âœ… â€œæ ‡ç‚¹è‰ç¨¿â€ç”Ÿæˆå®Œæˆã€‚\n")

    # æ­¥éª¤ 5.2: LLM æ ¡å¯¹ä¿®æ­£
    print(f"ğŸ¤– æ­¥éª¤ 5.2: æ­£åœ¨è°ƒç”¨ LLM '{llm_model_name}' è¿›è¡Œæ ¡å¯¹ä¿®æ­£...")
    final_text = correct_punctuation_with_llm(text_with_commas, llm_model_name)
    step5_end = datetime.datetime.now()
    step5_duration = step5_end - step5_start # æ•´ä¸ªæ­¥éª¤5çš„è€—æ—¶
    print(f"âœ… LLM æ ¡å¯¹å¤„ç†å®Œæˆã€‚è€—æ—¶: {step5_duration}\n")
    print("âœ¨ æœ€ç»ˆå…¨æ–‡ (ç»æ ¡å¯¹):\n" + "="*50 + f"\n{final_text}\n" + "="*50 + "\n")
    # <--- ä¿®æ”¹ç»“æŸ ---

    # åç»­æ­¥éª¤æ— å˜åŒ–
    # æ­¥éª¤ 6: LLM æ‘˜è¦æå–
    step6_start = datetime.datetime.now()
    print(f"ğŸ“œ æ­¥éª¤ 6: æ­£åœ¨è°ƒç”¨ LLM '{llm_model_name}' è¿›è¡Œæ‘˜è¦æå–...")
    summary_text = extract_summary_with_llm(final_text, llm_model_name)
    step6_end = datetime.datetime.now(); step6_duration = step6_end - step6_start
    print(f"âœ… LLM æ‘˜è¦æå–å®Œæˆã€‚è€—æ—¶: {step6_duration}\n")
    print("ğŸ’¡ æ–‡æœ¬æ‘˜è¦:\n" + "*"*50 + f"\n{summary_text}\n" + "*"*50 + "\n")
    
    # æ­¥éª¤ 7: ä¿å­˜æ–‡ä»¶
    # ... (ä»£ç çœç•¥ï¼Œä¸å‰ä¸€ç‰ˆç›¸åŒ)
    step7_start = datetime.datetime.now()
    print("ğŸ’¾ æ­¥éª¤ 7: æ­£åœ¨ä¿å­˜æ‰€æœ‰ç»“æœæ–‡ä»¶...")
    base_name = os.path.splitext(os.path.basename(media_path))[0]
    
    final_txt_path = os.path.join(output_dir, f"{base_name}_final_text.txt")
    save_text_file(final_text, final_txt_path)
    
    summary_txt_path = os.path.join(output_dir, f"{base_name}_summary.txt")
    save_text_file(summary_text, summary_txt_path)
    
    original_txt_path = os.path.join(output_dir, f"{base_name}_original.txt")
    save_text_file(original_text_display, original_txt_path)
    
    step7_end = datetime.datetime.now(); step7_duration = step7_end - step7_start
    print(f"âœ… æ–‡ä»¶ä¿å­˜å®Œæˆã€‚è€—æ—¶: {step7_duration}\n")

    # æ€»ç»“ç»Ÿè®¡
    # ... (ä»£ç çœç•¥ï¼Œä¸å‰ä¸€ç‰ˆç›¸åŒ)
    total_end_time = datetime.datetime.now()
    total_duration = total_end_time - total_start_time
    
    print("ğŸ‰ å…¨éƒ¨æµç¨‹å®Œæˆ!")
    print("=" * 70)
    print("â±ï¸ å„æ­¥éª¤è€—æ—¶ç»Ÿè®¡:")
    print(f"   æ­¥éª¤1 (æ¨¡å‹åŠ è½½): {step1_duration}")
    print(f"   æ­¥éª¤2 (éŸ³é¢‘è½¬å½•): {step2_duration}")
    print(f"   æ­¥éª¤3&4 (é¢„å¤„ç†):  {step3_4_duration}")
    print(f"   æ­¥éª¤5 (æ ¡å¯¹æµç¨‹): {step5_duration}")
    print(f"   æ­¥éª¤6 (LLMæ‘˜è¦):   {step6_duration}")
    print(f"   æ­¥éª¤7 (æ–‡ä»¶ä¿å­˜): {step7_duration}")
    print(f"   â° æ€»è®¡è€—æ—¶:      {total_duration}")
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   - æœ€ç»ˆå…¨æ–‡: {final_txt_path}")
    print(f"   - æ–‡æœ¬æ‘˜è¦: {summary_txt_path}")
    print(f"   - åŸå§‹æ–‡æœ¬: {original_txt_path}")
    print("=" * 70)


# --- 3. é…ç½®ä¸æ‰§è¡Œ ---
if __name__ == "__main__":
    # ... (æ— å˜åŒ–)
    REPLACEMENT_MAP = {
        "ä¹Œç­å›¾": "Ubuntu", "å‰ç‰¹å“ˆå¸ƒ": "GitHub", "Moddy Talk": "MultiTalk",
        "é©¬å…‹é“": "Markdown", "VS æ‰£å¾·": "VS Code", "æ´¾æ£®": "Python",
    }
    
    WHISPER_MODEL_NAME = "large-v3"
    MEDIA_FILE = "../audio/139.wav"
    LANGUAGE = "zh"
    LLM_MODEL_NAME = "qwen3:32b"
    
    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else '.'
    media_path_full = os.path.join(script_dir, MEDIA_FILE)
    
    if not os.path.exists(media_path_full):
        print(f"âŒ é”™è¯¯: éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ -> {media_path_full}")
    else:
        run_transcription_pipeline(
            model_name=WHISPER_MODEL_NAME, media_path=media_path_full,
            language=LANGUAGE, replacement_map=REPLACEMENT_MAP,
            llm_model_name=LLM_MODEL_NAME
        )