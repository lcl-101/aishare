#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
M6-5-æ–°æ¨¡å‹æµ‹è¯•.py - å¾®è°ƒæ¨¡å‹ä¸åŸå§‹æ¨¡å‹å¯¹æ¯”æµ‹è¯•

åŠŸèƒ½ï¼š
1. åŠ è½½å¾®è°ƒåçš„custom.ptæ¨¡å‹å’ŒåŸå§‹çš„large-v3.ptæ¨¡å‹
2. å¯¹åŒä¸€éŸ³é¢‘æ–‡ä»¶è¿›è¡Œæ¨ç†
3. å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„è¯†åˆ«ç»“æœ
4. è¯„ä¼°å‡†ç¡®ç‡å’Œç›¸ä¼¼åº¦
5. æ”¯æŒæ‰¹é‡æµ‹è¯•å¤šä¸ªéŸ³é¢‘æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
    # ç›´æ¥è¿è¡Œæµ‹è¯•ï¼ˆæ— éœ€å‘½ä»¤è¡Œå‚æ•°ï¼‰
    python M6-5-æ–°æ¨¡å‹æµ‹è¯•.py
    
    # å¯åœ¨main()å‡½æ•°ä¸­ä¿®æ”¹ä»¥ä¸‹é…ç½®ï¼š
    # - test_mode: "single"(å•æ–‡ä»¶) æˆ– "batch"(æ‰¹é‡æµ‹è¯•)
    # - audio_path: å•ä¸ªéŸ³é¢‘æ–‡ä»¶è·¯å¾„
    # - audio_dir: éŸ³é¢‘ç›®å½•è·¯å¾„
    # - custom_model_path: å¾®è°ƒæ¨¡å‹è·¯å¾„
    # - original_model_path: åŸå§‹æ¨¡å‹è·¯å¾„

ç¯å¢ƒè¦æ±‚ï¼š
    pip install openai-whisper jiwer

ä½œè€…ï¼šGitHub Copilot
æ—¥æœŸï¼š2025-07-07
"""

import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import whisper
import torch
import numpy as np
from jiwer import wer, cer


def load_single_model(model_path: str, model_name: str) -> whisper.Whisper:
    """
    åŠ è½½å•ä¸ªæ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
    Returns:
        whisper.Whisper: åŠ è½½çš„æ¨¡å‹
    """
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"âŒ {model_name}ä¸å­˜åœ¨: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½{model_name}: {model_path}")
    start_time = time.time()
    model = whisper.load_model(model_path)
    load_time = time.time() - start_time
    print(f"âœ… {model_name}åŠ è½½å®Œæˆ ({load_time:.2f}s)")
    
    return model


def release_model(model: whisper.Whisper, model_name: str) -> None:
    """
    é‡Šæ”¾æ¨¡å‹èµ„æº
    
    Args:
        model: è¦é‡Šæ”¾çš„æ¨¡å‹
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    try:
        # å°†æ¨¡å‹ç§»åˆ°CPUå¹¶åˆ é™¤å¼•ç”¨
        if hasattr(model, 'device'):
            model.to('cpu')
        del model
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        print(f"ğŸ§¹ {model_name}èµ„æºå·²é‡Šæ”¾")
    except Exception as e:
        print(f"âš ï¸  é‡Šæ”¾{model_name}èµ„æºæ—¶å‡ºé”™: {e}")


def transcribe_audio(model: whisper.Whisper, audio_path: str, model_name: str) -> Dict:
    """
    ä½¿ç”¨æŒ‡å®šæ¨¡å‹å¯¹éŸ³é¢‘è¿›è¡Œè½¬å½•
    
    Args:
        model: Whisperæ¨¡å‹
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
    Returns:
        dict: è½¬å½•ç»“æœ
    """
    print(f"ğŸµ ä½¿ç”¨{model_name}è½¬å½•: {os.path.basename(audio_path)}")
    
    start_time = time.time()
    
    # è¿›è¡Œè½¬å½•ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„å‚æ•°ï¼Œè§£å†³æˆªæ–­é—®é¢˜ï¼‰
    result = model.transcribe(
        audio_path,
        language="zh",  # ä¸­æ–‡
        task="transcribe",
        temperature=0.3,  # é€‚åº¦éšæœºæ€§ï¼Œé¿å…è¿‡æ—©æˆªæ–­
        beam_size=1,      # è´ªå©ªæœç´¢
        best_of=1,        # ç®€åŒ–æœç´¢
        patience=1.0,     # è€å¿ƒå‚æ•°
        length_penalty=0.0,  # ä¸­æ€§é•¿åº¦æƒ©ç½š
        suppress_tokens=[],  # ä¸æŠ‘åˆ¶ä»»ä½•token
        initial_prompt="",   # ç©ºåˆå§‹æç¤º
        condition_on_previous_text=False,  # ä¸ä¾èµ–å‰æ–‡ï¼Œé¿å…æˆªæ–­
        fp16=torch.cuda.is_available(),   # ä½¿ç”¨åŠç²¾åº¦ï¼ˆå¦‚æœGPUå¯ç”¨ï¼‰
        compression_ratio_threshold=2.4,  # å‹ç¼©æ¯”é˜ˆå€¼
        logprob_threshold=-1.0,           # å¯¹æ•°æ¦‚ç‡é˜ˆå€¼
        no_speech_threshold=0.6,          # æ— è¯­éŸ³é˜ˆå€¼
    )
    
    inference_time = time.time() - start_time
    
    # æå–å…³é”®ä¿¡æ¯
    transcription = {
        'text': result['text'].strip(),
        'language': result['language'],
        'segments': result['segments'],
        'inference_time': inference_time,
        'model_name': model_name
    }
    
    print(f"â±ï¸  {model_name}æ¨ç†è€—æ—¶: {inference_time:.2f}s")
    print(f"ğŸ“ {model_name}è¯†åˆ«ç»“æœ: {transcription['text']}")
    
    return transcription


def calculate_similarity_metrics(text1: str, text2: str) -> Dict[str, float]:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦æŒ‡æ ‡
    
    Args:
        text1: æ–‡æœ¬1
        text2: æ–‡æœ¬2
        
    Returns:
        dict: ç›¸ä¼¼åº¦æŒ‡æ ‡
    """
    if not text1.strip() or not text2.strip():
        return {
            'wer': 1.0,  # è¯é”™è¯¯ç‡100%
            'cer': 1.0,  # å­—ç¬¦é”™è¯¯ç‡100%
            'similarity': 0.0  # ç›¸ä¼¼åº¦0%
        }
    
    try:
        # è®¡ç®—è¯é”™è¯¯ç‡ (Word Error Rate)
        word_error_rate = wer(text1, text2)
        
        # è®¡ç®—å­—ç¬¦é”™è¯¯ç‡ (Character Error Rate)
        char_error_rate = cer(text1, text2)
        
        # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºäºå­—ç¬¦åŒ¹é…ï¼‰
        common_chars = set(text1.lower()) & set(text2.lower())
        total_chars = set(text1.lower()) | set(text2.lower())
        similarity = len(common_chars) / len(total_chars) if total_chars else 0.0
        
        return {
            'wer': word_error_rate,
            'cer': char_error_rate,
            'similarity': similarity
        }
    except Exception as e:
        print(f"âš ï¸  è®¡ç®—ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
        return {
            'wer': 1.0,
            'cer': 1.0,
            'similarity': 0.0
        }


def compare_results(custom_result: Dict, original_result: Dict, audio_path: str) -> Dict:
    """
    å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ¨ç†ç»“æœ
    
    Args:
        custom_result: å¾®è°ƒæ¨¡å‹ç»“æœ
        original_result: åŸå§‹æ¨¡å‹ç»“æœ
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: å¯¹æ¯”ç»“æœ
    """
    print(f"\n" + "="*80)
    print(f"ğŸ“Š éŸ³é¢‘æ–‡ä»¶å¯¹æ¯”ç»“æœ: {os.path.basename(audio_path)}")
    print(f"="*80)
    
    custom_text = custom_result['text']
    original_text = original_result['text']
    
    # æ‰“å°è¯†åˆ«ç»“æœ
    print(f"ğŸ¯ å¾®è°ƒæ¨¡å‹ç»“æœ: {custom_text}")
    print(f"ğŸ”„ åŸå§‹æ¨¡å‹ç»“æœ: {original_text}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡
    metrics = calculate_similarity_metrics(custom_text, original_text)
    
    # æ‰“å°æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
    print(f"   å¾®è°ƒæ¨¡å‹æ¨ç†æ—¶é—´: {custom_result['inference_time']:.2f}s")
    print(f"   åŸå§‹æ¨¡å‹æ¨ç†æ—¶é—´: {original_result['inference_time']:.2f}s")
    
    speed_improvement = (original_result['inference_time'] - custom_result['inference_time']) / original_result['inference_time'] * 100
    if speed_improvement > 0:
        print(f"   âš¡ å¾®è°ƒæ¨¡å‹é€Ÿåº¦æå‡: {speed_improvement:.1f}%")
    else:
        print(f"   â³ å¾®è°ƒæ¨¡å‹é€Ÿåº¦é™ä½: {abs(speed_improvement):.1f}%")
    
    # æ‰“å°ç›¸ä¼¼åº¦æŒ‡æ ‡
    print(f"\nğŸ” æ–‡æœ¬å¯¹æ¯”æŒ‡æ ‡:")
    print(f"   è¯é”™è¯¯ç‡ (WER): {metrics['wer']:.3f}")
    print(f"   å­—ç¬¦é”™è¯¯ç‡ (CER): {metrics['cer']:.3f}")
    print(f"   æ–‡æœ¬ç›¸ä¼¼åº¦: {metrics['similarity']:.3f}")
    
    # æ‰“å°æ®µè½çº§åˆ«ä¿¡æ¯
    print(f"\nğŸ“ æ®µè½æ•°é‡å¯¹æ¯”:")
    print(f"   å¾®è°ƒæ¨¡å‹æ®µè½æ•°: {len(custom_result['segments'])}")
    print(f"   åŸå§‹æ¨¡å‹æ®µè½æ•°: {len(original_result['segments'])}")
    
    comparison = {
        'audio_file': os.path.basename(audio_path),
        'custom_text': custom_text,
        'original_text': original_text,
        'custom_time': custom_result['inference_time'],
        'original_time': original_result['inference_time'],
        'speed_improvement': speed_improvement,
        'metrics': metrics,
        'custom_segments': len(custom_result['segments']),
        'original_segments': len(original_result['segments'])
    }
    
    return comparison


def test_single_audio_optimized(custom_model_path: str, original_model_path: str, 
                               audio_path: str) -> Dict:
    """
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šé€ä¸ªåŠ è½½æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼ŒèŠ‚çœå†…å­˜
    
    Args:
        custom_model_path: å¾®è°ƒæ¨¡å‹è·¯å¾„
        original_model_path: åŸå§‹æ¨¡å‹è·¯å¾„
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: å¯¹æ¯”ç»“æœ
    """
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
    
    print(f"\nğŸµ å¼€å§‹æµ‹è¯•éŸ³é¢‘: {audio_path}")
    print("ğŸ’¡ é‡‡ç”¨é€ä¸ªåŠ è½½ç­–ç•¥ï¼ŒèŠ‚çœæ˜¾å­˜å ç”¨...")
    
    # ç¬¬ä¸€æ­¥ï¼šåŠ è½½å¾®è°ƒæ¨¡å‹å¹¶æ¨ç†
    print(f"\n{'='*50} ç¬¬1æ­¥ï¼šå¾®è°ƒæ¨¡å‹æ¨ç† {'='*50}")
    custom_model = load_single_model(custom_model_path, "å¾®è°ƒæ¨¡å‹")
    custom_result = transcribe_audio(custom_model, audio_path, "å¾®è°ƒæ¨¡å‹")
    release_model(custom_model, "å¾®è°ƒæ¨¡å‹")
    
    # ç¬¬äºŒæ­¥ï¼šåŠ è½½åŸå§‹æ¨¡å‹å¹¶æ¨ç†
    print(f"\n{'='*50} ç¬¬2æ­¥ï¼šåŸå§‹æ¨¡å‹æ¨ç† {'='*50}")
    original_model = load_single_model(original_model_path, "åŸå§‹æ¨¡å‹")
    original_result = transcribe_audio(original_model, audio_path, "åŸå§‹æ¨¡å‹")
    release_model(original_model, "åŸå§‹æ¨¡å‹")
    
    # ç¬¬ä¸‰æ­¥ï¼šå¯¹æ¯”ç»“æœ
    print(f"\n{'='*50} ç¬¬3æ­¥ï¼šç»“æœå¯¹æ¯”åˆ†æ {'='*50}")
    comparison = compare_results(custom_result, original_result, audio_path)
    
    return comparison


def test_audio_directory_optimized(custom_model_path: str, original_model_path: str,
                                 audio_dir: str) -> List[Dict]:
    """
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šé€ä¸ªåŠ è½½æ¨¡å‹æµ‹è¯•éŸ³é¢‘ç›®å½•
    
    Args:
        custom_model_path: å¾®è°ƒæ¨¡å‹è·¯å¾„  
        original_model_path: åŸå§‹æ¨¡å‹è·¯å¾„
        audio_dir: éŸ³é¢‘ç›®å½•è·¯å¾„
        
    Returns:
        list: æ‰€æœ‰å¯¹æ¯”ç»“æœ
    """
    if not os.path.exists(audio_dir):
        raise FileNotFoundError(f"âŒ éŸ³é¢‘ç›®å½•ä¸å­˜åœ¨: {audio_dir}")
    
    # æ”¯æŒçš„éŸ³é¢‘æ ¼å¼
    audio_extensions = {'.wav', '.mp3', '.m4a', '.flac', '.aac', '.ogg'}
    
    # æŸ¥æ‰¾æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
    audio_files = []
    for ext in audio_extensions:
        audio_files.extend(Path(audio_dir).glob(f"*{ext}"))
        audio_files.extend(Path(audio_dir).glob(f"*{ext.upper()}"))
    
    if not audio_files:
        print(f"âš ï¸  åœ¨ {audio_dir} ç›®å½•ä¸‹æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
        return []
    
    print(f"ğŸ“ åœ¨ {audio_dir} ç›®å½•ä¸‹æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    print("ğŸ’¡ é‡‡ç”¨é€ä¸ªåŠ è½½ç­–ç•¥ï¼Œæ¯ä¸ªæ–‡ä»¶ç‹¬ç«‹æµ‹è¯•...")
    
    results = []
    for i, audio_file in enumerate(audio_files, 1):
        print(f"\n{'='*20} æµ‹è¯•è¿›åº¦: {i}/{len(audio_files)} {'='*20}")
        
        try:
            result = test_single_audio_optimized(custom_model_path, original_model_path, str(audio_file))
            results.append(result)
        except Exception as e:
            print(f"âŒ æµ‹è¯• {audio_file} æ—¶å‡ºé”™: {e}")
            continue
    
    return results


def print_summary(results: List[Dict]) -> None:
    """
    æ‰“å°æµ‹è¯•æ€»ç»“
    
    Args:
        results: æ‰€æœ‰æµ‹è¯•ç»“æœ
    """
    if not results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    print(f"\n" + "="*80)
    print(f"ğŸ“Š æµ‹è¯•æ€»ç»“ (å…± {len(results)} ä¸ªéŸ³é¢‘æ–‡ä»¶)")
    print(f"="*80)
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_wer = np.mean([r['metrics']['wer'] for r in results])
    avg_cer = np.mean([r['metrics']['cer'] for r in results])
    avg_similarity = np.mean([r['metrics']['similarity'] for r in results])
    avg_speed_improvement = np.mean([r['speed_improvement'] for r in results])
    
    total_custom_time = sum([r['custom_time'] for r in results])
    total_original_time = sum([r['original_time'] for r in results])
    
    print(f"ğŸ“ˆ å¹³å‡æŒ‡æ ‡:")
    print(f"   å¹³å‡è¯é”™è¯¯ç‡ (WER): {avg_wer:.3f}")
    print(f"   å¹³å‡å­—ç¬¦é”™è¯¯ç‡ (CER): {avg_cer:.3f}")
    print(f"   å¹³å‡æ–‡æœ¬ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
    print(f"   å¹³å‡é€Ÿåº¦æå‡: {avg_speed_improvement:.1f}%")
    
    print(f"\nâ±ï¸  æ€»æ¨ç†æ—¶é—´:")
    print(f"   å¾®è°ƒæ¨¡å‹æ€»æ—¶é—´: {total_custom_time:.2f}s")
    print(f"   åŸå§‹æ¨¡å‹æ€»æ—¶é—´: {total_original_time:.2f}s")
    print(f"   æ€»æ—¶é—´èŠ‚çœ: {total_original_time - total_custom_time:.2f}s")
    
    # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®ç»“æœ
    best_similarity = max(results, key=lambda x: x['metrics']['similarity'])
    worst_similarity = min(results, key=lambda x: x['metrics']['similarity'])
    
    print(f"\nğŸ† æœ€ä½³ä¸€è‡´æ€§: {best_similarity['audio_file']} (ç›¸ä¼¼åº¦: {best_similarity['metrics']['similarity']:.3f})")
    print(f"ğŸ’¡ æœ€å¤§å·®å¼‚: {worst_similarity['audio_file']} (ç›¸ä¼¼åº¦: {worst_similarity['metrics']['similarity']:.3f})")


def test_checkpoint_vs_custom(checkpoint_path: str, custom_model_path: str, audio_path: str) -> Dict:
    """
    ä¸“é—¨å¯¹æ¯”checkpoint-4000å’Œcustom.ptçš„å·®å¼‚
    
    Args:
        checkpoint_path: HuggingFace checkpointè·¯å¾„
        custom_model_path: è½¬æ¢åçš„OpenAIæ ¼å¼æ¨¡å‹è·¯å¾„
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: å¯¹æ¯”ç»“æœ
    """
    from transformers import WhisperForConditionalGeneration, WhisperProcessor
    import librosa
    
    print(f"\nğŸ” å¼€å§‹å¯¹æ¯” checkpoint-4000 vs custom.pt")
    print(f"ğŸ“‚ HF Checkpoint: {checkpoint_path}")
    print(f"ğŸ“‚ OpenAI Model: {custom_model_path}")
    print(f"ğŸµ æµ‹è¯•éŸ³é¢‘: {audio_path}")
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000)
    
    # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨HuggingFace checkpoint
    print(f"\n{'='*50} ç¬¬1æ­¥ï¼šHF Checkpointæ¨ç† {'='*50}")
    hf_model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path)
    hf_processor = WhisperProcessor.from_pretrained(checkpoint_path)
    
    # HFæ¨¡å‹æ¨ç†
    input_features = hf_processor(audio, sampling_rate=sr, return_tensors="pt").input_features
    
    # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°é¿å…é•¿åº¦é”™è¯¯
    start_time = time.time()
    generated_ids = hf_model.generate(
        input_features,
        language="zh",
        task="transcribe", 
        max_new_tokens=400,  # å‡å°‘åˆ°400é¿å…è¶…å‡ºé™åˆ¶
        num_beams=5,
        temperature=0.0,
        do_sample=False,
        return_dict_in_generate=True,
        output_scores=True
    )
    hf_time = time.time() - start_time
    
    hf_text = hf_processor.batch_decode(generated_ids.sequences, skip_special_tokens=True)[0].strip()
    print(f"âœ… HF Checkpointç»“æœ: {hf_text}")
    print(f"â±ï¸  HFæ¨ç†è€—æ—¶: {hf_time:.2f}s")
    
    # é‡Šæ”¾HFæ¨¡å‹
    del hf_model, hf_processor
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨OpenAIæ ¼å¼æ¨¡å‹
    print(f"\n{'='*50} ç¬¬2æ­¥ï¼šOpenAI Modelæ¨ç† {'='*50}")
    openai_model = load_single_model(custom_model_path, "OpenAIæ¨¡å‹")
    
    start_time = time.time()
    # ä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°è§£å†³æˆªæ–­é—®é¢˜
    openai_result = openai_model.transcribe(
        audio_path,
        language="zh",
        task="transcribe",
        temperature=0.3,  # å¢åŠ éšæœºæ€§ï¼Œé¿å…æˆªæ–­
        beam_size=1,      # è´ªå©ªæœç´¢
        best_of=1,
        patience=1.0,
        length_penalty=0.0,  # ä¸­æ€§é•¿åº¦æƒ©ç½š
        suppress_tokens=[],  # ä¸æŠ‘åˆ¶ä»»ä½•token
        initial_prompt="",   # ç©ºåˆå§‹æç¤º
        condition_on_previous_text=False,  # ä¸ä¾èµ–å‰æ–‡ï¼Œå…³é”®ï¼
        fp16=torch.cuda.is_available(),
        compression_ratio_threshold=2.4,
        logprob_threshold=-1.0,
        no_speech_threshold=0.6,
    )
    openai_time = time.time() - start_time
    
    openai_text = openai_result['text'].strip()
    print(f"âœ… OpenAI Modelç»“æœ: {openai_text}")
    print(f"â±ï¸  OpenAIæ¨ç†è€—æ—¶: {openai_time:.2f}s")
    
    release_model(openai_model, "OpenAIæ¨¡å‹")
    
    # ç¬¬ä¸‰æ­¥ï¼šè¯¦ç»†å¯¹æ¯”åˆ†æ
    print(f"\n{'='*50} ç¬¬3æ­¥ï¼šè¯¦ç»†å¯¹æ¯”åˆ†æ {'='*50}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡
    metrics = calculate_similarity_metrics(hf_text, openai_text)
    
    print(f"ğŸ” æ–‡æœ¬å¯¹æ¯”:")
    print(f"   HF Checkpoint: '{hf_text}'")
    print(f"   OpenAI Model:  '{openai_text}'")
    print(f"   æ–‡æœ¬é•¿åº¦å·®å¼‚: {len(hf_text)} vs {len(openai_text)} å­—ç¬¦")
    
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦æŒ‡æ ‡:")
    print(f"   è¯é”™è¯¯ç‡ (WER): {metrics['wer']:.3f}")
    print(f"   å­—ç¬¦é”™è¯¯ç‡ (CER): {metrics['cer']:.3f}")
    print(f"   æ–‡æœ¬ç›¸ä¼¼åº¦: {metrics['similarity']:.3f}")
    
    print(f"\nâ±ï¸  æ€§èƒ½å¯¹æ¯”:")
    print(f"   HFæ¨ç†æ—¶é—´: {hf_time:.2f}s")
    print(f"   OpenAIæ¨ç†æ—¶é—´: {openai_time:.2f}s")
    
    if metrics['wer'] > 0.1 or metrics['cer'] > 0.1:
        print(f"\nâš ï¸  æ£€æµ‹åˆ°æ˜¾è‘—å·®å¼‚ï¼")
        print(f"   è¿™å¯èƒ½è¡¨æ˜æ¨¡å‹è½¬æ¢è¿‡ç¨‹ä¸­å­˜åœ¨é—®é¢˜")
        print(f"   å»ºè®®æ£€æŸ¥è½¬æ¢è„šæœ¬æˆ–é‡æ–°è½¬æ¢æ¨¡å‹")
    else:
        print(f"\nâœ… æ¨¡å‹è½¬æ¢è´¨é‡è‰¯å¥½ï¼Œå·®å¼‚åœ¨å¯æ¥å—èŒƒå›´å†…")
    
    return {
        'hf_text': hf_text,
        'openai_text': openai_text,
        'hf_time': hf_time,
        'openai_time': openai_time,
        'metrics': metrics,
        'audio_file': os.path.basename(audio_path)
    }


def main():
    """ä¸»å‡½æ•°"""
    # ===== ç›´æ¥åœ¨ä»£ç ä¸­é…ç½®æµ‹è¯•å‚æ•° =====
    # æ¨¡å‹è·¯å¾„é…ç½®
    custom_model_path = '../models/custom.pt'      # å¾®è°ƒæ¨¡å‹è·¯å¾„
    original_model_path = '../models/large-v3.pt'  # åŸå§‹æ¨¡å‹è·¯å¾„
    checkpoint_path = 'whisper-large-v3-finetuned/checkpoint-4000'  # HF checkpointè·¯å¾„
    
    # æµ‹è¯•æ¨¡å¼é…ç½®
    test_mode = "single"  # "single", "batch", "checkpoint_compare"
    
    if test_mode == "checkpoint_compare":
        # checkpoint vs custom.pt å¯¹æ¯”æ¨¡å¼
        audio_path = "../audio/00000034.wav"
        audio_dir = None
    elif test_mode == "single":
        # å•æ–‡ä»¶æµ‹è¯•æ¨¡å¼
        audio_path = "../audio/00000034.wav"  # æŒ‡å®šè¦æµ‹è¯•çš„éŸ³é¢‘æ–‡ä»¶
        audio_dir = None
    else:
        # æ‰¹é‡æµ‹è¯•æ¨¡å¼
        audio_path = None
        audio_dir = "../audio"  # æŒ‡å®šè¦æµ‹è¯•çš„éŸ³é¢‘ç›®å½•
    
    print("ğŸš€ å¼€å§‹å¾®è°ƒæ¨¡å‹ä¸åŸå§‹æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    
    if test_mode == "checkpoint_compare":
        print(f"ğŸ” æµ‹è¯•æ¨¡å¼: Checkpoint vs Custom.pt å¯¹æ¯”")
        print(f"âš™ï¸  HF Checkpoint: {checkpoint_path}")
        print(f"âš™ï¸  OpenAI Model: {custom_model_path}")
        print(f"ğŸµ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    elif test_mode == "single":
        print(f"ğŸµ æµ‹è¯•æ¨¡å¼: å•æ–‡ä»¶æµ‹è¯•")
        print(f"âš™ï¸  å¾®è°ƒæ¨¡å‹è·¯å¾„: {custom_model_path}")
        print(f"âš™ï¸  åŸå§‹æ¨¡å‹è·¯å¾„: {original_model_path}")
        print(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    else:
        print(f"ğŸµ æµ‹è¯•æ¨¡å¼: æ‰¹é‡æµ‹è¯•")
        print(f"âš™ï¸  å¾®è°ƒæ¨¡å‹è·¯å¾„: {custom_model_path}")
        print(f"âš™ï¸  åŸå§‹æ¨¡å‹è·¯å¾„: {original_model_path}")
        print(f"ğŸ“ éŸ³é¢‘ç›®å½•: {audio_dir}")
    
    try:
        # æ‰§è¡Œæµ‹è¯• - ä½¿ç”¨ä¼˜åŒ–çš„é€ä¸ªåŠ è½½ç­–ç•¥
        if test_mode == "checkpoint_compare":
            # checkpoint vs custom.pt ä¸“é¡¹å¯¹æ¯”
            result = test_checkpoint_vs_custom(checkpoint_path, custom_model_path, audio_path)
            print(f"\n{'='*80}")
            print(f"ğŸ“Š å¯¹æ¯”æµ‹è¯•å®Œæˆ")
            print(f"{'='*80}")
        elif test_mode == "single":
            # å•ä¸ªéŸ³é¢‘æ–‡ä»¶æµ‹è¯•
            result = test_single_audio_optimized(custom_model_path, original_model_path, audio_path)
            print_summary([result])
        else:
            # æ‰¹é‡éŸ³é¢‘æµ‹è¯•
            results = test_audio_directory_optimized(custom_model_path, original_model_path, audio_dir)
            print_summary(results)
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
