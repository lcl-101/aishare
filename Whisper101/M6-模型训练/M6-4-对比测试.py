#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
M6-5-æ–°æ¨¡å‹æµ‹è¯•.py - å¾®è°ƒæ¨¡å‹ä¸åŸå§‹æ¨¡å‹å¯¹æ¯”æµ‹è¯•

åŠŸèƒ½ï¼š
1. åŠ è½½å¾®è°ƒåçš„custom.ptæ¨¡å‹å’ŒåŸå§‹çš„large-v3.ptæ¨¡å‹
2. å¯¹åŒä¸€éŸ³é¢‘æ–‡ä»¶è¿›è¡Œæ¨ç†
3. å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„è¯†åˆ«ç»“æœ
4. è¯„ä¼°å‡†ç¡®ç‡å’Œç›¸ä¼¼åº¦
5. æ”¯æŒæ‰¹é‡æµ‹è¯•å¤šä¸ªéŸ³é¢‘æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
    # ç›´æ¥è¿è¡Œæµ‹è¯•ï¼ˆæ— éœ€å‘½ä»¤è¡Œå‚æ•°ï¼‰
    python M6-5-æ–°æ¨¡å‹æµ‹è¯•.py
    
    # å¯åœ¨main()å‡½æ•°ä¸­ä¿®æ”¹ä»¥ä¸‹é…ç½®ï¼š
    # - test_mode: "single"(å•æ–‡ä»¶) æˆ– "batch"(æ‰¹é‡æµ‹è¯•)
    # - audio_path: å•ä¸ªéŸ³é¢‘æ–‡ä»¶è·¯å¾„
    # - audio_dir: éŸ³é¢‘ç›®å½•è·¯å¾„
    # - custom_model_path: å¾®è°ƒæ¨¡å‹è·¯å¾„
    # - original_model_path: åŸå§‹æ¨¡å‹è·¯å¾„

ç¯å¢ƒè¦æ±‚ï¼š
    pip install openai-whisper jiwer

ä½œè€…ï¼šGitHub Copilot
æ—¥æœŸï¼š2025-07-07
"""

import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import whisper
import torch
import numpy as np
from jiwer import wer, cer


def load_single_model(model_path: str, model_name: str) -> whisper.Whisper:
    """
    åŠ è½½å•ä¸ªæ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
    Returns:
        whisper.Whisper: åŠ è½½çš„æ¨¡å‹
    """
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"âŒ {model_name}ä¸å­˜åœ¨: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½{model_name}: {model_path}")
    start_time = time.time()
    model = whisper.load_model(model_path)
    load_time = time.time() - start_time
    print(f"âœ… {model_name}åŠ è½½å®Œæˆ ({load_time:.2f}s)")
    
    return model


def release_model(model: whisper.Whisper, model_name: str) -> None:
    """
    é‡Šæ”¾æ¨¡å‹èµ„æº
    
    Args:
        model: è¦é‡Šæ”¾çš„æ¨¡å‹
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    try:
        # å°†æ¨¡å‹ç§»åˆ°CPUå¹¶åˆ é™¤å¼•ç”¨
        if hasattr(model, 'device'):
            model.to('cpu')
        del model
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        print(f"ğŸ§¹ {model_name}èµ„æºå·²é‡Šæ”¾")
    except Exception as e:
        print(f"âš ï¸  é‡Šæ”¾{model_name}èµ„æºæ—¶å‡ºé”™: {e}")


def transcribe_audio(model: whisper.Whisper, audio_path: str, model_name: str) -> Dict:
    """
    ä½¿ç”¨æŒ‡å®šæ¨¡å‹å¯¹éŸ³é¢‘è¿›è¡Œè½¬å½•
    
    Args:
        model: Whisperæ¨¡å‹
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
    Returns:
        dict: è½¬å½•ç»“æœ
    """
    print(f"ğŸµ ä½¿ç”¨{model_name}è½¬å½•: {os.path.basename(audio_path)}")
    
    start_time = time.time()
    
    # è¿›è¡Œè½¬å½•ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„å‚æ•°ï¼Œè§£å†³æˆªæ–­é—®é¢˜ï¼‰
    result = model.transcribe(
        audio_path,
        language="zh",  # ä¸­æ–‡
        task="transcribe",
        temperature=0.3,  # é€‚åº¦éšæœºæ€§ï¼Œé¿å…è¿‡æ—©æˆªæ–­
        beam_size=1,      # è´ªå©ªæœç´¢
        best_of=1,        # ç®€åŒ–æœç´¢
        patience=1.0,     # è€å¿ƒå‚æ•°
        length_penalty=0.0,  # ä¸­æ€§é•¿åº¦æƒ©ç½š
        suppress_tokens=[],  # ä¸æŠ‘åˆ¶ä»»ä½•token
        initial_prompt="",   # ç©ºåˆå§‹æç¤º
        condition_on_previous_text=False,  # ä¸ä¾èµ–å‰æ–‡ï¼Œé¿å…æˆªæ–­
        fp16=torch.cuda.is_available(),   # ä½¿ç”¨åŠç²¾åº¦ï¼ˆå¦‚æœGPUå¯ç”¨ï¼‰
        compression_ratio_threshold=2.4,  # å‹ç¼©æ¯”é˜ˆå€¼
        logprob_threshold=-1.0,           # å¯¹æ•°æ¦‚ç‡é˜ˆå€¼
        no_speech_threshold=0.6,          # æ— è¯­éŸ³é˜ˆå€¼
    )
    
    inference_time = time.time() - start_time
    
    # æå–å…³é”®ä¿¡æ¯
    transcription = {
        'text': result['text'].strip(),
        'language': result['language'],
        'segments': result['segments'],
        'inference_time': inference_time,
        'model_name': model_name
    }
    
    print(f"â±ï¸  {model_name}æ¨ç†è€—æ—¶: {inference_time:.2f}s")
    print(f"ğŸ“ {model_name}è¯†åˆ«ç»“æœ: {transcription['text']}")
    
    return transcription


def calculate_similarity_metrics(text1: str, text2: str) -> Dict[str, float]:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦æŒ‡æ ‡
    
    Args:
        text1: æ–‡æœ¬1
        text2: æ–‡æœ¬2
        
    Returns:
        dict: ç›¸ä¼¼åº¦æŒ‡æ ‡
    """
    if not text1.strip() or not text2.strip():
        return {
            'wer': 1.0,  # è¯é”™è¯¯ç‡100%
            'cer': 1.0,  # å­—ç¬¦é”™è¯¯ç‡100%
            'similarity': 0.0  # ç›¸ä¼¼åº¦0%
        }
    
    try:
        # è®¡ç®—è¯é”™è¯¯ç‡ (Word Error Rate)
        word_error_rate = wer(text1, text2)
        
        # è®¡ç®—å­—ç¬¦é”™è¯¯ç‡ (Character Error Rate)
        char_error_rate = cer(text1, text2)
        
        # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºäºå­—ç¬¦åŒ¹é…ï¼‰
        common_chars = set(text1.lower()) & set(text2.lower())
        total_chars = set(text1.lower()) | set(text2.lower())
        similarity = len(common_chars) / len(total_chars) if total_chars else 0.0
        
        return {
            'wer': word_error_rate,
            'cer': char_error_rate,
            'similarity': similarity
        }
    except Exception as e:
        print(f"âš ï¸  è®¡ç®—ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
        return {
            'wer': 1.0,
            'cer': 1.0,
            'similarity': 0.0
        }


def compare_results_with_truth(custom_result: Dict, original_result: Dict, audio_path: str, 
                              ground_truth: str = None) -> Dict:
    """
    å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ¨ç†ç»“æœï¼ŒåŒ…å«çœŸå®æ ‡ç­¾è¯„ä¼°
    
    Args:
        custom_result: å¾®è°ƒæ¨¡å‹ç»“æœ
        original_result: åŸå§‹æ¨¡å‹ç»“æœ
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        ground_truth: çœŸå®æ ‡ç­¾æ–‡æœ¬
        
    Returns:
        dict: å¯¹æ¯”ç»“æœ
    """
    print(f"\n" + "="*80)
    print(f"ğŸ“Š éŸ³é¢‘æ–‡ä»¶å¯¹æ¯”ç»“æœ: {os.path.basename(audio_path)}")
    print(f"="*80)
    
    custom_text = custom_result['text']
    original_text = original_result['text']
    
    # å°è¯•è‡ªåŠ¨åŠ è½½çœŸå®æ ‡ç­¾
    if ground_truth is None:
        ground_truth = load_ground_truth(audio_path)
    
    # æ‰“å°è¯†åˆ«ç»“æœ
    if ground_truth:
        print(f"âœ… çœŸå®æ ‡ç­¾: {ground_truth}")
        print(f"ğŸ¯ å¾®è°ƒæ¨¡å‹ç»“æœ: {custom_text}")
        print(f"ğŸ”„ åŸå§‹æ¨¡å‹ç»“æœ: {original_text}")
    else:
        print(f"ğŸ¯ å¾®è°ƒæ¨¡å‹ç»“æœ: {custom_text}")
        print(f"ğŸ”„ åŸå§‹æ¨¡å‹ç»“æœ: {original_text}")
        print(f"âš ï¸ æœªæ‰¾åˆ°çœŸå®æ ‡ç­¾æ–‡ä»¶")
    
    # è®¡ç®—ç›¸å¯¹äºçœŸå®æ ‡ç­¾çš„å‡†ç¡®æ€§
    custom_accuracy = None
    original_accuracy = None
    
    if ground_truth:
        print(f"\nğŸ¯ ç›¸å¯¹äºçœŸå®æ ‡ç­¾çš„å‡†ç¡®æ€§è¯„ä¼°:")
        
        custom_accuracy = calculate_accuracy_metrics(custom_text, ground_truth)
        original_accuracy = calculate_accuracy_metrics(original_text, ground_truth)
        
        print(f"   å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡: {custom_accuracy['accuracy']:.1%}")
        print(f"   å¾®è°ƒæ¨¡å‹WER: {custom_accuracy['wer']:.3f}")
        print(f"   å¾®è°ƒæ¨¡å‹ç²¾ç¡®åŒ¹é…: {'âœ… æ˜¯' if custom_accuracy['exact_match'] else 'âŒ å¦'}")
        
        print(f"   åŸå§‹æ¨¡å‹å‡†ç¡®ç‡: {original_accuracy['accuracy']:.1%}")
        print(f"   åŸå§‹æ¨¡å‹WER: {original_accuracy['wer']:.3f}")
        print(f"   åŸå§‹æ¨¡å‹ç²¾ç¡®åŒ¹é…: {'âœ… æ˜¯' if original_accuracy['exact_match'] else 'âŒ å¦'}")
        
        # åˆ¤æ–­å“ªä¸ªæ¨¡å‹æ›´å‡†ç¡®
        if custom_accuracy['accuracy'] > original_accuracy['accuracy']:
            improvement = (custom_accuracy['accuracy'] - original_accuracy['accuracy'])
            print(f"\nğŸ† å¾®è°ƒæ¨¡å‹æ›´å‡†ç¡® (å‡†ç¡®ç‡é«˜ {improvement:.1%})")
            
            # ç‰¹åˆ«åˆ†æï¼šå¦‚æœå¾®è°ƒæ¨¡å‹å®Œå…¨æ­£ç¡®è€ŒåŸå§‹æ¨¡å‹é”™è¯¯
            if custom_accuracy['exact_match'] and not original_accuracy['exact_match']:
                print(f"âœ¨ é‡è¦å‘ç°: å¾®è°ƒæ¨¡å‹å®ç°ç²¾ç¡®åŒ¹é…ï¼Œè€ŒåŸå§‹æ¨¡å‹è¯†åˆ«é”™è¯¯")
                print(f"ğŸ“ˆ è¿™è¡¨æ˜å¾®è°ƒè®­ç»ƒæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ­¤ç±»éŸ³é¢‘ä¸Šçš„å‡†ç¡®æ€§")
                
        elif original_accuracy['accuracy'] > custom_accuracy['accuracy']:
            degradation = (original_accuracy['accuracy'] - custom_accuracy['accuracy'])
            print(f"\nâš ï¸ åŸå§‹æ¨¡å‹æ›´å‡†ç¡® (å‡†ç¡®ç‡é«˜ {degradation:.1%})")
            print(f"ğŸ” å»ºè®®: å¾®è°ƒæ¨¡å‹åœ¨æ­¤æ ·æœ¬ä¸Šè¡¨ç°ä¸ä½³ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥è®­ç»ƒæ•°æ®")
        else:
            print(f"\nğŸ¤ ä¸¤ä¸ªæ¨¡å‹å‡†ç¡®ç‡ç›¸åŒ")
        
        # åˆ†ææ¨ç†é€Ÿåº¦å·®å¼‚çš„åˆç†æ€§
        print(f"\nâš¡ æ¨ç†é€Ÿåº¦åˆ†æ:")
        speed_ratio = custom_result['inference_time'] / original_result['inference_time']
        if speed_ratio > 1.5:
            print(f"âš ï¸ å¾®è°ƒæ¨¡å‹æ¨ç†è¾ƒæ…¢ ({speed_ratio:.1f}x)ï¼Œå¯èƒ½åŸå› :")
            print(f"   â€¢ æ¨¡å‹æƒé‡å˜åŒ–å¯¼è‡´è®¡ç®—å¤æ‚åº¦å¢åŠ ")
            print(f"   â€¢ è½¬æ¢è¿‡ç¨‹ä¸­çš„ç²¾åº¦å˜åŒ–")
            print(f"   â€¢ å†…å­˜è®¿é—®æ¨¡å¼çš„å·®å¼‚")
            print(f"ğŸ’¡ å»ºè®®: å¦‚æœå‡†ç¡®ç‡æå‡æ˜¾è‘—ï¼Œé€‚åº¦çš„é€Ÿåº¦é™ä½æ˜¯å¯æ¥å—çš„")
        elif speed_ratio > 1.2:
            print(f"ğŸ“Š å¾®è°ƒæ¨¡å‹æ¨ç†ç¨æ…¢ ({speed_ratio:.1f}x)ï¼Œå±äºæ­£å¸¸èŒƒå›´")
        else:
            print(f"âœ… æ¨ç†é€Ÿåº¦è¡¨ç°è‰¯å¥½ ({speed_ratio:.1f}x)")
    
    # è®¡ç®—æ¨¡å‹é—´ç›¸ä¼¼åº¦æŒ‡æ ‡ï¼ˆç”¨äºäº†è§£ä¸¤ä¸ªæ¨¡å‹çš„å·®å¼‚ç¨‹åº¦ï¼‰
    inter_model_metrics = calculate_similarity_metrics(custom_text, original_text)
    
    # æ‰“å°æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
    print(f"   å¾®è°ƒæ¨¡å‹æ¨ç†æ—¶é—´: {custom_result['inference_time']:.2f}s")
    print(f"   åŸå§‹æ¨¡å‹æ¨ç†æ—¶é—´: {original_result['inference_time']:.2f}s")
    
    speed_improvement = (original_result['inference_time'] - custom_result['inference_time']) / original_result['inference_time'] * 100
    if speed_improvement > 0:
        print(f"   âš¡ å¾®è°ƒæ¨¡å‹é€Ÿåº¦æå‡: {speed_improvement:.1f}%")
    else:
        print(f"   â³ å¾®è°ƒæ¨¡å‹é€Ÿåº¦é™ä½: {abs(speed_improvement):.1f}%")
    
    # æ‰“å°æ¨¡å‹é—´å·®å¼‚æŒ‡æ ‡
    print(f"\nğŸ” æ¨¡å‹é—´å·®å¼‚æŒ‡æ ‡:")
    print(f"   è¯é”™è¯¯ç‡ (WER): {inter_model_metrics['wer']:.3f}")
    print(f"   å­—ç¬¦é”™è¯¯ç‡ (CER): {inter_model_metrics['cer']:.3f}")
    print(f"   æ–‡æœ¬ç›¸ä¼¼åº¦: {inter_model_metrics['similarity']:.3f}")
    
    # æ‰“å°æ®µè½çº§åˆ«ä¿¡æ¯
    print(f"\nğŸ“ æ®µè½æ•°é‡å¯¹æ¯”:")
    print(f"   å¾®è°ƒæ¨¡å‹æ®µè½æ•°: {len(custom_result['segments'])}")
    print(f"   åŸå§‹æ¨¡å‹æ®µè½æ•°: {len(original_result['segments'])}")
    
    comparison = {
        'audio_file': os.path.basename(audio_path),
        'ground_truth': ground_truth,
        'custom_text': custom_text,
        'original_text': original_text,
        'custom_time': custom_result['inference_time'],
        'original_time': original_result['inference_time'],
        'speed_improvement': speed_improvement,
        'inter_model_metrics': inter_model_metrics,
        'custom_accuracy': custom_accuracy,
        'original_accuracy': original_accuracy,
        'custom_segments': len(custom_result['segments']),
        'original_segments': len(original_result['segments'])
    }
    
    return comparison


def compare_results(custom_result: Dict, original_result: Dict, audio_path: str) -> Dict:
    """
    å‘åå…¼å®¹çš„å¯¹æ¯”å‡½æ•°
    """
    return compare_results_with_truth(custom_result, original_result, audio_path)


def test_single_audio_optimized(custom_model_path: str, original_model_path: str, 
                               audio_path: str) -> Dict:
    """
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šé€ä¸ªåŠ è½½æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼ŒèŠ‚çœå†…å­˜
    
    Args:
        custom_model_path: å¾®è°ƒæ¨¡å‹è·¯å¾„
        original_model_path: åŸå§‹æ¨¡å‹è·¯å¾„
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: å¯¹æ¯”ç»“æœ
    """
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
    
    print(f"\nğŸµ å¼€å§‹æµ‹è¯•éŸ³é¢‘: {audio_path}")
    print("ğŸ’¡ é‡‡ç”¨é€ä¸ªåŠ è½½ç­–ç•¥ï¼ŒèŠ‚çœæ˜¾å­˜å ç”¨...")
    
    # ç¬¬ä¸€æ­¥ï¼šåŠ è½½å¾®è°ƒæ¨¡å‹å¹¶æ¨ç†
    print(f"\n{'='*50} ç¬¬1æ­¥ï¼šå¾®è°ƒæ¨¡å‹æ¨ç† {'='*50}")
    custom_model = load_single_model(custom_model_path, "å¾®è°ƒæ¨¡å‹")
    custom_result = transcribe_audio(custom_model, audio_path, "å¾®è°ƒæ¨¡å‹")
    release_model(custom_model, "å¾®è°ƒæ¨¡å‹")
    
    # ç¬¬äºŒæ­¥ï¼šåŠ è½½åŸå§‹æ¨¡å‹å¹¶æ¨ç†
    print(f"\n{'='*50} ç¬¬2æ­¥ï¼šåŸå§‹æ¨¡å‹æ¨ç† {'='*50}")
    original_model = load_single_model(original_model_path, "åŸå§‹æ¨¡å‹")
    original_result = transcribe_audio(original_model, audio_path, "åŸå§‹æ¨¡å‹")
    release_model(original_model, "åŸå§‹æ¨¡å‹")
    
    # ç¬¬ä¸‰æ­¥ï¼šå¯¹æ¯”ç»“æœ
    print(f"\n{'='*50} ç¬¬3æ­¥ï¼šç»“æœå¯¹æ¯”åˆ†æ {'='*50}")
    comparison = compare_results_with_truth(custom_result, original_result, audio_path)
    
    return comparison


def test_audio_directory_optimized(custom_model_path: str, original_model_path: str,
                                 audio_dir: str) -> List[Dict]:
    """
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šé€ä¸ªåŠ è½½æ¨¡å‹æµ‹è¯•éŸ³é¢‘ç›®å½•
    
    Args:
        custom_model_path: å¾®è°ƒæ¨¡å‹è·¯å¾„  
        original_model_path: åŸå§‹æ¨¡å‹è·¯å¾„
        audio_dir: éŸ³é¢‘ç›®å½•è·¯å¾„
        
    Returns:
        list: æ‰€æœ‰å¯¹æ¯”ç»“æœ
    """
    if not os.path.exists(audio_dir):
        raise FileNotFoundError(f"âŒ éŸ³é¢‘ç›®å½•ä¸å­˜åœ¨: {audio_dir}")
    
    # æ”¯æŒçš„éŸ³é¢‘æ ¼å¼
    audio_extensions = {'.wav', '.mp3', '.m4a', '.flac', '.aac', '.ogg'}
    
    # æŸ¥æ‰¾æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
    audio_files = []
    for ext in audio_extensions:
        audio_files.extend(Path(audio_dir).glob(f"*{ext}"))
        audio_files.extend(Path(audio_dir).glob(f"*{ext.upper()}"))
    
    if not audio_files:
        print(f"âš ï¸  åœ¨ {audio_dir} ç›®å½•ä¸‹æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
        return []
    
    print(f"ğŸ“ åœ¨ {audio_dir} ç›®å½•ä¸‹æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    print("ğŸ’¡ é‡‡ç”¨é€ä¸ªåŠ è½½ç­–ç•¥ï¼Œæ¯ä¸ªæ–‡ä»¶ç‹¬ç«‹æµ‹è¯•...")
    
    results = []
    for i, audio_file in enumerate(audio_files, 1):
        print(f"\n{'='*20} æµ‹è¯•è¿›åº¦: {i}/{len(audio_files)} {'='*20}")
        
        try:
            result = test_single_audio_optimized(custom_model_path, original_model_path, str(audio_file))
            results.append(result)
        except Exception as e:
            print(f"âŒ æµ‹è¯• {audio_file} æ—¶å‡ºé”™: {e}")
            continue
    
    return results


def print_summary_with_truth(results: List[Dict]) -> None:
    """
    æ‰“å°åŒ…å«çœŸå®æ ‡ç­¾è¯„ä¼°çš„æµ‹è¯•æ€»ç»“
    
    Args:
        results: æ‰€æœ‰æµ‹è¯•ç»“æœ
    """
    if not results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•ç»“æœ")
        return
    
    print(f"\n" + "="*80)
    print(f"ğŸ“Š æµ‹è¯•æ€»ç»“ (å…± {len(results)} ä¸ªéŸ³é¢‘æ–‡ä»¶)")
    print(f"="*80)
    
    # è®¡ç®—æ¨¡å‹é—´æ¯”è¾ƒæŒ‡æ ‡
    inter_model_wer = np.mean([r['inter_model_metrics']['wer'] for r in results])
    inter_model_cer = np.mean([r['inter_model_metrics']['cer'] for r in results])
    inter_model_similarity = np.mean([r['inter_model_metrics']['similarity'] for r in results])
    avg_speed_improvement = np.mean([r['speed_improvement'] for r in results])
    
    total_custom_time = sum([r['custom_time'] for r in results])
    total_original_time = sum([r['original_time'] for r in results])
    
    print(f"ğŸ” æ¨¡å‹é—´å·®å¼‚æŒ‡æ ‡:")
    print(f"   å¹³å‡è¯é”™è¯¯ç‡ (WER): {inter_model_wer:.3f}")
    print(f"   å¹³å‡å­—ç¬¦é”™è¯¯ç‡ (CER): {inter_model_cer:.3f}")
    print(f"   å¹³å‡æ–‡æœ¬ç›¸ä¼¼åº¦: {inter_model_similarity:.3f}")
    print(f"   å¹³å‡é€Ÿåº¦æå‡: {avg_speed_improvement:.1f}%")
    
    print(f"\nâ±ï¸ æ€»æ¨ç†æ—¶é—´:")
    print(f"   å¾®è°ƒæ¨¡å‹æ€»æ—¶é—´: {total_custom_time:.2f}s")
    print(f"   åŸå§‹æ¨¡å‹æ€»æ—¶é—´: {total_original_time:.2f}s")
    print(f"   æ€»æ—¶é—´èŠ‚çœ: {total_original_time - total_custom_time:.2f}s")
    
    # åˆ†ææœ‰çœŸå®æ ‡ç­¾çš„ç»“æœ
    truth_results = [r for r in results if r.get('ground_truth') and r.get('custom_accuracy') and r.get('original_accuracy')]
    
    if truth_results:
        print(f"\nğŸ¯ çœŸå®æ ‡ç­¾å‡†ç¡®æ€§åˆ†æ (å…± {len(truth_results)} ä¸ªæœ‰æ ‡ç­¾æ–‡ä»¶):")
        
        custom_accuracies = [r['custom_accuracy']['accuracy'] for r in truth_results]
        original_accuracies = [r['original_accuracy']['accuracy'] for r in truth_results]
        
        avg_custom_accuracy = np.mean(custom_accuracies)
        avg_original_accuracy = np.mean(original_accuracies)
        
        custom_exact_matches = sum([1 for r in truth_results if r['custom_accuracy']['exact_match']])
        original_exact_matches = sum([1 for r in truth_results if r['original_accuracy']['exact_match']])
        
        print(f"   å¾®è°ƒæ¨¡å‹å¹³å‡å‡†ç¡®ç‡: {avg_custom_accuracy:.1%}")
        print(f"   åŸå§‹æ¨¡å‹å¹³å‡å‡†ç¡®ç‡: {avg_original_accuracy:.1%}")
        print(f"   å¾®è°ƒæ¨¡å‹ç²¾ç¡®åŒ¹é…: {custom_exact_matches}/{len(truth_results)} ({custom_exact_matches/len(truth_results):.1%})")
        print(f"   åŸå§‹æ¨¡å‹ç²¾ç¡®åŒ¹é…: {original_exact_matches}/{len(truth_results)} ({original_exact_matches/len(truth_results):.1%})")
        
        if avg_custom_accuracy > avg_original_accuracy:
            improvement = avg_custom_accuracy - avg_original_accuracy
            print(f"\nğŸ† å¾®è°ƒæ¨¡å‹æ•´ä½“æ›´å‡†ç¡®ï¼Œå‡†ç¡®ç‡æå‡: {improvement:.1%}")
        elif avg_original_accuracy > avg_custom_accuracy:
            degradation = avg_original_accuracy - avg_custom_accuracy
            print(f"\nâš ï¸ å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡ä¸‹é™: {degradation:.1%}")
        else:
            print(f"\nğŸ¤ ä¸¤ä¸ªæ¨¡å‹å‡†ç¡®ç‡ç›¸åŒ")
        
        # æ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„æ ·æœ¬
        best_custom = max(truth_results, key=lambda x: x['custom_accuracy']['accuracy'])
        worst_custom = min(truth_results, key=lambda x: x['custom_accuracy']['accuracy'])
        
        print(f"\nğŸ“ˆ å¾®è°ƒæ¨¡å‹è¡¨ç°:")
        print(f"   æœ€ä½³æ ·æœ¬: {best_custom['audio_file']} (å‡†ç¡®ç‡: {best_custom['custom_accuracy']['accuracy']:.1%})")
        print(f"   æœ€å·®æ ·æœ¬: {worst_custom['audio_file']} (å‡†ç¡®ç‡: {worst_custom['custom_accuracy']['accuracy']:.1%})")
    
    # æ‰¾å‡ºæœ€ç›¸ä¼¼å’Œæœ€ä¸åŒçš„ç»“æœ
    best_similarity = max(results, key=lambda x: x['inter_model_metrics']['similarity'])
    worst_similarity = min(results, key=lambda x: x['inter_model_metrics']['similarity'])
    
    print(f"\nğŸ” æ¨¡å‹é—´ä¸€è‡´æ€§:")
    print(f"   æœ€ä¸€è‡´æ ·æœ¬: {best_similarity['audio_file']} (ç›¸ä¼¼åº¦: {best_similarity['inter_model_metrics']['similarity']:.3f})")
    print(f"   æœ€åˆ†æ­§æ ·æœ¬: {worst_similarity['audio_file']} (ç›¸ä¼¼åº¦: {worst_similarity['inter_model_metrics']['similarity']:.3f})")
    
    # è°ƒç”¨æ·±åº¦åˆ†æ
    analyze_finetuning_effectiveness(results)


def print_summary(results: List[Dict]) -> None:
    """
    å‘åå…¼å®¹çš„æ€»ç»“å‡½æ•°
    """
    # æ£€æŸ¥ç»“æœæ ¼å¼ï¼Œå†³å®šä½¿ç”¨å“ªä¸ªæ€»ç»“å‡½æ•°
    if results and any('inter_model_metrics' in r for r in results):
        print_summary_with_truth(results)
    else:
        # åŸæœ‰çš„æ€»ç»“é€»è¾‘
        print(f"\n" + "="*80)
        print(f"ğŸ“Š æµ‹è¯•æ€»ç»“ (å…± {len(results)} ä¸ªéŸ³é¢‘æ–‡ä»¶)")
        print(f"="*80)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if results and 'metrics' in results[0]:
            avg_wer = np.mean([r['metrics']['wer'] for r in results])
            avg_cer = np.mean([r['metrics']['cer'] for r in results])
            avg_similarity = np.mean([r['metrics']['similarity'] for r in results])
            avg_speed_improvement = np.mean([r['speed_improvement'] for r in results])
            
            total_custom_time = sum([r['custom_time'] for r in results])
            total_original_time = sum([r['original_time'] for r in results])
            
            print(f"ğŸ“ˆ å¹³å‡æŒ‡æ ‡:")
            print(f"   å¹³å‡è¯é”™è¯¯ç‡ (WER): {avg_wer:.3f}")
            print(f"   å¹³å‡å­—ç¬¦é”™è¯¯ç‡ (CER): {avg_cer:.3f}")
            print(f"   å¹³å‡æ–‡æœ¬ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
            print(f"   å¹³å‡é€Ÿåº¦æå‡: {avg_speed_improvement:.1f}%")
            
            print(f"\nâ±ï¸  æ€»æ¨ç†æ—¶é—´:")
            print(f"   å¾®è°ƒæ¨¡å‹æ€»æ—¶é—´: {total_custom_time:.2f}s")
            print(f"   åŸå§‹æ¨¡å‹æ€»æ—¶é—´: {total_original_time:.2f}s")
            print(f"   æ€»æ—¶é—´èŠ‚çœ: {total_original_time - total_custom_time:.2f}s")
            
            # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®ç»“æœ
            best_similarity = max(results, key=lambda x: x['metrics']['similarity'])
            worst_similarity = min(results, key=lambda x: x['metrics']['similarity'])
            
            print(f"\nğŸ† æœ€ä½³ä¸€è‡´æ€§: {best_similarity['audio_file']} (ç›¸ä¼¼åº¦: {best_similarity['metrics']['similarity']:.3f})")
            print(f"ğŸ’¡ æœ€å¤§å·®å¼‚: {worst_similarity['audio_file']} (ç›¸ä¼¼åº¦: {worst_similarity['metrics']['similarity']:.3f})")


def test_checkpoint_vs_custom(checkpoint_path: str, custom_model_path: str, audio_path: str) -> Dict:
    """
    ä¸“é—¨å¯¹æ¯”checkpoint-4000å’Œcustom.ptçš„å·®å¼‚
    
    Args:
        checkpoint_path: HuggingFace checkpointè·¯å¾„
        custom_model_path: è½¬æ¢åçš„OpenAIæ ¼å¼æ¨¡å‹è·¯å¾„
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: å¯¹æ¯”ç»“æœ
    """
    from transformers import WhisperForConditionalGeneration, WhisperProcessor
    import librosa
    
    print(f"\nğŸ” å¼€å§‹å¯¹æ¯” checkpoint-4000 vs custom.pt")
    print(f"ğŸ“‚ HF Checkpoint: {checkpoint_path}")
    print(f"ğŸ“‚ OpenAI Model: {custom_model_path}")
    print(f"ğŸµ æµ‹è¯•éŸ³é¢‘: {audio_path}")
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000)
    
    # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨HuggingFace checkpoint
    print(f"\n{'='*50} ç¬¬1æ­¥ï¼šHF Checkpointæ¨ç† {'='*50}")
    hf_model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path)
    hf_processor = WhisperProcessor.from_pretrained(checkpoint_path)
    
    # HFæ¨¡å‹æ¨ç†
    input_features = hf_processor(audio, sampling_rate=sr, return_tensors="pt").input_features
    
    # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°é¿å…é•¿åº¦é”™è¯¯
    start_time = time.time()
    generated_ids = hf_model.generate(
        input_features,
        language="zh",
        task="transcribe", 
        max_new_tokens=400,  # å‡å°‘åˆ°400é¿å…è¶…å‡ºé™åˆ¶
        num_beams=5,
        temperature=0.0,
        do_sample=False,
        return_dict_in_generate=True,
        output_scores=True
    )
    hf_time = time.time() - start_time
    
    hf_text = hf_processor.batch_decode(generated_ids.sequences, skip_special_tokens=True)[0].strip()
    print(f"âœ… HF Checkpointç»“æœ: {hf_text}")
    print(f"â±ï¸  HFæ¨ç†è€—æ—¶: {hf_time:.2f}s")
    
    # é‡Šæ”¾HFæ¨¡å‹
    del hf_model, hf_processor
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨OpenAIæ ¼å¼æ¨¡å‹
    print(f"\n{'='*50} ç¬¬2æ­¥ï¼šOpenAI Modelæ¨ç† {'='*50}")
    openai_model = load_single_model(custom_model_path, "OpenAIæ¨¡å‹")
    
    start_time = time.time()
    # ä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°è§£å†³æˆªæ–­é—®é¢˜
    openai_result = openai_model.transcribe(
        audio_path,
        language="zh",
        task="transcribe",
        temperature=0.3,  # å¢åŠ éšæœºæ€§ï¼Œé¿å…æˆªæ–­
        beam_size=1,      # è´ªå©ªæœç´¢
        best_of=1,
        patience=1.0,
        length_penalty=0.0,  # ä¸­æ€§é•¿åº¦æƒ©ç½š
        suppress_tokens=[],  # ä¸æŠ‘åˆ¶ä»»ä½•token
        initial_prompt="",   # ç©ºåˆå§‹æç¤º
        condition_on_previous_text=False,  # ä¸ä¾èµ–å‰æ–‡ï¼Œå…³é”®ï¼
        fp16=torch.cuda.is_available(),
        compression_ratio_threshold=2.4,
        logprob_threshold=-1.0,
        no_speech_threshold=0.6,
    )
    openai_time = time.time() - start_time
    
    openai_text = openai_result['text'].strip()
    print(f"âœ… OpenAI Modelç»“æœ: {openai_text}")
    print(f"â±ï¸  OpenAIæ¨ç†è€—æ—¶: {openai_time:.2f}s")
    
    release_model(openai_model, "OpenAIæ¨¡å‹")
    
    # ç¬¬ä¸‰æ­¥ï¼šè¯¦ç»†å¯¹æ¯”åˆ†æ
    print(f"\n{'='*50} ç¬¬3æ­¥ï¼šè¯¦ç»†å¯¹æ¯”åˆ†æ {'='*50}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦æŒ‡æ ‡
    metrics = calculate_similarity_metrics(hf_text, openai_text)
    
    print(f"ğŸ” æ–‡æœ¬å¯¹æ¯”:")
    print(f"   HF Checkpoint: '{hf_text}'")
    print(f"   OpenAI Model:  '{openai_text}'")
    print(f"   æ–‡æœ¬é•¿åº¦å·®å¼‚: {len(hf_text)} vs {len(openai_text)} å­—ç¬¦")
    
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦æŒ‡æ ‡:")
    print(f"   è¯é”™è¯¯ç‡ (WER): {metrics['wer']:.3f}")
    print(f"   å­—ç¬¦é”™è¯¯ç‡ (CER): {metrics['cer']:.3f}")
    print(f"   æ–‡æœ¬ç›¸ä¼¼åº¦: {metrics['similarity']:.3f}")
    
    print(f"\nâ±ï¸  æ€§èƒ½å¯¹æ¯”:")
    print(f"   HFæ¨ç†æ—¶é—´: {hf_time:.2f}s")
    print(f"   OpenAIæ¨ç†æ—¶é—´: {openai_time:.2f}s")
    
    if metrics['wer'] > 0.1 or metrics['cer'] > 0.1:
        print(f"\nâš ï¸  æ£€æµ‹åˆ°æ˜¾è‘—å·®å¼‚ï¼")
        print(f"   è¿™å¯èƒ½è¡¨æ˜æ¨¡å‹è½¬æ¢è¿‡ç¨‹ä¸­å­˜åœ¨é—®é¢˜")
        print(f"   å»ºè®®æ£€æŸ¥è½¬æ¢è„šæœ¬æˆ–é‡æ–°è½¬æ¢æ¨¡å‹")
    else:
        print(f"\nâœ… æ¨¡å‹è½¬æ¢è´¨é‡è‰¯å¥½ï¼Œå·®å¼‚åœ¨å¯æ¥å—èŒƒå›´å†…")
    
    return {
        'hf_text': hf_text,
        'openai_text': openai_text,
        'hf_time': hf_time,
        'openai_time': openai_time,
        'metrics': metrics,
        'audio_file': os.path.basename(audio_path)
    }


def diagnostic_test(model_path: str, audio_path: str, model_name: str) -> Dict:
    """
    è¯Šæ–­æ€§æµ‹è¯•ï¼Œæä¾›æ›´è¯¦ç»†çš„æ¨¡å‹åˆ†æ
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°
        
    Returns:
        dict: è¯¦ç»†çš„è¯Šæ–­ç»“æœ
    """
    print(f"\nğŸ” å¼€å§‹{model_name}è¯Šæ–­æ€§æµ‹è¯•...")
    
    # åŠ è½½æ¨¡å‹
    model = load_single_model(model_path, model_name)
    
    # è·å–æ¨¡å‹åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“‹ {model_name}åŸºæœ¬ä¿¡æ¯:")
    print(f"   æ¨¡å‹ç±»å‹: {type(model).__name__}")
    print(f"   è®¾å¤‡: {model.device}")
    
    # å¤šæ¬¡æ¨ç†æµ‹è¯•ä¸€è‡´æ€§
    print(f"\nğŸ”„ è¿›è¡Œ3æ¬¡æ¨ç†æµ‹è¯•ä¸€è‡´æ€§...")
    results = []
    times = []
    
    for i in range(3):
        start_time = time.time()
        result = model.transcribe(
            audio_path,
            language="zh",
            task="transcribe",
            temperature=0.0,  # ç¡®å®šæ€§æ¨ç†
            beam_size=1,
            best_of=1,
            condition_on_previous_text=False,
            fp16=torch.cuda.is_available(),
        )
        inference_time = time.time() - start_time
        
        text = result['text'].strip()
        results.append(text)
        times.append(inference_time)
        
        print(f"   ç¬¬{i+1}æ¬¡: {text} ({inference_time:.2f}s)")
    
    # æ£€æŸ¥ä¸€è‡´æ€§
    unique_results = set(results)
    is_consistent = len(unique_results) == 1
    avg_time = np.mean(times)
    
    print(f"âœ… ä¸€è‡´æ€§æ£€æŸ¥: {'é€šè¿‡' if is_consistent else 'å¤±è´¥'}")
    print(f"ğŸ“Š å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}s (Â±{np.std(times):.2f}s)")
    
    # å°è¯•ä¸åŒå‚æ•°çš„æ¨ç†
    print(f"\nğŸ§ª æµ‹è¯•ä¸åŒæ¨ç†å‚æ•°...")
    
    # æµ‹è¯•1ï¼šæ›´é«˜æ¸©åº¦
    result_temp = model.transcribe(
        audio_path,
        language="zh", 
        task="transcribe",
        temperature=0.5,
        beam_size=1,
        condition_on_previous_text=False,
    )
    print(f"   é«˜æ¸©åº¦(0.5): {result_temp['text'].strip()}")
    
    # æµ‹è¯•2ï¼šbeam search
    result_beam = model.transcribe(
        audio_path,
        language="zh",
        task="transcribe", 
        temperature=0.0,
        beam_size=5,
        condition_on_previous_text=False,
    )
    print(f"   Beamæœç´¢(5): {result_beam['text'].strip()}")
    
    release_model(model, model_name)
    
    return {
        'model_name': model_name,
        'consistent': is_consistent,
        'avg_time': avg_time,
        'std_time': np.std(times),
        'results': results,
        'temp_result': result_temp['text'].strip(),
        'beam_result': result_beam['text'].strip(),
    }

def evaluate_finetuned_model_quality(model_path: str, test_audio_dir: str) -> Dict:
    """
    è¯„ä¼°å¾®è°ƒæ¨¡å‹çš„æ•´ä½“è´¨é‡
    
    Args:
        model_path: å¾®è°ƒæ¨¡å‹è·¯å¾„
        test_audio_dir: æµ‹è¯•éŸ³é¢‘ç›®å½•
        
    Returns:
        dict: è´¨é‡è¯„ä¼°ç»“æœ
    """
    print(f"\nğŸ¯ å¼€å§‹å¾®è°ƒæ¨¡å‹è´¨é‡è¯„ä¼°...")
    print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ“ æµ‹è¯•ç›®å½•: {test_audio_dir}")
    
    if not os.path.exists(test_audio_dir):
        print(f"âŒ æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_audio_dir}")
        return {}
    
    # æŸ¥æ‰¾æµ‹è¯•éŸ³é¢‘æ–‡ä»¶
    audio_extensions = {'.wav', '.mp3', '.m4a', '.flac'}
    audio_files = []
    for ext in audio_extensions:
        audio_files.extend(Path(test_audio_dir).glob(f"*{ext}"))
    
    if not audio_files:
        print(f"âŒ åœ¨æµ‹è¯•ç›®å½•ä¸­æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
        return {}
    
    # é™åˆ¶æµ‹è¯•æ–‡ä»¶æ•°é‡ï¼ˆé¿å…æµ‹è¯•æ—¶é—´è¿‡é•¿ï¼‰
    test_files = audio_files[:5]  # åªæµ‹è¯•å‰5ä¸ªæ–‡ä»¶
    print(f"ğŸ“Š å°†æµ‹è¯• {len(test_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    
    # åŠ è½½æ¨¡å‹
    model = load_single_model(model_path, "å¾®è°ƒæ¨¡å‹")
    
    results = []
    total_time = 0
    
    for i, audio_file in enumerate(test_files, 1):
        print(f"\nğŸ“ æµ‹è¯•æ–‡ä»¶ {i}/{len(test_files)}: {audio_file.name}")
        
        try:
            start_time = time.time()
            result = model.transcribe(
                str(audio_file),
                language="zh",
                task="transcribe",
                temperature=0.0,
                beam_size=1,
                condition_on_previous_text=False,
                fp16=torch.cuda.is_available(),
            )
            inference_time = time.time() - start_time
            total_time += inference_time
            
            text = result['text'].strip()
            segments = len(result['segments'])
            
            print(f"   ç»“æœ: {text}")
            print(f"   æ¨ç†æ—¶é—´: {inference_time:.2f}s")
            print(f"   æ®µè½æ•°: {segments}")
            
            # ç®€å•çš„è´¨é‡æ£€æŸ¥
            quality_score = 0
            
            # æ£€æŸ¥1ï¼šæ–‡æœ¬é•¿åº¦åˆç†æ€§
            if 5 <= len(text) <= 200:
                quality_score += 1
            
            # æ£€æŸ¥2ï¼šåŒ…å«ä¸­æ–‡å­—ç¬¦
            if any('\u4e00' <= char <= '\u9fff' for char in text):
                quality_score += 1
            
            # æ£€æŸ¥3ï¼šä¸æ˜¯é‡å¤å­—ç¬¦
            if len(set(text)) > len(text) * 0.3:
                quality_score += 1
            
            # æ£€æŸ¥4ï¼šæ¨ç†æ—¶é—´åˆç†
            if inference_time < 5.0:  # å°äº5ç§’è®¤ä¸ºåˆç†
                quality_score += 1
            
            results.append({
                'file': audio_file.name,
                'text': text,
                'time': inference_time,
                'segments': segments,
                'quality_score': quality_score
            })
            
        except Exception as e:
            print(f"âŒ æµ‹è¯• {audio_file.name} æ—¶å‡ºé”™: {e}")
            continue
    
    release_model(model, "å¾®è°ƒæ¨¡å‹")
    
    if not results:
        print(f"âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")
        return {}
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    avg_time = total_time / len(results)
    avg_quality = np.mean([r['quality_score'] for r in results])
    avg_segments = np.mean([r['segments'] for r in results])
    avg_text_length = np.mean([len(r['text']) for r in results])
    
    print(f"\nğŸ“Š å¾®è°ƒæ¨¡å‹è´¨é‡è¯„ä¼°æŠ¥å‘Š:")
    print(f"   æµ‹è¯•æ–‡ä»¶æ•°: {len(results)}")
    print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}s")
    print(f"   å¹³å‡è´¨é‡å¾—åˆ†: {avg_quality:.1f}/4.0")
    print(f"   å¹³å‡æ®µè½æ•°: {avg_segments:.1f}")
    print(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {avg_text_length:.0f} å­—ç¬¦")
    
    # è´¨é‡åˆ¤æ–­
    if avg_quality >= 3.5:
        print(f"âœ… æ¨¡å‹è´¨é‡: ä¼˜ç§€")
    elif avg_quality >= 2.5:
        print(f"âš ï¸  æ¨¡å‹è´¨é‡: è‰¯å¥½")
    else:
        print(f"âŒ æ¨¡å‹è´¨é‡: éœ€è¦æ”¹è¿›")
    
    return {
        'test_count': len(results),
        'avg_time': avg_time,
        'avg_quality': avg_quality,
        'avg_segments': avg_segments,
        'avg_text_length': avg_text_length,
        'results': results
    }

def load_ground_truth(audio_file: str, text_dir: str = None) -> Optional[str]:
    """
    åŠ è½½éŸ³é¢‘æ–‡ä»¶å¯¹åº”çš„çœŸå®æ ‡ç­¾æ–‡æœ¬
    
    Args:
        audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        text_dir: æ–‡æœ¬æ–‡ä»¶ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ¨æ–­
        
    Returns:
        str: çœŸå®æ ‡ç­¾æ–‡æœ¬ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
    """
    # è·å–éŸ³é¢‘æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    audio_path = Path(audio_file)
    audio_name = audio_path.stem
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡æœ¬ç›®å½•ï¼Œå°è¯•è‡ªåŠ¨æ¨æ–­
    if text_dir is None:
        # å°è¯•å‡ ç§å¸¸è§çš„æ–‡æœ¬ç›®å½•ç»“æ„
        possible_text_dirs = [
            audio_path.parent / "text",  # åŒçº§textç›®å½•
            audio_path.parent / "../text",  # ä¸Šçº§textç›®å½•
            audio_path.parent.parent / "text",  # ä¸Šä¸Šçº§textç›®å½•
            Path("dataset/test/text"),  # é»˜è®¤æµ‹è¯•æ•°æ®é›†è·¯å¾„
        ]
        
        for text_dir_candidate in possible_text_dirs:
            if text_dir_candidate.exists():
                text_dir = text_dir_candidate
                break
    
    if text_dir is None:
        return None
    
    # æŸ¥æ‰¾å¯¹åº”çš„æ–‡æœ¬æ–‡ä»¶
    text_file = Path(text_dir) / f"{audio_name}.txt"
    
    if not text_file.exists():
        return None
    
    try:
        with open(text_file, 'r', encoding='utf-8') as f:
            ground_truth = f.read().strip()
        return ground_truth
    except Exception as e:
        print(f"âš ï¸ è¯»å–çœŸå®æ ‡ç­¾æ–‡ä»¶ {text_file} æ—¶å‡ºé”™: {e}")
        return None


def calculate_accuracy_metrics(pred_text: str, ground_truth: str) -> Dict[str, float]:
    """
    è®¡ç®—é¢„æµ‹æ–‡æœ¬ç›¸å¯¹äºçœŸå®æ ‡ç­¾çš„å‡†ç¡®æ€§æŒ‡æ ‡
    
    Args:
        pred_text: é¢„æµ‹æ–‡æœ¬
        ground_truth: çœŸå®æ ‡ç­¾æ–‡æœ¬
        
    Returns:
        dict: å‡†ç¡®æ€§æŒ‡æ ‡
    """
    if not pred_text.strip() or not ground_truth.strip():
        return {
            'wer': 1.0,
            'cer': 1.0,
            'accuracy': 0.0,
            'exact_match': False
        }
    
    try:
        # è®¡ç®—é”™è¯¯ç‡
        word_error_rate = wer(ground_truth, pred_text)
        char_error_rate = cer(ground_truth, pred_text)
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆ1 - é”™è¯¯ç‡ï¼‰
        word_accuracy = max(0.0, 1.0 - word_error_rate)
        char_accuracy = max(0.0, 1.0 - char_error_rate)
        
        # ç»¼åˆå‡†ç¡®ç‡
        overall_accuracy = (word_accuracy + char_accuracy) / 2
        
        # ç²¾ç¡®åŒ¹é…
        exact_match = pred_text.strip().lower() == ground_truth.strip().lower()
        
        return {
            'wer': word_error_rate,
            'cer': char_error_rate,
            'word_accuracy': word_accuracy,
            'char_accuracy': char_accuracy,
            'accuracy': overall_accuracy,
            'exact_match': exact_match
        }
    except Exception as e:
        print(f"âš ï¸ è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        return {
            'wer': 1.0,
            'cer': 1.0,
            'word_accuracy': 0.0,
            'char_accuracy': 0.0,
            'accuracy': 0.0,
            'exact_match': False
        }

def main():
    """ä¸»å‡½æ•°"""
    # ===== ç›´æ¥åœ¨ä»£ç ä¸­é…ç½®æµ‹è¯•å‚æ•° =====
    # æ¨¡å‹è·¯å¾„é…ç½®
    custom_model_path = '../models/custom.pt'      # å¾®è°ƒæ¨¡å‹è·¯å¾„
    original_model_path = '../models/large-v3.pt'  # åŸå§‹æ¨¡å‹è·¯å¾„
    checkpoint_path = 'whisper-large-v3-finetuned/checkpoint-20000'  # HF checkpointè·¯å¾„
    
    # æµ‹è¯•æ¨¡å¼é…ç½®
    test_mode = "single"  # "single", "batch", "checkpoint_compare", "diagnostic", "quality_eval"
    
    if test_mode == "checkpoint_compare":
        # checkpoint vs custom.pt å¯¹æ¯”æ¨¡å¼
        audio_path = "../audio/00000034.wav"  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        audio_dir = None
    elif test_mode == "quality_eval":
        # è´¨é‡è¯„ä¼°æ¨¡å¼
        audio_path = None
        audio_dir = "dataset/test/audio"  # ä½¿ç”¨æµ‹è¯•æ•°æ®é›†
    elif test_mode == "diagnostic":
        # è¯Šæ–­æ¨¡å¼
        audio_path = "../audio/00000034.wav"  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        audio_dir = None
    elif test_mode == "single":
        # å•æ–‡ä»¶æµ‹è¯•æ¨¡å¼
        audio_path = "../audio/00000034.wav"  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        audio_dir = None
    else:
        # æ‰¹é‡æµ‹è¯•æ¨¡å¼
        audio_path = None
        audio_dir = "dataset/test/audio"  # ä½¿ç”¨æœ‰æ ‡ç­¾çš„æµ‹è¯•æ•°æ®é›†
    
    print("ğŸš€ å¼€å§‹å¾®è°ƒæ¨¡å‹ä¸åŸå§‹æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    
    if test_mode == "checkpoint_compare":
        print(f"ğŸ” æµ‹è¯•æ¨¡å¼: Checkpoint vs Custom.pt å¯¹æ¯”")
        print(f"âš™ï¸  HF Checkpoint: {checkpoint_path}")
        print(f"âš™ï¸  OpenAI Model: {custom_model_path}")
        print(f"ğŸµ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    elif test_mode == "quality_eval":
        print(f"ğŸ¯ æµ‹è¯•æ¨¡å¼: å¾®è°ƒæ¨¡å‹è´¨é‡è¯„ä¼°")
        print(f"âš™ï¸  å¾®è°ƒæ¨¡å‹è·¯å¾„: {custom_model_path}")
        print(f"ğŸ“ æµ‹è¯•ç›®å½•: {audio_dir}")
    elif test_mode == "diagnostic":
        print(f"ğŸ” æµ‹è¯•æ¨¡å¼: è¯Šæ–­æ€§æµ‹è¯•")
        print(f"âš™ï¸  å¾®è°ƒæ¨¡å‹è·¯å¾„: {custom_model_path}")
        print(f"âš™ï¸  åŸå§‹æ¨¡å‹è·¯å¾„: {original_model_path}")
        print(f"ğŸµ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    elif test_mode == "single":
        print(f"ğŸµ æµ‹è¯•æ¨¡å¼: å•æ–‡ä»¶æµ‹è¯•")
        print(f"âš™ï¸  å¾®è°ƒæ¨¡å‹è·¯å¾„: {custom_model_path}")
        print(f"âš™ï¸  åŸå§‹æ¨¡å‹è·¯å¾„: {original_model_path}")
        print(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
    else:
        print(f"ğŸµ æµ‹è¯•æ¨¡å¼: æ‰¹é‡æµ‹è¯•")
        print(f"âš™ï¸  å¾®è°ƒæ¨¡å‹è·¯å¾„: {custom_model_path}")
        print(f"âš™ï¸  åŸå§‹æ¨¡å‹è·¯å¾„: {original_model_path}")
        print(f"ğŸ“ éŸ³é¢‘ç›®å½•: {audio_dir}")
    
    try:
        # æ‰§è¡Œæµ‹è¯• - ä½¿ç”¨ä¼˜åŒ–çš„é€ä¸ªåŠ è½½ç­–ç•¥
        if test_mode == "checkpoint_compare":
            # checkpoint vs custom.pt ä¸“é¡¹å¯¹æ¯”
            result = test_checkpoint_vs_custom(checkpoint_path, custom_model_path, audio_path)
            print(f"\n{'='*80}")
            print(f"ğŸ“Š å¯¹æ¯”æµ‹è¯•å®Œæˆ")
            print(f"{'='*80}")
        elif test_mode == "quality_eval":
            # å¾®è°ƒæ¨¡å‹è´¨é‡è¯„ä¼°
            quality_result = evaluate_finetuned_model_quality(custom_model_path, audio_dir)
            if quality_result:
                print(f"\nâœ… è´¨é‡è¯„ä¼°å®Œæˆ")
            else:
                print(f"\nâŒ è´¨é‡è¯„ä¼°å¤±è´¥")
        elif test_mode == "diagnostic":
            # è¯Šæ–­æ€§æµ‹è¯•
            print(f"\n{'='*60} å¾®è°ƒæ¨¡å‹è¯Šæ–­ {'='*60}")
            custom_diag = diagnostic_test(custom_model_path, audio_path, "å¾®è°ƒæ¨¡å‹")
            
            print(f"\n{'='*60} åŸå§‹æ¨¡å‹è¯Šæ–­ {'='*60}")
            original_diag = diagnostic_test(original_model_path, audio_path, "åŸå§‹æ¨¡å‹")
            
            # è¯Šæ–­æ€»ç»“
            print(f"\n{'='*60} è¯Šæ–­æ€»ç»“ {'='*60}")
            print(f"ğŸ” å¾®è°ƒæ¨¡å‹ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if custom_diag['consistent'] else 'âŒ å¤±è´¥'}")
            print(f"ğŸ” åŸå§‹æ¨¡å‹ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if original_diag['consistent'] else 'âŒ å¤±è´¥'}")
            print(f"â±ï¸  å¹³å‡æ¨ç†æ—¶é—´å¯¹æ¯”:")
            print(f"   å¾®è°ƒæ¨¡å‹: {custom_diag['avg_time']:.2f}s (Â±{custom_diag['std_time']:.2f}s)")
            print(f"   åŸå§‹æ¨¡å‹: {original_diag['avg_time']:.2f}s (Â±{original_diag['std_time']:.2f}s)")
            
            if custom_diag['avg_time'] > original_diag['avg_time'] * 1.5:
                print(f"âš ï¸  å¾®è°ƒæ¨¡å‹æ¨ç†é€Ÿåº¦å¼‚å¸¸ç¼“æ…¢ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹è½¬æ¢è¿‡ç¨‹")
            
            # æ£€æŸ¥ä¸åŒå‚æ•°ä¸‹çš„ç»“æœä¸€è‡´æ€§
            custom_variants = {custom_diag['results'][0], custom_diag['temp_result'], custom_diag['beam_result']}
            original_variants = {original_diag['results'][0], original_diag['temp_result'], original_diag['beam_result']}
            
            print(f"\nğŸ§ª å‚æ•°æ•æ„Ÿæ€§åˆ†æ:")
            print(f"   å¾®è°ƒæ¨¡å‹è¾“å‡ºå˜å¼‚æ•°: {len(custom_variants)} ç§")
            print(f"   åŸå§‹æ¨¡å‹è¾“å‡ºå˜å¼‚æ•°: {len(original_variants)} ç§")
            
            if len(custom_variants) > 2:
                print(f"âš ï¸  å¾®è°ƒæ¨¡å‹è¾“å‡ºä¸ç¨³å®šï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜")
            
        elif test_mode == "single":
            # å•ä¸ªéŸ³é¢‘æ–‡ä»¶æµ‹è¯•
            result = test_single_audio_optimized(custom_model_path, original_model_path, audio_path)
            print_summary([result])
        else:
            # æ‰¹é‡éŸ³é¢‘æµ‹è¯•
            results = test_audio_directory_optimized(custom_model_path, original_model_path, audio_dir)
            print_summary(results)
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def analyze_finetuning_effectiveness(results: List[Dict]) -> None:
    """
    ä¸“é—¨åˆ†æå¾®è°ƒæ•ˆæœçš„å‡½æ•°
    
    Args:
        results: æµ‹è¯•ç»“æœåˆ—è¡¨
    """
    if not results:
        return
    
    print(f"\n" + "="*80)
    print(f"ğŸ¯ å¾®è°ƒæ¨¡å‹æ•ˆæœæ·±åº¦åˆ†æ")
    print(f"="*80)
    
    # ç»Ÿè®¡æœ‰çœŸå®æ ‡ç­¾çš„ç»“æœ
    truth_results = [r for r in results if r.get('ground_truth') and r.get('custom_accuracy') and r.get('original_accuracy')]
    
    if not truth_results:
        print(f"âŒ æ²¡æœ‰å¯ç”¨çš„çœŸå®æ ‡ç­¾æ•°æ®è¿›è¡Œåˆ†æ")
        return
    
    # åˆ†ç±»ç»Ÿè®¡
    custom_better = 0  # å¾®è°ƒæ¨¡å‹æ›´å¥½
    original_better = 0  # åŸå§‹æ¨¡å‹æ›´å¥½
    equal_performance = 0  # æ€§èƒ½ç›¸åŒ
    
    custom_exact_wins = 0  # å¾®è°ƒæ¨¡å‹ç²¾ç¡®åŒ¹é…è€ŒåŸå§‹æ¨¡å‹é”™è¯¯
    original_exact_wins = 0  # åŸå§‹æ¨¡å‹ç²¾ç¡®åŒ¹é…è€Œå¾®è°ƒæ¨¡å‹é”™è¯¯
    both_exact = 0  # ä¸¤è€…éƒ½ç²¾ç¡®åŒ¹é…
    neither_exact = 0  # ä¸¤è€…éƒ½ä¸ç²¾ç¡®åŒ¹é…
    
    speed_comparisons = []
    accuracy_improvements = []
    
    for result in truth_results:
        custom_acc = result['custom_accuracy']['accuracy']
        original_acc = result['original_accuracy']['accuracy']
        
        custom_exact = result['custom_accuracy']['exact_match']
        original_exact = result['original_accuracy']['exact_match']
        
        # ç»Ÿè®¡å‡†ç¡®ç‡æ¯”è¾ƒ
        if custom_acc > original_acc:
            custom_better += 1
            accuracy_improvements.append(custom_acc - original_acc)
        elif original_acc > custom_acc:
            original_better += 1
            accuracy_improvements.append(original_acc - custom_acc)
        else:
            equal_performance += 1
        
        # ç»Ÿè®¡ç²¾ç¡®åŒ¹é…æƒ…å†µ
        if custom_exact and original_exact:
            both_exact += 1
        elif custom_exact and not original_exact:
            custom_exact_wins += 1
        elif original_exact and not custom_exact:
            original_exact_wins += 1
        else:
            neither_exact += 1
        
        # ç»Ÿè®¡é€Ÿåº¦æ¯”è¾ƒ
        speed_ratio = result['custom_time'] / result['original_time']
        speed_comparisons.append(speed_ratio)
    
    total_samples = len(truth_results)
    
    print(f"ğŸ“Š å‡†ç¡®ç‡å¯¹æ¯”ç»Ÿè®¡ ({total_samples} ä¸ªæ ·æœ¬):")
    print(f"   ğŸ† å¾®è°ƒæ¨¡å‹æ›´å‡†ç¡®: {custom_better} æ ·æœ¬ ({custom_better/total_samples:.1%})")
    print(f"   ğŸ”„ åŸå§‹æ¨¡å‹æ›´å‡†ç¡®: {original_better} æ ·æœ¬ ({original_better/total_samples:.1%})")
    print(f"   ğŸ¤ å‡†ç¡®ç‡ç›¸åŒ: {equal_performance} æ ·æœ¬ ({equal_performance/total_samples:.1%})")
    
    print(f"\nğŸ¯ ç²¾ç¡®åŒ¹é…ç»Ÿè®¡:")
    print(f"   âœ¨ å¾®è°ƒæ¨¡å‹ç²¾ç¡®åŒ¹é…(åŸå§‹é”™è¯¯): {custom_exact_wins} æ ·æœ¬ ({custom_exact_wins/total_samples:.1%})")
    print(f"   ğŸ”„ åŸå§‹æ¨¡å‹ç²¾ç¡®åŒ¹é…(å¾®è°ƒé”™è¯¯): {original_exact_wins} æ ·æœ¬ ({original_exact_wins/total_samples:.1%})")
    print(f"   ğŸ¤ ä¸¤è€…éƒ½ç²¾ç¡®åŒ¹é…: {both_exact} æ ·æœ¬ ({both_exact/total_samples:.1%})")
    print(f"   âŒ ä¸¤è€…éƒ½æœªç²¾ç¡®åŒ¹é…: {neither_exact} æ ·æœ¬ ({neither_exact/total_samples:.1%})")
    
    # è®¡ç®—æ•´ä½“æ”¹å–„æƒ…å†µ
    if custom_better > original_better:
        net_improvement = custom_better - original_better
        print(f"\nğŸ‰ å¾®è°ƒæ•ˆæœè¯„ä¼°: ç§¯æ")
        print(f"   å‡€æ”¹å–„æ ·æœ¬æ•°: {net_improvement} ä¸ª")
        print(f"   æ”¹å–„ç‡: {net_improvement/total_samples:.1%}")
        
        if custom_exact_wins > 0:
            print(f"   ç‰¹åˆ«äº®ç‚¹: {custom_exact_wins} ä¸ªæ ·æœ¬ä»é”™è¯¯å˜ä¸ºå®Œå…¨æ­£ç¡®")
    
    elif original_better > custom_better:
        net_degradation = original_better - custom_better
        print(f"\nâš ï¸ å¾®è°ƒæ•ˆæœè¯„ä¼°: éœ€è¦æ”¹è¿›")
        print(f"   å‡€é€€åŒ–æ ·æœ¬æ•°: {net_degradation} ä¸ª")
        print(f"   é€€åŒ–ç‡: {net_degradation/total_samples:.1%}")
    else:
        print(f"\nğŸ¤ å¾®è°ƒæ•ˆæœè¯„ä¼°: ä¸­æ€§")
        print(f"   æ•´ä½“å‡†ç¡®ç‡æ— æ˜¾è‘—å˜åŒ–")
    
    # é€Ÿåº¦åˆ†æ
    avg_speed_ratio = np.mean(speed_comparisons)
    speed_std = np.std(speed_comparisons)
    
    print(f"\nâš¡ æ¨ç†é€Ÿåº¦åˆ†æ:")
    print(f"   å¹³å‡é€Ÿåº¦æ¯”ä¾‹: {avg_speed_ratio:.2f}x (å¾®è°ƒ/åŸå§‹)")
    print(f"   é€Ÿåº¦å˜åŒ–æ ‡å‡†å·®: {speed_std:.2f}")
    
    if avg_speed_ratio > 1.5:
        print(f"   ğŸ“ é€Ÿåº¦è¯„ä¼°: æ˜æ˜¾å˜æ…¢ï¼Œå»ºè®®ä¼˜åŒ–")
        print(f"   ğŸ’¡ å¯èƒ½åŸå› : æ¨¡å‹å¤æ‚åº¦å¢åŠ ã€è½¬æ¢ç²¾åº¦æŸå¤±")
    elif avg_speed_ratio > 1.2:
        print(f"   ğŸ“ é€Ÿåº¦è¯„ä¼°: è½»å¾®å˜æ…¢ï¼Œå¯æ¥å—èŒƒå›´")
    elif avg_speed_ratio > 0.8:
        print(f"   ğŸ“ é€Ÿåº¦è¯„ä¼°: å˜åŒ–ä¸å¤§ï¼Œè¡¨ç°è‰¯å¥½")
    else:
        print(f"   ğŸ“ é€Ÿåº¦è¯„ä¼°: åè€Œå˜å¿«ï¼Œå¯èƒ½å­˜åœ¨æµ‹é‡è¯¯å·®")
    
    # ç»¼åˆè¯„ä¼°å»ºè®®
    print(f"\nğŸ’¡ ç»¼åˆè¯„ä¼°å»ºè®®:")
    
    if custom_exact_wins > original_exact_wins and custom_better >= original_better:
        print(f"   âœ… å¾®è°ƒè®­ç»ƒæˆåŠŸï¼æ¨¡å‹åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½")
        print(f"   ğŸ“ˆ å»ºè®®ç»§ç»­ä½¿ç”¨å¾®è°ƒæ¨¡å‹è¿›è¡Œæ¨ç†")
        if avg_speed_ratio > 1.3:
            print(f"   âš¡ è€ƒè™‘è¿›è¡Œæ¨¡å‹é‡åŒ–æˆ–ä¼˜åŒ–ä»¥æ”¹å–„æ¨ç†é€Ÿåº¦")
    
    elif custom_better > original_better * 1.5:
        print(f"   âœ… å¾®è°ƒæ•ˆæœæ˜¾è‘—ï¼Œå‡†ç¡®ç‡æ˜æ˜¾æå‡")
        print(f"   ğŸ“Š å‡†ç¡®ç‡æ”¹å–„è¶…è¿‡50%çš„æ ·æœ¬å ä¸»å¯¼")
    
    elif custom_exact_wins == 0 and original_exact_wins > 0:
        print(f"   âš ï¸ å¾®è°ƒå¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥:")
        print(f"     â€¢ è®­ç»ƒæ•°æ®è´¨é‡å’Œåˆ†å¸ƒ")
        print(f"     â€¢ è®­ç»ƒè¶…å‚æ•°è®¾ç½®")
        print(f"     â€¢ æ¨¡å‹è½¬æ¢è¿‡ç¨‹")
    
    else:
        print(f"   ğŸ“Š å¾®è°ƒæ•ˆæœä¸€èˆ¬ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–:")
        print(f"     â€¢ å¢åŠ è®­ç»ƒæ•°æ®é‡")
        print(f"     â€¢ è°ƒæ•´å­¦ä¹ ç‡å’Œè®­ç»ƒè½®æ¬¡")
        print(f"     â€¢ æ£€æŸ¥æ•°æ®æ ‡æ³¨è´¨é‡")


if __name__ == "__main__":
    main()
