"""
M6-2: Whisper æ¨¡å‹å¾®è°ƒè®­ç»ƒ

ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†å¯¹ Whisper æ¨¡å‹è¿›è¡Œå¾®è°ƒè®­ç»ƒã€‚

æ•°æ®é›†ç»“æ„:
- dataset/train/audio/ - è®­ç»ƒéŸ³é¢‘æ–‡ä»¶ (wavæ ¼å¼)  
- dataset/train/text/  - è®­ç»ƒæ–‡æœ¬æ–‡ä»¶ (txtæ ¼å¼)
- dataset/test/audio/  - æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ (wavæ ¼å¼)
- dataset/test/text/   - æµ‹è¯•æ–‡æœ¬æ–‡ä»¶ (txtæ ¼å¼)

ä¾èµ–å®‰è£…:
pip install transformers datasets evaluate accelerate jiwer
"""
import torch
import os
import numpy as np
import json
import datetime
from dataclasses import dataclass
from typing import Any, Dict, List, Union

try:
    from transformers import (
        WhisperTokenizer, WhisperFeatureExtractor, WhisperProcessor,
        WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer
    )
    from datasets import load_dataset, Audio
    from huggingface_hub import login
    import evaluate
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("è¯·å®‰è£…ä¾èµ–: pip install torch transformers datasets evaluate huggingface_hub accelerate")
    exit(1)

# --- é…ç½®å‚æ•° ---
DATASET_ROOT = "dataset"
TRAIN_FOLDER = "train"
TEST_FOLDER = "test"

# æ¨¡å‹è·¯å¾„ï¼ˆéœ€æå‰ä¸‹è½½åˆ°æœ¬åœ°ï¼‰
MODEL_NAME = "../models/whisper-large-v3"  # æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œæˆ–æ”¹ä¸º "openai/whisper-large-v3" ä½¿ç”¨åœ¨çº¿æ¨¡å‹

LANGUAGE = "zh"
TASK = "transcribe"
OUTPUT_DIR = "whisper-large-v3-finetuned"

def create_dataset_json(folder_path, json_file_path):
    """åˆ›å»ºæ•°æ®é›†JSONæ–‡ä»¶"""
    print(f"ğŸ“ æ­£åœ¨åˆ›å»ºæ•°æ®é›†æ–‡ä»¶: {json_file_path}")
    
    # ç§»é™¤æ—§æ–‡ä»¶
    if os.path.exists(json_file_path):
        os.remove(json_file_path)
    
    audio_path = os.path.join(folder_path, "audio")
    text_path = os.path.join(folder_path, "text")
    
    if not os.path.exists(audio_path) or not os.path.exists(text_path):
        raise FileNotFoundError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {folder_path}")
    
    count = 0
    with open(json_file_path, 'w', encoding='utf-8') as json_file:
        for audio_file in os.listdir(audio_path):
            if not audio_file.endswith('.wav'):
                continue
                
            audio_file_path = os.path.join(audio_path, audio_file)
            text_file_path = os.path.join(text_path, audio_file.replace('.wav', '.txt'))
            
            if not os.path.exists(text_file_path):
                print(f"âš ï¸ ç¼ºå°‘å¯¹åº”æ–‡æœ¬æ–‡ä»¶: {text_file_path}")
                continue
            
            try:
                with open(text_file_path, 'r', encoding='utf-8') as txt_file:
                    txt_sentence = txt_file.read().strip()
                
                name = audio_file.replace('.wav', '')
                data = {
                    "name": name,
                    "audio_file_path": audio_file_path,
                    "txt_sentence": txt_sentence
                }
                
                json_file.write(json.dumps(data, ensure_ascii=False) + '\n')
                count += 1
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {audio_file} æ—¶å‡ºé”™: {e}")
    
    print(f"âœ… å®Œæˆï¼Œå…±å¤„ç† {count} ä¸ªæ ·æœ¬")
    return count

def prepare_dataset(batch, feature_extractor, tokenizer):
    """æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼ˆæ”¯æŒæ‰¹å¤„ç†ï¼‰"""
    if isinstance(batch['audio_file_path'], list):
        # æ‰¹å¤„ç†æ¨¡å¼
        input_features = []
        labels = []
        for audio in batch['audio_file_path']:
            features = feature_extractor(
                audio['array'], 
                sampling_rate=audio['sampling_rate']
            ).input_features[0]
            input_features.append(features)
        
        for text in batch['txt_sentence']:
            label = tokenizer(text).input_ids
            labels.append(label)
            
        batch['input_features'] = input_features
        batch['labels'] = labels
    else:
        # å•ä¸ªæ ·æœ¬æ¨¡å¼
        audio = batch['audio_file_path']
        batch['input_features'] = feature_extractor(
            audio['array'], 
            sampling_rate=audio['sampling_rate']
        ).input_features[0]
        batch['labels'] = tokenizer(batch['txt_sentence']).input_ids
    
    return batch

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """è¯­éŸ³åºåˆ—åˆ°åºåˆ—æ•°æ®æ•´ç†å™¨"""
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # åˆ†ç¦»è¾“å…¥å’Œæ ‡ç­¾ï¼Œå› ä¸ºå®ƒä»¬é•¿åº¦ä¸åŒéœ€è¦ä¸åŒçš„å¡«å……æ–¹æ³•
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # è·å–æ ‡è®°åŒ–çš„æ ‡ç­¾åºåˆ—
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # ç”¨ -100 æ›¿æ¢å¡«å……ä»¥æ­£ç¡®å¿½ç•¥æŸå¤±
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # å¦‚æœåœ¨ä¹‹å‰çš„æ ‡è®°åŒ–æ­¥éª¤ä¸­æ·»åŠ äº† bos tokenï¼Œåœ¨è¿™é‡Œåˆ é™¤å®ƒ
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

def compute_metrics(pred, tokenizer):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # ç”¨ pad_token_id æ›¿æ¢ -100
    label_ids[label_ids == -100] = tokenizer.pad_token_id

    # è§£ç é¢„æµ‹å’Œæ ‡ç­¾
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    # è®¡ç®— WER
    metric = evaluate.load("wer")
    wer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

def monitor_gpu_utilization():
    """ç›‘æ§GPUåˆ©ç”¨ç‡"""
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            utilization = int(result.stdout.strip())
            return utilization
    except:
        pass
    return None

def check_gpu_memory():
    """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
            allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
            cached = torch.cuda.memory_reserved(i) / 1024**3  # GB
            utilization = monitor_gpu_utilization()
            
            print(f"ğŸ® GPU {i} ({gpu_name}):")
            print(f"   ğŸ’¾ æ€»å†…å­˜: {total_memory:.1f} GB")
            print(f"   ğŸ”¥ å·²åˆ†é…: {allocated:.1f} GB")
            print(f"   ğŸ“¦ å·²ç¼“å­˜: {cached:.1f} GB")
            print(f"   ğŸ†“ å¯ç”¨: {total_memory - cached:.1f} GB")
            if utilization is not None:
                print(f"   âš¡ GPUåˆ©ç”¨ç‡: {utilization}%")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡")

def check_datasets_cache():
    """æ£€æŸ¥æ•°æ®é›†ç¼“å­˜ä½¿ç”¨æƒ…å†µ"""
    import shutil
    
    cache_dir = os.path.expanduser("~/.cache/huggingface/datasets")
    if os.path.exists(cache_dir):
        # è®¡ç®—ç¼“å­˜å¤§å°
        total_size = shutil.disk_usage(cache_dir).used
        total_size_gb = total_size / (1024**3)
        
        print(f"ğŸ“¦ æ•°æ®é›†ç¼“å­˜ä¿¡æ¯:")
        print(f"   ğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
        print(f"   ğŸ’¾ ç¼“å­˜å¤§å°: {total_size_gb:.1f} GB")
        
        # æ£€æŸ¥å…·ä½“çš„jsonç¼“å­˜
        json_cache_dir = os.path.join(cache_dir, "json")
        if os.path.exists(json_cache_dir):
            json_dirs = [d for d in os.listdir(json_cache_dir) if d.startswith("default-")]
            print(f"   ğŸ—‚ï¸ JSONç¼“å­˜ç›®å½•æ•°é‡: {len(json_dirs)}")
            
            if total_size_gb > 50:  # è¶…è¿‡50GBæé†’
                print(f"   âš ï¸ ç¼“å­˜è¾ƒå¤§ï¼Œå¯è€ƒè™‘æ¸…ç†æ—§ç¼“å­˜")
                print(f"   ğŸ§¹ æ¸…ç†å‘½ä»¤: rm -rf {cache_dir}/json/default-*")
        
        return total_size_gb
    else:
        print("ğŸ“¦ æ•°æ®é›†ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return 0

def clear_datasets_cache():
    """æ¸…ç†æ•°æ®é›†ç¼“å­˜"""
    import shutil
    
    cache_dir = os.path.expanduser("~/.cache/huggingface/datasets")
    if not os.path.exists(cache_dir):
        print("ğŸ“¦ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
        return 0
    
    # è®¡ç®—æ¸…ç†å‰å¤§å°
    try:
        before_size = shutil.disk_usage(cache_dir).used / (1024**3)
    except:
        before_size = 0
    
    cleaned_count = 0
    cleaned_size = 0
    
    # æ¸…ç†jsonç¼“å­˜ç›®å½•
    json_cache_dir = os.path.join(cache_dir, "json")
    if os.path.exists(json_cache_dir):
        for item in os.listdir(json_cache_dir):
            if item.startswith("default-"):
                item_path = os.path.join(json_cache_dir, item)
                try:
                    if os.path.isdir(item_path):
                        # è®¡ç®—ç›®å½•å¤§å°
                        for root, dirs, files in os.walk(item_path):
                            for file in files:
                                try:
                                    cleaned_size += os.path.getsize(os.path.join(root, file))
                                except:
                                    pass
                        shutil.rmtree(item_path)
                        cleaned_count += 1
                    else:
                        cleaned_size += os.path.getsize(item_path)
                        os.remove(item_path)
                        cleaned_count += 1
                except Exception as e:
                    print(f"   âš ï¸ æ¸…ç† {item} æ—¶å‡ºé”™: {e}")
    
    # æ¸…ç†é”æ–‡ä»¶
    for item in os.listdir(cache_dir):
        if item.endswith(".lock") and "json_default-" in item:
            lock_path = os.path.join(cache_dir, item)
            try:
                os.remove(lock_path)
                cleaned_count += 1
            except Exception as e:
                print(f"   âš ï¸ æ¸…ç†é”æ–‡ä»¶ {item} æ—¶å‡ºé”™: {e}")
    
    cleaned_size_gb = cleaned_size / (1024**3)
    
    if cleaned_count > 0:
        print(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ:")
        print(f"   ğŸ—‘ï¸ æ¸…ç†æ–‡ä»¶/ç›®å½•æ•°é‡: {cleaned_count}")
        print(f"   ğŸ’¾ é‡Šæ”¾ç©ºé—´: {cleaned_size_gb:.1f} GB")
    else:
        print("ğŸ“¦ æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„ç¼“å­˜æ–‡ä»¶")
    
    return cleaned_size_gb

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("ğŸš€ å¼€å§‹ Whisper æ¨¡å‹å¾®è°ƒè®­ç»ƒ")
    print("=" * 60)
    
    # æ£€æŸ¥GPUå†…å­˜å’Œç¼“å­˜ä½¿ç”¨æƒ…å†µ
    check_gpu_memory()
    print()
    
    # è‡ªåŠ¨æ¸…ç†æ•°æ®é›†ç¼“å­˜
    print("ğŸ§¹ è‡ªåŠ¨æ¸…ç†æ•°æ®é›†ç¼“å­˜...")
    cleared_size = clear_datasets_cache()
    print()
    
    # æ£€æŸ¥æ¸…ç†åçš„ç¼“å­˜çŠ¶æ€
    if cleared_size > 0:
        print("ğŸ“¦ æ¸…ç†åç¼“å­˜çŠ¶æ€:")
        check_datasets_cache()
        print()
    
    # å‡†å¤‡æ•°æ®é›†è·¯å¾„
    train_dataset_path = os.path.join(DATASET_ROOT, TRAIN_FOLDER)
    test_dataset_path = os.path.join(DATASET_ROOT, TEST_FOLDER)
    train_json_file = os.path.join(DATASET_ROOT, "train_dataset.json")
    test_json_file = os.path.join(DATASET_ROOT, "test_dataset.json")
    
    try:
        # åˆ›å»ºæ•°æ®é›†JSONæ–‡ä»¶
        print("ğŸ“‚ æ­¥éª¤ 1: å‡†å¤‡æ•°æ®é›†...")
        train_count = create_dataset_json(train_dataset_path, train_json_file)
        test_count = create_dataset_json(test_dataset_path, test_json_file)
        
        if train_count == 0 or test_count == 0:
            raise ValueError("æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
        
        # åŠ è½½æ•°æ®é›†
        print("\nğŸ“¥ æ­¥éª¤ 2: åŠ è½½æ•°æ®é›†...")
        dataset = load_dataset('json', data_files={
            'train': train_json_file, 
            'test': test_json_file
        })
        dataset = dataset.cast_column('audio_file_path', Audio(sampling_rate=16000))
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ - è®­ç»ƒ: {train_count}, æµ‹è¯•: {test_count}")
        
        # åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶
        print("\nğŸ”§ æ­¥éª¤ 3: åˆå§‹åŒ–æ¨¡å‹ç»„ä»¶...")
        tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)
        feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)
        processor = WhisperProcessor.from_pretrained(MODEL_NAME)
        print("âœ… æ¨¡å‹ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
        
        # æ•°æ®é¢„å¤„ç†
        print("\nâš™ï¸ æ­¥éª¤ 4: æ•°æ®é¢„å¤„ç†...")
        print("ğŸ“ ç¼“å­˜æœºåˆ¶è¯´æ˜:")
        print("   ğŸ§¹ è‡ªåŠ¨æ¸…ç†: æ¯æ¬¡å¯åŠ¨æ—¶æ¸…ç†æ—§ç¼“å­˜ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°æ•°æ®")
        print("   ğŸ”„ é¦–æ¬¡è¿è¡Œ: å¤„ç†æ•°æ®å¹¶ç¼“å­˜åˆ° ~/.cache/huggingface/datasets/")
        print("   âš¡ åç»­è¿è¡Œ: ç›´æ¥ä»ç¼“å­˜åŠ è½½ï¼Œé€Ÿåº¦æå¿«")
        print("   ğŸ·ï¸ ç¼“å­˜é”®å€¼: åŸºäºæ•°æ®å†…å®¹å’Œå¤„ç†å‡½æ•°çš„å“ˆå¸Œå€¼")
        print("   ğŸ’¾ å­˜å‚¨æ ¼å¼: Apache Arrow åˆ—å¼å­˜å‚¨")
        print("   ğŸš€ æ€§èƒ½ä¼˜åŒ–: å¤§æ‰¹é‡é¢„å¤„ç† + å†…å­˜å›ºå®šä¼ è¾“")
        print("   ğŸ“Š é¢„æœŸæ•ˆæœ: åˆ©ç”¨2TBå†…å­˜ä¼˜åŠ¿ï¼Œæœ€å¤§åŒ–å•çº¿ç¨‹æ€§èƒ½")
        
        def prepare_batch(batch):
            return prepare_dataset(batch, feature_extractor, tokenizer)
        
        dataset = dataset.map(
            prepare_batch, 
            remove_columns=dataset.column_names["train"], 
            num_proc=1,  # å•è¿›ç¨‹é¿å…å…±äº«å†…å­˜é™åˆ¶
            batched=True,  # å¯ç”¨æ‰¹å¤„ç†
            batch_size=200,  # å¢å¤§æ‰¹å¤„ç†å¤§å°ï¼Œåˆ©ç”¨å¤§å†…å­˜ä¼˜åŠ¿
            desc="ğŸ”„ å¤„ç†éŸ³é¢‘ç‰¹å¾å’Œæ–‡æœ¬æ ‡ç­¾ (å¤§æ‰¹é‡ä¼˜åŒ–)",
            load_from_cache_file=True,  # å¯ç”¨ç¼“å­˜åŠ é€Ÿ
        )
        print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        
        # è®¾ç½®æ•°æ®æ•´ç†å™¨å’Œè¯„ä¼°å‡½æ•°
        data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
        
        def compute_metrics_wrapped(pred):
            return compute_metrics(pred, tokenizer)
        
        # åŠ è½½æ¨¡å‹
        print("\nğŸ¤– æ­¥éª¤ 5: åŠ è½½æ¨¡å‹...")
        model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)
        model.generation_config.language = LANGUAGE
        model.generation_config.task = TASK
        model.generation_config.forced_decoder_ids = None
        model.config.suppress_tokens = []
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # è®­ç»ƒå‚æ•°
        print("\nâš™ï¸ æ­¥éª¤ 6: é…ç½®è®­ç»ƒå‚æ•°...")
        training_args = Seq2SeqTrainingArguments(
            output_dir=OUTPUT_DIR,
            
            # --- æ‰¹å¤„ç†å¤§å°ä¼˜åŒ– (æ˜¾å­˜å……è¶³æ—¶å¯ä»¥å¢å¤§) ---
            per_device_train_batch_size=20,        # å¢å¤§åˆ°20 (åˆ©ç”¨å¤§å†…å­˜å’Œé«˜æ˜¾å­˜)
            per_device_eval_batch_size=12,         # å¢å¤§åˆ°12
            gradient_accumulation_steps=3,         # è°ƒæ•´ä¸º3ï¼Œæœ‰æ•ˆæ‰¹å¤§å°=20*3=60
            
            # --- å­¦ä¹ ç‡å’Œè®­ç»ƒæ­¥æ•°ä¼˜åŒ– ---
            learning_rate=5e-5,                    # ç¨å¾®æé«˜å­¦ä¹ ç‡
            warmup_steps=2000,                     # å¢åŠ é¢„çƒ­æ­¥æ•°
            max_steps=20000,                       # å¢åŠ è®­ç»ƒæ­¥æ•°ï¼Œæ›´å……åˆ†è®­ç»ƒ
            
            # --- å†…å­˜å’Œè®¡ç®—ä¼˜åŒ– ---
            gradient_checkpointing=False,          # ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹é¿å…å†²çª
            fp16=False,                            # å…³é—­fp16
            bf16=True,                             # ä½¿ç”¨bf16ï¼Œç²¾åº¦æ›´å¥½
            dataloader_pin_memory=True,            # å¯ç”¨pin_memoryæå‡GPUä¼ è¾“é€Ÿåº¦
            dataloader_num_workers=0,              # ä¿æŒå•è¿›ç¨‹é¿å…å…±äº«å†…å­˜ä¸è¶³
            # dataloader_prefetch_factor=2,        # æ³¨é‡Šæ‰ï¼Œå› ä¸ºworkers=0æ—¶ä¸éœ€è¦
            
            # --- è¯„ä¼°å’Œä¿å­˜ç­–ç•¥ä¼˜åŒ– ---
            eval_strategy="steps",
            eval_steps=2000,                       # æ¯2000æ­¥è¯„ä¼°ä¸€æ¬¡
            save_steps=2000,                       # æ¯2000æ­¥ä¿å­˜ä¸€æ¬¡
            save_total_limit=3,                    # åªä¿ç•™æœ€è¿‘3ä¸ªæ£€æŸ¥ç‚¹
            load_best_model_at_end=True,           # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹
            
            # --- ç”Ÿæˆå‚æ•°ä¼˜åŒ– ---
            predict_with_generate=True,
            generation_max_length=448,             # å¢åŠ æœ€å¤§ç”Ÿæˆé•¿åº¦
            
            # --- ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ---
            optim="adamw_torch",                   # ä½¿ç”¨AdamWä¼˜åŒ–å™¨
            weight_decay=0.01,                     # æ·»åŠ æƒé‡è¡°å‡
            lr_scheduler_type="cosine",            # ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
            
            # --- æ—¥å¿—å’Œç›‘æ§ ---
            logging_steps=100,                     # å¢åŠ æ—¥å¿—é¢‘ç‡
            report_to=None,                        # å¯æ”¹ä¸º["tensorboard"]å¯ç”¨tensorboard
            
            # --- å…¶ä»–ä¼˜åŒ–è®¾ç½® ---
            metric_for_best_model="wer",
            greater_is_better=False,
            push_to_hub=False,
            remove_unused_columns=False,           # ä¿ç•™æ‰€æœ‰åˆ—
            
            # --- é«˜æ˜¾å­˜ä¸“ç”¨è®¾ç½® ---
            max_grad_norm=1.0,                     # æ¢¯åº¦è£å‰ª
            warmup_ratio=0.1,                      # é¢„çƒ­æ¯”ä¾‹
        )
        
        processor.save_pretrained(training_args.output_dir)
        print("âœ… è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ")
        
        # æ˜¾ç¤ºè®­ç»ƒé…ç½®ä¿¡æ¯
        print(f"\nğŸ“Š è®­ç»ƒé…ç½®ä¿¡æ¯:")
        print(f"   ğŸ¯ æ¨¡å‹: {MODEL_NAME}")
        print(f"   ğŸ’¾ æ‰¹å¤§å°: {training_args.per_device_train_batch_size} (è®­ç»ƒ) / {training_args.per_device_eval_batch_size} (è¯„ä¼°)")
        print(f"   ğŸ“ˆ æœ‰æ•ˆæ‰¹å¤§å°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
        print(f"   ğŸ”¥ å­¦ä¹ ç‡: {training_args.learning_rate}")
        print(f"   ğŸƒ è®­ç»ƒæ­¥æ•°: {training_args.max_steps}")
        print(f"   ğŸ² é¢„çƒ­æ­¥æ•°: {training_args.warmup_steps}")
        print(f"   ğŸ’¾ ç²¾åº¦æ¨¡å¼: {'BF16' if training_args.bf16 else 'FP16' if training_args.fp16 else 'FP32'}")
        print(f"   ğŸ’¿ è¾“å‡ºç›®å½•: {training_args.output_dir}")
        
        # ä¼°ç®—è®­ç»ƒæ—¶é—´ (åŸºäºå®é™…æ¯æ­¥æ—¶é—´)
        estimated_time_hours = (training_args.max_steps * 14.21) / 3600  # 14.21ç§’/æ­¥
        print(f"   â° é¢„ä¼°è®­ç»ƒæ—¶é—´: {estimated_time_hours:.1f} å°æ—¶ (åŸºäº14.21ç§’/æ­¥)")
        print()
        
        # åˆ›å»ºè®­ç»ƒå™¨
        print("\nğŸ‹ï¸ æ­¥éª¤ 7: å¼€å§‹è®­ç»ƒ...")
        trainer = Seq2SeqTrainer(
            args=training_args,
            model=model,
            train_dataset=dataset["train"],
            eval_dataset=dataset["test"],
            data_collator=data_collator,
            compute_metrics=compute_metrics_wrapped,
            processing_class=processor,  # ä½¿ç”¨æ–°çš„å‚æ•°å
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print("\nğŸ’¾ æ­¥éª¤ 8: ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        final_model_dir = f"{OUTPUT_DIR}/final"
        trainer.save_model(final_model_dir)
        processor.save_pretrained(final_model_dir)
        print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_model_dir}")
        
        # è¿è¡Œæœ€ç»ˆè¯„ä¼°
        print("\nğŸ“Š æ­¥éª¤ 9: æœ€ç»ˆè¯„ä¼°...")
        eval_results = trainer.evaluate()
        print("ğŸ“‹ æœ€ç»ˆè¯„ä¼°ç»“æœ:")
        for key, value in eval_results.items():
            print(f"   {key}: {value:.4f}")
        
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ¯ æœ€ä½³æ¨¡å‹WER: {eval_results.get('eval_wer', 'N/A'):.4f}")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {final_model_dir}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise
    
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        print("\nğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for file_path in [train_json_file, test_json_file]:
            if os.path.exists(file_path):
                os.remove(file_path)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {file_path}")

if __name__ == "__main__":
    main()

