import os
import tempfile
import traceback
from argparse import ArgumentParser
from pathlib import Path
from typing import Optional, List, Tuple

import gradio as gr

from stepaudio2 import StepAudio2
from token2wav import Token2wav


# ----------------------- Shared Utilities -----------------------

def save_tmp_audio_bytes(audio_bytes: bytes, cache_dir: str) -> str:
    os.makedirs(cache_dir, exist_ok=True)
    with tempfile.NamedTemporaryFile(dir=cache_dir, suffix=".wav", delete=False) as f:
        f.write(audio_bytes)
        return f.name


# ----------------------- Transcription Logic -----------------------

class Transcriber:
    def __init__(self, model: StepAudio2):
        self.model = model

    def transcribe(
        self,
        audio_path: Optional[str],
        system_prompt: str,
        max_new_tokens: int,
        temperature: float,
        repetition_penalty: float,
    ) -> str:
        if not audio_path:
            return "è¯·å…ˆä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ã€‚"
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "human", "content": [{"type": "audio", "audio": audio_path}]},
            {"role": "assistant", "content": None},
        ]
        try:
            _, text, _ = self.model(
                messages,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                repetition_penalty=repetition_penalty,
                do_sample=True,
            )
            return text.strip()
        except Exception as e:
            return f"è½¬å†™å‡ºé”™: {e}"


# ----------------------- Chat (Q&A / Voice) Logic -----------------------

def add_message(chatbot, history, mic_path, text_input):
    if not mic_path and not text_input:
        return chatbot, history, "è¾“å…¥ä¸ºç©º"
    if text_input:
        chatbot.append({"role": "user", "content": text_input})
        history.append({"role": "human", "content": text_input})
    elif mic_path and Path(mic_path).exists():
        chatbot.append({"role": "user", "content": {"path": mic_path}})
        history.append({"role": "human", "content": [{"type": "audio", "audio": mic_path}]})
    return chatbot, history, None


def predict_response(chatbot, history, audio_model: StepAudio2, token2wav: Token2wav, prompt_wav: str, cache_dir: str):
    try:
        history.append({"role": "assistant", "content": [{"type": "text", "text": "<tts_start>"}], "eot": False})
        tokens, text, audio_tokens = audio_model(history, max_new_tokens=4096, temperature=0.7, repetition_penalty=1.05, do_sample=True)
        audio_bytes = token2wav(audio_tokens, prompt_wav)
        audio_path = save_tmp_audio_bytes(audio_bytes, cache_dir)
        chatbot.append({"role": "assistant", "content": {"path": audio_path}})
        history[-1]["content"].append({"type": "token", "token": tokens})
        history[-1]["eot"] = True
    except Exception:
        print(traceback.format_exc())
        gr.Warning("å‡ºç°é”™è¯¯ï¼Œè¯·é‡è¯•ã€‚")
    return chatbot, history


def reset_chat(system_prompt: str):
    return [], [{"role": "system", "content": system_prompt}]


# ----------------------- UI Construction -----------------------

def build_demo(model: StepAudio2, token2wav: Token2wav, args) -> gr.Blocks:
    transcriber = Transcriber(model)

    with gr.Blocks(title="Step Audio 2 å¤šåŠŸèƒ½ Demo", delete_cache=(86400, 86400)) as demo:
        gr.Markdown("""# Step Audio 2 å¤šåŠŸèƒ½ Demo
æä¾›ä¸‰ä¸ªåŠŸèƒ½ Tabï¼š
1. **è¯­éŸ³è½¬å†™** â€”â€” å°†è¯­éŸ³ç²¾ç¡®è½¬æˆçº¯æ–‡æœ¬ã€‚
2. **è¯­éŸ³é—®ç­” (Chat)** â€”â€” è¯­éŸ³/æ–‡æœ¬å¤šè½®å¯¹è¯ï¼Œå¯è¿”å›è¯­éŸ³å›å¤ã€‚
3. **è¯­éŸ³ç¿»è¯‘** â€”â€” è·¨ä¸­è‹±æ–‡çš„è¯­éŸ³åˆ°æ–‡æœ¬ / è¯­éŸ³ç¿»è¯‘ã€‚

æç¤ºï¼šè‹¥åªéœ€å¹²å‡€è½¬å†™ï¼Œè¯·å‹¾é€‰â€œä¸¥æ ¼ä»…è¾“å‡ºåŸå§‹æ–‡å­—â€ï¼›å½“å‰ç‰ˆæœ¬æœªæä¾›é€è¯æ—¶é—´æˆ³ï¼Œå¯åç»­æ¥å…¥å¯¹é½å·¥å…·ç”Ÿæˆå­—å¹•ã€‚""")
        with gr.Tabs():
            # --------- Transcription Tab ---------
            with gr.TabItem("è¯­éŸ³è½¬å†™"):
                with gr.Row():
                    audio_input = gr.Audio(
                        label="ä¸Šä¼ éŸ³é¢‘ (16k wav ä¼˜å…ˆ)",
                        type="filepath",
                        sources=["upload", "microphone"],
                    )
                    transcript_box = gr.Textbox(label="è½¬å†™ç»“æœ", lines=12)
                with gr.Accordion("é«˜çº§å‚æ•°", open=False):
                    system_prompt_tr = gr.Textbox(
                        label="System Prompt",
                        value=(
                            "You are an automatic speech recognition (ASR) system. "
                            "Transcribe the user's speech verbatim. Do NOT add speaker labels, time info, summaries, analysis or any extra words. Output ONLY the raw transcript text."
                        ),
                        lines=4,
                    )
                    strict_mode = gr.Checkbox(value=True, label="ä¸¥æ ¼ä»…è¾“å‡ºåŸå§‹æ–‡å­—(è¦†ç›–ä¸Šé¢æç¤º)")
                    max_new_tokens_tr = gr.Slider(32, 1024, value=256, step=32, label="max_new_tokens")
                    temperature_tr = gr.Slider(0.1, 1.5, value=0.7, step=0.05, label="temperature")
                    repetition_penalty_tr = gr.Slider(1.0, 2.0, value=1.05, step=0.01, label="repetition_penalty")
                with gr.Row():
                    transcribe_btn = gr.Button("ğŸ” è½¬å†™")
                    clear_tr_btn = gr.Button("ğŸ§¹ æ¸…ç©º")

                def _clean_output(text: str) -> str:
                    # è½»é‡æ¸…æ´—ï¼šå»é™¤å¸¸è§çš„æè¿°æ€§å‰ç¼€è¡Œï¼ˆè‹¥æ¨¡å‹ä»ç”Ÿæˆï¼‰
                    lines = [l.strip() for l in text.splitlines() if l.strip()]
                    drop_prefixes = ("è¿™æ˜¯ç¬¬ä¸€", "è¿™æ˜¯ç¬¬ä¸€ä¸ª", "è¿™æ˜¯ä¸€ä½", "è¯´è¯çš„å†…å®¹æ˜¯", "è¯´è¯å†…å®¹æ˜¯")
                    if len(lines) > 1:
                        filtered = [l for l in lines if not any(l.startswith(p) for p in drop_prefixes)]
                        if filtered:
                            lines = filtered
                    cleaned = " ".join(lines)
                    # å»æ‰å¯èƒ½çš„å¼•å·åŒ…è£¹
                    if (cleaned.startswith('"') and cleaned.endswith('"')) or (cleaned.startswith('â€œ') and cleaned.endswith('â€')):
                        cleaned = cleaned[1:-1].strip()
                    return cleaned

                def do_transcribe(audio_path, sp, strict, mnt, temp, rp):
                    if strict:
                        sp = (
                            "You are a precise speech-to-text engine. Transcribe exactly what is spoken. "
                            "No speaker labels, no timestamps, no commentary, no summarization. Output ONLY the transcribed text."
                        )
                        temp = min(float(temp), 0.5)  # é™ä½æ¸©åº¦æ›´ç¨³å®š
                    raw = transcriber.transcribe(audio_path, sp, int(mnt), float(temp), float(rp))
                    if strict:
                        raw = _clean_output(raw)
                    return raw

                transcribe_btn.click(
                    do_transcribe,
                    inputs=[audio_input, system_prompt_tr, strict_mode, max_new_tokens_tr, temperature_tr, repetition_penalty_tr],
                    outputs=[transcript_box],
                    concurrency_id="gpu_queue",
                )
                clear_tr_btn.click(lambda: (None, ""), outputs=[audio_input, transcript_box])
                # è½¬å†™ Tab ä¸å†æ”¾ç½®ç¤ºä¾‹éŸ³é¢‘ï¼ˆç¤ºä¾‹ç§»åŠ¨åˆ° Chat Tabï¼‰

            # --------- Chat Tab ---------
            with gr.TabItem("è¯­éŸ³é—®ç­” / Chat"):
                system_prompt_chat = gr.Textbox(
                    label="System Prompt",
                    value=(
                        "ä½ çš„åå­—å«åšå°è·ƒï¼Œæ˜¯ç”±é˜¶è·ƒæ˜Ÿè¾°å…¬å¸è®­ç»ƒå‡ºæ¥çš„è¯­éŸ³å¤§æ¨¡å‹ã€‚\n"
                        "ä½ æƒ…æ„Ÿç»†è…»ï¼Œå–„è§£äººæ„ï¼Œå¯Œæœ‰åŒç†å¿ƒã€‚è¯·ç”¨é»˜è®¤å¥³å£°ä¸ç”¨æˆ·äº¤æµã€‚"
                    ),
                    lines=3,
                )
                chatbot = gr.Chatbot(elem_id="chatbot", min_height=700, type="messages")
                # åˆå§‹åŒ–ä¸ºç©ºï¼Œä¼šåœ¨é¦–æ¬¡å‘é€æ—¶æ’å…¥ system promptï¼Œé¿å…æŠŠå‡½æ•°å¯¹è±¡å†™å…¥å†å²
                history_state = gr.State([])
                mic = gr.Audio(type="filepath", label="è¯­éŸ³è¾“å…¥ (å¯é€‰)")
                text_input = gr.Textbox(placeholder="è¾“å…¥æ–‡å­—ç„¶åæäº¤ ...")
                with gr.Row():
                    submit_btn = gr.Button("ğŸš€ å‘é€")
                    regen_btn = gr.Button("ğŸ¤” é‡è¯•")
                    clear_chat_btn = gr.Button("ğŸ§¹ æ¸…ç©ºä¼šè¯")
                gr.Examples(
                    examples=[["assets/give_me_a_brief_introduction_to_the_great_wall.wav"]],
                    inputs=[mic],
                    label="ç¤ºä¾‹éŸ³é¢‘",
                )

                def _build_system_prompt(base_sp: str) -> str:
                    return base_sp

                def on_submit(chatbot_data, history_data, mic_path, text_val, system_prompt_value):
                    system_prompt_cur = _build_system_prompt(system_prompt_value)
                    # ç¡®ä¿ system åœ¨ç´¢å¼•0 å¹¶ä¸å½“å‰é€‰æ‹©åŒæ­¥
                    if not history_data or history_data[0]["role"] != "system":
                        history_data.insert(0, {"role": "system", "content": system_prompt_cur})
                    else:
                        history_data[0]["content"] = system_prompt_cur
                    chatbot_data, history_data, err = add_message(chatbot_data, history_data, mic_path, text_val)
                    if err:
                        gr.Warning(err)
                        return chatbot_data, history_data, None, None
                    chatbot_data, history_data = predict_response(chatbot_data, history_data, model, token2wav, args.prompt_wav, args.cache_dir)
                    return chatbot_data, history_data, None, None

                submit_btn.click(
                    on_submit,
                    inputs=[chatbot, history_state, mic, text_input, system_prompt_chat],
                    outputs=[chatbot, history_state, mic, text_input],
                    concurrency_id="gpu_queue",
                )

                def regenerate(chatbot_data, history_data):
                    while chatbot_data and chatbot_data[-1]["role"] == "assistant":
                        chatbot_data.pop()
                    while history_data and history_data[-1]["role"] == "assistant":
                        history_data.pop()
                    return predict_response(chatbot_data, history_data, model, token2wav, args.prompt_wav, args.cache_dir)

                regen_btn.click(
                    regenerate,
                    inputs=[chatbot, history_state],
                    outputs=[chatbot, history_state],
                    concurrency_id="gpu_queue",
                )

                def _clear_chat(sp):
                    return reset_chat(_build_system_prompt(sp))
                clear_chat_btn.click(_clear_chat, inputs=[system_prompt_chat], outputs=[chatbot, history_state])

            # --------- Speech Translation Tab ---------
            with gr.TabItem("è¯­éŸ³ç¿»è¯‘"):
                gr.Markdown("""### è¯­éŸ³ç¿»è¯‘ (Speech Translation)
ä¸Šä¼ æˆ–å½•åˆ¶æºè¯­éŸ³ï¼Œé€‰æ‹©ç›®æ ‡è¯­è¨€ï¼Œå¯é€‰ç”Ÿæˆè¯­éŸ³å½¢å¼çš„ç¿»è¯‘ç»“æœã€‚
- ä»…æ–‡æœ¬ï¼šä¸€æ¬¡ç”Ÿæˆç¿»è¯‘æ–‡æœ¬ã€‚
- æ–‡æœ¬+è¯­éŸ³ï¼šä¸€æ¬¡å¯¹è¯è¯·æ±‚ï¼Œä½¿ç”¨ `<tts_start>` è§¦å‘è¯­éŸ³ç¿»è¯‘è¾“å‡ºã€‚
""")
                with gr.Row():
                    audio_tr = gr.Audio(label="æºéŸ³é¢‘", type="filepath", sources=["upload", "microphone"])
                    with gr.Column():
                        translated_text = gr.Textbox(label="ç¿»è¯‘ç»“æœ", lines=10)
                        translated_audio = gr.Audio(label="ç¿»è¯‘è¯­éŸ³", interactive=False)
                with gr.Row():
                    source_lang = gr.Dropdown(["Auto", "English", "Chinese"], value="Auto", label="æºè¯­è¨€")
                    target_lang = gr.Dropdown(["Chinese", "English"], value="Chinese", label="ç›®æ ‡è¯­è¨€")
                    voice_output = gr.Checkbox(value=False, label="ç”Ÿæˆè¯­éŸ³è¾“å‡º")
                with gr.Accordion("é«˜çº§å‚æ•°", open=False):
                    system_prompt_trs = gr.Textbox(label="System Prompt", value="You are a helpful and professional speech translation assistant.", lines=2)
                    custom_prompt = gr.Textbox(label="è‡ªå®šä¹‰ç¿»è¯‘æŒ‡ä»¤ (å¯ç•™ç©º)", placeholder="ç•™ç©ºåˆ™è‡ªåŠ¨ç”Ÿæˆï¼Œå¦‚: è¯·æŠŠä¸‹é¢çš„è‹±è¯­è¯­éŸ³ç¿»è¯‘æˆåœ°é“çš„ä¸­æ–‡ï¼Œåªè¾“å‡ºç¿»è¯‘ã€‚")
                    max_new_tokens_trs = gr.Slider(32, 1024, value=256, step=32, label="max_new_tokens")
                    temperature_trs = gr.Slider(0.1, 1.5, value=0.7, step=0.05, label="temperature")
                    repetition_penalty_trs = gr.Slider(1.0, 2.0, value=1.05, step=0.01, label="repetition_penalty")
                with gr.Row():
                    translate_btn = gr.Button("ğŸŒ ç¿»è¯‘")
                    clear_trs_btn = gr.Button("ğŸ§¹ æ¸…ç©º")

                def build_instruction(src: str, tgt: str, voice: bool) -> str:
                    tgt_cn = "ä¸­æ–‡" if tgt == "Chinese" else "English"
                    if src == "Auto":
                        base = f"Please translate the following speech into {tgt}. Output only the {tgt} translation." if tgt != "Chinese" else "è¯·æŠŠä¸‹é¢çš„è¯­éŸ³å†…å®¹ç¿»è¯‘æˆè‡ªç„¶æµç•…çš„ä¸­æ–‡ï¼Œåªè¾“å‡ºç¿»è¯‘ã€‚"
                    else:
                        if src == "English" and tgt == "Chinese":
                            base = "è¯·æŠŠä¸‹é¢çš„è‹±è¯­è¯­éŸ³ç¿»è¯‘æˆè‡ªç„¶ã€å‡†ç¡®ä¸”ç®€æ´çš„ä¸­æ–‡ï¼Œåªè¾“å‡ºç¿»è¯‘ã€‚"
                        elif src == "Chinese" and tgt == "English":
                            base = "Please translate the following Chinese speech into concise, natural English. Output only the translation." 
                        else:
                            base = f"Please translate the following {src} speech into {tgt}. Output only the {tgt} translation."
                    if voice:
                        base += " Provide the translated content as speech (TTS) as well." if tgt != "Chinese" else " å¹¶ä½¿ç”¨è¯­éŸ³å½¢å¼æœ—è¯»è¯‘æ–‡ã€‚"
                    return base

                def speech_translate(audio_path, src_lang, tgt_lang, voice_out, system_prompt, custom_inst, max_new, temp, rep):
                    if not audio_path:
                        return "è¯·å…ˆä¸Šä¼ éŸ³é¢‘ã€‚", None
                    instruction = custom_inst.strip() if custom_inst and custom_inst.strip() else build_instruction(src_lang, tgt_lang, voice_out)
                    human_content = [
                        {"type": "text", "text": instruction},
                        {"type": "audio", "audio": audio_path},
                    ]
                    assistant_content = "<tts_start>" if voice_out else None
                    assistant_kwargs = {"eot": False} if voice_out else {}
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "human", "content": human_content},
                        {"role": "assistant", "content": assistant_content, **assistant_kwargs},
                    ]
                    try:
                        tokens, text, audio_tokens = model(
                            messages,
                            max_new_tokens=int(max_new),
                            temperature=float(temp),
                            repetition_penalty=float(rep),
                            do_sample=True,
                        )
                        audio_path_out = None
                        if voice_out and audio_tokens:
                            try:
                                audio_bytes = token2wav(audio_tokens, args.prompt_wav)
                                audio_path_out = save_tmp_audio_bytes(audio_bytes, args.cache_dir)
                            except Exception as e:
                                gr.Warning(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
                        return text.strip(), audio_path_out
                    except Exception as e:
                        return f"ç¿»è¯‘å‡ºé”™: {e}", None

                translate_btn.click(
                    speech_translate,
                    inputs=[audio_tr, source_lang, target_lang, voice_output, system_prompt_trs, custom_prompt, max_new_tokens_trs, temperature_trs, repetition_penalty_trs],
                    outputs=[translated_text, translated_audio],
                    concurrency_id="gpu_queue",
                )
                clear_trs_btn.click(lambda: (None, "", None), outputs=[audio_tr, translated_text, translated_audio])
                gr.Examples(
                    examples=[["assets/give_me_a_brief_introduction_to_the_great_wall.wav"]],
                    inputs=[audio_tr],
                    label="ç¤ºä¾‹éŸ³é¢‘",
                )

        gr.Markdown(
            """**æç¤º**: å½“å‰æ¨¡å‹æœªè¾“å‡ºé€è¯æ—¶é—´æˆ³ï¼Œè‹¥éœ€è¦å­—å¹•å¯å•ç‹¬åšåˆ†æ®µ+è½¬å†™æˆ–é›†æˆå¤–éƒ¨å¯¹é½å·¥å…· (WhisperX/MFA)ã€‚"""
        )

    return demo


def parse_args():
    parser = ArgumentParser()
    parser.add_argument("--model-path", type=str, default="Step-Audio-2-mini", help="æ¨¡å‹ç›®å½•")
    parser.add_argument("--server-port", type=int, default=7860, help="Server ç«¯å£")
    parser.add_argument("--server-name", type=str, default="0.0.0.0", help="Server ç»‘å®šåœ°å€")
    parser.add_argument("--prompt-wav", type=str, default="assets/default_female.wav", help="èŠå¤©è¯­éŸ³å›å¤çš„æç¤ºéŸ³è‰² wav")
    parser.add_argument("--cache-dir", type=str, default="/tmp/stepaudio2", help="ç¼“å­˜ç›®å½•")
    return parser.parse_args()


def main():
    args = parse_args()
    os.environ["GRADIO_TEMP_DIR"] = args.cache_dir
    model = StepAudio2(args.model_path)
    token2wav = Token2wav(f"{args.model_path}/token2wav")
    demo = build_demo(model, token2wav, args)
    demo.queue().launch(server_port=args.server_port, server_name=args.server_name)


if __name__ == "__main__":
    main()
