"""
VideoMaMa Gradio Demo
äº¤äº’å¼è§†é¢‘æŠ å›¾ä¸ SAM2 æ©ç è·Ÿè¸ª
"""

import sys
sys.path.append("./")
sys.path.append("./demo")

import os
import json
import time
import cv2
import torch
import numpy as np
import gradio as gr
from PIL import Image
from pathlib import Path
import tempfile
import shutil

from demo.tools.painter import mask_painter, point_painter
from pipeline_svd_mask import VideoInferencePipeline
from sam2.build_sam import build_sam2_video_predictor


class SAM2VideoTracker:
    """SAM2 è§†é¢‘è·Ÿè¸ªå™¨åŒ…è£…ç±»"""
    
    def __init__(self, checkpoint_path, config_file, device="cuda"):
        """
        åˆå§‹åŒ– SAM2 è§†é¢‘è·Ÿè¸ªå™¨
        
        Args:
            checkpoint_path: SAM2 æ£€æŸ¥ç‚¹è·¯å¾„
            config_file: SAM2 é…ç½®æ–‡ä»¶è·¯å¾„
            device: è¿è¡Œè®¾å¤‡
        """
        self.device = device
        self.predictor = build_sam2_video_predictor(
            config_file=config_file,
            ckpt_path=checkpoint_path,
            device=device
        )
        print(f"SAM2 è§†é¢‘è·Ÿè¸ªå™¨å·²åœ¨ {device} ä¸Šåˆå§‹åŒ–")
    
    def track_video(self, frames, points, labels):
        """
        ä½¿ç”¨ SAM2 è·Ÿè¸ªè§†é¢‘ä¸­çš„å¯¹è±¡
        
        Args:
            frames: numpy æ•°ç»„åˆ—è¡¨, [(H,W,3)]*n, uint8 RGB å¸§
            points: æç¤ºç‚¹çš„ [x, y] åæ ‡åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨ (1 ä¸ºæ­£å‘, 0 ä¸ºè´Ÿå‘)
            
        Returns:
            masks: numpy æ•°ç»„åˆ—è¡¨, [(H,W)]*n, uint8 äºŒå€¼æ©ç 
        """
        # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾å¸§
        temp_dir = Path(tempfile.mkdtemp())
        frames_dir = temp_dir / "frames"
        frames_dir.mkdir(exist_ok=True)
        
        try:
            # ä¿å­˜å¸§åˆ°ä¸´æ—¶ç›®å½•
            print(f"æ­£åœ¨ä¿å­˜ {len(frames)} å¸§åˆ°ä¸´æ—¶ç›®å½•...")
            for i, frame in enumerate(frames):
                frame_path = frames_dir / f"{i:05d}.jpg"
                Image.fromarray(frame).save(frame_path, quality=95)
            
            # åˆå§‹åŒ– SAM2 è§†é¢‘é¢„æµ‹å™¨
            print("æ­£åœ¨åˆå§‹åŒ– SAM2 æ¨ç†çŠ¶æ€...")
            inference_state = self.predictor.init_state(video_path=str(frames_dir))
            
            # åœ¨ç¬¬ä¸€å¸§ä¸Šæ·»åŠ æç¤º
            points_array = np.array(points, dtype=np.float32)
            labels_array = np.array(labels, dtype=np.int32)
            
            print(f"åœ¨ç¬¬ä¸€å¸§ä¸Šæ·»åŠ  {len(points)} ä¸ªç‚¹æç¤º...")
            _, out_obj_ids, out_mask_logits = self.predictor.add_new_points(
                inference_state=inference_state,
                frame_idx=0,
                obj_id=1,
                points=points_array,
                labels=labels_array,
            )
            
            # åœ¨è§†é¢‘ä¸­ä¼ æ’­
            print("æ­£åœ¨è§†é¢‘ä¸­ä¼ æ’­æ©ç ...")
            masks = []
            for frame_idx, object_ids, mask_logits in self.predictor.propagate_in_video(inference_state):
                obj_ids_list = object_ids.tolist() if hasattr(object_ids, 'tolist') else object_ids
                
                if 1 in obj_ids_list:
                    mask_idx = obj_ids_list.index(1)
                    mask = (mask_logits[mask_idx] > 0.0).cpu().numpy()
                    mask_uint8 = (mask.squeeze() * 255).astype(np.uint8)
                    masks.append(mask_uint8)
                else:
                    h, w = frames[0].shape[:2]
                    masks.append(np.zeros((h, w), dtype=np.uint8))
            
            print(f"å·²ç”Ÿæˆ {len(masks)} ä¸ªæ©ç ")
            return masks
            
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    def get_first_frame_mask(self, frame, points, labels):
        """
        ä»…è·å–ç¬¬ä¸€å¸§çš„æ©ç ï¼ˆç”¨äºé¢„è§ˆï¼‰
        
        Args:
            frame: np.ndarray, (H, W, 3), uint8 RGB å¸§
            points: [x, y] åæ ‡åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨ (1 ä¸ºæ­£å‘, 0 ä¸ºè´Ÿå‘)
            
        Returns:
            mask: np.ndarray, (H, W), uint8 äºŒå€¼æ©ç 
        """
        temp_dir = Path(tempfile.mkdtemp())
        frames_dir = temp_dir / "frames"
        frames_dir.mkdir(exist_ok=True)
        
        try:
            frame_path = frames_dir / "00000.jpg"
            Image.fromarray(frame).save(frame_path, quality=95)
            
            inference_state = self.predictor.init_state(video_path=str(frames_dir))
            
            points_array = np.array(points, dtype=np.float32)
            labels_array = np.array(labels, dtype=np.int32)
            
            _, out_obj_ids, out_mask_logits = self.predictor.add_new_points(
                inference_state=inference_state,
                frame_idx=0,
                obj_id=1,
                points=points_array,
                labels=labels_array,
            )
            
            if len(out_mask_logits) > 0:
                mask = (out_mask_logits[0] > 0.0).cpu().numpy()
                mask_uint8 = (mask.squeeze() * 255).astype(np.uint8)
                return mask_uint8
            else:
                return np.zeros(frame.shape[:2], dtype=np.uint8)
                
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)


def load_sam2_tracker(device="cuda"):
    """
    åŠ è½½ SAM2 è§†é¢‘è·Ÿè¸ªå™¨
    
    Args:
        device: è¿è¡Œè®¾å¤‡
        
    Returns:
        SAM2VideoTracker å®ä¾‹
    """
    checkpoint_path = "checkpoints/sam2/sam2.1_hiera_large.pt"
    # ä½¿ç”¨ SAM2 åŒ…å†…çš„é…ç½®è·¯å¾„æ ¼å¼
    config_file = "configs/sam2.1/sam2.1_hiera_l.yaml"
    
    print(f"æ­£åœ¨ä» {checkpoint_path} åŠ è½½ SAM2...")
    tracker = SAM2VideoTracker(checkpoint_path, config_file, device)
    
    return tracker


def load_videomama_pipeline(device="cuda"):
    """
    åŠ è½½ VideoMaMa ç®¡é“å¹¶ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
    
    Args:
        device: è¿è¡Œè®¾å¤‡
        
    Returns:
        VideoInferencePipeline å®ä¾‹
    """
    # ä½¿ç”¨æœ¬åœ° checkpoints ç›®å½•
    base_model_path = "checkpoints/stable-video-diffusion-img2vid-xt"
    unet_checkpoint_path = "checkpoints/VideoMaMa"
    
    print(f"æ­£åœ¨ä» {unet_checkpoint_path} åŠ è½½ VideoMaMa ç®¡é“...")
    
    pipeline = VideoInferencePipeline(
        base_model_path=base_model_path,
        unet_checkpoint_path=unet_checkpoint_path,
        weight_dtype=torch.float16,
        device=device
    )
    
    print("VideoMaMa ç®¡é“åŠ è½½æˆåŠŸï¼")
    
    return pipeline


def videomama(pipeline, frames_np, mask_frames_np):
    """
    ä½¿ç”¨æ©ç æ¡ä»¶è¿è¡Œ VideoMaMa æ¨ç†
    
    Args:
        pipeline: VideoInferencePipeline å®ä¾‹
        frames_np: numpy æ•°ç»„åˆ—è¡¨, [(H,W,3)]*n, uint8 RGB å¸§
        mask_frames_np: numpy æ•°ç»„åˆ—è¡¨, [(H,W)]*n, uint8 ç°åº¦æ©ç 
        
    Returns:
        output_frames: numpy æ•°ç»„åˆ—è¡¨, [(H,W,3)]*n, uint8 RGB è¾“å‡º
    """
    # å°† numpy æ•°ç»„è½¬æ¢ä¸º PIL å›¾åƒ
    frames_pil = [Image.fromarray(f) for f in frames_np]
    mask_frames_pil = [Image.fromarray(m, mode='L') for m in mask_frames_np]
    
    # è°ƒæ•´åˆ°æ¨¡å‹è¾“å…¥å¤§å°
    target_width, target_height = 1024, 576
    frames_resized = [f.resize((target_width, target_height), Image.Resampling.BILINEAR) 
                     for f in frames_pil]
    masks_resized = [m.resize((target_width, target_height), Image.Resampling.BILINEAR) 
                    for m in mask_frames_pil]
    
    # è¿è¡Œæ¨ç†
    print(f"åœ¨ {len(frames_resized)} å¸§ä¸Šè¿è¡Œ VideoMaMa æ¨ç†...")
    output_frames_pil = pipeline.run(
        cond_frames=frames_resized,
        mask_frames=masks_resized,
        seed=42,
        mask_cond_mode="vae"
    )
    
    # è°ƒæ•´å›åŸå§‹åˆ†è¾¨ç‡
    original_size = frames_pil[0].size
    output_frames_resized = [f.resize(original_size, Image.Resampling.BILINEAR) 
                            for f in output_frames_pil]
    
    # è½¬æ¢å› numpy æ•°ç»„
    output_frames_np = [np.array(f) for f in output_frames_resized]
    
    return output_frames_np

import warnings
warnings.filterwarnings("ignore")

# å…¨å±€æ¨¡å‹
sam2_tracker = None
videomama_pipeline = None

# å¸¸é‡
MASK_COLOR = 3
MASK_ALPHA = 0.7
CONTOUR_COLOR = 1
CONTOUR_WIDTH = 5
POINT_COLOR_POS = 8   # æ­£å‘ç‚¹ - æ©™è‰²
POINT_COLOR_NEG = 1   # è´Ÿå‘ç‚¹ - çº¢è‰²
POINT_ALPHA = 0.9
POINT_RADIUS = 15

def initialize_models():
    """åˆå§‹åŒ– SAM2 å’Œ VideoMaMa æ¨¡å‹"""
    global sam2_tracker, videomama_pipeline
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½ SAM2
    sam2_tracker = load_sam2_tracker(device=device)
    
    # åŠ è½½ VideoMaMa
    videomama_pipeline = load_videomama_pipeline(device=device)
    
    print("æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")


def extract_frames_from_video(video_path, max_frames=24):
    """
    ä»è§†é¢‘æ–‡ä»¶ä¸­æå–å¸§
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        max_frames: æœ€å¤§æå–å¸§æ•°ï¼ˆé»˜è®¤ï¼š24ï¼‰
        
    Returns:
        frames: numpy æ•°ç»„åˆ—è¡¨ (H,W,3), uint8 RGB
        adjusted_fps: è°ƒæ•´åçš„ FPSï¼Œç”¨äºè¾“å‡ºè§†é¢‘ä¿æŒæ­£å¸¸æ’­æ”¾é€Ÿåº¦
    """
    cap = cv2.VideoCapture(video_path)
    original_fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # é¦–å…ˆè¯»å–æ‰€æœ‰å¸§
    all_frames = []
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        # å°† BGR è½¬æ¢ä¸º RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        all_frames.append(frame_rgb)
    
    cap.release()
    
    # å¦‚æœè§†é¢‘å¸§æ•°è¶…è¿‡ max_framesï¼Œåˆ™éšæœºé‡‡æ ·
    if len(all_frames) > max_frames:
        print(f"è§†é¢‘æœ‰ {len(all_frames)} å¸§ï¼Œéšæœºé‡‡æ · {max_frames} å¸§...")
        # æ’åºç´¢å¼•ä»¥ä¿æŒæ—¶é—´é¡ºåº
        sampled_indices = sorted(np.random.choice(len(all_frames), max_frames, replace=False))
        frames = [all_frames[i] for i in sampled_indices]
        print(f"é‡‡æ ·å¸§ç´¢å¼•: {sampled_indices}")
        
        # è°ƒæ•´ FPS ä»¥ä¿æŒæ­£å¸¸æ’­æ”¾é€Ÿåº¦
        # å¦‚æœä» M æ€»å¸§ä¸­é‡‡æ ·äº† N å¸§ï¼Œåˆ™æŒ‰æ¯”ä¾‹è°ƒæ•´ FPS
        adjusted_fps = original_fps * (len(frames) / len(all_frames))
    else:
        frames = all_frames
        adjusted_fps = original_fps
        print(f"è§†é¢‘æœ‰ {len(frames)} å¸§ (â‰¤ {max_frames})ï¼Œä½¿ç”¨å…¨éƒ¨å¸§")
    
    print(f"ä½¿ç”¨è§†é¢‘çš„ {len(frames)} å¸§ (åŸå§‹ FPS: {original_fps:.2f}ï¼Œè°ƒæ•´å FPS: {adjusted_fps:.2f})")
    
    return frames, adjusted_fps


def get_prompt(click_state, click_input):
    """
    å°†ç‚¹å‡»è¾“å…¥è½¬æ¢ä¸ºæç¤ºæ ¼å¼
    
    Args:
        click_state: [[points], [labels]]
        click_input: JSON å­—ç¬¦ä¸² "[[x, y, label]]"
        
    Returns:
        æ›´æ–°åçš„ click_state
    """
    inputs = json.loads(click_input)
    points = click_state[0]
    labels = click_state[1]
    
    for input_item in inputs:
        points.append(input_item[:2])
        labels.append(input_item[2])
    
    click_state[0] = points
    click_state[1] = labels
    
    return click_state


def get_video_info(video_path):
    """
    è·å–è§†é¢‘ä¿¡æ¯ï¼ˆå¸§æ•°ã€FPSç­‰ï¼‰
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        
    Returns:
        total_frames: æ€»å¸§æ•°
        fps: å¸§ç‡
        duration: æ—¶é•¿ï¼ˆç§’ï¼‰
    """
    if video_path is None:
        return 0, 0, 0
    
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    duration = total_frames / fps if fps > 0 else 0
    cap.release()
    
    return total_frames, fps, duration


def on_video_upload(video_input):
    """
    å½“ç”¨æˆ·ä¸Šä¼ è§†é¢‘æ—¶ï¼Œè‡ªåŠ¨è·å–è§†é¢‘ä¿¡æ¯å¹¶æ›´æ–°æ»‘å—
    """
    if video_input is None:
        return gr.update(maximum=100, value=24, info="ä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·çš„å¸§æ•°ã€‚æ¨è 24-50 å¸§ï¼Œæ›´å¤šå¸§éœ€è¦æ›´å¤šæ˜¾å­˜ã€‚"), \
               ""
    
    total_frames, fps, duration = get_video_info(video_input)
    
    if total_frames == 0:
        return gr.update(maximum=100, value=24, info="ä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·çš„å¸§æ•°ã€‚æ¨è 24-50 å¸§ï¼Œæ›´å¤šå¸§éœ€è¦æ›´å¤šæ˜¾å­˜ã€‚"), \
               ""
    
    # è®¡ç®—æ¨èå¸§æ•°ï¼šæœ€å¤š50å¸§ï¼Œæˆ–è€…è§†é¢‘æœ¬èº«çš„å¸§æ•°ï¼ˆå¦‚æœå°‘äº50ï¼‰
    recommended_frames = min(50, total_frames)
    
    # æ›´æ–°æ»‘å—çš„æœ€å¤§å€¼ä¸ºè§†é¢‘çš„æ€»å¸§æ•°ï¼ˆä½†ä¸è¶…è¿‡200å¸§ä»¥é˜²æ˜¾å­˜æº¢å‡ºï¼‰
    max_frames = min(total_frames, 200)
    
    # ç”Ÿæˆè§†é¢‘ä¿¡æ¯æ–‡å­—
    video_info = f"ğŸ“¹ è§†é¢‘ä¿¡æ¯ï¼šå…± {total_frames} å¸§ | {fps:.1f} FPS | æ—¶é•¿ {duration:.1f} ç§’"
    
    slider_info = f"ä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·çš„å¸§æ•°ã€‚æ¨è {recommended_frames} å¸§ï¼ˆæ˜¾å­˜å……è¶³å¯å¢åŠ ï¼Œå»ºè®®ä¸è¶…è¿‡100å¸§ï¼‰"
    
    return gr.update(maximum=max_frames, value=recommended_frames, info=slider_info), \
           video_info


def load_video(video_input, video_state, num_frames):
    """
    åŠ è½½è§†é¢‘å¹¶æå–ç¬¬ä¸€å¸§ç”¨äºç”Ÿæˆæ©ç 
    """
    # æ¸…ç†æ—§çš„è¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if video_state is not None and "output_paths" in video_state:
        cleanup_old_videos(video_state["output_paths"])
    
    if video_input is None:
        return video_state, None, \
               gr.update(visible=False), gr.update(visible=False), \
               gr.update(visible=False), gr.update(visible=False)
    
    # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å¸§æ•°æå–å¸§
    frames, fps = extract_frames_from_video(video_input, max_frames=num_frames)
    
    if len(frames) == 0:
        return video_state, None, \
               gr.update(visible=False), gr.update(visible=False), \
               gr.update(visible=False), gr.update(visible=False)
    
    # åˆå§‹åŒ–è§†é¢‘çŠ¶æ€
    video_state = {
        "frames": frames,
        "fps": fps,
        "first_frame_mask": None,
        "masks": None,
    }
    
    first_frame_pil = Image.fromarray(frames[0])
    
    return video_state, first_frame_pil, \
           gr.update(visible=True), gr.update(visible=True), \
           gr.update(visible=True), gr.update(visible=False)


def sam_refine(video_state, point_prompt, click_state, evt: gr.SelectData):
    """
    æ·»åŠ ç‚¹å‡»å¹¶æ›´æ–°ç¬¬ä¸€å¸§ä¸Šçš„æ©ç 
    
    Args:
        video_state: åŒ…å«è§†é¢‘æ•°æ®çš„å­—å…¸
        point_prompt: "æ­£å‘ç‚¹" æˆ– "è´Ÿå‘ç‚¹"
        click_state: [[points], [labels]]
        evt: Gradio SelectData äº‹ä»¶ï¼ŒåŒ…å«ç‚¹å‡»åæ ‡
    """
    if video_state is None or "frames" not in video_state:
        return None, video_state, click_state
    
    # æ·»åŠ æ–°ç‚¹å‡»
    x, y = evt.index[0], evt.index[1]
    label = 1 if point_prompt == "æ­£å‘ç‚¹" else 0
    
    click_state[0].append([x, y])
    click_state[1].append(label)
    
    print(f"æ·»åŠ  {point_prompt} ç‚¹å‡»ä½ç½® ({x}, {y})ã€‚æ€»ç‚¹å‡»æ•°: {len(click_state[0])}")
    
    # ä½¿ç”¨ SAM2 ç”Ÿæˆæ©ç 
    first_frame = video_state["frames"][0]
    mask = sam2_tracker.get_first_frame_mask(
        frame=first_frame,
        points=click_state[0],
        labels=click_state[1]
    )
    
    # å°†æ©ç å­˜å‚¨åœ¨è§†é¢‘çŠ¶æ€ä¸­
    video_state["first_frame_mask"] = mask
    
    # å¯è§†åŒ–æ©ç å’Œç‚¹
    painted_image = mask_painter(
        first_frame.copy(),
        mask,
        MASK_COLOR,
        MASK_ALPHA,
        CONTOUR_COLOR,
        CONTOUR_WIDTH
    )
    
    # ç»˜åˆ¶æ­£å‘ç‚¹
    positive_points = np.array([click_state[0][i] for i in range(len(click_state[0])) 
                               if click_state[1][i] == 1])
    if len(positive_points) > 0:
        painted_image = point_painter(
            painted_image,
            positive_points,
            POINT_COLOR_POS,
            POINT_ALPHA,
            POINT_RADIUS,
            CONTOUR_COLOR,
            CONTOUR_WIDTH
        )
    
    # ç»˜åˆ¶è´Ÿå‘ç‚¹
    negative_points = np.array([click_state[0][i] for i in range(len(click_state[0])) 
                               if click_state[1][i] == 0])
    if len(negative_points) > 0:
        painted_image = point_painter(
            painted_image,
            negative_points,
            POINT_COLOR_NEG,
            POINT_ALPHA,
            POINT_RADIUS,
            CONTOUR_COLOR,
            CONTOUR_WIDTH
        )
    
    painted_pil = Image.fromarray(painted_image)
    
    return painted_pil, video_state, click_state


def clear_clicks(video_state, click_state):
    """æ¸…é™¤æ‰€æœ‰ç‚¹å‡»å¹¶é‡ç½®ä¸ºåŸå§‹ç¬¬ä¸€å¸§"""
    click_state = [[], []]
    
    if video_state is not None and "frames" in video_state:
        first_frame = video_state["frames"][0]
        video_state["first_frame_mask"] = None
        return Image.fromarray(first_frame), video_state, click_state
    
    return None, video_state, click_state


def propagate_masks(video_state, click_state):
    """
    ä½¿ç”¨ SAM2 åœ¨æ•´ä¸ªè§†é¢‘ä¸­ä¼ æ’­ç¬¬ä¸€å¸§æ©ç 
    """
    if video_state is None or "frames" not in video_state:
        return video_state, "æœªåŠ è½½è§†é¢‘", gr.update(visible=False)
    
    if len(click_state[0]) == 0:
        return video_state, "âš ï¸ è¯·å…ˆæ·»åŠ è‡³å°‘ä¸€ä¸ªç‚¹", gr.update(visible=False)
    
    frames = video_state["frames"]
    
    # åœ¨è§†é¢‘ä¸­è·Ÿè¸ª
    print(f"åœ¨ {len(frames)} å¸§ä¸­è·Ÿè¸ªå¯¹è±¡...")
    masks = sam2_tracker.track_video(
        frames=frames,
        points=click_state[0],
        labels=click_state[1]
    )
    
    video_state["masks"] = masks
    
    status_msg = f"âœ“ å·²ç”Ÿæˆ {len(masks)} ä¸ªæ©ç ã€‚å‡†å¤‡è¿è¡Œ VideoMaMaï¼"
    
    return video_state, status_msg, gr.update(visible=True)


def run_videomama_with_sam2(video_state, click_state):
    """
    ä¸€èµ·è¿è¡Œ SAM2 ä¼ æ’­å’Œ VideoMaMa æ¨ç†
    """
    if video_state is None or "frames" not in video_state:
        return video_state, None, None, None, "âš ï¸ æœªåŠ è½½è§†é¢‘"
    
    if len(click_state[0]) == 0:
        return video_state, None, None, None, "âš ï¸ è¯·å…ˆæ·»åŠ è‡³å°‘ä¸€ä¸ªç‚¹"
    
    frames = video_state["frames"]
    
    # æ­¥éª¤ 1: ä½¿ç”¨ SAM2 åœ¨è§†é¢‘ä¸­è·Ÿè¸ª
    print(f"ğŸ¯ ä½¿ç”¨ SAM2 åœ¨ {len(frames)} å¸§ä¸­è·Ÿè¸ªå¯¹è±¡...")
    masks = sam2_tracker.track_video(
        frames=frames,
        points=click_state[0],
        labels=click_state[1]
    )
    
    video_state["masks"] = masks
    print(f"âœ“ å·²ç”Ÿæˆ {len(masks)} ä¸ªæ©ç ")
    
    # æ­¥éª¤ 2: è¿è¡Œ VideoMaMa
    print(f"ğŸ¨ åœ¨ {len(frames)} å¸§ä¸Šè¿è¡Œ VideoMaMa...")
    output_frames = videomama(videomama_pipeline, frames, masks)
    
    # ä¿å­˜è¾“å‡ºè§†é¢‘
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)
    
    timestamp = int(time.time())
    output_video_path = output_dir / f"output_{timestamp}.mp4"
    mask_video_path = output_dir / f"masks_{timestamp}.mp4"
    greenscreen_path = output_dir / f"greenscreen_{timestamp}.mp4"
    
    # ä¿å­˜æŠ å›¾ç»“æœ
    save_video(output_frames, output_video_path, video_state["fps"])
    
    # ä¿å­˜æ©ç è§†é¢‘ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    mask_frames_rgb = [np.stack([m, m, m], axis=-1) for m in masks]
    save_video(mask_frames_rgb, mask_video_path, video_state["fps"])
    
    # åˆ›å»ºç»¿å±åˆæˆï¼šRGB * VideoMaMa_alpha + green * (1 - VideoMaMa_alpha)
    # VideoMaMa output_frames å·²åŒ…å« alpha è’™ç‰ˆç»“æœ
    greenscreen_frames = []
    for orig_frame, output_frame in zip(frames, output_frames):
        # ä» VideoMaMa è¾“å‡ºä¸­æå– alpha è’™ç‰ˆ
        # VideoMaMa è¾“å‡ºæŠ å›¾åçš„å‰æ™¯ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¶å¼ºåº¦ä½œä¸º alpha
        gray = cv2.cvtColor(output_frame, cv2.COLOR_RGB2GRAY)
        alpha = np.clip(gray.astype(np.float32) / 255.0, 0, 1)
        alpha_3ch = np.stack([alpha, alpha, alpha], axis=-1)
        
        # åˆ›å»ºç»¿è‰²èƒŒæ™¯
        green_bg = np.zeros_like(orig_frame)
        green_bg[:, :] = [156, 251, 165]  # ç»¿å±é¢œè‰²
        
        # åˆæˆï¼šoriginal_RGB * alpha + green * (1 - alpha)
        composite = (orig_frame.astype(np.float32) * alpha_3ch + 
                    green_bg.astype(np.float32) * (1 - alpha_3ch)).astype(np.uint8)
        greenscreen_frames.append(composite)
    
    save_video(greenscreen_frames, greenscreen_path, video_state["fps"])
    
    status_msg = f"âœ“ å®Œæˆï¼å·²ç”Ÿæˆ {len(output_frames)} å¸§ã€‚"
    
    # å­˜å‚¨è·¯å¾„ä»¥ä¾¿ç¨åæ¸…ç†
    video_state["output_paths"] = [str(output_video_path), str(mask_video_path), str(greenscreen_path)]
    
    return video_state, str(output_video_path), str(mask_video_path), str(greenscreen_path), status_msg


def save_video(frames, output_path, fps):
    """å°†å¸§ä¿å­˜ä¸ºè§†é¢‘æ–‡ä»¶"""
    if len(frames) == 0:
        return
    
    height, width = frames[0].shape[:2]
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
    
    for frame in frames:
        if len(frame.shape) == 2:  # ç°åº¦å›¾
            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
        else:  # RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        out.write(frame)
    
    out.release()
    print(f"å·²ä¿å­˜è§†é¢‘åˆ° {output_path}")


def cleanup_old_videos(video_paths):
    """åˆ é™¤æ—§çš„è¾“å‡ºè§†é¢‘ä»¥èŠ‚çœå­˜å‚¨ç©ºé—´"""
    if video_paths is None:
        return
    
    for path in video_paths:
        try:
            if os.path.exists(path):
                os.remove(path)
                print(f"å·²æ¸…ç†: {path}")
        except Exception as e:
            print(f"åˆ é™¤å¤±è´¥ {path}: {e}")


def cleanup_old_outputs(max_age_minutes=30):
    """
    åˆ é™¤è¶…è¿‡ max_age_minutes çš„è¾“å‡ºæ–‡ä»¶ä»¥é˜²æ­¢å­˜å‚¨æº¢å‡º
    å®šæœŸè¿è¡Œä»¥æ¸…ç†åºŸå¼ƒæ–‡ä»¶
    """
    output_dir = Path("outputs")
    if not output_dir.exists():
        return
    
    current_time = time.time()
    max_age_seconds = max_age_minutes * 60
    
    for file_path in output_dir.glob("*.mp4"):
        try:
            file_age = current_time - file_path.stat().st_mtime
            if file_age > max_age_seconds:
                file_path.unlink()
                print(f"å·²æ¸…ç†æ—§æ–‡ä»¶: {file_path} (æ—¶é•¿: {file_age/60:.1f} åˆ†é’Ÿ)")
        except Exception as e:
            print(f"æ¸…ç†å¤±è´¥ {file_path}: {e}")


def restart():
    """é‡ç½®æ‰€æœ‰çŠ¶æ€"""
    return None, [[], []], None, \
           gr.update(visible=False), gr.update(visible=False), \
           gr.update(visible=False), None, None, None, "", \
           gr.update(maximum=100, value=24, info="ä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·çš„å¸§æ•°ã€‚æ¨è 24-50 å¸§ï¼Œæ›´å¤šå¸§éœ€è¦æ›´å¤šæ˜¾å­˜ã€‚"), ""


# CSS æ ·å¼
custom_css = """
.gradio-container {width: 90% !important; margin: 0 auto;}
.title-text {text-align: center; font-size: 48px; font-weight: bold; 
             background: linear-gradient(to right, #8b5cf6, #10b981); 
             -webkit-background-clip: text; -webkit-text-fill-color: transparent;}
.description-text {text-align: center; font-size: 18px; margin: 20px 0;}
.youtube-link {
    text-align: center; 
    font-size: 16px; 
    margin: 10px 0 20px 0; 
    padding: 10px;
    background: linear-gradient(135deg, #ff0000 0%, #cc0000 100%);
    border-radius: 10px;
}
.youtube-link a {
    color: white !important;
    text-decoration: none;
    font-weight: bold;
}
.youtube-link a:hover {
    text-decoration: underline;
}
button {border-radius: 8px !important;}
.green_button {background-color: #10b981 !important; color: white !important;}
.red_button {background-color: #ef4444 !important; color: white !important;}
.run_matting_button {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%) !important;
    color: white !important;
    font-weight: bold !important;
    font-size: 18px !important;
    padding: 20px !important;
    box-shadow: 0 4px 15px 0 rgba(102, 126, 234, 0.75) !important;
    border: none !important;
}
.run_matting_button:hover {
    background: linear-gradient(135deg, #764ba2 0%, #667eea 50%, #f093fb 100%) !important;
    box-shadow: 0 6px 20px 0 rgba(102, 126, 234, 0.9) !important;
    transform: translateY(-2px) !important;
}
"""

# æ„å»º Gradio ç•Œé¢
with gr.Blocks(css=custom_css, title="VideoMaMa æ¼”ç¤º") as demo:
    gr.HTML('<div class="youtube-link">ğŸ“º <a href="https://www.youtube.com/@rongyi-ai" target="_blank">AI æŠ€æœ¯åˆ†äº«é¢‘é“</a> - æ¬¢è¿è®¢é˜…ï¼</div>')
    gr.HTML('<div class="title-text">VideoMaMa äº¤äº’å¼æ¼”ç¤º</div>')
    gr.Markdown(
        '<div class="description-text">ğŸ¬ ä¸Šä¼ è§†é¢‘ â†’ ğŸ–±ï¸ ç‚¹å‡»æ ‡è®°å¯¹è±¡ â†’ âœ… ç”Ÿæˆæ©ç  â†’ ğŸ¨ è¿è¡Œ VideoMaMa</div>'
    )
    
    # çŠ¶æ€å˜é‡
    video_state = gr.State(None)
    click_state = gr.State([[], []])  # [[points], [labels]]
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### æ­¥éª¤ 1ï¼šä¸Šä¼ è§†é¢‘")
            video_input = gr.Video(label="è¾“å…¥è§†é¢‘")
            video_info_text = gr.Markdown("", elem_id="video_info")
            num_frames_slider = gr.Slider(
                minimum=1,
                maximum=100,
                value=24,
                step=1,
                label="è¦å¤„ç†çš„å¸§æ•°",
                info="ä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·çš„å¸§æ•°ã€‚æ¨è 24-50 å¸§ï¼Œæ›´å¤šå¸§éœ€è¦æ›´å¤šæ˜¾å­˜ã€‚"
            )
            load_button = gr.Button("ğŸ“ åŠ è½½è§†é¢‘", variant="primary")
            
            gr.Markdown("### æ­¥éª¤ 2ï¼šæ ‡è®°å¯¹è±¡")
            point_prompt = gr.Radio(
                choices=["æ­£å‘ç‚¹", "è´Ÿå‘ç‚¹"],
                value="æ­£å‘ç‚¹",
                label="ç‚¹å‡»ç±»å‹",
                info="æ­£å‘ç‚¹ï¼šå¯¹è±¡ï¼Œè´Ÿå‘ç‚¹ï¼šèƒŒæ™¯",
                visible=False
            )
            clear_button = gr.Button("ğŸ—‘ï¸ æ¸…é™¤ç‚¹å‡»", visible=False)
            
        with gr.Column(scale=1):
            gr.Markdown("### ç¬¬ä¸€å¸§ï¼ˆç‚¹å‡»æ·»åŠ ç‚¹ï¼‰")
            first_frame_display = gr.Image(
                label="ç¬¬ä¸€å¸§",
                type="pil",
                interactive=True
            )
            run_button = gr.Button("ğŸš€ è¿è¡ŒæŠ å›¾", visible=False, elem_classes="run_matting_button", size="lg")
    
    status_text = gr.Textbox(label="çŠ¶æ€", value="", interactive=False, visible=False)
    
    gr.Markdown("### è¾“å‡ºç»“æœ")
    with gr.Row():
        with gr.Column():
            output_video = gr.Video(label="æŠ å›¾ç»“æœï¼ˆVideoMaMa ç”Ÿæˆï¼‰", autoplay=True)
        with gr.Column():
            greenscreen_video = gr.Video(label="ç»¿å±åˆæˆ", autoplay=True)
        with gr.Column():
            mask_video = gr.Video(label="åˆ†å‰²æ©ç ï¼ˆSAM2 åˆ†å‰²ï¼‰", autoplay=True)
    
    # äº‹ä»¶å¤„ç†å™¨
    load_button.click(
        fn=load_video,
        inputs=[video_input, video_state, num_frames_slider],
        outputs=[video_state, first_frame_display, 
                point_prompt, clear_button, run_button, status_text]
    )
    
    first_frame_display.select(
        fn=sam_refine,
        inputs=[video_state, point_prompt, click_state],
        outputs=[first_frame_display, video_state, click_state]
    )
    
    clear_button.click(
        fn=clear_clicks,
        inputs=[video_state, click_state],
        outputs=[first_frame_display, video_state, click_state]
    )
    
    run_button.click(
        fn=run_videomama_with_sam2,
        inputs=[video_state, click_state],
        outputs=[video_state, output_video, mask_video, greenscreen_video, status_text]
    )
    
    # è§†é¢‘ä¸Šä¼ æ—¶è‡ªåŠ¨æ›´æ–°å¸§æ•°ä¿¡æ¯
    video_input.upload(
        fn=on_video_upload,
        inputs=[video_input],
        outputs=[num_frames_slider, video_info_text]
    )
    
    video_input.change(
        fn=restart,
        inputs=[],
        outputs=[video_state, click_state, first_frame_display,
                point_prompt, clear_button, run_button, 
                output_video, mask_video, greenscreen_video, status_text,
                num_frames_slider, video_info_text]
    )
    
    # ç¤ºä¾‹
    gr.Markdown("---\n### ğŸ“¦ ç¤ºä¾‹è§†é¢‘")
    example_dir = Path("assets")
    if example_dir.exists():
        examples = [str(p) for p in sorted(example_dir.glob("*.mp4"))]
        if examples:
            gr.Examples(examples=examples, inputs=[video_input])


if __name__ == "__main__":
    print("=" * 60)
    print("VideoMaMa äº¤äº’å¼æ¼”ç¤º")
    print("=" * 60)
    
    # å¯åŠ¨æ—¶æ¸…ç†æ—§çš„è¾“å‡ºæ–‡ä»¶
    cleanup_old_outputs(max_age_minutes=30)
    
    # åˆå§‹åŒ–æ¨¡å‹
    initialize_models()
    
    # å¯åŠ¨æ¼”ç¤º
    demo.queue()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
    )
