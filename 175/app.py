import gradio as gr
import os
import json
import subprocess
import time
import yaml
from datetime import datetime
from PIL import Image
import torch
from datasets import load_dataset
from diffusers import DiffusionPipeline
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging
import random
import numpy as np
from torch.utils.data import Dataset, DataLoader

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Custom dataset class to avoid modifying existing files
class InlineCustomImageDataset(Dataset):
    def __init__(self, img_dir, img_size=512, caption_type='txt',
                 random_ratio=False, caption_dropout_rate=0.1):
        self.images = [os.path.join(img_dir, i) for i in os.listdir(img_dir) if '.jpg' in i or '.png' in i]
        self.images.sort()
        self.img_size = img_size
        self.caption_type = caption_type
        self.random_ratio = random_ratio
        self.caption_dropout_rate = caption_dropout_rate
        print(f'Found {len(self.images)} images in {img_dir}')
    
    def __len__(self):
        return 999999  # Large number for continuous training
    
    def __getitem__(self, idx):
        try:
            idx = random.randint(0, len(self.images) - 1)
            
            # Load and process image
            img = Image.open(self.images[idx]).convert('RGB')
            
            if self.random_ratio:
                ratio = random.choice(["16:9", "default", "1:1", "4:3"])
                if ratio != "default":
                    img = self._crop_to_aspect_ratio(img, ratio)
            
            img = self._image_resize(img, self.img_size)
            w, h = img.size
            new_w = (w // 32) * 32
            new_h = (h // 32) * 32
            img = img.resize((new_w, new_h))
            img = torch.from_numpy((np.array(img) / 127.5) - 1)
            img = img.permute(2, 0, 1)
            
            # Load text caption with fixed path handling
            img_path = self.images[idx]
            txt_path = os.path.splitext(img_path)[0] + '.' + self.caption_type
            
            with open(txt_path, 'r', encoding='utf-8') as f:
                prompt = f.read().strip()
            
            # Random caption dropout
            if random.random() < self.caption_dropout_rate:
                return img, " "
            else:
                return img, prompt
                
        except Exception as e:
            print(f"Error loading data: {e}")
            return self.__getitem__(random.randint(0, len(self.images) - 1))
    
    def _image_resize(self, image, target_size):
        """Resize image maintaining aspect ratio"""
        width, height = image.size
        if width > height:
            new_width = target_size
            new_height = int(height * target_size / width)
        else:
            new_height = target_size
            new_width = int(width * target_size / height)
        return image.resize((new_width, new_height), Image.LANCZOS)
    
    def _crop_to_aspect_ratio(self, image, target_ratio):
        """Crop image to target aspect ratio"""
        width, height = image.size
        
        if target_ratio == "16:9":
            target_ratio_value = 16 / 9
        elif target_ratio == "1:1":
            target_ratio_value = 1
        elif target_ratio == "4:3":
            target_ratio_value = 4 / 3
        else:
            return image
        
        current_ratio = width / height
        
        if current_ratio > target_ratio_value:
            # Image is too wide, crop from sides
            new_width = int(height * target_ratio_value)
            offset = (width - new_width) // 2
            crop_box = (offset, 0, offset + new_width, height)
        else:
            # Image is too tall, crop from top/bottom
            new_height = int(width / target_ratio_value)
            offset = (height - new_height) // 2
            crop_box = (0, offset, width, offset + new_height)
        
        return image.crop(crop_box)

def inline_data_loader(img_dir, img_size, caption_type, random_ratio, caption_dropout_rate, train_batch_size, num_workers):
    """Custom data loader function to replace the original loader"""
    dataset = InlineCustomImageDataset(
        img_dir=img_dir,
        img_size=img_size,
        caption_type=caption_type,
        random_ratio=random_ratio,
        caption_dropout_rate=caption_dropout_rate
    )
    return DataLoader(dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True)

class LoRATrainerWebUI:
    def __init__(self):
        self.dataset_dir = "./datasets"
        self.training_data_dir = "./my_training_data"
        self.checkpoints_dir = "./checkpoints"
        self.config_dir = "./train_configs"
        self.output_dir = "./output"
        
        # Create directories if they don't exist
        for dir_path in [self.dataset_dir, self.training_data_dir, self.checkpoints_dir, self.config_dir, self.output_dir]:
            os.makedirs(dir_path, exist_ok=True)
    
    def download_dataset(self, dataset_name, split, num_samples, progress=gr.Progress()):
        """ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®é›†"""
        try:
            progress(0, desc="æ­£åœ¨ä¸‹è½½æ•°æ®é›†...")
            
            # Download dataset
            dataset = load_dataset(dataset_name, split=split, streaming=False)
            
            if num_samples > 0:
                dataset = dataset.select(range(min(num_samples, len(dataset))))
            
            progress(0.3, desc="æ­£åœ¨å¤„ç†å›¾åƒ...")
            
            # Save images and create text descriptions
            image_count = 0
            total_images = len(dataset)
            
            for i, item in enumerate(dataset):
                if 'image' in item:
                    # Save image
                    image_filename = f"image_{i+1:03d}.png"
                    image_path = os.path.join(self.training_data_dir, image_filename)
                    item['image'].save(image_path)
                    
                    # Create text description from filename or metadata
                    text_filename = f"image_{i+1:03d}.txt"
                    text_path = os.path.join(self.training_data_dir, text_filename)
                    
                    # Extract description from filename or create one
                    if dataset_name == "monadical-labs/minecraft-preview":
                        # Extract character name from image filename
                        original_filename = item.get('filename', f'character_{i+1}')
                        char_name = original_filename.replace('.png', '').replace('_', ' ')
                        description = f"A minecraft character: {char_name}, minecraft style, pixelated, blocky, game character"
                    else:
                        description = f"An image from {dataset_name}, detailed, high quality"
                    
                    with open(text_path, 'w') as f:
                        f.write(description)
                    
                    image_count += 1
                    progress((i + 1) / total_images * 0.7 + 0.3, desc=f"å·²å¤„ç† {i+1}/{total_images} å¼ å›¾åƒ")
            
            progress(1.0, desc="æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
            return f"æˆåŠŸä¸‹è½½å¹¶å‡†å¤‡äº† {image_count} å¼ å›¾åƒï¼Œæ¥è‡ª {dataset_name}ï¼\nå›¾åƒä¿å­˜ä½ç½®ï¼š{self.training_data_dir}"
        
        except Exception as e:
            return f"æ•°æ®é›†ä¸‹è½½é”™è¯¯ï¼š{str(e)}"
    
    def validate_dataset(self):
        """éªŒè¯å‡†å¤‡å¥½çš„æ•°æ®é›†"""
        try:
            if not os.path.exists(self.training_data_dir):
                return "âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ç›®å½•ã€‚è¯·å…ˆä¸‹è½½æ•°æ®é›†ã€‚"
            
            image_files = [f for f in os.listdir(self.training_data_dir) if f.endswith('.png')]
            text_files = [f for f in os.listdir(self.training_data_dir) if f.endswith('.txt')]
            
            image_count = len(image_files)
            text_count = len(text_files)
            
            # Check for paired files
            paired_count = 0
            paired_files = []
            for img_file in image_files:
                txt_file = img_file.replace('.png', '.txt')
                if txt_file in text_files:
                    paired_count += 1
                    paired_files.append((img_file, txt_file))
            
            # Sample a few text descriptions
            sample_descriptions = []
            for i, (img_file, txt_file) in enumerate(paired_files[:3]):
                try:
                    with open(os.path.join(self.training_data_dir, txt_file), 'r') as f:
                        desc = f.read().strip()
                        sample_descriptions.append(f"{img_file}: {desc}")
                except:
                    continue
            
            validation_result = f"""
ğŸ“Š æ•°æ®é›†éªŒè¯ç»“æœï¼š
â€¢ å‘ç°å›¾åƒï¼š{image_count} å¼ 
â€¢ å‘ç°æ–‡æœ¬æ–‡ä»¶ï¼š{text_count} ä¸ª  
â€¢ æ­£ç¡®é…å¯¹ï¼š{paired_count} å¯¹
â€¢ æ•°æ®é›†ç›®å½•ï¼š{self.training_data_dir}

ğŸ“ ç¤ºä¾‹æè¿°ï¼š
{chr(10).join(sample_descriptions)}

{'âœ… æ•°æ®é›†å·²å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼' if paired_count > 0 else 'âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å›¾åƒ-æ–‡æœ¬é…å¯¹ï¼'}
"""
            return validation_result
        
        except Exception as e:
            return f"âŒ æ•°æ®é›†éªŒè¯é”™è¯¯ï¼š{str(e)}"
    
    def get_dataset_status(self):
        """è·å–å½“å‰æ•°æ®é›†çŠ¶æ€"""
        try:
            if not os.path.exists(self.training_data_dir):
                return "âŒ æœªæ‰¾åˆ°æ•°æ®é›†"
            
            image_files = [f for f in os.listdir(self.training_data_dir) if f.endswith('.png')]
            text_files = [f for f in os.listdir(self.training_data_dir) if f.endswith('.txt')]
            
            if len(image_files) > 0 and len(text_files) > 0:
                return f"âœ… {len(image_files)} å¼ å›¾åƒå·²å‡†å¤‡å¥½"
            else:
                return "âš ï¸ æ•°æ®é›†ä¸å®Œæ•´"
        except:
            return "âŒ æ•°æ®é›†é”™è¯¯"
    
    def get_dataset_preview(self, num_samples=5):
        """è·å–æ•°æ®é›†é¢„è§ˆï¼ŒåŒ…å«å›¾åƒå’Œæ ‡é¢˜"""
        try:
            if not os.path.exists(self.training_data_dir):
                return [], "âŒ æœªæ‰¾åˆ°æ•°æ®é›†"
            
            image_files = [f for f in os.listdir(self.training_data_dir) if f.endswith('.png')]
            image_files.sort()  # Sort for consistent ordering
            
            preview_images = []
            preview_captions = []
            
            # Get first few samples for preview
            for i, img_file in enumerate(image_files[:num_samples]):
                img_path = os.path.join(self.training_data_dir, img_file)
                txt_file = img_file.replace('.png', '.txt')
                txt_path = os.path.join(self.training_data_dir, txt_file)
                
                # Load image
                if os.path.exists(img_path):
                    preview_images.append(img_path)
                
                # Load caption
                if os.path.exists(txt_path):
                    with open(txt_path, 'r') as f:
                        caption = f.read().strip()
                        preview_captions.append(f"#{i+1}: {caption}")
                else:
                    preview_captions.append(f"#{i+1}: æœªæ‰¾åˆ°æ ‡é¢˜")
            
            preview_text = "\n\n".join(preview_captions)
            return preview_images, preview_text
        
        except Exception as e:
            return [], f"âŒ åŠ è½½é¢„è§ˆæ—¶å‡ºé”™ï¼š{str(e)}"
    
    def refresh_preview(self):
        """åˆ·æ–°æ•°æ®é›†é¢„è§ˆ"""
        return self.get_dataset_preview()
    
    def start_training(self, model_name, learning_rate, batch_size, epochs, resolution, 
                      lora_rank, lora_alpha, gradient_accumulation_steps, save_steps, logging_steps, 
                      progress=gr.Progress()):
        """ç›´æ¥å¯åŠ¨ LoRA è®­ç»ƒï¼ˆå®Œå…¨å†…åµŒï¼Œæ— éœ€é…ç½®æ–‡ä»¶ï¼‰"""
        try:
            import copy
            from copy import deepcopy
            from accelerate import Accelerator
            from accelerate.utils import ProjectConfiguration
            import datasets
            import diffusers
            from diffusers import FlowMatchEulerDiscreteScheduler
            from diffusers import (
                AutoencoderKLQwenImage,
                QwenImagePipeline,
                QwenImageTransformer2DModel,
            )
            from diffusers.optimization import get_scheduler
            from diffusers.training_utils import (
                compute_density_for_timestep_sampling,
                compute_loss_weighting_for_sd3,
            )
            from diffusers.utils import convert_state_dict_to_diffusers
            from diffusers.utils.torch_utils import is_compiled_module
            from peft import LoraConfig
            from peft.utils import get_peft_model_state_dict
            import transformers
            from tqdm.auto import tqdm
            
            progress(0, desc="å‡†å¤‡è®­ç»ƒç¯å¢ƒ...")
            
            # Validate data directory
            if not os.path.exists(self.training_data_dir):
                return "âŒ è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®é›†ï¼"
            
            # Validate model directory
            if not os.path.exists(model_name):
                return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼š{model_name}"
            
            # Ensure output directory exists
            os.makedirs(self.output_dir, exist_ok=True)
            logging_dir = os.path.join(self.output_dir, "logs")
            
            progress(0.1, desc="åˆå§‹åŒ–è®­ç»ƒè®¾ç½®...")
            
            # Calculate max steps
            image_files = [f for f in os.listdir(self.training_data_dir) if f.endswith('.png')]
            dataset_size = len(image_files)
            max_train_steps = max(100, (dataset_size // batch_size) * epochs)
            
            # Initialize accelerator with bf16 to avoid gradient scaling issues
            accelerator_project_config = ProjectConfiguration(project_dir=self.output_dir, logging_dir=logging_dir)
            accelerator = Accelerator(
                gradient_accumulation_steps=gradient_accumulation_steps,
                mixed_precision="bf16",  # Use bf16 instead of fp16 to avoid gradient scaling issues
                project_config=accelerator_project_config,
            )
            
            def unwrap_model(model):
                model = accelerator.unwrap_model(model)
                model = model._orig_mod if is_compiled_module(model) else model
                return model
            
            # Set logging
            if accelerator.is_local_main_process:
                datasets.utils.logging.set_verbosity_warning()
                transformers.utils.logging.set_verbosity_warning()
                diffusers.utils.logging.set_verbosity_info()
            else:
                datasets.utils.logging.set_verbosity_error()
                transformers.utils.logging.set_verbosity_error()
                diffusers.utils.logging.set_verbosity_error()
            
            progress(0.2, desc="åŠ è½½æ¨¡å‹...")
            
            # Set dtype based on mixed precision
            weight_dtype = torch.float32
            if accelerator.mixed_precision == "fp16":
                weight_dtype = torch.float16
            elif accelerator.mixed_precision == "bf16":
                weight_dtype = torch.bfloat16
            
            # Load models
            text_encoding_pipeline = QwenImagePipeline.from_pretrained(
                model_name, transformer=None, vae=None, torch_dtype=weight_dtype
            )
            vae = AutoencoderKLQwenImage.from_pretrained(
                model_name,
                subfolder="vae",
            )
            flux_transformer = QwenImageTransformer2DModel.from_pretrained(
                model_name,
                subfolder="transformer",
            )
            
            progress(0.3, desc="é…ç½® LoRA...")
            
            # LoRA configuration
            lora_config = LoraConfig(
                r=lora_rank,
                lora_alpha=lora_rank,
                init_lora_weights="gaussian",
                target_modules=["to_k", "to_q", "to_v", "to_out.0"],
            )
            
            noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
                model_name,
                subfolder="scheduler",
            )
            
            flux_transformer.to(accelerator.device, dtype=weight_dtype)
            flux_transformer.add_adapter(lora_config)
            text_encoding_pipeline.to(accelerator.device)
            noise_scheduler_copy = copy.deepcopy(noise_scheduler)
            
            def get_sigmas(timesteps, n_dim=4, dtype=torch.float32):
                sigmas = noise_scheduler_copy.sigmas.to(device=accelerator.device, dtype=dtype)
                schedule_timesteps = noise_scheduler_copy.timesteps.to(accelerator.device)
                timesteps = timesteps.to(accelerator.device)
                step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]
                sigma = sigmas[step_indices].flatten()
                while len(sigma.shape) < n_dim:
                    sigma = sigma.unsqueeze(-1)
                return sigma
            
            # Freeze base model parameters
            vae.requires_grad_(False)
            flux_transformer.requires_grad_(False)
            flux_transformer.train()
            
            progress(0.4, desc="é…ç½®ä¼˜åŒ–å™¨...")
            
            # Set trainable parameters
            trainable_params = []
            for n, param in flux_transformer.named_parameters():
                if 'lora' not in n:
                    param.requires_grad = False
                else:
                    param.requires_grad = True
                    trainable_params.append(param)
            
            print(f"Trainable parameters: {sum([p.numel() for p in trainable_params]) / 1000000:.1f}M")
            
            flux_transformer.enable_gradient_checkpointing()
            optimizer = torch.optim.AdamW(
                trainable_params,
                lr=learning_rate,
                betas=(0.9, 0.999),
                weight_decay=0.01,
                eps=1e-8,
            )
            
            progress(0.5, desc="å‡†å¤‡æ•°æ®...")
            
            # Data configuration - using inline data loader  
            train_dataloader = inline_data_loader(
                img_dir=self.training_data_dir,
                img_size=resolution,
                caption_type='txt',
                random_ratio=False,
                caption_dropout_rate=0.1,
                train_batch_size=batch_size,
                num_workers=1
            )
            
            lr_scheduler = get_scheduler(
                "constant",
                optimizer=optimizer,
                num_warmup_steps=0,
                num_training_steps=max_train_steps * accelerator.num_processes,
            )
            
            progress(0.6, desc="å‡†å¤‡è®­ç»ƒ...")
            
            # Prepare for training
            vae.to(accelerator.device, dtype=weight_dtype)
            flux_transformer, optimizer, _, lr_scheduler = accelerator.prepare(
                flux_transformer, optimizer, deepcopy(train_dataloader), lr_scheduler
            )
            
            global_step = 0
            total_batch_size = batch_size * accelerator.num_processes * gradient_accumulation_steps
            
            print(f"***** Running training *****")
            print(f"  Instantaneous batch size per device = {batch_size}")
            print(f"  Total train batch size = {total_batch_size}")
            print(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
            print(f"  Max training steps = {max_train_steps}")
            
            progress(0.7, desc="å¼€å§‹è®­ç»ƒ...")
            
            vae_scale_factor = 2 ** len(vae.temperal_downsample)
            
            for epoch in range(1):
                train_loss = 0.0
                step_count = 0
                
                for step, batch in enumerate(train_dataloader):
                    with accelerator.accumulate(flux_transformer):
                        img, prompts = batch
                        
                        with torch.no_grad():
                            pixel_values = img.to(dtype=weight_dtype).to(accelerator.device)
                            pixel_values = pixel_values.unsqueeze(2)
                            
                            pixel_latents = vae.encode(pixel_values).latent_dist.sample()
                            pixel_latents = pixel_latents.permute(0, 2, 1, 3, 4)
                            
                            latents_mean = (
                                torch.tensor(vae.config.latents_mean)
                                .view(1, 1, vae.config.z_dim, 1, 1)
                                .to(pixel_latents.device, pixel_latents.dtype)
                            )
                            latents_std = 1.0 / torch.tensor(vae.config.latents_std).view(1, 1, vae.config.z_dim, 1, 1).to(
                                pixel_latents.device, pixel_latents.dtype
                            )
                            pixel_latents = (pixel_latents - latents_mean) * latents_std
                            
                            bsz = pixel_latents.shape[0]
                            noise = torch.randn_like(pixel_latents, device=accelerator.device, dtype=weight_dtype)
                            u = compute_density_for_timestep_sampling(
                                weighting_scheme="none",
                                batch_size=bsz,
                                logit_mean=0.0,
                                logit_std=1.0,
                                mode_scale=1.29,
                            )
                            indices = (u * noise_scheduler_copy.config.num_train_timesteps).long()
                            timesteps = noise_scheduler_copy.timesteps[indices].to(device=pixel_latents.device)
                            
                            sigmas = get_sigmas(timesteps, n_dim=pixel_latents.ndim, dtype=pixel_latents.dtype)
                            noisy_model_input = (1.0 - sigmas) * pixel_latents + sigmas * noise
                            
                            # Pack the latents
                            packed_noisy_model_input = QwenImagePipeline._pack_latents(
                                noisy_model_input,
                                bsz, 
                                noisy_model_input.shape[2],
                                noisy_model_input.shape[3],
                                noisy_model_input.shape[4],
                            )
                            
                            # Encode prompts
                            img_shapes = [(1, noisy_model_input.shape[3] // 2, noisy_model_input.shape[4] // 2)] * bsz
                            prompt_embeds, prompt_embeds_mask = text_encoding_pipeline.encode_prompt(
                                prompt=prompts,
                                device=packed_noisy_model_input.device,
                                num_images_per_prompt=1,
                                max_sequence_length=1024,
                            )
                            txt_seq_lens = prompt_embeds_mask.sum(dim=1).tolist()
                        
                        # Model prediction
                        model_pred = flux_transformer(
                            hidden_states=packed_noisy_model_input,
                            timestep=timesteps / 1000,
                            guidance=None,
                            encoder_hidden_states_mask=prompt_embeds_mask,
                            encoder_hidden_states=prompt_embeds,
                            img_shapes=img_shapes,
                            txt_seq_lens=txt_seq_lens,
                            return_dict=False,
                        )[0]
                        
                        model_pred = QwenImagePipeline._unpack_latents(
                            model_pred,
                            height=noisy_model_input.shape[3] * vae_scale_factor,
                            width=noisy_model_input.shape[4] * vae_scale_factor,
                            vae_scale_factor=vae_scale_factor,
                        )
                        
                        # Compute loss
                        weighting = compute_loss_weighting_for_sd3(weighting_scheme="none", sigmas=sigmas)
                        target = noise - pixel_latents
                        target = target.permute(0, 2, 1, 3, 4)
                        loss = torch.mean(
                            (weighting.float() * (model_pred.float() - target.float()) ** 2).reshape(target.shape[0], -1),
                            1,
                        )
                        loss = loss.mean()
                        
                        avg_loss = accelerator.gather(loss.repeat(batch_size)).mean()
                        train_loss += avg_loss.item() / gradient_accumulation_steps
                        
                        # Backpropagate
                        accelerator.backward(loss)
                        if accelerator.sync_gradients:
                            accelerator.clip_grad_norm_(flux_transformer.parameters(), 1.0)
                        optimizer.step()
                        lr_scheduler.step()
                        optimizer.zero_grad()
                    
                    # Update progress and save checkpoints
                    if accelerator.sync_gradients:
                        global_step += 1
                        step_count += 1
                        train_loss = 0.0
                        
                        # Update progress
                        training_progress = 0.7 + (global_step / max_train_steps) * 0.25  # 70% to 95%
                        progress(training_progress, desc=f"è®­ç»ƒä¸­... æ­¥éª¤ {global_step}/{max_train_steps}")
                        
                        if global_step % save_steps == 0:
                            if accelerator.is_main_process:
                                save_path = os.path.join(self.output_dir, f"checkpoint-{global_step}")
                                os.makedirs(save_path, exist_ok=True)
                                
                                unwrapped_flux_transformer = unwrap_model(flux_transformer)
                                flux_transformer_lora_state_dict = convert_state_dict_to_diffusers(
                                    get_peft_model_state_dict(unwrapped_flux_transformer)
                                )
                                
                                QwenImagePipeline.save_lora_weights(
                                    save_path,
                                    flux_transformer_lora_state_dict,
                                    safe_serialization=True,
                                )
                                
                                print(f"Saved checkpoint to {save_path}")
                        
                        if global_step >= max_train_steps:
                            break
                    
                    if global_step >= max_train_steps:
                        break
            
            accelerator.wait_for_everyone()
            accelerator.end_training()
            
            progress(1.0, desc="è®­ç»ƒå®Œæˆï¼")
            return f"âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼\n\nè®­ç»ƒæ­¥æ•°: {global_step}\nè¾“å‡ºç›®å½•: {self.output_dir}"
            
        except Exception as e:
            import traceback
            error_msg = f"âŒ è®­ç»ƒæ—¶å‡ºé”™ï¼š{str(e)}\n\n{traceback.format_exc()}"
            print(error_msg)
            return error_msg
    
    def load_inference_model(self, model_path):
        """åŠ è½½æ¨ç†æ¨¡å‹"""
        try:
            self.pipe = None
            if os.path.exists(model_path):
                # è®¾ç½®è®¾å¤‡å’Œæ•°æ®ç±»å‹
                if torch.cuda.is_available():
                    torch_dtype = torch.bfloat16
                    device = "cuda"
                    device_info = f"GPU: {torch.cuda.get_device_name()}"
                else:
                    torch_dtype = torch.float32
                    device = "cpu"
                    device_info = "CPU"
                
                # åŠ è½½åŸºç¡€æ¨¡å‹ - ä½¿ç”¨æ­£ç¡®çš„ DiffusionPipeline
                self.pipe = DiffusionPipeline.from_pretrained(
                    model_path,
                    torch_dtype=torch_dtype,
                    safety_checker=None,
                    requires_safety_checker=False
                )
                self.pipe = self.pipe.to(device)
                
                # æŸ¥æ‰¾ LoRA æƒé‡æ–‡ä»¶
                lora_path = os.path.join(model_path, "pytorch_lora_weights.safetensors")
                if not os.path.exists(lora_path):
                    # å°è¯•åœ¨å­ç›®å½•ä¸­æŸ¥æ‰¾
                    lora_path = os.path.join(model_path, "lora", "pytorch_lora_weights.safetensors")
                
                # åŠ è½½ LoRA æƒé‡
                if os.path.exists(lora_path):
                    self.pipe.load_lora_weights(lora_path, adapter_name="minecraft_lora")
                    return f"âœ… æ¨¡å‹å’Œ LoRA åŠ è½½æˆåŠŸï¼\nè®¾å¤‡ï¼š{device_info}\nLoRA è·¯å¾„ï¼š{lora_path}"
                else:
                    return f"âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸï¼\nè®¾å¤‡ï¼š{device_info}\nâš ï¸ æœªæ‰¾åˆ° LoRA æƒé‡æ–‡ä»¶"
            else:
                return f"âŒ æœªæ‰¾åˆ°æ¨¡å‹è·¯å¾„ï¼š{model_path}"
        
        except Exception as e:
            return f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}"
    
    def generate_image(self, prompt, negative_prompt, steps, cfg_scale, seed, width, height, progress=gr.Progress()):
        """ä½¿ç”¨åŠ è½½çš„æ¨¡å‹ç”Ÿæˆå›¾åƒ"""
        try:
            if not hasattr(self, 'pipe') or self.pipe is None:
                return None, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
            
            progress(0, desc="æ­£åœ¨ç”Ÿæˆå›¾åƒ...")
            
            # è®¾ç½®éšæœºç§å­
            if seed < 0:
                seed = torch.randint(0, 1000000, (1,)).item()
            
            # è·å–è®¾å¤‡ä¿¡æ¯
            device = "cuda" if torch.cuda.is_available() else "cpu"
            generator = torch.Generator(device=device).manual_seed(seed)
            
            progress(0.2, desc="æ­£åœ¨æ¨ç†...")
            
            # ç”Ÿæˆå›¾åƒ - ä½¿ç”¨ä¸ inference.py ç›¸åŒçš„å‚æ•°
            result = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=int(width),
                height=int(height),
                num_inference_steps=int(steps),
                guidance_scale=float(cfg_scale),
                generator=generator
            )
            
            progress(0.8, desc="æ­£åœ¨ä¿å­˜å›¾åƒ...")
            
            # ä¿å­˜ç”Ÿæˆçš„å›¾åƒ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(self.output_dir, f"generated_{timestamp}_seed_{seed}.png")
            result.images[0].save(output_path)
            
            progress(1.0, desc="å›¾åƒç”Ÿæˆå®Œæˆï¼")
            
            generation_info = f"""âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼
ä¿å­˜è·¯å¾„ï¼š{output_path}
æç¤ºè¯ï¼š{prompt}
è´Ÿé¢æç¤ºè¯ï¼š{negative_prompt}
å°ºå¯¸ï¼š{width}x{height}
æ¨ç†æ­¥æ•°ï¼š{steps}
CFG Scaleï¼š{cfg_scale}
éšæœºç§å­ï¼š{seed}"""
            
            return result.images[0], generation_info
        
        except Exception as e:
            return None, f"âŒ ç”Ÿæˆå›¾åƒæ—¶å‡ºé”™ï¼š{str(e)}"
    
    def get_sample_prompts(self):
        """è·å– Minecraft è§’è‰²çš„ç¤ºä¾‹æç¤ºè¯"""
        prompts = [
            "A powerful wizard in Minecraft style, magical staff, blocky robes, pixelated, voxel art, fantasy theme",
            "A brave knight in Minecraft style, iron armor, diamond sword, blocky design, pixelated, voxel art, medieval theme",
            "A fire mage in Minecraft style, flame staff, orange robes, blocky fire effects, pixelated, voxel art, magic theme",
            "An ice warrior in Minecraft style, frost armor, ice sword, blocky ice crystals, pixelated, voxel art, winter theme",
            "A forest archer in Minecraft style, wooden bow, green cloak, blocky trees background, pixelated, voxel art, nature theme"
        ]
        return prompts

# Initialize the WebUI class
webui = LoRATrainerWebUI()

# Create Gradio interface
def create_interface():
    with gr.Blocks(title="LoRA è®­ç»ƒå™¨ WebUI", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ® LoRA è®­ç»ƒå™¨ WebUI")
        gr.Markdown("å®Œæ•´çš„æ•°æ®é›†å‡†å¤‡ã€LoRAè®­ç»ƒå’Œæ¨ç†å·¥å…·åŒ…")
        
        with gr.Tabs():
            # Tab 1: Dataset Download
            with gr.TabItem("ğŸ“¥ æ•°æ®é›†ä¸‹è½½"):
                gr.Markdown("## ä¸‹è½½å’Œå‡†å¤‡è®­ç»ƒæ•°æ®é›†")
                
                # Add dataset status display
                with gr.Row():
                    dataset_status = gr.Textbox(
                        label="å½“å‰æ•°æ®é›†çŠ¶æ€",
                        value=webui.get_dataset_status(),
                        interactive=False
                    )
                    refresh_status_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€")
                
                with gr.Row():
                    with gr.Column():
                        dataset_name = gr.Textbox(
                            label="æ•°æ®é›†åç§°",
                            value="monadical-labs/minecraft-preview",
                            placeholder="ä¾‹å¦‚ï¼šmonadical-labs/minecraft-preview"
                        )
                        split = gr.Dropdown(
                            choices=["train", "test", "validation"],
                            value="train",
                            label="æ•°æ®é›†åˆ†å‰²"
                        )
                        num_samples = gr.Number(
                            label="æ ·æœ¬æ•°é‡ (0 = å…¨éƒ¨)",
                            value=100,
                            minimum=0
                        )
                        
                        download_btn = gr.Button("ğŸ“¥ ä¸‹è½½æ•°æ®é›†", variant="primary")
                        validate_btn = gr.Button("ğŸ” éªŒè¯æ•°æ®é›†")
                    
                    with gr.Column():
                        download_output = gr.Textbox(
                            label="ä¸‹è½½çŠ¶æ€",
                            lines=10,
                            interactive=False
                        )
                
                download_btn.click(
                    webui.download_dataset,
                    inputs=[dataset_name, split, num_samples],
                    outputs=[download_output]
                )
                
                validate_btn.click(
                    webui.validate_dataset,
                    outputs=[download_output]
                )
                
                refresh_status_btn.click(
                    webui.get_dataset_status,
                    outputs=[dataset_status]
                )
                
                # Add dataset preview section
                gr.Markdown("## æ•°æ®é›†é¢„è§ˆ")
                with gr.Row():
                    with gr.Column():
                        preview_btn = gr.Button("ğŸ‘ï¸ é¢„è§ˆæ•°æ®é›†")
                        preview_gallery = gr.Gallery(
                            label="ç¤ºä¾‹å›¾åƒ",
                            show_label=True,
                            elem_id="preview_gallery",
                            columns=3,
                            rows=2,
                            height="auto"
                        )
                    
                    with gr.Column():
                        preview_captions = gr.Textbox(
                            label="ç¤ºä¾‹æ ‡é¢˜",
                            lines=10,
                            interactive=False,
                            placeholder="ç‚¹å‡»'é¢„è§ˆæ•°æ®é›†'æŸ¥çœ‹ç¤ºä¾‹æ ‡é¢˜..."
                        )
                
                preview_btn.click(
                    webui.get_dataset_preview,
                    outputs=[preview_gallery, preview_captions]
                )
            
            # Tab 2: Training
            with gr.TabItem("ğŸš€ è®­ç»ƒ"):
                gr.Markdown("## LoRA è®­ç»ƒé…ç½®")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### åŸºç¡€é…ç½®")
                        model_name = gr.Textbox(
                            label="åŸºç¡€æ¨¡å‹è·¯å¾„",
                            value="./checkpoints/Qwen-Image",
                            placeholder="åŸºç¡€æ¨¡å‹è·¯å¾„"
                        )
                        learning_rate = gr.Number(
                            label="å­¦ä¹ ç‡",
                            value=1e-4,
                            minimum=1e-6,
                            maximum=1e-2,
                            step=1e-5
                        )
                        batch_size = gr.Number(
                            label="æ‰¹é‡å¤§å°",
                            value=1,
                            minimum=1,
                            maximum=8
                        )
                        epochs = gr.Number(
                            label="è®­ç»ƒè½®æ•°",
                            value=10,
                            minimum=1,
                            maximum=100
                        )
                        resolution = gr.Dropdown(
                            choices=[512, 768, 1024],
                            value=1024,
                            label="è®­ç»ƒåˆ†è¾¨ç‡"
                        )
                        
                        gr.Markdown("### LoRA é…ç½®")
                        lora_rank = gr.Number(
                            label="LoRA ç§©",
                            value=32,
                            minimum=1,
                            maximum=128
                        )
                        lora_alpha = gr.Number(
                            label="LoRA Alpha",
                            value=16,
                            minimum=1,
                            maximum=64
                        )
                        
                        gr.Markdown("### é«˜çº§è®¾ç½®")
                        gradient_accumulation_steps = gr.Number(
                            label="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°",
                            value=1,
                            minimum=1,
                            maximum=8
                        )
                        save_steps = gr.Number(
                            label="ä¿å­˜æ­¥æ•°",
                            value=500,
                            minimum=100,
                            maximum=2000
                        )
                        logging_steps = gr.Number(
                            label="æ—¥å¿—æ­¥æ•°",
                            value=50,
                            minimum=10,
                            maximum=200
                        )
                        
                        train_btn = gr.Button("ğŸš€ å¼€å§‹è®­ç»ƒ", variant="primary")
                    
                    with gr.Column():
                        training_output = gr.Textbox(
                            label="è®­ç»ƒæ—¥å¿—",
                            lines=20,
                            interactive=False
                        )
                
                train_btn.click(
                    webui.start_training,
                    inputs=[model_name, learning_rate, batch_size, epochs, resolution, 
                            lora_rank, lora_alpha, gradient_accumulation_steps, save_steps, logging_steps],
                    outputs=[training_output]
                )
            
            # Tab 3: Inference
            with gr.TabItem("ğŸ¨ æ¨ç†"):
                gr.Markdown("## ä½¿ç”¨è®­ç»ƒå¥½çš„ LoRA ç”Ÿæˆå›¾åƒ")
                
                with gr.Row():
                    with gr.Column():
                        model_path = gr.Textbox(
                            label="æ¨¡å‹è·¯å¾„",
                            value="./checkpoints/Qwen-Image",
                            placeholder="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„"
                        )
                        load_model_btn = gr.Button("ğŸ“‚ åŠ è½½æ¨¡å‹")
                        model_status = gr.Textbox(
                            label="æ¨¡å‹çŠ¶æ€",
                            interactive=False
                        )
                        
                        gr.Markdown("### ç”Ÿæˆè®¾ç½®")
                        prompt = gr.Textbox(
                            label="æç¤ºè¯",
                            lines=3,
                            placeholder="æè¿°ä½ æƒ³è¦ç”Ÿæˆçš„å†…å®¹..."
                        )
                        
                        sample_prompts = gr.Dropdown(
                            choices=webui.get_sample_prompts(),
                            label="ç¤ºä¾‹æç¤ºè¯",
                            interactive=True
                        )
                        
                        negative_prompt = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯",
                            value="blurry, low quality, distorted, ugly, bad anatomy",
                            lines=2
                        )
                        
                        with gr.Row():
                            steps = gr.Slider(
                                label="æ¨ç†æ­¥æ•°",
                                minimum=10,
                                maximum=100,
                                value=30,
                                step=1
                            )
                            cfg_scale = gr.Slider(
                                label="CFG æ¯”ä¾‹",
                                minimum=1.0,
                                maximum=20.0,
                                value=7.5,
                                step=0.5
                            )
                        
                        with gr.Row():
                            seed = gr.Number(
                                label="éšæœºç§å­ (-1 ä¸ºéšæœº)",
                                value=47,
                                minimum=-1
                            )
                            width = gr.Dropdown(
                                choices=[512, 768, 1024],
                                value=1024,
                                label="å®½åº¦"
                            )
                            height = gr.Dropdown(
                                choices=[512, 768, 1024],
                                value=1024,
                                label="é«˜åº¦"
                            )
                        
                        generate_btn = gr.Button("ğŸ¨ ç”Ÿæˆå›¾åƒ", variant="primary")
                    
                    with gr.Column():
                        generated_image = gr.Image(
                            label="ç”Ÿæˆçš„å›¾åƒ",
                            height=400
                        )
                        generation_status = gr.Textbox(
                            label="ç”ŸæˆçŠ¶æ€",
                            interactive=False
                        )
                
                # Event handlers
                load_model_btn.click(
                    webui.load_inference_model,
                    inputs=[model_path],
                    outputs=[model_status]
                )
                
                sample_prompts.change(
                    lambda x: x,
                    inputs=[sample_prompts],
                    outputs=[prompt]
                )
                
                generate_btn.click(
                    webui.generate_image,
                    inputs=[prompt, negative_prompt, steps, cfg_scale, seed, width, height],
                    outputs=[generated_image, generation_status]
                )
        
        gr.Markdown("---")
        gr.Markdown("ğŸ’¡ **æç¤ºï¼š** é¦–å…ˆä¸‹è½½æ•°æ®é›†ï¼Œç„¶åè®­ç»ƒä½ çš„ LoRAï¼Œæœ€åç”¨äºæ¨ç†ç”Ÿæˆï¼")
    
    return demo

if __name__ == "__main__":
    # Add gradio to requirements if not present
    try:
        import gradio
    except ImportError:
        print("æ­£åœ¨å®‰è£… gradio...")
        subprocess.run(["pip", "install", "gradio"], check=True)
    
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        debug=True
    )
