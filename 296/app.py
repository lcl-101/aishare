#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
UniVideo Gradio Web æ¼”ç¤ºç¨‹åº
åŸºäº UniVideo çš„å¤šä»»åŠ¡è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘
"""

import os
import torch
import numpy as np
import yaml
import gradio as gr
from PIL import Image
from pathlib import Path

from diffusers.utils import export_to_video
from diffusers.models.autoencoders.autoencoder_kl_hunyuan_video import AutoencoderKLHunyuanVideo
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler
from transformer_univideo_hunyuan_video import HunyuanVideoTransformer3DModel, TwoLayerMLP
from mllm_encoder import MLLMInContext, MLLMInContextConfig
from pipeline_univideo import UniVideoPipeline, UniVideoPipelineConfig

from utils import pad_image_pil_to_square, load_model


# å…¨å±€å˜é‡å­˜å‚¨ pipeline
pipeline = None
current_variant = None

# æœ¬åœ°æ¨¡å‹è·¯å¾„é…ç½®
LOCAL_MODEL_BASE = "checkpoints"
LOCAL_HUNYUAN_PATH = os.path.join(LOCAL_MODEL_BASE, "HunyuanVideo")
LOCAL_QWEN_PATH = os.path.join(LOCAL_MODEL_BASE, "Qwen2.5-VL-7B-Instruct")
LOCAL_UNIVIDEO_PATH = os.path.join(LOCAL_MODEL_BASE, "UniVideo")

# é»˜è®¤è´Ÿé¢æç¤ºè¯
NEGATIVE_PROMPT = "Bright tones, overexposed, oversharpening, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, walking backwards, computer-generated environment, weak dynamics, distorted and erratic motions, unstable framing and a disorganized composition."

# Demo è·¯å¾„
DEMO_PATH = "demo"

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_PATH = "configs"


def load_pipeline(variant="variant1"):
    """åŠ è½½æŒ‡å®šå˜ä½“çš„ pipeline"""
    global pipeline, current_variant
    
    if pipeline is not None and current_variant == variant:
        return pipeline
    
    # é‡Šæ”¾ä¹‹å‰çš„ pipeline
    if pipeline is not None:
        del pipeline
        torch.cuda.empty_cache()
    
    # é€‰æ‹©é…ç½®æ–‡ä»¶
    if variant == "variant1":
        config_path = os.path.join(CONFIG_PATH, "univideo_qwen2p5vl7b_hidden_hunyuanvideo.yaml")
        transformer_ckpt = os.path.join(LOCAL_UNIVIDEO_PATH, "univideo_qwen2p5vl7b_hidden_hunyuanvideo/model.ckpt")
        mllm_encoder_ckpt = None
    else:
        config_path = os.path.join(CONFIG_PATH, "univideo_qwen2p5vl7b_queries_hunyuanvideo.yaml")
        transformer_ckpt = os.path.join(LOCAL_UNIVIDEO_PATH, "univideo_qwen2p5vl7b_queries_hunyuanvideo/model.ckpt")
        mllm_encoder_ckpt = os.path.join(LOCAL_UNIVIDEO_PATH, "univideo_qwen2p5vl7b_queries_hunyuanvideo/mllm.ckpt")
    
    with open(config_path, "r") as f:
        raw = yaml.safe_load(f)
    
    # ä¿®æ”¹é…ç½®ä»¥ä½¿ç”¨æœ¬åœ°è·¯å¾„
    raw["mllm_config"]["mllm_id"] = LOCAL_QWEN_PATH
    raw["pipeline_config"]["hunyuan_model_id"] = LOCAL_HUNYUAN_PATH
    
    mllm_config = MLLMInContextConfig(**raw["mllm_config"])
    pipe_cfg = UniVideoPipelineConfig(**raw["pipeline_config"])
    
    # åˆ›å»º MLLM encoder
    mllm_encoder = MLLMInContext(mllm_config)
    
    # åŠ è½½ mllm_encoder checkpoint (variant2 éœ€è¦)
    if mllm_encoder_ckpt is not None:
        print(f"[åˆå§‹åŒ–] æ­£åœ¨åŠ è½½ mllm_encoder æ£€æŸ¥ç‚¹: {mllm_encoder_ckpt}")
        mllm_encoder = load_model(mllm_encoder, mllm_encoder_ckpt)
    mllm_encoder.requires_grad_(False)
    mllm_encoder.eval()
    
    # åŠ è½½ VAE
    vae = AutoencoderKLHunyuanVideo.from_pretrained(
        LOCAL_HUNYUAN_PATH,
        subfolder="vae",
        low_cpu_mem_usage=False,
        device_map=None
    )
    vae.eval()
    
    # åŠ è½½ transformer
    qwenvl_txt_dim = 3584
    transformer = HunyuanVideoTransformer3DModel.from_pretrained(
        LOCAL_HUNYUAN_PATH,
        subfolder="transformer",
        low_cpu_mem_usage=False,
        device_map=None,
        text_embed_dim=qwenvl_txt_dim
    )
    transformer.qwen_project_in = TwoLayerMLP(qwenvl_txt_dim, qwenvl_txt_dim * 4, 4096)
    with torch.no_grad():
        torch.nn.init.ones_(transformer.qwen_project_in.ln.weight)
        for layer in transformer.qwen_project_in.mlp:
            if isinstance(layer, torch.nn.Linear):
                torch.nn.init.xavier_uniform_(layer.weight, gain=1.0)
                if layer.bias is not None:
                    torch.nn.init.zeros_(layer.bias)
    
    # åŠ è½½ transformer checkpoint
    def rename_func(state_dict):
        new_state_dict = {}
        for k, v in state_dict.items():
            new_k = k.replace("transformer.", "", 1) if k.startswith("transformer.") else k
            new_state_dict[new_k] = v
        return new_state_dict
    
    print(f"[åˆå§‹åŒ–] æ­£åœ¨åŠ è½½ transformer æ£€æŸ¥ç‚¹: {transformer_ckpt}")
    transformer = load_model(transformer, transformer_ckpt, rename_func=rename_func)
    
    # åŠ è½½ scheduler
    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(
        LOCAL_HUNYUAN_PATH,
        subfolder="scheduler"
    )
    
    # æ„å»º pipeline
    pipeline = UniVideoPipeline(
        transformer=transformer,
        vae=vae,
        scheduler=scheduler,
        mllm_encoder=mllm_encoder,
        univideo_config=pipe_cfg
    ).to(device="cuda", dtype=torch.bfloat16)
    
    current_variant = variant
    print(f"[åˆå§‹åŒ–] Pipeline åŠ è½½å®Œæˆï¼Œä½¿ç”¨å˜ä½“: {variant}")
    
    return pipeline


def process_output(output, output_path):
    """å¤„ç† pipeline è¾“å‡º"""
    # æ–‡æœ¬è¾“å‡º
    if hasattr(output, "text") and output.text is not None:
        return output.text[0] if output.text else ""
    
    # å›¾åƒ/è§†é¢‘è¾“å‡º
    elif hasattr(output, "frames"):
        frames = output.frames[0]  # (F, H, W, C)
        
        if hasattr(frames, "detach"):
            frames = frames.detach().cpu().float().numpy()
        
        F, H, W, C = frames.shape
        
        # å›¾åƒè¾“å‡º
        if F == 1:
            img = frames[0]
            if img.min() < 0:
                img = (img + 1.0) / 2.0
            img = (img * 255).clip(0, 255).astype(np.uint8)
            Image.fromarray(img).save(output_path)
            return output_path
        # è§†é¢‘è¾“å‡º
        else:
            export_to_video(frames, output_path, fps=24)
            return output_path
    
    return None


# ==================== ä»»åŠ¡å¤„ç†å‡½æ•° ====================

def run_understanding(variant, video_file, prompt, max_video_frames, seed):
    """è§†é¢‘ç†è§£ä»»åŠ¡"""
    pipe = load_pipeline(variant)
    
    if video_file is None:
        return "è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶"
    
    # é™åˆ¶è§†é¢‘å¸§æ•°ä»¥é¿å…æ˜¾å­˜ä¸è¶³
    # Qwen2.5-VLå¤„ç†è§†é¢‘æ—¶ä¼šç”Ÿæˆå¤§é‡è§†è§‰tokensï¼Œå¸§æ•°è¿‡å¤šä¼šå¯¼è‡´OOM
    max_frames = int(max_video_frames)
    
    output = pipe(
        prompts=[prompt],
        cond_video_path=video_file,
        num_frames=max_frames,
        height=480,  # é™ä½åˆ†è¾¨ç‡å‡å°‘æ˜¾å­˜å ç”¨
        width=854,
        seed=int(seed),
        task="understanding",
    )
    
    if hasattr(output, "text") and output.text is not None:
        return output.text[0] if output.text else "æ— è¾“å‡º"
    return "æ— è¾“å‡º"


def run_t2v(variant, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed):
    """æ–‡æœ¬ç”Ÿæˆè§†é¢‘"""
    pipe = load_pipeline(variant)
    
    output_path = "outputs/t2v_output.mp4"
    os.makedirs("outputs", exist_ok=True)
    
    output = pipe(
        prompts=[prompt],
        negative_prompt=NEGATIVE_PROMPT,
        height=int(height),
        width=int(width),
        num_frames=int(num_frames),
        num_inference_steps=int(num_steps),
        guidance_scale=float(guidance_scale),
        image_guidance_scale=float(image_guidance_scale),
        seed=int(seed),
        timestep_shift=7.0,
        task="t2v",
    )
    
    return process_output(output, output_path)


def run_i2v(variant, image_file, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed):
    """å›¾åƒç”Ÿæˆè§†é¢‘"""
    pipe = load_pipeline(variant)
    
    if image_file is None:
        return None
    
    output_path = "outputs/i2v_output.mp4"
    os.makedirs("outputs", exist_ok=True)
    
    output = pipe(
        prompts=[prompt],
        negative_prompt=NEGATIVE_PROMPT,
        cond_image_path=image_file,
        height=int(height),
        width=int(width),
        num_frames=int(num_frames),
        num_inference_steps=int(num_steps),
        guidance_scale=float(guidance_scale),
        image_guidance_scale=float(image_guidance_scale),
        seed=int(seed),
        timestep_shift=7.0,
        task="i2v",
    )
    
    return process_output(output, output_path)


def run_i2i_edit(variant, image_file, prompt, height, width, num_steps, guidance_scale, image_guidance_scale, seed):
    """å›¾åƒç¼–è¾‘"""
    pipe = load_pipeline(variant)
    
    if image_file is None:
        return None
    
    output_path = "outputs/i2i_edit_output.jpg"
    os.makedirs("outputs", exist_ok=True)
    
    output = pipe(
        prompts=[prompt],
        negative_prompt=NEGATIVE_PROMPT,
        cond_image_path=image_file,
        height=int(height),
        width=int(width),
        num_frames=1,
        num_inference_steps=int(num_steps),
        guidance_scale=float(guidance_scale),
        image_guidance_scale=float(image_guidance_scale),
        seed=int(seed),
        timestep_shift=7.0,
        task="i2i_edit",
    )
    
    return process_output(output, output_path)


def run_v2v_edit(variant, video_file, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed):
    """è§†é¢‘ç¼–è¾‘"""
    pipe = load_pipeline(variant)
    
    if video_file is None:
        return None
    
    output_path = "outputs/v2v_edit_output.mp4"
    os.makedirs("outputs", exist_ok=True)
    
    output = pipe(
        prompts=[prompt],
        negative_prompt=NEGATIVE_PROMPT,
        cond_video_path=video_file,
        height=int(height),
        width=int(width),
        num_frames=int(num_frames),
        num_inference_steps=int(num_steps),
        guidance_scale=float(guidance_scale),
        image_guidance_scale=float(image_guidance_scale),
        seed=int(seed),
        timestep_shift=7.0,
        task="v2v_edit",
    )
    
    return process_output(output, output_path)


def run_multiid(variant, ref_images, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed):
    """å¤šèº«ä»½ä¸Šä¸‹æ–‡ç”Ÿæˆ"""
    pipe = load_pipeline(variant)
    
    if ref_images is None or len(ref_images) == 0:
        return None
    
    output_path = "outputs/multiid_output.mp4"
    os.makedirs("outputs", exist_ok=True)
    
    # å¤„ç†å‚è€ƒå›¾åƒ (Gallery è¿”å›çš„æ˜¯ [(filepath, caption), ...] æˆ– [filepath, ...])
    pil_images = []
    for img in ref_images:
        if isinstance(img, tuple):
            # Gallery format: (filepath, caption)
            pil_images.append(pad_image_pil_to_square(Image.open(img[0]).convert("RGB")))
        elif isinstance(img, str):
            pil_images.append(pad_image_pil_to_square(Image.open(img).convert("RGB")))
        elif isinstance(img, Image.Image):
            pil_images.append(pad_image_pil_to_square(img.convert("RGB")))
        elif hasattr(img, 'name'):
            pil_images.append(pad_image_pil_to_square(Image.open(img.name).convert("RGB")))
    ref_images_pil_list = [pil_images]
    
    output = pipe(
        prompts=[prompt],
        negative_prompt=NEGATIVE_PROMPT,
        ref_images=ref_images_pil_list,
        height=int(height),
        width=int(width),
        num_frames=int(num_frames),
        num_inference_steps=int(num_steps),
        guidance_scale=float(guidance_scale),
        image_guidance_scale=float(image_guidance_scale),
        seed=int(seed),
        timestep_shift=7.0,
        task="multiid",
    )
    
    return process_output(output, output_path)


def run_iv2v_edit(variant, ref_images, video_file, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed):
    """å›¾åƒ+è§†é¢‘ç¼–è¾‘ (ä¸Šä¸‹æ–‡ V2V)"""
    pipe = load_pipeline(variant)
    
    if ref_images is None or len(ref_images) == 0 or video_file is None:
        return None
    
    output_path = "outputs/iv2v_edit_output.mp4"
    os.makedirs("outputs", exist_ok=True)
    
    # å¤„ç†å‚è€ƒå›¾åƒ (Gallery è¿”å›çš„æ˜¯ [(filepath, caption), ...] æˆ– [filepath, ...])
    pil_images = []
    for img in ref_images:
        if isinstance(img, tuple):
            # Gallery format: (filepath, caption)
            pil_images.append(pad_image_pil_to_square(Image.open(img[0]).convert("RGB")))
        elif isinstance(img, str):
            pil_images.append(pad_image_pil_to_square(Image.open(img).convert("RGB")))
        elif isinstance(img, Image.Image):
            pil_images.append(pad_image_pil_to_square(img.convert("RGB")))
        elif hasattr(img, 'name'):
            pil_images.append(pad_image_pil_to_square(Image.open(img.name).convert("RGB")))
    ref_images_pil_list = [pil_images]
    
    output = pipe(
        prompts=[prompt],
        negative_prompt=NEGATIVE_PROMPT,
        ref_images=ref_images_pil_list,
        cond_video_path=video_file,
        height=int(height),
        width=int(width),
        num_frames=int(num_frames),
        num_inference_steps=int(num_steps),
        guidance_scale=float(guidance_scale),
        image_guidance_scale=float(image_guidance_scale),
        seed=int(seed),
        timestep_shift=7.0,
        task="i+v2v_edit",
    )
    
    return process_output(output, output_path)


# ==================== æ„å»º Gradio ç•Œé¢ ====================

def create_header():
    """åˆ›å»ºé¡µé¢å¤´éƒ¨"""
    return gr.HTML("""
    <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 10px; margin-bottom: 20px;">
        <h1 style="color: white; margin: 0; font-size: 2.5em;">ğŸ¬ UniVideo æ¼”ç¤º</h1>
        <p style="color: #f0f0f0; margin: 10px 0 0 0; font-size: 1.1em;">ç»Ÿä¸€çš„è§†é¢‘ç”Ÿæˆä¸ç†è§£æ¨¡å‹</p>
        <div style="margin-top: 15px; padding: 10px; background: rgba(255,255,255,0.2); border-radius: 8px;">
            <p style="color: white; margin: 0; font-size: 1em;">
                ğŸ“º <strong>AI æŠ€æœ¯åˆ†äº«é¢‘é“</strong> - 
                <a href="https://www.youtube.com/@rongyikanshijie-ai" target="_blank" style="color: #ffeb3b; text-decoration: none;">
                    https://www.youtube.com/@rongyikanshijie-ai
                </a>
            </p>
        </div>
    </div>
    """)


def create_variant_selector():
    """åˆ›å»ºæ¨¡å‹å˜ä½“é€‰æ‹©å™¨"""
    return gr.Radio(
        choices=[("å˜ä½“1 (éšè—çŠ¶æ€)", "variant1"), ("å˜ä½“2 (æŸ¥è¯¢å‘é‡)", "variant2")],
        value="variant1",
        label="æ¨¡å‹å˜ä½“",
        info="å˜ä½“1: å›¾åƒ/è§†é¢‘/æ–‡æœ¬ â†’ MLLM â†’ æœ€åä¸€å±‚éšè—çŠ¶æ€ â†’ MMDiT | å˜ä½“2: å›¾åƒ/è§†é¢‘/æ–‡æœ¬/æŸ¥è¯¢ â†’ MLLM â†’ æ–‡æœ¬+æŸ¥è¯¢éšè—çŠ¶æ€ â†’ MMDiT"
    )


def create_understanding_tab():
    """åˆ›å»ºè§†é¢‘ç†è§£æ ‡ç­¾é¡µ"""
    with gr.TabItem("ğŸ¥ è§†é¢‘ç†è§£"):
        gr.Markdown("### è§†é¢‘ç†è§£\nä¸Šä¼ è§†é¢‘ï¼Œè®© AI æè¿°è§†é¢‘å†…å®¹")
        
        with gr.Row():
            with gr.Column():
                variant = create_variant_selector()
                video_input = gr.Video(label="è¾“å…¥è§†é¢‘")
                prompt = gr.Textbox(
                    label="æç¤ºè¯",
                    value="Describe this video in detail",
                    lines=2
                )
                max_video_frames = gr.Slider(
                    label="æœ€å¤§è§†é¢‘å¸§æ•° (å¸§æ•°è¶Šå°‘æ˜¾å­˜å ç”¨è¶Šå°)",
                    minimum=5,
                    maximum=65,
                    step=4,
                    value=17,
                    info="å»ºè®®: 17å¸§çº¦éœ€40GBæ˜¾å­˜ï¼Œ33å¸§çº¦éœ€80GBæ˜¾å­˜"
                )
                seed = gr.Number(label="éšæœºç§å­", value=42)
                run_btn = gr.Button("ğŸš€ å¼€å§‹åˆ†æ", variant="primary")
            
            with gr.Column():
                output_text = gr.Textbox(label="åˆ†æç»“æœ", lines=10)
        
        # ç¤ºä¾‹
        gr.Examples(
            examples=[
                [os.path.join(DEMO_PATH, "understanding/1.mp4"), "Describe this video in detail", 17, 42]
            ],
            inputs=[video_input, prompt, max_video_frames, seed],
            label="ç¤ºä¾‹"
        )
        
        run_btn.click(
            fn=run_understanding,
            inputs=[variant, video_input, prompt, max_video_frames, seed],
            outputs=output_text
        )


def create_t2v_tab():
    """åˆ›å»ºæ–‡æœ¬ç”Ÿæˆè§†é¢‘æ ‡ç­¾é¡µ"""
    with gr.TabItem("ğŸ“ æ–‡æœ¬ç”Ÿæˆè§†é¢‘"):
        gr.Markdown("### æ–‡æœ¬ç”Ÿæˆè§†é¢‘\næ ¹æ®æ–‡å­—æè¿°ç”Ÿæˆè§†é¢‘")
        
        with gr.Row():
            with gr.Column():
                variant = create_variant_selector()
                prompt = gr.Textbox(
                    label="æç¤ºè¯",
                    value="a stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.",
                    lines=4
                )
                
                with gr.Row():
                    height = gr.Number(label="é«˜åº¦", value=480)
                    width = gr.Number(label="å®½åº¦", value=854)
                    num_frames = gr.Number(label="å¸§æ•°", value=61)
                
                with gr.Row():
                    num_steps = gr.Slider(label="æ¨ç†æ­¥æ•°", minimum=10, maximum=100, value=30, step=1)
                    guidance_scale = gr.Slider(label="å¼•å¯¼ç³»æ•°", minimum=1, maximum=20, value=6.0, step=0.5)
                
                with gr.Row():
                    image_guidance_scale = gr.Slider(label="å›¾åƒå¼•å¯¼ç³»æ•°", minimum=0, maximum=10, value=1.0, step=0.5)
                    seed = gr.Number(label="éšæœºç§å­", value=42)
                
                run_btn = gr.Button("ğŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary")
            
            with gr.Column():
                output_video = gr.Video(label="ç”Ÿæˆç»“æœ")
        
        # ç¤ºä¾‹
        gr.Examples(
            examples=[
                ["a stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.", 480, 854, 61, 30, 6.0, 1.0, 42]
            ],
            inputs=[prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed],
            label="ç¤ºä¾‹"
        )
        
        run_btn.click(
            fn=run_t2v,
            inputs=[variant, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed],
            outputs=output_video
        )


def create_i2v_tab():
    """åˆ›å»ºå›¾åƒç”Ÿæˆè§†é¢‘æ ‡ç­¾é¡µ"""
    with gr.TabItem("ğŸ–¼ï¸ å›¾åƒç”Ÿæˆè§†é¢‘"):
        gr.Markdown("### å›¾åƒç”Ÿæˆè§†é¢‘\næ ¹æ®å›¾åƒå’Œæ–‡å­—æè¿°ç”Ÿæˆè§†é¢‘")
        
        with gr.Row():
            with gr.Column():
                variant = create_variant_selector()
                image_input = gr.Image(label="è¾“å…¥å›¾åƒ", type="filepath")
                prompt = gr.Textbox(
                    label="æç¤ºè¯",
                    value="The video shows a small capybara wearing round glasses, holding a book titled 'UniVideo' on its cover. The capybara keeps the book lifted in front of its face, gently turning pages as it reads, its head making small, focused nods that match the rhythm of careful study. Its posture remains steady as both paws grip the book, and its ears tilt slightly with each subtle movement. Soft, warm lighting and a simple blurred background stay secondary to the close-up focus on the capybara, its glasses, and the reading motion.",
                    lines=4
                )
                
                with gr.Row():
                    height = gr.Number(label="é«˜åº¦", value=480)
                    width = gr.Number(label="å®½åº¦", value=854)
                    num_frames = gr.Number(label="å¸§æ•°", value=129)
                
                with gr.Row():
                    num_steps = gr.Slider(label="æ¨ç†æ­¥æ•°", minimum=10, maximum=100, value=30, step=1)
                    guidance_scale = gr.Slider(label="å¼•å¯¼ç³»æ•°", minimum=1, maximum=20, value=5.0, step=0.5)
                
                with gr.Row():
                    image_guidance_scale = gr.Slider(label="å›¾åƒå¼•å¯¼ç³»æ•°", minimum=0, maximum=10, value=1.0, step=0.5)
                    seed = gr.Number(label="éšæœºç§å­", value=42)
                
                run_btn = gr.Button("ğŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary")
            
            with gr.Column():
                output_video = gr.Video(label="ç”Ÿæˆç»“æœ")
        
        # ç¤ºä¾‹
        gr.Examples(
            examples=[
                [os.path.join(DEMO_PATH, "i2v/1.png"), "The video shows a small capybara wearing round glasses, holding a book titled 'UniVideo' on its cover. The capybara keeps the book lifted in front of its face, gently turning pages as it reads, its head making small, focused nods that match the rhythm of careful study. Its posture remains steady as both paws grip the book, and its ears tilt slightly with each subtle movement. Soft, warm lighting and a simple blurred background stay secondary to the close-up focus on the capybara, its glasses, and the reading motion.", 480, 854, 129, 30, 5.0, 1.0, 42]
            ],
            inputs=[image_input, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed],
            label="ç¤ºä¾‹"
        )
        
        run_btn.click(
            fn=run_i2v,
            inputs=[variant, image_input, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed],
            outputs=output_video
        )


def create_i2i_edit_tab():
    """åˆ›å»ºå›¾åƒç¼–è¾‘æ ‡ç­¾é¡µ"""
    with gr.TabItem("âœï¸ å›¾åƒç¼–è¾‘"):
        gr.Markdown("### å›¾åƒç¼–è¾‘\næ ¹æ®æ–‡å­—æŒ‡ä»¤ç¼–è¾‘å›¾åƒ")
        
        with gr.Row():
            with gr.Column():
                variant = create_variant_selector()
                image_input = gr.Image(label="è¾“å…¥å›¾åƒ", type="filepath")
                prompt = gr.Textbox(
                    label="ç¼–è¾‘æç¤ºè¯",
                    value="Change the background to dessert.",
                    lines=2
                )
                
                with gr.Row():
                    height = gr.Number(label="é«˜åº¦", value=480)
                    width = gr.Number(label="å®½åº¦", value=832)
                
                with gr.Row():
                    num_steps = gr.Slider(label="æ¨ç†æ­¥æ•°", minimum=10, maximum=100, value=50, step=1)
                    guidance_scale = gr.Slider(label="å¼•å¯¼ç³»æ•°", minimum=1, maximum=20, value=7.0, step=0.5)
                
                with gr.Row():
                    image_guidance_scale = gr.Slider(label="å›¾åƒå¼•å¯¼ç³»æ•°", minimum=0, maximum=10, value=2.0, step=0.5)
                    seed = gr.Number(label="éšæœºç§å­", value=42)
                
                run_btn = gr.Button("ğŸš€ å¼€å§‹ç¼–è¾‘", variant="primary")
            
            with gr.Column():
                output_image = gr.Image(label="ç¼–è¾‘ç»“æœ")
        
        # ç¤ºä¾‹
        gr.Examples(
            examples=[
                [os.path.join(DEMO_PATH, "i2i_edit/1.jpg"), "Change the background to dessert.", 480, 832, 50, 7.0, 2.0, 42]
            ],
            inputs=[image_input, prompt, height, width, num_steps, guidance_scale, image_guidance_scale, seed],
            label="ç¤ºä¾‹"
        )
        
        run_btn.click(
            fn=run_i2i_edit,
            inputs=[variant, image_input, prompt, height, width, num_steps, guidance_scale, image_guidance_scale, seed],
            outputs=output_image
        )


def create_v2v_edit_tab():
    """åˆ›å»ºè§†é¢‘ç¼–è¾‘æ ‡ç­¾é¡µ"""
    with gr.TabItem("ğŸ¬ è§†é¢‘ç¼–è¾‘"):
        gr.Markdown("### è§†é¢‘ç¼–è¾‘\næ ¹æ®æ–‡å­—æŒ‡ä»¤ç¼–è¾‘è§†é¢‘")
        
        with gr.Row():
            with gr.Column():
                variant = create_variant_selector()
                video_input = gr.Video(label="è¾“å…¥è§†é¢‘")
                prompt = gr.Textbox(
                    label="ç¼–è¾‘æç¤ºè¯",
                    value="Change the man to look like he is sculpted from chocolate.",
                    lines=2
                )
                
                with gr.Row():
                    height = gr.Number(label="é«˜åº¦", value=480)
                    width = gr.Number(label="å®½åº¦", value=854)
                    num_frames = gr.Number(label="å¸§æ•°", value=129)
                
                with gr.Row():
                    num_steps = gr.Slider(label="æ¨ç†æ­¥æ•°", minimum=10, maximum=100, value=50, step=1)
                    guidance_scale = gr.Slider(label="å¼•å¯¼ç³»æ•°", minimum=1, maximum=20, value=7.0, step=0.5)
                
                with gr.Row():
                    image_guidance_scale = gr.Slider(label="å›¾åƒå¼•å¯¼ç³»æ•°", minimum=0, maximum=10, value=2.0, step=0.5)
                    seed = gr.Number(label="éšæœºç§å­", value=42)
                
                run_btn = gr.Button("ğŸš€ å¼€å§‹ç¼–è¾‘", variant="primary")
            
            with gr.Column():
                output_video = gr.Video(label="ç¼–è¾‘ç»“æœ")
        
        # ç¤ºä¾‹
        gr.Examples(
            examples=[
                [os.path.join(DEMO_PATH, "v2v_edit/video.mp4"), "Change the man to look like he is sculpted from chocolate.", 480, 854, 129, 50, 7.0, 2.0, 42]
            ],
            inputs=[video_input, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed],
            label="ç¤ºä¾‹"
        )
        
        run_btn.click(
            fn=run_v2v_edit,
            inputs=[variant, video_input, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed],
            outputs=output_video
        )


def create_multiid_tab():
    """åˆ›å»ºå¤šèº«ä»½ä¸Šä¸‹æ–‡ç”Ÿæˆæ ‡ç­¾é¡µ"""
    with gr.TabItem("ğŸ‘¥ å¤šèº«ä»½ç”Ÿæˆ"):
        gr.Markdown("### å¤šèº«ä»½ä¸Šä¸‹æ–‡ç”Ÿæˆ\nä¸Šä¼ å¤šå¼ å‚è€ƒå›¾åƒï¼Œç”ŸæˆåŒ…å«è¿™äº›èº«ä»½çš„è§†é¢‘")
        
        with gr.Row():
            with gr.Column():
                variant = create_variant_selector()
                ref_images = gr.Gallery(
                    label="å‚è€ƒå›¾åƒ (å¯ä¸Šä¼ å¤šå¼ ï¼Œç‚¹å‡»ä¸Šä¼ æˆ–æ‹–æ‹½)",
                    columns=3,
                    rows=1,
                    height="auto",
                    interactive=True,
                    type="filepath"
                )
                prompt = gr.Textbox(
                    label="æç¤ºè¯",
                    value="A man with short, light brown hair and light skin, now dressed in a vibrant Hawaiian shirt with a colorful floral pattern, sits comfortably on a beach lounge chair. On his right shoulder, a fluffy, yellow Pikachu with a small detective hat perches, looking alertly at the camera. The man holds an ice cream cone piled high with vanilla ice cream and colorful sprinkles, taking a bite with a relaxed, happy expression. His smile is gentle and content, reflecting the ease of the moment. The camera slowly circles around them, capturing the leisurely scene from various perspectives.",
                    lines=4
                )
                
                with gr.Row():
                    height = gr.Number(label="é«˜åº¦", value=480)
                    width = gr.Number(label="å®½åº¦", value=832)
                    num_frames = gr.Number(label="å¸§æ•°", value=129)
                
                with gr.Row():
                    num_steps = gr.Slider(label="æ¨ç†æ­¥æ•°", minimum=10, maximum=100, value=50, step=1)
                    guidance_scale = gr.Slider(label="å¼•å¯¼ç³»æ•°", minimum=1, maximum=20, value=5.0, step=0.5)
                
                with gr.Row():
                    image_guidance_scale = gr.Slider(label="å›¾åƒå¼•å¯¼ç³»æ•°", minimum=0, maximum=10, value=3.0, step=0.5)
                    seed = gr.Number(label="éšæœºç§å­", value=42)
                
                run_btn = gr.Button("ğŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary")
            
            with gr.Column():
                output_video = gr.Video(label="ç”Ÿæˆç»“æœ")
        
        # ç¤ºä¾‹
        example_images = [
            os.path.join(DEMO_PATH, "in-context-generation/1.png"),
            os.path.join(DEMO_PATH, "in-context-generation/2.png"),
            os.path.join(DEMO_PATH, "in-context-generation/3.jpg")
        ]
        gr.Examples(
            examples=[
                [
                    example_images,
                    "A man with short, light brown hair and light skin, now dressed in a vibrant Hawaiian shirt with a colorful floral pattern, sits comfortably on a beach lounge chair. On his right shoulder, a fluffy, yellow Pikachu with a small detective hat perches, looking alertly at the camera. The man holds an ice cream cone piled high with vanilla ice cream and colorful sprinkles, taking a bite with a relaxed, happy expression. His smile is gentle and content, reflecting the ease of the moment. The camera slowly circles around them, capturing the leisurely scene from various perspectives.",
                    480, 832, 129, 50, 5.0, 3.0, 42
                ]
            ],
            inputs=[ref_images, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed],
            label="ç¤ºä¾‹"
        )
        
        run_btn.click(
            fn=run_multiid,
            inputs=[variant, ref_images, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed],
            outputs=output_video
        )


def create_iv2v_edit_tab():
    """åˆ›å»ºå›¾åƒ+è§†é¢‘ç¼–è¾‘æ ‡ç­¾é¡µ"""
    with gr.TabItem("ğŸ”„ ä¸Šä¸‹æ–‡è§†é¢‘ç¼–è¾‘"):
        gr.Markdown("### ä¸Šä¸‹æ–‡è§†é¢‘ç¼–è¾‘ (å›¾åƒ+è§†é¢‘)\nä½¿ç”¨å‚è€ƒå›¾åƒå¯¹è§†é¢‘è¿›è¡Œç¼–è¾‘ï¼Œä¾‹å¦‚èº«ä»½æ›¿æ¢")
        
        with gr.Row():
            with gr.Column():
                variant = create_variant_selector()
                ref_images = gr.Gallery(
                    label="å‚è€ƒå›¾åƒ (å¯ä¸Šä¼ å¤šå¼ ï¼Œç‚¹å‡»ä¸Šä¼ æˆ–æ‹–æ‹½)",
                    columns=3,
                    rows=1,
                    height="auto",
                    interactive=True,
                    type="filepath"
                )
                video_input = gr.Video(label="è¾“å…¥è§†é¢‘")
                prompt = gr.Textbox(
                    label="ç¼–è¾‘æç¤ºè¯",
                    value="Use the man's face in the reference image to replace the man's face in the video.",
                    lines=2
                )
                
                with gr.Row():
                    height = gr.Number(label="é«˜åº¦", value=480)
                    width = gr.Number(label="å®½åº¦", value=832)
                    num_frames = gr.Number(label="å¸§æ•° (å¯¼å‡º24fps: 137å¸§â‰ˆ5.7s)", value=137)
                
                with gr.Row():
                    num_steps = gr.Slider(label="æ¨ç†æ­¥æ•°", minimum=10, maximum=100, value=50, step=1)
                    guidance_scale = gr.Slider(label="å¼•å¯¼ç³»æ•°", minimum=1, maximum=20, value=7.0, step=0.5)
                
                with gr.Row():
                    image_guidance_scale = gr.Slider(label="å›¾åƒå¼•å¯¼ç³»æ•°", minimum=0, maximum=10, value=2.0, step=0.5)
                    seed = gr.Number(label="éšæœºç§å­", value=42)
                
                run_btn = gr.Button("ğŸš€ å¼€å§‹ç¼–è¾‘", variant="primary")
            
            with gr.Column():
                output_video = gr.Video(label="ç¼–è¾‘ç»“æœ")
        
        # ç¤ºä¾‹
        gr.Examples(
            examples=[
                [
                    [os.path.join(DEMO_PATH, "in-context-v2v/id_swap/ID.jpeg")],
                    os.path.join(DEMO_PATH, "in-context-v2v/id_swap/origin.mp4"),
                    "Use the man's face in the reference image to replace the man's face in the video.",
                    480, 832, 137, 50, 7.0, 2.0, 42
                ]
            ],
            inputs=[ref_images, video_input, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed],
            label="ç¤ºä¾‹"
        )
        
        run_btn.click(
            fn=run_iv2v_edit,
            inputs=[variant, ref_images, video_input, prompt, height, width, num_frames, num_steps, guidance_scale, image_guidance_scale, seed],
            outputs=output_video
        )


def create_app():
    """åˆ›å»º Gradio åº”ç”¨"""
    with gr.Blocks(
        title="UniVideo æ¼”ç¤º",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1400px !important;
        }
        """
    ) as app:
        create_header()
        
        gr.Markdown("""
        ### ğŸ“– ä½¿ç”¨è¯´æ˜
        
        1. **é€‰æ‹©æ¨¡å‹å˜ä½“**: åœ¨æ¯ä¸ªä»»åŠ¡æ ‡ç­¾é¡µä¸­ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨å˜ä½“1æˆ–å˜ä½“2
           - **å˜ä½“1**: å›¾åƒ/è§†é¢‘/æ–‡æœ¬ â†’ MLLM â†’ æœ€åä¸€å±‚éšè—çŠ¶æ€ â†’ MMDiT
           - **å˜ä½“2**: å›¾åƒ/è§†é¢‘/æ–‡æœ¬/æŸ¥è¯¢ â†’ MLLM â†’ æ–‡æœ¬+æŸ¥è¯¢éšè—çŠ¶æ€ â†’ MMDiT
        
        2. **ä¸Šä¼ è¾“å…¥**: æ ¹æ®ä»»åŠ¡ç±»å‹ä¸Šä¼ å›¾åƒæˆ–è§†é¢‘
        
        3. **è®¾ç½®å‚æ•°**: è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼ˆå¯é€‰ï¼‰
        
        4. **ç‚¹å‡»ç”Ÿæˆ**: ç­‰å¾…ç»“æœç”Ÿæˆ
        
        ---
        """)
        
        with gr.Tabs():
            create_understanding_tab()
            create_t2v_tab()
            create_i2v_tab()
            create_i2i_edit_tab()
            create_v2v_edit_tab()
            create_multiid_tab()
            create_iv2v_edit_tab()
        
        gr.Markdown("""
        ---
        ### âš ï¸ æ³¨æ„äº‹é¡¹
        
        - é¦–æ¬¡è¿è¡Œæ—¶ï¼Œæ¨¡å‹åŠ è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        - è§†é¢‘ç”Ÿæˆéœ€è¦å¤§é‡æ˜¾å­˜ï¼Œå»ºè®®ä½¿ç”¨å…·æœ‰ 24GB+ æ˜¾å­˜çš„ GPU
        - åˆ‡æ¢æ¨¡å‹å˜ä½“ä¼šé‡æ–°åŠ è½½æ¨¡å‹
        
        ---
        <div style="text-align: center; color: #888; padding: 20px;">
            Powered by UniVideo | 
            <a href="https://www.youtube.com/@rongyikanshijie-ai" target="_blank">AI æŠ€æœ¯åˆ†äº«é¢‘é“</a>
        </div>
        """)
    
    return app


if __name__ == "__main__":
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("outputs", exist_ok=True)
    
    # å¯åŠ¨åº”ç”¨
    app = create_app()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
