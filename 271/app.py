#!/usr/bin/env python3
"""
Gradio WebUI for HY-WorldPlay Video Generation
æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. äº¤äº’æ¨¡å¼ - ä½¿ç”¨WASDæŒ‰é”®å®žæ—¶æŽ§åˆ¶ç›¸æœº
2. è½¨è¿¹æ¨¡å¼ - ä½¿ç”¨é¢„å®šä¹‰çš„ç›¸æœºè½¨è¿¹JSONç”Ÿæˆè§†é¢‘

æ­¤æ–‡ä»¶æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œä¸éœ€è¦ä¿®æ”¹åŽŸå§‹ä»£ç åº“ä¸­çš„ä»»ä½•æ–‡ä»¶ã€‚
é€šè¿‡ monkey patching åœ¨è¿è¡Œæ—¶ä¿®å¤ pipeline ä¸­çš„é—®é¢˜ã€‚
"""

import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

import gradio as gr
import torch
import json
import numpy as np
import imageio
import einops
from pathlib import Path
import tempfile
from datetime import datetime
from collections import deque
from PIL import Image


# ============== Monkey Patching ä¿®å¤ Pipeline ==============
# åœ¨å¯¼å…¥ pipeline ä¹‹å‰ï¼Œå…ˆå‡†å¤‡å¥½è¡¥ä¸

def apply_pipeline_patches():
    """
    åº”ç”¨è¡¥ä¸ä¿®å¤ worldplay_video_pipeline.py ä¸­çš„é—®é¢˜ï¼š
    1. action tensor å¿…é¡»æ˜¯ 1Dï¼ˆ.reshape(-1)ï¼‰ï¼Œå¦åˆ™ action_in æ¨¡å—ä¼šæŠ¥é”™
    2. ç¡®ä¿ viewmats, Ks, action åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    """
    from hyvideo.pipelines import worldplay_video_pipeline
    from hyvideo.commons import auto_offload_model
    from hyvideo.utils.retrieval_context import select_aligned_memory_frames
    
    def patched_ar_rollout(self, latents, timesteps, prompt_embeds, prompt_mask, 
                           vision_states, cond_latents, task_type, extra_kwargs,
                           viewmats, Ks, action, device):
        """
        ä¿®å¤åŽçš„ ar_rollout æ–¹æ³•
        ä¸»è¦ä¿®å¤: action tensor å¿…é¡»æ˜¯ 1D (.reshape(-1))
        """
        self.init_kv_cache()
        positive_idx = 1 if self.do_classifier_free_guidance else 0
        stabilization_level = 15
        
        # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        if viewmats is not None:
            viewmats = viewmats.to(device)
        if Ks is not None:
            Ks = Ks.to(device)
        if action is not None:
            action = action.to(device)
        
        # text, siglip, byt5 embedding cache
        with (torch.autocast(device_type="cuda", dtype=self.target_dtype, enabled=self.autocast_enabled),
              auto_offload_model(self.transformer, self.execution_device, enabled=self.enable_offloading)):
            extra_kwargs_pos = {
                "byt5_text_states": extra_kwargs["byt5_text_states"][positive_idx, None, ...],
                "byt5_text_mask": extra_kwargs["byt5_text_mask"][positive_idx, None, ...],
            }
            t_expand_txt = torch.tensor([0]).to(device).to(latents.dtype)
            self._kv_cache = self.transformer(
                bi_inference=False,
                ar_txt_inference=True,
                ar_vision_inference=False,
                timestep_txt=t_expand_txt,
                text_states=prompt_embeds[positive_idx, None, ...],
                encoder_attention_mask=prompt_mask[positive_idx, None, ...],
                vision_states=vision_states[positive_idx, None, ...],
                mask_type=task_type,
                extra_kwargs=extra_kwargs_pos,
                kv_cache=self._kv_cache,
                cache_txt=True,
            )
            if self.do_classifier_free_guidance:
                extra_kwargs_neg = {
                    "byt5_text_states": extra_kwargs["byt5_text_states"][0, None, ...],
                    "byt5_text_mask": extra_kwargs["byt5_text_mask"][0, None, ...],
                }
                t_expand_txt = torch.tensor([0]).to(device).to(latents.dtype)
                self._kv_cache_neg = self.transformer(
                    bi_inference=False,
                    ar_txt_inference=True,
                    ar_vision_inference=False,
                    timestep_txt=t_expand_txt,
                    text_states=prompt_embeds[0, None, ...],
                    encoder_attention_mask=prompt_mask[0, None, ...],
                    vision_states=vision_states[0, None, ...],
                    mask_type=task_type,
                    extra_kwargs=extra_kwargs_neg,
                    kv_cache=self._kv_cache_neg,
                    cache_txt=True,
                )

        selected_frame_indices = []

        for chunk_i in range(self.chunk_num):
            if chunk_i > 0:
                current_frame_idx = chunk_i * self.chunk_latent_frames

                selected_frame_indices = []
                for chunk_start_idx in range(current_frame_idx, current_frame_idx + self.chunk_latent_frames, 4):
                    selected_history_frame_id = select_aligned_memory_frames(
                        viewmats[0].cpu().detach().numpy(),
                        chunk_start_idx,
                        memory_frames=20,
                        temporal_context_size=12,
                        pred_latent_size=4,
                        points_local=self.points_local,
                        device=device)
                    selected_frame_indices += selected_history_frame_id
                selected_frame_indices = sorted(list(set(selected_frame_indices)))
                to_remove = list(range(current_frame_idx, current_frame_idx + self.chunk_latent_frames))
                selected_frame_indices = [x for x in selected_frame_indices if x not in to_remove]

                context_latents = latents[:, :, selected_frame_indices]
                context_cond_latents_input = cond_latents[:, :, selected_frame_indices]
                context_latents_input = torch.concat([context_latents, context_cond_latents_input], dim=1)

                context_viewmats = viewmats[:, selected_frame_indices].to(device)
                context_Ks = Ks[:, selected_frame_indices].to(device)
                # å…³é”®ä¿®å¤: action å¿…é¡»æ˜¯ 1D tensor
                context_action = action[:, selected_frame_indices].reshape(-1).to(device)

                context_timestep = torch.full((len(selected_frame_indices),), stabilization_level - 1,
                                              device=device, dtype=timesteps.dtype)
                # compute kv cache
                with (torch.autocast(device_type="cuda", dtype=self.target_dtype, enabled=self.autocast_enabled),
                      auto_offload_model(self.transformer, self.execution_device, enabled=self.enable_offloading)):
                    self._kv_cache = self.transformer(
                        bi_inference=False,
                        ar_txt_inference=False,
                        ar_vision_inference=True,
                        hidden_states=context_latents_input,
                        timestep=context_timestep,
                        timestep_r=None,
                        mask_type=task_type,
                        return_dict=False,
                        viewmats=context_viewmats.to(self.target_dtype),
                        Ks=context_Ks.to(self.target_dtype),
                        action=context_action.to(self.target_dtype),
                        kv_cache=self._kv_cache,
                        cache_vision=True,
                        rope_temporal_size=context_latents_input.shape[2],
                        start_rope_start_idx=0,
                    )
                    if self.do_classifier_free_guidance:
                        self._kv_cache_neg = self.transformer(
                            bi_inference=False,
                            ar_txt_inference=False,
                            ar_vision_inference=True,
                            hidden_states=context_latents_input,
                            timestep=context_timestep,
                            timestep_r=None,
                            mask_type=task_type,
                            return_dict=False,
                            viewmats=context_viewmats.to(self.target_dtype),
                            Ks=context_Ks.to(self.target_dtype),
                            action=context_action.to(self.target_dtype),
                            kv_cache=self._kv_cache_neg,
                            cache_vision=True,
                            rope_temporal_size=context_latents_input.shape[2],
                            start_rope_start_idx=0,
                        )

                self.scheduler.set_timesteps(self.num_inference_steps, device=device)

            start_idx = chunk_i * self.chunk_latent_frames
            end_idx = chunk_i * self.chunk_latent_frames + self.chunk_latent_frames

            with self.progress_bar(total=self.num_inference_steps) as progress_bar, \
                 auto_offload_model(self.transformer, self.execution_device, enabled=self.enable_offloading):
                for i, t in enumerate(timesteps):
                    timestep_input = torch.full((self.chunk_latent_frames,), t, device=device,
                                                dtype=timesteps.dtype)
                    latent_model_input = latents[:, :, start_idx: end_idx]
                    cond_latents_input = cond_latents[:, :, start_idx: end_idx]

                    viewmats_input = viewmats[:, start_idx: end_idx].to(device)
                    Ks_input = Ks[:, start_idx: end_idx].to(device)
                    # å…³é”®ä¿®å¤: action å¿…é¡»æ˜¯ 1D tensor
                    action_input = action[:, start_idx: end_idx].reshape(-1).to(device)

                    latents_concat = torch.concat([latent_model_input, cond_latents_input], dim=1)
                    latents_concat = self.scheduler.scale_model_input(latents_concat, t)

                    with torch.autocast(device_type="cuda", dtype=self.target_dtype, enabled=self.autocast_enabled):
                        noise_pred = self.transformer(
                            bi_inference=False,
                            ar_txt_inference=False,
                            ar_vision_inference=True,
                            hidden_states=latents_concat,
                            timestep=timestep_input,
                            timestep_r=None,
                            mask_type=task_type,
                            return_dict=False,
                            viewmats=viewmats_input.to(self.target_dtype),
                            Ks=Ks_input.to(self.target_dtype),
                            action=action_input.to(self.target_dtype),
                            kv_cache=self._kv_cache,
                            cache_vision=False,
                            rope_temporal_size=latents_concat.shape[2] + len(selected_frame_indices),
                            start_rope_start_idx=len(selected_frame_indices),
                        )[0]
                        if self.do_classifier_free_guidance:
                            noise_pred_uncond = self.transformer(
                                bi_inference=False,
                                ar_txt_inference=False,
                                ar_vision_inference=True,
                                hidden_states=latents_concat,
                                timestep=timestep_input,
                                timestep_r=None,
                                mask_type=task_type,
                                return_dict=False,
                                viewmats=viewmats_input.to(self.target_dtype),
                                Ks=Ks_input.to(self.target_dtype),
                                action=action_input.to(self.target_dtype),
                                kv_cache=self._kv_cache_neg,
                                cache_vision=False,
                                rope_temporal_size=latents_concat.shape[2] + len(selected_frame_indices),
                                start_rope_start_idx=len(selected_frame_indices),
                            )[0]

                    if self.do_classifier_free_guidance:
                        noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred - noise_pred_uncond)

                    latent_model_input = self.scheduler.step(noise_pred, t, latent_model_input, return_dict=False)[0]
                    latents[:, :, start_idx: end_idx] = latent_model_input[:, :, -self.chunk_latent_frames:]

                    if i == len(timesteps) - 1 or ((i + 1) > self.num_warmup_steps
                                                   and (i + 1) % self.scheduler.order == 0):
                        if progress_bar is not None:
                            progress_bar.update()
        
        return latents
    
    def patched_bi_rollout(self, latents, timesteps, prompt_embeds, prompt_mask, 
                           vision_states, cond_latents, task_type, extra_kwargs,
                           viewmats, Ks, action, device):
        """
        ä¿®å¤åŽçš„ bi_rollout æ–¹æ³•
        ä¸»è¦ä¿®å¤: ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        """
        from hyvideo.commons import auto_offload_model
        from hyvideo.utils.retrieval_context import select_aligned_memory_frames
        from hyvideo.pipelines.pipeline_utils import rescale_noise_cfg
        from einops import repeat
        
        # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        if viewmats is not None:
            viewmats = viewmats.to(device)
        if Ks is not None:
            Ks = Ks.to(device)
        if action is not None:
            action = action.to(device)
        
        stabilization_level = 15
        for chunk_i in range(self.chunk_num):
            if chunk_i > 0:
                current_frame_idx = chunk_i * self.chunk_latent_frames

                selected_frame_indices = []
                for chunk_start_idx in range(current_frame_idx, current_frame_idx + self.chunk_latent_frames, 4):
                    selected_history_frame_id = select_aligned_memory_frames(
                        viewmats[0].cpu().detach().numpy(),
                        chunk_start_idx,
                        memory_frames=20,
                        temporal_context_size=12,
                        pred_latent_size=4,
                        points_local=self.points_local,
                        device=device)
                    selected_frame_indices = selected_frame_indices + selected_history_frame_id
                selected_frame_indices = sorted(list(set(selected_frame_indices)))
                to_remove = list(range(current_frame_idx, current_frame_idx + self.chunk_latent_frames))
                selected_frame_indices = [x for x in selected_frame_indices if x not in to_remove]

                context_latents = latents[:, :, selected_frame_indices]
                context_w2c = viewmats[:, selected_frame_indices]
                context_Ks = Ks[:, selected_frame_indices]
                context_action = action[:, selected_frame_indices]

                self.scheduler.set_timesteps(self.num_inference_steps, device=device)

            start_idx = chunk_i * self.chunk_latent_frames
            end_idx = chunk_i * self.chunk_latent_frames + self.chunk_latent_frames

            with (self.progress_bar(total=self.num_inference_steps) as progress_bar,
                  auto_offload_model(self.transformer, self.execution_device, enabled=self.enable_offloading)):
                for i, t in enumerate(timesteps):
                    if chunk_i == 0:
                        timestep_input = torch.full((self.chunk_latent_frames,), t,
                                                    device=device, dtype=timesteps.dtype)
                        latent_model_input = latents[:, :, :self.chunk_latent_frames]
                        cond_latents_input = cond_latents[:, :, :self.chunk_latent_frames]
                    else:
                        t_now = torch.full((self.chunk_latent_frames,), t,
                                           device=device, dtype=timesteps.dtype)
                        t_ctx = torch.full((len(selected_frame_indices),), stabilization_level - 1,
                                           device=device, dtype=timesteps.dtype)
                        timestep_input = torch.cat([t_ctx, t_now], dim=0)

                        latents_model_now = latents[:, :, start_idx: end_idx]
                        latent_model_input = torch.cat([context_latents, latents_model_now], dim=2)
                        cond_latents_input = cond_latents[:, :, :latent_model_input.shape[2]]

                    viewmats_input = viewmats[:, start_idx: end_idx]
                    Ks_input = Ks[:, start_idx: end_idx]
                    action_input = action[:, start_idx: end_idx]

                    if chunk_i > 0:
                        viewmats_input = torch.cat([context_w2c, viewmats_input], dim=1)
                        Ks_input = torch.cat([context_Ks, Ks_input], dim=1)
                        action_input = torch.cat([context_action, action_input], dim=1)

                    latents_concat = torch.concat([latent_model_input, cond_latents_input], dim=1)
                    if self.do_classifier_free_guidance:
                        latents_concat = torch.cat([latents_concat] * 2)
                    latents_concat = self.scheduler.scale_model_input(latents_concat, t)

                    batch_size = latents_concat.shape[0]
                    t_expand_txt = t.repeat(batch_size)
                    t_expand = timestep_input.repeat(batch_size)
                    viewmats_input = repeat(viewmats_input, 'B L H W -> (B R) L H W', R=batch_size).to(device)
                    Ks_input = repeat(Ks_input, 'B L H W -> (B R) L H W', R=batch_size).to(device)
                    # å…³é”®: ä½¿ç”¨ repeat åŽ reshape æˆ 1D
                    action_input = repeat(action_input, 'B L -> (B R) L', R=batch_size).reshape(-1).to(device)

                    with torch.autocast(device_type="cuda", dtype=self.target_dtype, enabled=self.autocast_enabled):
                        output = self.transformer(
                            bi_inference=True,
                            ar_txt_inference=False,
                            ar_vision_inference=False,
                            hidden_states=latents_concat,
                            timestep=t_expand,
                            timestep_txt=t_expand_txt,
                            text_states=prompt_embeds,
                            text_states_2=None,
                            encoder_attention_mask=prompt_mask,
                            timestep_r=None,
                            vision_states=vision_states,
                            mask_type=task_type,
                            guidance=None,
                            return_dict=False,
                            extra_kwargs=extra_kwargs,
                            viewmats=viewmats_input.to(self.target_dtype),
                            Ks=Ks_input.to(self.target_dtype),
                            action=action_input.to(self.target_dtype),
                        )
                        noise_pred = output[0]

                    if self.do_classifier_free_guidance:
                        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                        noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)

                    if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:
                        noise_pred = rescale_noise_cfg(
                            noise_pred,
                            noise_pred_text,
                            guidance_rescale=self.guidance_rescale,
                        )

                    latent_model_input = self.scheduler.step(noise_pred, t, latent_model_input, return_dict=False)[0]
                    latents[:, :, start_idx: end_idx] = latent_model_input[:, :, -self.chunk_latent_frames:]

                    if i == len(timesteps) - 1 or ((i + 1) > self.num_warmup_steps
                                                   and (i + 1) % self.scheduler.order == 0):
                        if progress_bar is not None:
                            progress_bar.update()

        return latents
    
    # åº”ç”¨è¡¥ä¸
    worldplay_video_pipeline.HunyuanVideo_1_5_Pipeline.ar_rollout = patched_ar_rollout
    worldplay_video_pipeline.HunyuanVideo_1_5_Pipeline.bi_rollout = patched_bi_rollout
    
    print("âœ… Pipeline patches applied successfully!")

# åº”ç”¨è¡¥ä¸
apply_pipeline_patches()

# çŽ°åœ¨å®‰å…¨åœ°å¯¼å…¥
from hyvideo.pipelines.worldplay_video_pipeline import HunyuanVideo_1_5_Pipeline
from hyvideo.commons.parallel_states import initialize_parallel_state
from hyvideo.commons.infer_state import initialize_infer_state
from scipy.spatial.transform import Rotation as R


# Global pipeline cache
pipeline_cache = {}
current_config = {}

# Default paths - update these based on your setup
DEFAULT_MODEL_PATH = "./checkpoints/HunyuanVideo-1.5"
DEFAULT_BI_ACTION_PATH = "./checkpoints/HY-WorldPlay/bidirectional_model/diffusion_pytorch_model.safetensors"
DEFAULT_AR_ACTION_PATH = "./checkpoints/HY-WorldPlay/ar_model/diffusion_pytorch_model.safetensors"
DEFAULT_AR_DISTILL_ACTION_PATH = "./checkpoints/HY-WorldPlay/ar_distilled_action_model/model.safetensors"


# ============== äº¤äº’æ¨¡å¼çŠ¶æ€ç®¡ç† ==============
class InteractiveState:
    """ç®¡ç†äº¤äº’æ¨¡å¼çš„å…¨å±€çŠ¶æ€"""
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.is_active = False
        self.image_path = None
        self.prompt = ""
        self.action_queue = []  # åŠ¨ä½œé˜Ÿåˆ—
        self.current_c2w = np.eye(4)  # å½“å‰ç›¸æœºä½å§¿ (camera to world)
        self.video_chunks = []  # ç”Ÿæˆçš„è§†é¢‘ç‰‡æ®µè·¯å¾„
        self.frame_count = 0
        self.all_videos = []  # æ‰€æœ‰ç”Ÿæˆçš„è§†é¢‘tensor

interactive_state = InteractiveState()


# ============== å·¥å…·å‡½æ•° ==============
def initialize_env():
    """Initialize parallel state for single GPU"""
    if 'parallel_initialized' not in globals():
        parallel_dims = initialize_parallel_state(sp=1)
        torch.cuda.set_device(0)
        globals()['parallel_initialized'] = True


def pose_to_input(pose_json_path, latent_chunk_num, tps=False):
    """Convert pose JSON to model input format"""
    mapping = {
        (0,0,0,0): 0, (1,0,0,0): 1, (0,1,0,0): 2, (0,0,1,0): 3,
        (0,0,0,1): 4, (1,0,1,0): 5, (1,0,0,1): 6, (0,1,1,0): 7,
        (0,1,0,1): 8,
    }
    
    def one_hot_to_one_dimension(one_hot):
        y = torch.tensor([mapping[tuple(row.tolist())] for row in one_hot])
        return y
    
    pose_json = json.load(open(pose_json_path, 'r'))
    pose_keys = list(pose_json.keys())
    intrinsic_list = []
    w2c_list = []
    
    for i in range(latent_chunk_num):
        t_key = pose_keys[i]
        c2w = np.array(pose_json[t_key]["extrinsic"])
        w2c = np.linalg.inv(c2w)
        w2c_list.append(w2c)
        intrinsic = np.array(pose_json[t_key]["K"])
        intrinsic[0, 0] /= intrinsic[0, 2] * 2
        intrinsic[1, 1] /= intrinsic[1, 2] * 2
        intrinsic[0, 2] = 0.5
        intrinsic[1, 2] = 0.5
        intrinsic_list.append(intrinsic)

    w2c_list = np.array(w2c_list)
    intrinsic_list = torch.tensor(np.array(intrinsic_list))

    c2ws = np.linalg.inv(w2c_list)
    C_inv = np.linalg.inv(c2ws[:-1])
    relative_c2w = np.zeros_like(c2ws)
    relative_c2w[0, ...] = c2ws[0, ...]
    relative_c2w[1:, ...] = C_inv @ c2ws[1:, ...]
    trans_one_hot = np.zeros((relative_c2w.shape[0], 4), dtype=np.int32)
    rotate_one_hot = np.zeros((relative_c2w.shape[0], 4), dtype=np.int32)

    move_norm_valid = 0.0001
    for i in range(1, relative_c2w.shape[0]):
        move_dirs = relative_c2w[i, :3, 3]
        move_norms = np.linalg.norm(move_dirs)
        if move_norms > move_norm_valid:
            move_norm_dirs = move_dirs / move_norms
            angles_rad = np.arccos(move_norm_dirs.clip(-1.0, 1.0))
            trans_angles_deg = angles_rad * (180.0 / torch.pi)
        else:
            trans_angles_deg = torch.zeros(3)

        R_rel = relative_c2w[i, :3, :3]
        r = R.from_matrix(R_rel)
        rot_angles_deg = r.as_euler('xyz', degrees=True)

        if move_norms > move_norm_valid:
            if (not tps) or (tps == True and abs(rot_angles_deg[1]) < 5e-2 and abs(rot_angles_deg[0]) < 5e-2):
                if trans_angles_deg[2] < 60:
                    trans_one_hot[i, 0] = 1
                elif trans_angles_deg[2] > 120:
                    trans_one_hot[i, 1] = 1
                if trans_angles_deg[0] < 60:
                    trans_one_hot[i, 2] = 1
                elif trans_angles_deg[0] > 120:
                    trans_one_hot[i, 3] = 1

        if rot_angles_deg[1] > 5e-2:
            rotate_one_hot[i, 0] = 1
        elif rot_angles_deg[1] < -5e-2:
            rotate_one_hot[i, 1] = 1
        if rot_angles_deg[0] > 5e-2:
            rotate_one_hot[i, 2] = 1
        elif rot_angles_deg[0] < -5e-2:
            rotate_one_hot[i, 3] = 1
            
    trans_one_hot = torch.tensor(trans_one_hot)
    rotate_one_hot = torch.tensor(rotate_one_hot)
    trans_one_label = one_hot_to_one_dimension(trans_one_hot)
    rotate_one_label = one_hot_to_one_dimension(rotate_one_hot)
    action_one_label = trans_one_label * 9 + rotate_one_label

    return torch.tensor(w2c_list), torch.tensor(intrinsic_list), action_one_label


def save_video(video, path):
    """Save video tensor to file"""
    if video.ndim == 5:
        assert video.shape[0] == 1
        video = video[0]
    vid = (video * 255).clamp(0, 255).to(torch.uint8)
    vid = einops.rearrange(vid, 'c f h w -> f h w c')
    imageio.mimwrite(path, vid, fps=24)
    return path


def load_pipeline(model_path, action_ckpt, model_type, dtype, enable_sr, enable_offloading=False):
    """Load or retrieve cached pipeline"""
    cache_key = f"{model_path}_{action_ckpt}_{model_type}_{dtype}_{enable_sr}_{enable_offloading}"
    
    if cache_key in pipeline_cache:
        return pipeline_cache[cache_key]
    
    # Clear old cache to save memory
    pipeline_cache.clear()
    
    transformer_dtype = torch.bfloat16 if dtype == 'bf16' else torch.float32
    
    pipe = HunyuanVideo_1_5_Pipeline.create_pipeline(
        pretrained_model_name_or_path=model_path,
        transformer_version="480p_i2v",
        enable_offloading=enable_offloading,
        enable_group_offloading=enable_offloading,
        create_sr_pipeline=enable_sr,
        force_sparse_attn=False,
        transformer_dtype=transformer_dtype,
        action_ckpt=action_ckpt,
    )
    
    pipeline_cache[cache_key] = pipe
    return pipe


# ============== äº¤äº’æ¨¡å¼å‡½æ•° ==============

# åŠ¨ä½œå®šä¹‰
# trans_one_hot: [å‰è¿›, åŽé€€, å·¦ç§», å³ç§»]
# rotate_one_hot: [å·¦è½¬, å³è½¬, ä¸Šçœ‹, ä¸‹çœ‹]
# action = trans * 9 + rotate

def get_action_label(trans_idx, rot_idx):
    """
    è®¡ç®—åŠ¨ä½œæ ‡ç­¾
    trans_idx: 0=æ— , 1=å‰è¿›, 2=åŽé€€, 3=å·¦ç§», 4=å³ç§»
    rot_idx: 0=æ— , 1=å·¦è½¬, 2=å³è½¬, 3=ä¸Šçœ‹, 4=ä¸‹çœ‹
    """
    # æ˜ å°„åˆ°one-hotç´¢å¼•
    trans_mapping = {
        0: 0,  # æ— ç§»åŠ¨ -> (0,0,0,0) -> 0
        1: 1,  # å‰è¿› -> (1,0,0,0) -> 1
        2: 2,  # åŽé€€ -> (0,1,0,0) -> 2
        3: 3,  # å·¦ç§» -> (0,0,1,0) -> 3
        4: 4,  # å³ç§» -> (0,0,0,1) -> 4
    }
    rot_mapping = {
        0: 0,  # æ— æ—‹è½¬ -> (0,0,0,0) -> 0
        1: 1,  # å·¦è½¬ -> (1,0,0,0) -> 1
        2: 2,  # å³è½¬ -> (0,1,0,0) -> 2
        3: 3,  # ä¸Šçœ‹ -> (0,0,1,0) -> 3
        4: 4,  # ä¸‹çœ‹ -> (0,0,0,1) -> 4
    }
    return trans_mapping[trans_idx] * 9 + rot_mapping[rot_idx]


def action_to_transform(action_name, move_dist=0.1, rot_deg=5.0):
    """
    å°†åŠ¨ä½œåç§°è½¬æ¢ä¸ºç›¸æœºå˜æ¢çŸ©é˜µ
    è¿”å›žç›¸å¯¹å˜æ¢ (relative c2w transform)
    """
    transform = np.eye(4)
    
    if action_name == 'W':  # å‰è¿› (æ²¿+Zæ–¹å‘)
        transform[2, 3] = move_dist
    elif action_name == 'S':  # åŽé€€ (æ²¿-Zæ–¹å‘)
        transform[2, 3] = -move_dist
    elif action_name == 'A':  # å·¦ç§»
        transform[0, 3] = -move_dist
    elif action_name == 'D':  # å³ç§»
        transform[0, 3] = move_dist
    elif action_name == 'LEFT':  # å·¦è½¬
        r = R.from_euler('y', rot_deg, degrees=True)
        transform[:3, :3] = r.as_matrix()
    elif action_name == 'RIGHT':  # å³è½¬
        r = R.from_euler('y', -rot_deg, degrees=True)
        transform[:3, :3] = r.as_matrix()
    elif action_name == 'UP':  # ä¸Šçœ‹
        r = R.from_euler('x', rot_deg, degrees=True)
        transform[:3, :3] = r.as_matrix()
    elif action_name == 'DOWN':  # ä¸‹çœ‹
        r = R.from_euler('x', -rot_deg, degrees=True)
        transform[:3, :3] = r.as_matrix()
    
    return transform


def action_name_to_label(action_name):
    """å°†åŠ¨ä½œåç§°è½¬æ¢ä¸ºæ¨¡åž‹éœ€è¦çš„action label"""
    action_map = {
        'NONE': get_action_label(0, 0),  # æ— åŠ¨ä½œ
        'W': get_action_label(1, 0),     # å‰è¿›
        'S': get_action_label(2, 0),     # åŽé€€
        'A': get_action_label(3, 0),     # å·¦ç§»
        'D': get_action_label(4, 0),     # å³ç§»
        'LEFT': get_action_label(0, 1),  # å·¦è½¬
        'RIGHT': get_action_label(0, 2), # å³è½¬
        'UP': get_action_label(0, 3),    # ä¸Šçœ‹
        'DOWN': get_action_label(0, 4),  # ä¸‹çœ‹
        # ç»„åˆåŠ¨ä½œ
        'W+LEFT': get_action_label(1, 1),
        'W+RIGHT': get_action_label(1, 2),
        'S+LEFT': get_action_label(2, 1),
        'S+RIGHT': get_action_label(2, 2),
    }
    return action_map.get(action_name, 0)


def create_trajectory_from_actions(actions, start_c2w=None, latent_chunk_num=None):
    """
    ä»ŽåŠ¨ä½œåºåˆ—åˆ›å»ºç›¸æœºè½¨è¿¹
    è¿”å›ž: w2c_list, K_list, action_labels
    
    latent_chunk_num: æ¨¡åž‹éœ€è¦çš„ latent frame æ•°é‡
    """
    if start_c2w is None:
        start_c2w = np.eye(4)
    
    c2w_list = [start_c2w.copy()]
    action_labels = [action_name_to_label('NONE')]  # ç¬¬ä¸€å¸§æ— åŠ¨ä½œ
    
    current_c2w = start_c2w.copy()
    
    for action_name in actions:
        # è®¡ç®—ç›¸å¯¹å˜æ¢
        rel_transform = action_to_transform(action_name)
        # æ›´æ–°ä½å§¿: new_c2w = current_c2w @ rel_transform
        current_c2w = current_c2w @ rel_transform
        c2w_list.append(current_c2w.copy())
        action_labels.append(action_name_to_label(action_name))
    
    # å¦‚æžœæŒ‡å®šäº† latent_chunk_numï¼Œç¡®ä¿é•¿åº¦åŒ¹é…
    if latent_chunk_num is not None:
        # èŽ·å–æœ€åŽä¸€ä¸ªåŠ¨ä½œç”¨äºŽå¡«å……ï¼ˆå¦‚æžœæ²¡æœ‰åŠ¨ä½œåˆ™ç”¨NONEï¼‰
        last_action = actions[-1] if len(actions) > 0 else 'NONE'
        last_action_label = action_name_to_label(last_action)
        
        while len(c2w_list) < latent_chunk_num:
            # ç»§ç»­ç”¨æœ€åŽä¸€ä¸ªåŠ¨ä½œå¡«å……ï¼Œä¿æŒè¿åŠ¨è¿žè´¯æ€§
            rel_transform = action_to_transform(last_action)
            current_c2w = current_c2w @ rel_transform
            c2w_list.append(current_c2w.copy())
            action_labels.append(last_action_label)
        # æˆªæ–­åˆ°æ­£ç¡®é•¿åº¦
        c2w_list = c2w_list[:latent_chunk_num]
        action_labels = action_labels[:latent_chunk_num]
    
    # è½¬æ¢ä¸ºw2c
    c2w_array = np.array(c2w_list)
    w2c_array = np.linalg.inv(c2w_array)
    
    # åˆ›å»ºå†…å‚çŸ©é˜µ (å½’ä¸€åŒ–)
    K = np.array([
        [0.5, 0, 0.5],
        [0, 0.5, 0.5],
        [0, 0, 1]
    ])
    K_list = np.array([K] * len(c2w_list))
    
    return (
        torch.tensor(w2c_array, dtype=torch.float32),
        torch.tensor(K_list, dtype=torch.float32),
        torch.tensor(action_labels, dtype=torch.long)
    )


# ============== è½¨è¿¹æ¨¡å¼ç”Ÿæˆå‡½æ•° ==============
def generate_video_gradio(
    image,
    prompt,
    pose_json_path,
    model_type,
    seed,
    video_length,
    num_inference_steps,
    enable_sr,
    negative_prompt,
    aspect_ratio,
    model_path,
    action_ckpt,
    dtype,
    few_step,
    enable_offloading,
    progress=gr.Progress()
):
    """Generate video using Gradio interface"""
    try:
        initialize_env()
        
        # Create a simple InferState-like object
        class SimpleArgs:
            def __init__(self, offload):
                self.offloading = offload
                self.group_offloading = offload
                self.enable_torch_compile = False
                
        initialize_infer_state(SimpleArgs(enable_offloading))
        
        progress(0.1, desc="åŠ è½½æ¨¡åž‹ä¸­...")
        
        # Load pipeline
        pipe = load_pipeline(model_path, action_ckpt, model_type, dtype, enable_sr, enable_offloading)
        
        progress(0.3, desc="Processing pose data...")
        
        # Save uploaded image temporarily
        with tempfile.NamedTemporaryFile(delete=False, suffix=".png") as tmp_img:
            image.save(tmp_img.name)
            image_path = tmp_img.name
        
        # Load pose data
        latent_chunk_num = (video_length - 1) // 4 + 1
        viewmats, Ks, action = pose_to_input(pose_json_path, latent_chunk_num)
        
        progress(0.5, desc="ç”Ÿæˆè§†é¢‘ä¸­...")
        
        # Generate video
        out = pipe(
            enable_sr=enable_sr,
            prompt=prompt,
            aspect_ratio=aspect_ratio,
            num_inference_steps=num_inference_steps,
            sr_num_inference_steps=None,
            video_length=video_length,
            negative_prompt=negative_prompt,
            seed=seed,
            output_type="pt",
            prompt_rewrite=False,  # No vLLM
            return_pre_sr_video=True,
            viewmats=viewmats.unsqueeze(0),
            Ks=Ks.unsqueeze(0),
            action=action.unsqueeze(0),
            few_step=few_step,
            chunk_latent_frames=4 if model_type == "ar" else 16,
            model_type=model_type,
            reference_image=image_path,
        )
        
        progress(0.9, desc="ä¿å­˜è§†é¢‘ä¸­...")
        
        # Save output
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("./outputs") / timestamp
        output_dir.mkdir(parents=True, exist_ok=True)
        
        video_path = str(output_dir / "generated.mp4")
        sr_video_path = str(output_dir / "generated_sr.mp4")
        
        if enable_sr and hasattr(out, 'sr_videos'):
            save_video(out.sr_videos, sr_video_path)
            save_video(out.videos, video_path)
            result_video = sr_video_path
            info = f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼\n\nðŸ“ è¶…åˆ†è§†é¢‘: {sr_video_path}\nðŸ“ åŽŸå§‹è§†é¢‘: {video_path}"
        else:
            save_video(out.videos, video_path)
            result_video = video_path
            info = f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸï¼\n\nðŸ“ è¾“å‡º: {video_path}"
        
        # Cleanup
        os.unlink(image_path)
        
        progress(1.0, desc="å®Œæˆï¼")
        return result_video, info
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ Error: {str(e)}\n\n{traceback.format_exc()}"
        return None, error_msg


# ============== äº¤äº’æ¨¡å¼å‡½æ•° ==============
def interactive_init(image, prompt, model_path, action_ckpt, dtype):
    """åˆå§‹åŒ–äº¤äº’ä¼šè¯"""
    global interactive_state
    
    if image is None:
        return "âŒ è¯·å…ˆä¸Šä¼ èµ·å§‹å›¾ç‰‡ï¼", "", None
    
    try:
        initialize_env()
        
        class SimpleArgs:
            def __init__(self, offload):
                self.offloading = offload
                self.group_offloading = offload
                self.enable_torch_compile = False
        
        initialize_infer_state(SimpleArgs(False))  # ä¸ä½¿ç”¨å¸è½½
        
        # é¢„åŠ è½½æ¨¡åž‹
        pipe = load_pipeline(model_path, action_ckpt, "ar", dtype, False, False)  # ä¸ä½¿ç”¨å¸è½½
        
        # ä¿å­˜å›¾ç‰‡
        with tempfile.NamedTemporaryFile(delete=False, suffix=".png") as tmp_img:
            image.save(tmp_img.name)
            image_path = tmp_img.name
        
        # åˆå§‹åŒ–çŠ¶æ€
        interactive_state.reset()
        interactive_state.is_active = True
        interactive_state.image_path = image_path
        interactive_state.prompt = prompt
        interactive_state.current_c2w = np.eye(4)
        
        info = """âœ… ä¼šè¯å·²å¯åŠ¨ï¼

ðŸŽ® **æ“ä½œæŒ‡å—**:
1. å…ˆé€‰æ‹©ã€Œè§†é¢‘å¸§æ•°ã€(è®¾ç½®ä¸­)
2. æŸ¥çœ‹éœ€è¦å¤šå°‘ä¸ªåŠ¨ä½œ
3. ç‚¹å‡»æ–¹å‘æŒ‰é’®æ·»åŠ åŠ¨ä½œ
4. ç‚¹å‡»ã€Œç”Ÿæˆè§†é¢‘ã€æ‰§è¡Œ

âš¡ W/S: å‰è¿›/åŽé€€ | A/D: å·¦ç§»/å³ç§»
âš¡ â†/â†’: å·¦è½¬/å³è½¬ | â†‘/â†“: ä¸Šçœ‹/ä¸‹çœ‹"""
        
        # é»˜è®¤29å¸§éœ€è¦7ä¸ªåŠ¨ä½œ
        return info, get_queue_display(29), image
        
    except Exception as e:
        import traceback
        return f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}\n{traceback.format_exc()}", "", None


def interactive_add_action(action_name, video_frames=29):
    """æ·»åŠ åŠ¨ä½œåˆ°é˜Ÿåˆ—"""
    global interactive_state
    
    if not interactive_state.is_active:
        return "âš ï¸ è¯·å…ˆç‚¹å‡»ã€Œå¯åŠ¨ä¼šè¯ã€", get_queue_display(video_frames)
    
    interactive_state.action_queue.append(action_name)
    
    action_display = {
        'W': 'â†‘å‰è¿›', 'S': 'â†“åŽé€€', 'A': 'â†å·¦ç§»', 'D': 'â†’å³ç§»',
        'LEFT': 'â†°å·¦è½¬', 'RIGHT': 'â†±å³è½¬', 'UP': 'â†‘ä¸Šçœ‹', 'DOWN': 'â†“ä¸‹çœ‹'
    }
    
    status = f"âœ… å·²æ·»åŠ : {action_display.get(action_name, action_name)}"
    
    return status, get_queue_display(video_frames)


def get_queue_display(video_frames=29):
    """èŽ·å–é˜Ÿåˆ—æ˜¾ç¤ºå­—ç¬¦ä¸²ï¼ŒåŒ…å«åŠ¨ä½œè®¡æ•°"""
    action_display = {
        'W': 'â†‘å‰è¿›', 'S': 'â†“åŽé€€', 'A': 'â†å·¦ç§»', 'D': 'â†’å³ç§»',
        'LEFT': 'â†°å·¦è½¬', 'RIGHT': 'â†±å³è½¬', 'UP': 'â†‘ä¸Šçœ‹', 'DOWN': 'â†“ä¸‹çœ‹'
    }
    
    # è®¡ç®—éœ€è¦çš„åŠ¨ä½œæ•°é‡
    latent_chunk_num = (video_frames - 1) // 4 + 1
    required_actions = latent_chunk_num - 1  # ç¬¬ä¸€å¸§æ˜¯é™æ­¢çš„
    current_count = len(interactive_state.action_queue)
    
    if not interactive_state.action_queue:
        return f"ðŸ“‹ åŠ¨ä½œé˜Ÿåˆ—: [ç©º]\nðŸ“Š å·²é€‰: 0/{required_actions} ä¸ªåŠ¨ä½œ"
    
    queue_str = " â†’ ".join([action_display.get(a, a) for a in interactive_state.action_queue])
    
    if current_count < required_actions:
        hint = f"âš ï¸ è¿˜éœ€ {required_actions - current_count} ä¸ªåŠ¨ä½œ (ä¸è¶³éƒ¨åˆ†å°†é‡å¤æœ€åŽåŠ¨ä½œ)"
    elif current_count == required_actions:
        hint = "âœ… åŠ¨ä½œæ•°é‡åˆšå¥½!"
    else:
        hint = f"âš ï¸ è¶…å‡º {current_count - required_actions} ä¸ªåŠ¨ä½œ (å°†è¢«å¿½ç•¥)"
    
    return f"ðŸ“‹ åŠ¨ä½œé˜Ÿåˆ—: [{queue_str}]\nðŸ“Š å·²é€‰: {current_count}/{required_actions} ä¸ªåŠ¨ä½œ\n{hint}"


def interactive_clear_queue(video_frames=29):
    """æ¸…ç©ºåŠ¨ä½œé˜Ÿåˆ—"""
    global interactive_state
    interactive_state.action_queue = []
    return "âœ… é˜Ÿåˆ—å·²æ¸…ç©º", get_queue_display(video_frames)


def interactive_generate(model_path, action_ckpt, dtype, num_steps, video_frames, progress=gr.Progress()):
    """æ ¹æ®é˜Ÿåˆ—ä¸­çš„åŠ¨ä½œç”Ÿæˆè§†é¢‘"""
    global interactive_state
    
    video_frames = int(video_frames)
    
    if not interactive_state.is_active:
        return "âš ï¸ è¯·å…ˆç‚¹å‡»ã€Œå¯åŠ¨ä¼šè¯ã€åˆå§‹åŒ–", get_queue_display(video_frames), None
    
    if len(interactive_state.action_queue) == 0:
        return "âš ï¸ åŠ¨ä½œé˜Ÿåˆ—ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ åŠ¨ä½œ", get_queue_display(video_frames), None
    
    try:
        progress(0.1, desc="å‡†å¤‡ç”Ÿæˆ...")
        
        # èŽ·å–åŠ¨ä½œ
        actions = interactive_state.action_queue.copy()
        
        # è®¡ç®—è§†é¢‘é•¿åº¦å’Œ latent chunk æ•°é‡
        # video_length = latent_chunk_num * 4 + 1 (æ¯ä¸ªlatent chunkå¯¹åº”4ä¸ªpixel frames)
        video_length = int(video_frames)  # ç”¨æˆ·è®¾ç½®çš„å¸§æ•°
        latent_chunk_num = (video_length - 1) // 4 + 1
        
        # æ ¹æ®å¸§æ•°è®¡ç®—éœ€è¦çš„åŠ¨ä½œæ•°é‡
        max_actions = latent_chunk_num - 1  # ç¬¬ä¸€å¸§æ˜¯NONEï¼Œå…¶ä½™æ¯ä¸ªchunkä¸€ä¸ªåŠ¨ä½œ
        current_actions = actions[:max_actions]
        
        # åˆ›å»ºè½¨è¿¹ï¼Œç¡®ä¿é•¿åº¦åŒ¹é…
        w2c_list, K_list, action_labels = create_trajectory_from_actions(
            current_actions, 
            interactive_state.current_c2w,
            latent_chunk_num=latent_chunk_num
        )
        
        progress(0.3, desc="åŠ è½½æ¨¡åž‹...")
        
        # é‡æ–°åˆå§‹åŒ–çŽ¯å¢ƒç¡®ä¿çŠ¶æ€æ­£ç¡®
        class SimpleArgs:
            def __init__(self, offload):
                self.offloading = offload
                self.group_offloading = offload
                self.enable_torch_compile = False
        
        initialize_infer_state(SimpleArgs(False))  # ä¸ä½¿ç”¨å¸è½½
        
        # åŠ è½½pipeline
        pipe = load_pipeline(model_path, action_ckpt, "ar", dtype, False, False)  # ä¸ä½¿ç”¨å¸è½½
        
        progress(0.5, desc="ç”Ÿæˆè§†é¢‘ä¸­...")
        
        # ç”Ÿæˆ
        out = pipe(
            enable_sr=False,
            prompt=interactive_state.prompt,
            aspect_ratio="16:9",
            num_inference_steps=num_steps,
            video_length=video_length,
            seed=None,
            output_type="pt",
            prompt_rewrite=False,
            return_pre_sr_video=True,
            viewmats=w2c_list.unsqueeze(0),
            Ks=K_list.unsqueeze(0),
            action=action_labels.unsqueeze(0),
            few_step=False,  # å®Œæ•´æ¨¡åž‹ä¸ç”¨ few_step
            chunk_latent_frames=4,
            model_type='ar',
            reference_image=interactive_state.image_path,
        )
        
        progress(0.9, desc="ä¿å­˜è§†é¢‘...")
        
        # ä¿å­˜è§†é¢‘
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        output_dir = Path("./outputs/interactive") / timestamp
        output_dir.mkdir(parents=True, exist_ok=True)
        
        video_path = str(output_dir / "chunk.mp4")
        save_video(out.videos, video_path)
        
        # æ›´æ–°çŠ¶æ€
        # æ›´æ–°ç›¸æœºä½å§¿åˆ°æœ€åŽä¸€å¸§
        for action_name in current_actions:
            rel_transform = action_to_transform(action_name)
            interactive_state.current_c2w = interactive_state.current_c2w @ rel_transform
        
        # ä»Žé˜Ÿåˆ—ä¸­ç§»é™¤å·²æ‰§è¡Œçš„åŠ¨ä½œ
        interactive_state.action_queue = actions[max_actions:]
        
        # è®°å½•
        interactive_state.video_chunks.append(video_path)
        interactive_state.frame_count += video_length - 1
        
        progress(1.0, desc="å®Œæˆ!")
        
        used_actions = len(current_actions)
        remaining_actions = len(interactive_state.action_queue)
        
        status = f"""âœ… ç”ŸæˆæˆåŠŸï¼
ðŸ“½ï¸ æœ¬æ¬¡ç”Ÿæˆ: {video_length}å¸§ (çº¦{video_length/24:.1f}ç§’)
ðŸŽ® ä½¿ç”¨åŠ¨ä½œ: {used_actions}ä¸ªï¼Œå‰©ä½™: {remaining_actions}ä¸ª
ðŸŽ¬ ç´¯è®¡å¸§æ•°: {interactive_state.frame_count}
ðŸ“ ä¿å­˜è‡³: {video_path}"""
        
        return status, get_queue_display(video_frames), video_path
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        return error_msg, get_queue_display(video_frames), None


def interactive_stop():
    """åœæ­¢ä¼šè¯"""
    global interactive_state
    
    if not interactive_state.is_active:
        return "âš ï¸ æ²¡æœ‰æ´»åŠ¨çš„ä¼šè¯", None
    
    # æ¸…ç†
    if interactive_state.image_path and os.path.exists(interactive_state.image_path):
        try:
            os.unlink(interactive_state.image_path)
        except:
            pass
    
    total_frames = interactive_state.frame_count
    chunks = len(interactive_state.video_chunks)
    last_video = interactive_state.video_chunks[-1] if interactive_state.video_chunks else None
    
    interactive_state.reset()
    
    info = f"""âœ… ä¼šè¯å·²ç»“æŸ

ðŸ“Š ç»Ÿè®¡:
- æ€»å¸§æ•°: {total_frames}
- è§†é¢‘ç‰‡æ®µ: {chunks}ä¸ª

ðŸ’¾ è§†é¢‘å·²ä¿å­˜åˆ° ./outputs/interactive/ ç›®å½•"""
    
    return info, last_video


# ============== UI æž„å»º ==============
def create_ui():
    """Create Gradio UI with tabs"""
    
    with gr.Blocks(title="HY-WorldPlay è§†é¢‘ç”Ÿæˆå™¨", theme=gr.themes.Soft(), css="""
        .action-btn { min-width: 80px !important; }
        .big-btn { min-height: 50px !important; font-size: 18px !important; }
    """) as demo:
        
        gr.Markdown("""
        # ðŸŽ® HY-WorldPlay è§†é¢‘ç”Ÿæˆå™¨
        
        ä½¿ç”¨ HunyuanVideo-1.5 ç”Ÿæˆå¯æŽ§ç›¸æœºè½¨è¿¹è§†é¢‘ | æ”¯æŒ **äº¤äº’æ¨¡å¼** (WASDæŽ§åˆ¶) å’Œ **è½¨è¿¹æ¨¡å¼** (JSONå®šä¹‰)
        """)
        
        with gr.Tabs():
            # ============== äº¤äº’æ¨¡å¼ Tab ==============
            with gr.TabItem("ðŸŽ® äº¤äº’æ¨¡å¼ (WASDæŽ§åˆ¶)", id="interactive"):
                gr.Markdown("""
                ### å®žæ—¶æŽ§åˆ¶ç›¸æœºæŽ¢ç´¢ä¸–ç•Œ
                ä½¿ç”¨æŒ‰é’®æŽ§åˆ¶ç›¸æœºç§»åŠ¨å’Œæ—‹è½¬ï¼Œç”Ÿæˆäº¤äº’å¼è§†é¢‘ï¼
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### 1ï¸âƒ£ åˆå§‹åŒ–")
                        
                        inter_image = gr.Image(
                            label="èµ·å§‹å›¾ç‰‡",
                            type="pil",
                            height=250
                        )
                        
                        inter_prompt = gr.Textbox(
                            label="åœºæ™¯æè¿°",
                            value="A beautiful outdoor scene with natural lighting.",
                            lines=2
                        )
                        
                        with gr.Accordion("âš™ï¸ æ¨¡åž‹è®¾ç½®", open=False):
                            inter_model_path = gr.Textbox(
                                label="æ¨¡åž‹è·¯å¾„",
                                value=DEFAULT_MODEL_PATH
                            )
                            inter_action_ckpt = gr.Textbox(
                                label="åŠ¨ä½œæ¨¡åž‹",
                                value=DEFAULT_AR_ACTION_PATH
                            )
                            inter_dtype = gr.Radio(
                                choices=["bf16", "fp32"],
                                value="bf16",
                                label="ç²¾åº¦"
                            )
                            inter_steps = gr.Slider(
                                minimum=10, maximum=50, value=50, step=1,
                                label="æŽ¨ç†æ­¥æ•° (å®Œæ•´æ¨¡åž‹å»ºè®®50æ­¥)"
                            )
                            # åˆæ³•çš„å¸§æ•°: latent_frames å¿…é¡»èƒ½è¢« 4 æ•´é™¤
                            # 13å¸§(4latent), 29å¸§(8latent), 45å¸§(12latent), 61å¸§(16latent)
                            inter_video_frames = gr.Dropdown(
                                choices=[
                                    ("13å¸§ (3åŠ¨ä½œ) â‰ˆ0.5ç§’", 13),
                                    ("29å¸§ (7åŠ¨ä½œ) â‰ˆ1.2ç§’", 29),
                                    ("45å¸§ (11åŠ¨ä½œ) â‰ˆ1.9ç§’", 45),
                                    ("61å¸§ (15åŠ¨ä½œ) â‰ˆ2.5ç§’", 61),
                                ],
                                value=29,
                                label="è§†é¢‘å¸§æ•°",
                                info="å¿…é¡»é€‰æ‹©æœ‰æ•ˆå¸§æ•°ï¼Œå¦åˆ™æœ€åŽå‡ å¸§ä¼šèŠ±å±"
                            )
                        
                        init_btn = gr.Button("ðŸš€ å¯åŠ¨ä¼šè¯", variant="primary", elem_classes="big-btn")
                        
                        gr.Markdown("---")
                        gr.Markdown("#### 2ï¸âƒ£ æŽ§åˆ¶é¢æ¿")
                        
                        # WASD æŽ§åˆ¶æŒ‰é’®
                        with gr.Group():
                            gr.Markdown("**ç§»åŠ¨æŽ§åˆ¶**")
                            with gr.Row():
                                gr.Column(scale=1)
                                w_btn = gr.Button("W â†‘\nå‰è¿›", elem_classes="action-btn")
                                gr.Column(scale=1)
                            with gr.Row():
                                a_btn = gr.Button("A â†\nå·¦ç§»", elem_classes="action-btn")
                                s_btn = gr.Button("S â†“\nåŽé€€", elem_classes="action-btn")
                                d_btn = gr.Button("D â†’\nå³ç§»", elem_classes="action-btn")
                        
                        with gr.Group():
                            gr.Markdown("**è§†è§’æŽ§åˆ¶**")
                            with gr.Row():
                                gr.Column(scale=1)
                                up_btn = gr.Button("â†‘\nä¸Šçœ‹", elem_classes="action-btn")
                                gr.Column(scale=1)
                            with gr.Row():
                                left_btn = gr.Button("â†\nå·¦è½¬", elem_classes="action-btn")
                                down_btn = gr.Button("â†“\nä¸‹çœ‹", elem_classes="action-btn")
                                right_btn = gr.Button("â†’\nå³è½¬", elem_classes="action-btn")
                        
                        with gr.Row():
                            clear_btn = gr.Button("ðŸ—‘ï¸ æ¸…ç©ºé˜Ÿåˆ—")
                            gen_btn = gr.Button("ðŸŽ¬ ç”Ÿæˆè§†é¢‘", variant="primary", elem_classes="big-btn")
                        
                        stop_btn = gr.Button("â¹ï¸ ç»“æŸä¼šè¯", variant="stop")
                    
                    with gr.Column(scale=1):
                        gr.Markdown("#### è¾“å‡º")
                        
                        inter_status = gr.Textbox(
                            label="çŠ¶æ€",
                            lines=8,
                            interactive=False
                        )
                        
                        inter_queue = gr.Textbox(
                            label="åŠ¨ä½œé˜Ÿåˆ—",
                            value="ðŸ“‹ åŠ¨ä½œé˜Ÿåˆ—: [ç©º]\nðŸ“Š å·²é€‰: 0/7 ä¸ªåŠ¨ä½œ",
                            lines=3,
                            interactive=False
                        )
                        
                        inter_preview = gr.Image(
                            label="å½“å‰ç”»é¢",
                            height=200
                        )
                        
                        inter_video = gr.Video(
                            label="ç”Ÿæˆçš„è§†é¢‘",
                            height=300
                        )
                
                # ç»‘å®šäº‹ä»¶
                init_btn.click(
                    fn=interactive_init,
                    inputs=[inter_image, inter_prompt, inter_model_path, inter_action_ckpt, inter_dtype],
                    outputs=[inter_status, inter_queue, inter_preview]
                )
                
                # åŠ¨ä½œæŒ‰é’® - ä¼ é€’è§†é¢‘å¸§æ•°ä»¥æ˜¾ç¤ºæ­£ç¡®çš„åŠ¨ä½œæ•°é‡æç¤º
                w_btn.click(fn=lambda vf: interactive_add_action('W', vf), inputs=[inter_video_frames], outputs=[inter_status, inter_queue])
                s_btn.click(fn=lambda vf: interactive_add_action('S', vf), inputs=[inter_video_frames], outputs=[inter_status, inter_queue])
                a_btn.click(fn=lambda vf: interactive_add_action('A', vf), inputs=[inter_video_frames], outputs=[inter_status, inter_queue])
                d_btn.click(fn=lambda vf: interactive_add_action('D', vf), inputs=[inter_video_frames], outputs=[inter_status, inter_queue])
                left_btn.click(fn=lambda vf: interactive_add_action('LEFT', vf), inputs=[inter_video_frames], outputs=[inter_status, inter_queue])
                right_btn.click(fn=lambda vf: interactive_add_action('RIGHT', vf), inputs=[inter_video_frames], outputs=[inter_status, inter_queue])
                up_btn.click(fn=lambda vf: interactive_add_action('UP', vf), inputs=[inter_video_frames], outputs=[inter_status, inter_queue])
                down_btn.click(fn=lambda vf: interactive_add_action('DOWN', vf), inputs=[inter_video_frames], outputs=[inter_status, inter_queue])
                
                clear_btn.click(fn=lambda vf: interactive_clear_queue(vf), inputs=[inter_video_frames], outputs=[inter_status, inter_queue])
                
                # å½“è§†é¢‘å¸§æ•°æ”¹å˜æ—¶æ›´æ–°é˜Ÿåˆ—æ˜¾ç¤º
                inter_video_frames.change(fn=lambda vf: get_queue_display(vf), inputs=[inter_video_frames], outputs=[inter_queue])
                
                gen_btn.click(
                    fn=interactive_generate,
                    inputs=[inter_model_path, inter_action_ckpt, inter_dtype, inter_steps, inter_video_frames],
                    outputs=[inter_status, inter_queue, inter_video]
                )
                
                stop_btn.click(fn=interactive_stop, outputs=[inter_status, inter_video])
                
                # äº¤äº’æ¨¡å¼ç¤ºä¾‹
                gr.Examples(
                    examples=[
                        [
                            "./assets/img/test.png",
                            "A paved pathway leads towards a stone arch bridge spanning a calm body of water. Lush green trees and foliage line the path.",
                        ],
                    ],
                    inputs=[inter_image, inter_prompt],
                    label="ðŸ“· ç¤ºä¾‹å›¾ç‰‡"
                )
            
            # ============== è½¨è¿¹æ¨¡å¼ Tab ==============
            with gr.TabItem("ðŸ“ è½¨è¿¹æ¨¡å¼ (JSONå®šä¹‰)", id="trajectory"):
                gr.Markdown("""
                ### ä½¿ç”¨é¢„å®šä¹‰ç›¸æœºè½¨è¿¹ç”Ÿæˆè§†é¢‘
                é€šè¿‡ JSON æ–‡ä»¶ç²¾ç¡®æŽ§åˆ¶æ¯ä¸€å¸§çš„ç›¸æœºä½ç½®å’Œæœå‘
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### è¾“å…¥è®¾ç½®")
                        
                        image_input = gr.Image(
                            label="è¾“å…¥å›¾ç‰‡ï¼ˆI2V å¿…éœ€ï¼‰",
                            type="pil",
                            height=300
                        )
                        
                        prompt_input = gr.Textbox(
                            label="æç¤ºè¯",
                            placeholder="Describe your scene...",
                            lines=4,
                            value="A paved pathway leads towards a stone arch bridge spanning a calm body of water."
                        )
                        
                        negative_prompt_input = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯ï¼ˆå¯é€‰ï¼‰",
                            placeholder="What you don't want in the video...",
                            lines=2,
                            value=""
                        )
                        
                        pose_json_input = gr.Textbox(
                            label="ä½å§¿ JSON è·¯å¾„",
                            value="./assets/pose/test_forward_32_latents.json",
                            info="ç›¸æœºè½¨è¿¹ JSON æ–‡ä»¶è·¯å¾„"
                        )
                        
                        with gr.Row():
                            model_type_input = gr.Radio(
                                choices=["bi", "ar", "ar_distilled"],
                                value="bi",
                                label="æ¨¡åž‹ç±»åž‹",
                                info="bi=åŒå‘æ¨¡åž‹ï¼ˆè´¨é‡æ›´é«˜ï¼‰, ar=è‡ªå›žå½’æ¨¡åž‹ï¼ˆæ›´å¿«ï¼‰, ar_distilled=æœ€å¿«"
                            )
                        
                        with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                            seed_input = gr.Slider(
                                minimum=0,
                                maximum=2147483647,
                                value=1,
                                step=1,
                                label="éšæœºç§å­"
                            )
                            
                            video_length_input = gr.Slider(
                                minimum=33,
                                maximum=125,
                                value=125,
                                step=4,
                                label="è§†é¢‘é•¿åº¦ï¼ˆå¸§æ•°ï¼‰",
                                info="å¿…é¡»æ˜¯ 4n+1 æ ¼å¼ï¼ˆå¦‚ 33, 37, 41, ..., 125ï¼‰"
                            )
                            
                            num_steps_input = gr.Slider(
                                minimum=4,
                                maximum=50,
                                value=50,
                                step=1,
                                label="æŽ¨ç†æ­¥æ•°",
                                info="æ­¥æ•°è¶Šå¤šè´¨é‡è¶Šå¥½ï¼Œä½†é€Ÿåº¦è¶Šæ…¢"
                            )
                            
                            aspect_ratio_input = gr.Dropdown(
                                choices=["16:9", "9:16", "4:3", "3:4", "1:1"],
                                value="16:9",
                                label="å®½é«˜æ¯”"
                            )
                            
                            enable_sr_input = gr.Checkbox(
                                label="å¯ç”¨è¶…åˆ†è¾¨çŽ‡",
                                value=False,
                                info="ä»…åœ¨ video_length=121 æ—¶æœ‰æ•ˆ"
                            )
                            
                            few_step_input = gr.Checkbox(
                                label="å°‘æ­¥æ¨¡å¼",
                                value=False,
                                info="ä»…ç”¨äºŽè’¸é¦æ¨¡åž‹"
                            )
                            
                            dtype_input = gr.Radio(
                                choices=["bf16", "fp32"],
                                value="bf16",
                                label="ç²¾åº¦",
                                info="bf16=æ›´å¿«, fp32=è´¨é‡æ›´å¥½"
                            )
                            
                            enable_offloading_input = gr.Checkbox(
                                label="å¯ç”¨å¸è½½",
                                value=False,
                                info="å…³é—­ä»¥èŽ·å¾—æ›´å¿«é€Ÿåº¦ï¼ˆéœ€è¦å¤§æ˜¾å­˜ï¼‰"
                            )
                        
                        with gr.Accordion("æ¨¡åž‹è·¯å¾„", open=False):
                            model_path_input = gr.Textbox(
                                label="HunyuanVideo æ¨¡åž‹è·¯å¾„",
                                value=DEFAULT_MODEL_PATH
                            )
                            
                            action_ckpt_input = gr.Textbox(
                                label="åŠ¨ä½œæ¨¡åž‹è·¯å¾„",
                                value=DEFAULT_BI_ACTION_PATH,
                                info="ä¼šæ ¹æ®æ¨¡åž‹ç±»åž‹è‡ªåŠ¨æ›´æ–°"
                            )
                        
                        generate_btn = gr.Button("ðŸŽ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### è¾“å‡º")
                        
                        video_output = gr.Video(
                            label="ç”Ÿæˆçš„è§†é¢‘",
                            height=400
                        )
                        
                        info_output = gr.Textbox(
                            label="çŠ¶æ€",
                            lines=6,
                            max_lines=10
                        )
                        
                        gr.Markdown("""
                        ### ðŸ“ ä½¿ç”¨æç¤º:
                        - **å›¾ç‰‡**: I2V ç”Ÿæˆå¿…éœ€
                        - **ä½å§¿ JSON**: ä½¿ç”¨è‡ªå®šä¹‰ JSON å®šä¹‰ç›¸æœºè½¨è¿¹
                        - **æ¨¡åž‹ç±»åž‹**: 
                          - `bi` (åŒå‘): è´¨é‡æœ€å¥½ï¼Œè¾ƒæ…¢
                          - `ar` (è‡ªå›žå½’): å¹³è¡¡é€‰æ‹©
                          - `ar_distilled`: æœ€å¿«ï¼Œéœ€å¯ç”¨"å°‘æ­¥æ¨¡å¼"
                        - **è§†é¢‘é•¿åº¦**: å¿…é¡»æ˜¯ 4n+1ï¼ˆå¦‚ 33, 37, 41, ..., 125ï¼‰
                        
                        ### ðŸ“‚ é¢„è®¾è½¨è¿¹:
                        - `./assets/pose/test_forward_32_latents.json` - å‘å‰è¿åŠ¨
                        """)
                
                # Auto-update action checkpoint path based on model type
                def update_action_path(model_type):
                    if model_type == "bi":
                        return DEFAULT_BI_ACTION_PATH
                    elif model_type == "ar":
                        return DEFAULT_AR_ACTION_PATH
                    elif model_type == "ar_distilled":
                        return DEFAULT_AR_DISTILL_ACTION_PATH
                    return DEFAULT_BI_ACTION_PATH
                
                model_type_input.change(
                    fn=update_action_path,
                    inputs=[model_type_input],
                    outputs=[action_ckpt_input]
                )
                
                # Generate button click
                generate_btn.click(
                    fn=generate_video_gradio,
                    inputs=[
                        image_input,
                        prompt_input,
                        pose_json_input,
                        model_type_input,
                        seed_input,
                        video_length_input,
                        num_steps_input,
                        enable_sr_input,
                        negative_prompt_input,
                        aspect_ratio_input,
                        model_path_input,
                        action_ckpt_input,
                        dtype_input,
                        few_step_input,
                        enable_offloading_input,
                    ],
                    outputs=[video_output, info_output]
                )
                
                # è½¨è¿¹æ¨¡å¼ç¤ºä¾‹
                gr.Examples(
                    examples=[
                        [
                            "./assets/img/test.png",
                            "A paved pathway leads towards a stone arch bridge spanning a calm body of water. Lush green trees and foliage line the path.",
                            "./assets/pose/test_forward_32_latents.json",
                            "bi",
                            1,
                            125,
                            50,
                        ],
                    ],
                    inputs=[
                        image_input,
                        prompt_input,
                        pose_json_input,
                        model_type_input,
                        seed_input,
                        video_length_input,
                        num_steps_input,
                    ],
                    label="ðŸ“· ç¤ºä¾‹"
                )
        
        gr.Markdown("""
        ---
        ### ðŸ“š å…³äºŽ
        
        **HY-WorldPlay** æ˜¯è…¾è®¯æ··å…ƒå›¢é˜Ÿå¼€æºçš„å®žæ—¶äº¤äº’ä¸–ç•Œæ¨¡åž‹ã€‚
        
        - ðŸ”— [GitHub](https://github.com/Tencent-Hunyuan/HY-WorldPlay) 
        - ðŸŒ [å®˜æ–¹Demo](https://3d.hunyuan.tencent.com/sceneTo3D) (å®Œæ•´å®žæ—¶äº¤äº’ä½“éªŒ)
        - ðŸ“„ [æŠ€æœ¯æŠ¥å‘Š](https://3d-models.hunyuan.tencent.com/world/world1_5/HYWorld_1.5_Tech_Report.pdf)
        
        âš ï¸ **æ³¨æ„**: æœ¬åœ°äº¤äº’æ¨¡å¼æ˜¯ç®€åŒ–å®žçŽ°ï¼Œå®Œæ•´çš„å®žæ—¶æµå¼äº¤äº’è¯·è®¿é—®å®˜æ–¹Demoã€‚
        """)
    
    return demo


def preload_models():
    """å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡åž‹"""
    print("\n" + "="*60)
    print("ðŸš€ æ­£åœ¨é¢„åŠ è½½æ¨¡åž‹...")
    print("="*60 + "\n")
    
    initialize_env()
    
    class SimpleArgs:
        def __init__(self):
            self.offloading = False
            self.group_offloading = False
            self.enable_torch_compile = False
    
    initialize_infer_state(SimpleArgs())
    
    # é¢„åŠ è½½ AR æ¨¡åž‹ï¼ˆäº¤äº’æ¨¡å¼ä½¿ç”¨ï¼‰
    print("ðŸ“¦ åŠ è½½ AR æ¨¡åž‹ (äº¤äº’æ¨¡å¼)...")
    load_pipeline(
        DEFAULT_MODEL_PATH, 
        DEFAULT_AR_ACTION_PATH, 
        "ar", 
        "bf16", 
        enable_sr=False, 
        enable_offloading=False
    )
    print("âœ… AR æ¨¡åž‹åŠ è½½å®Œæˆ!\n")
    
    # é¢„åŠ è½½åŒå‘æ¨¡åž‹ï¼ˆè½¨è¿¹æ¨¡å¼ä½¿ç”¨ï¼‰
    print("ðŸ“¦ åŠ è½½åŒå‘æ¨¡åž‹ (è½¨è¿¹æ¨¡å¼)...")
    load_pipeline(
        DEFAULT_MODEL_PATH, 
        DEFAULT_BI_ACTION_PATH, 
        "bi", 
        "bf16", 
        enable_sr=False, 
        enable_offloading=False
    )
    print("âœ… åŒå‘æ¨¡åž‹åŠ è½½å®Œæˆ!\n")
    
    print("="*60)
    print("ðŸŽ‰ æ‰€æœ‰æ¨¡åž‹é¢„åŠ è½½å®Œæˆï¼å¯ä»¥å¼€å§‹ä½¿ç”¨äº†ã€‚")
    print("="*60 + "\n")


if __name__ == "__main__":
    # å¯åŠ¨ Gradio UIï¼ˆæ¨¡åž‹å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åŠ è½½ï¼‰
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
