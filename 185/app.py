import os
import cv2
import numpy as np
import torch
import gradio as gr
from PIL import Image
import torch.nn.functional as F
from torchvision.transforms import functional as TF 
from detectron2.checkpoint import DetectionCheckpointer
import warnings
import time

warnings.filterwarnings("ignore")

def load_model(model_name):
    """åŠ è½½æŒ‡å®šçš„æ¨¡å‹"""
    try:
        torch.set_grad_enabled(False)
        
        # é€‰æ‹©æƒé‡æ–‡ä»¶
        if model_name == "SDMatte":
            checkpoint_path = "checkpoints/SDMatte/SDMatte.pth"
            # ç›´æ¥å¯¼å…¥æ¨¡å‹ç±»
            from modeling.SDMatte import SDMatte
            
            # æ‰‹åŠ¨åˆ›å»ºæ¨¡å‹å‚æ•°ï¼Œé¿å…åŠ è½½é…ç½®æ–‡ä»¶
            model_kwargs = {
                'pretrained_model_name_or_path': "checkpoints/SDMatte",
                'load_weight': False,
                'conv_scale': 3,
                'num_inference_steps': 1,
                'aux_input': "bbox_mask",
                'add_noise': False,
                'use_dis_loss': True,
                'use_aux_input': True,
                'use_coor_input': True,
                'use_attention_mask': True,
                'residual_connection': False,
                'use_encoder_hidden_states': True,
                'use_attention_mask_list': [True, True, True],
                'use_encoder_hidden_states_list': [False, True, False],
            }
            model = SDMatte(**model_kwargs)
            
        elif model_name == "LiteSDMatte":
            checkpoint_path = "checkpoints/LiteSDMatte/LiteSDMatte.pth"
            # ç›´æ¥å¯¼å…¥æ¨¡å‹ç±»
            from modeling.LiteSDMatte import LiteSDMatte
            
            # æ‰‹åŠ¨åˆ›å»ºæ¨¡å‹å‚æ•°ï¼Œé¿å…åŠ è½½é…ç½®æ–‡ä»¶
            model_kwargs = {
                'pretrained_model_name_or_path': "checkpoints/LiteSDMatte",
                'load_weight': False,
                'conv_scale': 3,
                'num_inference_steps': 1,
                'aux_input': "bbox_mask",
                'add_noise': False,
                'use_dis_loss': True,
                'use_aux_input': True,
                'use_coor_input': True,
                'use_attention_mask': True,
                'residual_connection': False,
                'use_encoder_hidden_states': True,
                'use_attention_mask_list': [True, True, True],
                'use_encoder_hidden_states_list': [False, True, False],
            }
            model = LiteSDMatte(**model_kwargs)
            
        else:
            raise ValueError(f"Unsupported model: {model_name}")
        
        # è®¾ç½®è®¾å¤‡
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model.to(device)
        
        # åŠ è½½æƒé‡
        DetectionCheckpointer(model).load(checkpoint_path)
        model.eval()
        
        print(f"âœ… {model_name} æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, None  # ä¸è¿”å›cfgï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦å®ƒ
        
    except Exception as e:
        print(f"âŒ {model_name} æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None

def preprocess_image(image):
    """é¢„å¤„ç†è¾“å…¥å›¾åƒ"""
    try:
        # ç¡®ä¿å›¾åƒæ˜¯PIL Image
        if isinstance(image, str):
            # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œå°è¯•æ‰“å¼€
            image = Image.open(image)
        elif isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        elif not isinstance(image, Image.Image):
            raise ValueError("Input must be PIL Image, numpy array, or file path")
        
        # è½¬æ¢ä¸ºRGB
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # è·å–åŸå§‹å°ºå¯¸
        original_size = image.size
        
        # è½¬æ¢ä¸ºnumpy arrayï¼Œä¿æŒ0-255èŒƒå›´ï¼ˆåç»­åœ¨prepare_model_inputä¸­å¤„ç†å½’ä¸€åŒ–ï¼‰
        image_np = np.array(image).astype(np.float32)
        
        # è°ƒæ•´å¤§å°åˆ°512x512ï¼ˆä¸æ¨¡å‹è®­ç»ƒå°ºå¯¸ä¸€è‡´ï¼‰
        image_resized = cv2.resize(image_np, (512, 512), interpolation=cv2.INTER_LINEAR)
        
        return image_resized, original_size
        
    except Exception as e:
        print(f"å›¾åƒé¢„å¤„ç†é”™è¯¯: {str(e)}")
        return None, None

def generate_simple_mask(image):
    """ç”Ÿæˆç®€å•çš„å‰æ™¯æ©ç ï¼ˆåŸºäºå›¾åƒä¸­å¿ƒåŒºåŸŸï¼‰"""
    h, w = image.shape[:2]
    mask = np.zeros((h, w), dtype=np.float32)
    
    # åˆ›å»ºä¸€ä¸ªä¸­å¿ƒåŒºåŸŸçš„æ©ç ï¼ˆæ¤­åœ†å½¢ï¼‰
    center_h, center_w = h // 2, w // 2
    mask_h, mask_w = int(h * 0.6), int(w * 0.6)
    
    # åˆ›å»ºæ¤­åœ†æ©ç 
    y, x = np.ogrid[:h, :w]
    ellipse_mask = ((y - center_h) ** 2) / (mask_h/2) ** 2 + ((x - center_w) ** 2) / (mask_w/2) ** 2 <= 1
    mask[ellipse_mask] = 1.0
    
    return mask

def inference_single_image(image, model, model_name):
    """å¯¹å•å¼ å›¾åƒè¿›è¡Œæ¨ç†"""
    try:
        start_time = time.time()
        
        # é¢„å¤„ç†å›¾åƒ
        processed_image, original_size = preprocess_image(image)
        
        if processed_image is None or original_size is None:
            return None, 0
        
        # ç”Ÿæˆç®€å•æ©ç 
        mask = generate_simple_mask(processed_image)
        
        # å‡†å¤‡æ¨¡å‹è¾“å…¥ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸ä½¿ç”¨trimapï¼‰
        model_input = prepare_model_input(processed_image, mask, None)
        
        if model_input is None:
            return None, 0
        
        # æ¨¡å‹æ¨ç†
        inference_start = time.time()
        with torch.no_grad():
            output = model(model_input)
        inference_time = time.time() - inference_start
        
        # åå¤„ç†è¾“å‡º
        result_image = postprocess_output(output, original_size)
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        
        total_time = time.time() - start_time
        print(f"âœ… {model_name} æ¨ç†å®Œæˆ - æ€»æ—¶é—´: {total_time:.3f}s (æ¨¡å‹æ¨ç†: {inference_time:.3f}s)")
        
        return result_image, total_time
        
    except Exception as e:
        print(f"æ¨ç†é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, 0

def generate_bbox_from_mask(mask):
    """ä»æ©ç ç”Ÿæˆè¾¹ç•Œæ¡†åæ ‡"""
    h, w = mask.shape
    coords = np.nonzero(mask)
    
    if coords[0].size == 0 or coords[1].size == 0:
        return np.array([0, 0, 1, 1])
    
    y_min, x_min = np.argwhere(mask).min(axis=0)
    y_max, x_max = np.argwhere(mask).max(axis=0)
    
    # å½’ä¸€åŒ–åæ ‡
    y_min, y_max = y_min / h, y_max / h
    x_min, x_max = x_min / w, x_max / w
    
    return np.array([x_min, y_min, x_max, y_max])

def prepare_model_input(image, mask, trimap):
    """å‡†å¤‡æ¨¡å‹è¾“å…¥æ•°æ®"""
    try:
        # å›¾åƒä»0-255èŒƒå›´ç›´æ¥æ ‡å‡†åŒ–åˆ°[-1,1]
        image_normalized = (image / 255.0 - 0.5) / 0.5
        
        # è½¬æ¢å›¾åƒåˆ°tensor
        image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).unsqueeze(0).float().cuda()
        
        # è½¬æ¢æ©ç åˆ°tensor
        mask_tensor = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0).float().cuda()
        
        # ç”Ÿæˆè¾¹ç•Œæ¡†åæ ‡
        bbox_coords = generate_bbox_from_mask(mask)
        bbox_coords_tensor = torch.from_numpy(bbox_coords).unsqueeze(0).float().cuda()
        
        # åˆ›å»ºåŸºæœ¬æ•°æ®å­—å…¸
        data = {
            "image": image_tensor,
            "bbox_mask": mask_tensor,
            "bbox_coords": bbox_coords_tensor,
            "is_trans": torch.tensor([0]).cuda(),  # 0è¡¨ç¤ºä¸é€æ˜ç‰©ä½“
            "hw": torch.tensor([image.shape[0], image.shape[1]]).unsqueeze(0).cuda()
        }
        
        return data
        
    except Exception as e:
        print(f"å‡†å¤‡æ¨¡å‹è¾“å…¥é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def generate_point_coords(mask, num_points=10):
    """ç”Ÿæˆç‚¹åæ ‡ï¼ˆæ¨¡æ‹ŸGenPointåŠŸèƒ½ï¼‰"""
    height, width = mask.shape
    
    # æ‰¾åˆ°æ©ç ä¸­çš„å‰æ™¯åƒç´ 
    mask_binary = (mask > 0.5).astype(np.float32)
    y_coords, x_coords = np.where(mask_binary == 1)
    
    if len(y_coords) < num_points:
        # å¦‚æœå‰æ™¯åƒç´ ä¸å¤Ÿï¼Œè¿”å›é›¶ç‚¹åæ ‡
        return np.zeros(20, dtype=np.float32)
    
    # éšæœºé€‰æ‹©ç‚¹
    selected_indices = np.random.choice(len(y_coords), size=num_points, replace=False)
    
    point_coords = []
    for idx in selected_indices:
        y_center = y_coords[idx]
        x_center = x_coords[idx]
        
        # å½’ä¸€åŒ–åæ ‡
        y_norm = y_center / height
        x_norm = x_center / width
        point_coords.extend([x_norm, y_norm])
    
    # ç¡®ä¿æ€»æ˜¯20ä¸ªåæ ‡ï¼ˆ10ä¸ªç‚¹ï¼Œæ¯ä¸ªç‚¹2ä¸ªåæ ‡ï¼‰
    if len(point_coords) < 20:
        point_coords.extend([0.0] * (20 - len(point_coords)))
    
    return np.array(point_coords, dtype=np.float32)

def generate_point_mask(mask, point_coords, radius=25):
    """ç”Ÿæˆç‚¹æ©ç ï¼ˆæ¨¡æ‹ŸGenPointåŠŸèƒ½ï¼‰"""
    import scipy.ndimage
    
    height, width = mask.shape
    point_mask = np.zeros_like(mask, dtype=np.float32)
    
    # å¤„ç†ç‚¹åæ ‡ï¼ˆæ¯ä¸¤ä¸ªå€¼ä»£è¡¨ä¸€ä¸ªç‚¹çš„x,yåæ ‡ï¼‰
    for i in range(0, len(point_coords), 2):
        if i + 1 < len(point_coords):
            x_norm, y_norm = point_coords[i], point_coords[i + 1]
            if x_norm == 0 and y_norm == 0:
                continue
                
            # è½¬æ¢ä¸ºåƒç´ åæ ‡
            x_center = int(x_norm * width)
            y_center = int(y_norm * height)
            
            # åˆ›å»ºé«˜æ–¯æ©ç 
            tmp_mask = np.zeros_like(mask, dtype=np.float32)
            if 0 <= y_center < height and 0 <= x_center < width:
                tmp_mask[y_center, x_center] = 1
                tmp_mask = scipy.ndimage.gaussian_filter(tmp_mask, sigma=radius)
                if np.max(tmp_mask) > 0:
                    tmp_mask /= np.max(tmp_mask)
                point_mask = np.maximum(point_mask, tmp_mask)
    
    return point_mask

def prepare_model_input(image, mask, trimap):
    """å‡†å¤‡æ¨¡å‹è¾“å…¥æ•°æ®"""
    try:
        # å›¾åƒä»0-255èŒƒå›´ç›´æ¥æ ‡å‡†åŒ–åˆ°[-1,1]
        image_normalized = (image / 255.0 - 0.5) / 0.5
        
        # è½¬æ¢å›¾åƒåˆ°tensor
        image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).unsqueeze(0).float().cuda()
        
        # è½¬æ¢æ©ç åˆ°tensor
        mask_tensor = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0).float().cuda()
        
        # ç”Ÿæˆè¾¹ç•Œæ¡†åæ ‡
        bbox_coords = generate_bbox_from_mask(mask)
        bbox_coords_tensor = torch.from_numpy(bbox_coords).unsqueeze(0).float().cuda()
        
        # åˆ›å»ºåŸºæœ¬æ•°æ®å­—å…¸
        data = {
            "image": image_tensor,
            "bbox_mask": mask_tensor,
            "bbox_coords": bbox_coords_tensor,
            "is_trans": torch.tensor([0]).cuda(),  # 0è¡¨ç¤ºä¸é€æ˜ç‰©ä½“
            "hw": torch.tensor([image.shape[0], image.shape[1]]).unsqueeze(0).cuda()
        }
        
        return data
        
        return data
        
    except Exception as e:
        print(f"å‡†å¤‡æ¨¡å‹è¾“å…¥é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def postprocess_output(output, original_size):
    """åå¤„ç†æ¨¡å‹è¾“å‡º"""
    try:
        # æŒ‰ç…§å®˜æ–¹inference.pyçš„å¤„ç†æ–¹å¼
        # output = pred.flatten(0, 2) * 255
        output = output.flatten(0, 2) * 255
        
        # å°†è¾“å‡ºè½¬æ¢ä¸ºnumpy array
        output = output.detach().cpu().numpy()
        
        # ç¡®ä¿è¾“å‡ºåœ¨æ­£ç¡®èŒƒå›´å†…
        output = np.clip(output, 0, 255).astype(np.uint8)
        
        # è°ƒæ•´å›åŸå§‹å°ºå¯¸
        output_resized = cv2.resize(output, original_size, interpolation=cv2.INTER_LINEAR)
        
        # è½¬æ¢ä¸ºPIL Imageï¼ˆå®˜æ–¹ä½¿ç”¨F.to_pil_image(output).convert("RGB")ï¼‰
        from torchvision.transforms import functional as F
        output_pil = F.to_pil_image(output_resized).convert("RGB")
        
        return output_pil
        
    except Exception as e:
        print(f"åå¤„ç†é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def matting_inference(image):
    """ä¸»æ¨ç†å‡½æ•°ï¼ŒåŒæ—¶ä½¿ç”¨SDMatteå’ŒLiteSDMatteå¤„ç†å›¾åƒ"""
    if image is None:
        return None, None, "è¯·ä¸Šä¼ å›¾åƒ"
    
    try:
        total_start_time = time.time()
        print("ğŸš€ å¼€å§‹æ¨ç†...")
        
        # åŠ è½½SDMatteæ¨¡å‹
        print("ğŸ“¥ åŠ è½½ SDMatte æ¨¡å‹...")
        sdmatte_model, sdmatte_cfg = load_model("SDMatte")
        
        if sdmatte_model is None:
            return None, None, "SDMatte æ¨¡å‹åŠ è½½å¤±è´¥"
        
        print("ğŸ”„ ä½¿ç”¨ SDMatte è¿›è¡Œæ¨ç†...")
        sdmatte_result, sdmatte_time = inference_single_image(image, sdmatte_model, "SDMatte")
        
        # æ¸…ç†å†…å­˜
        del sdmatte_model
        torch.cuda.empty_cache()
        
        if sdmatte_result is None:
            return None, None, "SDMatte æ¨ç†å¤±è´¥"
        
        # åŠ è½½LiteSDMatteæ¨¡å‹
        print("ğŸ“¥ åŠ è½½ LiteSDMatte æ¨¡å‹...")
        litesdmatte_model, litesdmatte_cfg = load_model("LiteSDMatte")
        
        if litesdmatte_model is None:
            status_msg = f"LiteSDMatte æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½† SDMatte æ¨ç†æˆåŠŸ\nâ±ï¸ SDMatte æ¨ç†æ—¶é—´: {sdmatte_time:.3f}s"
            return sdmatte_result, None, status_msg
        
        print("ğŸ”„ ä½¿ç”¨ LiteSDMatte è¿›è¡Œæ¨ç†...")
        litesdmatte_result, litesdmatte_time = inference_single_image(image, litesdmatte_model, "LiteSDMatte")
        
        # æ¸…ç†å†…å­˜
        del litesdmatte_model
        torch.cuda.empty_cache()
        
        total_time = time.time() - total_start_time
        
        if litesdmatte_result is None:
            status_msg = f"LiteSDMatte æ¨ç†å¤±è´¥ï¼Œä½† SDMatte æ¨ç†æˆåŠŸ\nâ±ï¸ SDMatte æ¨ç†æ—¶é—´: {sdmatte_time:.3f}s"
            return sdmatte_result, None, status_msg
        
        # è®¡ç®—é€Ÿåº¦æ¯”è¾ƒ
        speed_comparison = ""
        if sdmatte_time > 0 and litesdmatte_time > 0:
            if sdmatte_time > litesdmatte_time:
                ratio = sdmatte_time / litesdmatte_time
                speed_comparison = f"\nğŸš€ LiteSDMatte æ¯” SDMatte å¿« {ratio:.1f}x"
            else:
                ratio = litesdmatte_time / sdmatte_time
                speed_comparison = f"\nğŸš€ SDMatte æ¯” LiteSDMatte å¿« {ratio:.1f}x"
        
        status_msg = f"""âœ… æ¨ç†å®Œæˆ
â±ï¸ æ¨ç†æ—¶é—´ç»Ÿè®¡:
   â€¢ SDMatte: {sdmatte_time:.3f}s
   â€¢ LiteSDMatte: {litesdmatte_time:.3f}s
   â€¢ æ€»æ—¶é—´: {total_time:.3f}s{speed_comparison}"""
        
        print("âœ… æ¨ç†å®Œæˆ")
        return sdmatte_result, litesdmatte_result, status_msg
        
    except Exception as e:
        error_msg = f"æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return None, None, error_msg

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    """åˆ›å»ºGradio Webç•Œé¢"""
    
    with gr.Blocks(title="SDMatte & LiteSDMatte Web UI", theme=gr.themes.Soft()) as demo:
        gr.HTML("""
        <div style="text-align: center; margin: 20px;">
            <h1>ğŸ­ SDMatte & LiteSDMatte å›¾åƒæŠ å›¾ç³»ç»Ÿ</h1>
            <p>ä¸Šä¼ å›¾åƒï¼Œä½¿ç”¨SDMatteå’ŒLiteSDMatteæ¨¡å‹è¿›è¡Œè‡ªåŠ¨æŠ å›¾å¤„ç†</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.HTML("<h3>ğŸ“¤ è¾“å…¥</h3>")
                input_image = gr.Image(
                    label="ä¸Šä¼ å›¾åƒ",
                    type="pil",
                    height=400
                )
                
                inference_button = gr.Button(
                    "ğŸš€ å¼€å§‹æ¨ç†",
                    variant="primary",
                    size="lg"
                )
                
                status_text = gr.Textbox(
                    label="çŠ¶æ€ä¿¡æ¯",
                    value="ç­‰å¾…ä¸Šä¼ å›¾åƒ...",
                    interactive=False,
                    lines=6,
                    max_lines=10
                )
            
            with gr.Column(scale=2):
                gr.HTML("<h3>ğŸ“¤ è¾“å‡ºç»“æœ</h3>")
                
                with gr.Row():
                    with gr.Column():
                        gr.HTML("<h4>SDMatte ç»“æœ</h4>")
                        sdmatte_output = gr.Image(
                            label="SDMatte æŠ å›¾ç»“æœ",
                            type="pil",
                            height=300
                        )
                    
                    with gr.Column():
                        gr.HTML("<h4>LiteSDMatte ç»“æœ</h4>")
                        litesdmatte_output = gr.Image(
                            label="LiteSDMatte æŠ å›¾ç»“æœ", 
                            type="pil",
                            height=300
                        )
        
        # ä½¿ç”¨è¯´æ˜
        gr.HTML("""
        <div style="margin: 20px; padding: 15px; background: rgba(255,255,255,0.1); border: 1px solid rgba(255,255,255,0.2); border-radius: 10px; backdrop-filter: blur(10px);">
            <h3 style="color: var(--body-text-color, #333); margin-bottom: 15px;">ğŸ“‹ ä½¿ç”¨è¯´æ˜</h3>
            <ul style="color: var(--body-text-color, #666); line-height: 1.6;">
                <li>ğŸ–¼ï¸ æ”¯æŒå¸¸è§å›¾åƒæ ¼å¼ï¼šJPGã€PNGã€JPEGç­‰</li>
                <li>ğŸ¯ ç³»ç»Ÿä¼šè‡ªåŠ¨ç”Ÿæˆå‰æ™¯åŒºåŸŸæ©ç </li>
                <li>âš¡ åŒæ—¶ä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œå¯¹æ¯”æ•ˆæœ</li>
                <li>ğŸ”„ å¤„ç†å®Œæˆåä¼šæ˜¾ç¤ºä¸¤ä¸ªæ¨¡å‹çš„æŠ å›¾ç»“æœ</li>
                <li>ğŸ’¾ å¯ä»¥å³é”®ä¿å­˜ç»“æœå›¾åƒ</li>
                <li>â±ï¸ æ¨ç†æ—¶é—´ä¼šæ˜¾ç¤ºåœ¨çŠ¶æ€æ ä¸­</li>
            </ul>
        </div>
        """)
        
        # ç»‘å®šäº‹ä»¶
        inference_button.click(
            fn=matting_inference,
            inputs=[input_image],
            outputs=[sdmatte_output, litesdmatte_output, status_text],
            show_progress=True
        )
        
        # ç¤ºä¾‹
        gr.Examples(
            examples=[],
            inputs=input_image,
            label="ğŸ“š ç¤ºä¾‹å›¾åƒï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰"
        )
    
    return demo

if __name__ == "__main__":
    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [
        "checkpoints/SDMatte/SDMatte.pth",
        "checkpoints/LiteSDMatte/LiteSDMatte.pth"
    ]
    
    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print("âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶:")
        for f in missing_files:
            print(f"   - {f}")
        print("\nè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨åé‡æ–°è¿è¡Œã€‚")
        exit(1)
    
    print("âœ… æ‰€æœ‰å¿…è¦æ–‡ä»¶å·²æ‰¾åˆ°")
    print("ğŸš€ å¯åŠ¨ Gradio Web UI...")
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,       # é»˜è®¤ç«¯å£
        share=False,            # ä¸åˆ›å»ºå…¬å…±é“¾æ¥
        debug=True              # è°ƒè¯•æ¨¡å¼
    )
