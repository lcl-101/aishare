# Copyright (c) 2025, Alibaba Cloud and its affiliates;
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Fun-Audio-Chat Gradio Web Demo
A simple Gradio-based web application for Fun-Audio-Chat model inference.
Supports both Speech-to-Text (S2T) and Speech-to-Speech (S2S) modes.
"""

import os
import sys
import uuid
import json
import torch
import librosa
import torchaudio
import gradio as gr
import numpy as np
from loguru import logger

# Register Fun-Audio-Chat model
from funaudiochat.register import register_funaudiochat
register_funaudiochat()

from transformers import AutoConfig, AutoProcessor, AutoModelForSeq2SeqLM

from utils.cosyvoice_detokenizer import get_audio_detokenizer, token2wav
from utils.constant import (
    DEFAULT_S2M_GEN_KWARGS,
    DEFAULT_SP_GEN_KWARGS,
    DEFAULT_S2T_PROMPT,
    SPOKEN_S2M_PROMPT,
    AUDIO_TEMPLATE,
)

# ============= Configuration =============
MODEL_PATH = "checkpoints/Fun-Audio-Chat-8B"
TTS_MODEL_PATH = "checkpoints/Fun-CosyVoice3-0.5B-2512"
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
SAMPLE_RATE = 16000
OUTPUT_SAMPLE_RATE = 24000  # CosyVoice output sample rate

# ============= Example Audio Files =============
# You can add more example audio files here
EXAMPLE_AUDIO_DIR = "examples"
EXAMPLE_AUDIOS = [
    "examples/ck7vv9ag.wav",  # Default example from the repo
]

# ============= Preset System Prompts =============
PRESET_PROMPTS = {
    "default": ("é»˜è®¤å¯¹è¯", DEFAULT_S2T_PROMPT),
    "transcribe": ("è¯­éŸ³è½¬å†™ (ASR)", "Please transcribe the audio content accurately."),
    "translate_en": ("ç¿»è¯‘æˆè‹±æ–‡", "Please translate the audio content into English."),
    "translate_zh": ("ç¿»è¯‘æˆä¸­æ–‡", "Please translate the audio content into Chinese."),
    "summarize": ("å†…å®¹æ€»ç»“", "Please summarize the main points of the audio content."),
    "qa": ("é—®ç­”åŠ©æ‰‹", "You are a helpful assistant. Please answer the question in the audio."),
}

# ============= Knowledge Base Configuration =============
# Simple in-memory knowledge base (can be replaced with vector DB like FAISS, Milvus, etc.)
KNOWLEDGE_BASE = {}

def load_knowledge_base(file_path: str) -> dict:
    """Load knowledge base from a text file. Each line is a knowledge entry."""
    kb = {}
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                line = line.strip()
                if line:
                    kb[f"doc_{i}"] = line
    return kb

def simple_search(query: str, knowledge_base: dict, top_k: int = 3) -> list:
    """
    Simple search in knowledge base with Chinese support.
    For production, use vector similarity search (FAISS, Milvus, etc.)
    """
    if not knowledge_base:
        return []
    
    results = []
    query_lower = query.lower()
    
    # For Chinese: use character-level matching
    # Remove common punctuation and whitespace
    import re
    query_chars = set(re.sub(r'[ï¼Œã€‚ï¼Ÿï¼ã€\s\?\!\.\,]', '', query_lower))
    
    for doc_id, content in knowledge_base.items():
        content_lower = content.lower()
        
        # Score 1: Check if any query character appears in content
        char_score = sum(1 for char in query_chars if char in content_lower)
        
        # Score 2: Check for substring match (important for Chinese)
        # Extract key phrases (2-4 character combinations)
        substring_score = 0
        for i in range(len(query_lower)):
            for length in [2, 3, 4]:
                if i + length <= len(query_lower):
                    phrase = query_lower[i:i+length]
                    # Skip if phrase is all punctuation
                    if re.match(r'^[ï¼Œã€‚ï¼Ÿï¼ã€\s\?\!\.\,]+$', phrase):
                        continue
                    if phrase in content_lower:
                        substring_score += length  # Longer matches score higher
        
        total_score = char_score + substring_score * 2  # Weight substring matches higher
        
        if total_score > 0:
            results.append((total_score, content))
    
    # Sort by score and return top_k
    results.sort(key=lambda x: x[0], reverse=True)
    return [content for score, content in results[:top_k]]

RAG_SYSTEM_PROMPT_TEMPLATE = """You are a helpful assistant. Answer the user's question based on the following knowledge base content.

## Knowledge Base:
{knowledge_content}

## Instructions:
- Answer based on the knowledge provided above
- If the knowledge doesn't contain relevant information, say you don't know
- Be concise and accurate
- Respond in the same language as the user's question"""

# ============= Global Model Variables =============
model = None
processor = None
cosyvoice_model = None

def load_models():
    """Load Fun-Audio-Chat model and CosyVoice TTS model"""
    global model, processor, cosyvoice_model
    
    logger.info(f"Loading Fun-Audio-Chat model from {MODEL_PATH}...")
    config = AutoConfig.from_pretrained(MODEL_PATH)
    processor = AutoProcessor.from_pretrained(MODEL_PATH)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        MODEL_PATH, 
        config=config, 
        torch_dtype=torch.bfloat16, 
        device_map=DEVICE
    )
    logger.info("Fun-Audio-Chat model loaded successfully!")
    
    # Modify CosyVoice model path in cosyvoice_detokenizer
    logger.info(f"Loading CosyVoice TTS model from {TTS_MODEL_PATH}...")
    
    # Import CosyVoice modules
    current_dir = os.path.dirname(os.path.abspath(__file__))
    submodule_path = os.path.join(current_dir, 'third_party/CosyVoice')
    sys.path.insert(0, submodule_path)
    matcha_tts_path = os.path.join(current_dir, 'third_party/CosyVoice/third_party/Matcha-TTS')
    sys.path.insert(0, matcha_tts_path)
    
    from cosyvoice.cli.cosyvoice import CosyVoice3
    
    cosyvoice_model = CosyVoice3(
        TTS_MODEL_PATH,
        load_trt=False, 
        load_vllm=False, 
        fp16=False
    )
    cosyvoice_model.model.flow.decoder.estimator.static_chunk_size = 2 * 25 * 30
    logger.info("CosyVoice TTS model loaded successfully!")
    
    return "Models loaded successfully!"

def process_audio_input(audio_path):
    """Process audio input and return audio array"""
    if audio_path is None:
        return None
    
    # Load audio with librosa (resample to 16kHz)
    audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE)
    return audio

def speech_to_text(audio_path, system_prompt=None):
    """
    Speech-to-Text inference: Generate text response from audio input.
    
    Args:
        audio_path: Path to input audio file
        system_prompt: Optional custom system prompt
    
    Returns:
        Generated text response
    """
    global model, processor
    
    if model is None or processor is None:
        return "Error: Models not loaded. Please click 'Load Models' first."
    
    if audio_path is None:
        return "Error: Please upload or record an audio file."
    
    try:
        # Load audio
        audio = [process_audio_input(audio_path)]
        
        # Set generation parameters for text-only mode
        model.sp_gen_kwargs.update({
            'text_greedy': True, 
            'disable_speech': True,
        })
        
        # Build conversation
        prompt = system_prompt if system_prompt else DEFAULT_S2T_PROMPT
        conversation = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": AUDIO_TEMPLATE},
        ]
        
        # Process input
        text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
        inputs = processor(text=text, audio=audio, return_tensors="pt", return_token_type_ids=False).to(model.device)
        
        # Generate
        generate_ids, _ = model.generate(**inputs)
        generate_ids = generate_ids[:, inputs.input_ids.size(1):]
        generate_text = processor.decode(generate_ids[0], skip_special_tokens=True)
        
        return generate_text
        
    except Exception as e:
        logger.error(f"Error in speech_to_text: {e}")
        return f"Error: {str(e)}"

def speech_to_speech(audio_path, system_prompt=None):
    """
    Speech-to-Speech inference: Generate both text and audio response from audio input.
    
    Args:
        audio_path: Path to input audio file
        system_prompt: Optional custom system prompt
    
    Returns:
        Tuple of (generated_text, audio_output_path)
    """
    global model, processor, cosyvoice_model
    
    if model is None or processor is None:
        return "Error: Models not loaded. Please click 'Load Models' first.", None
    
    if cosyvoice_model is None:
        return "Error: TTS model not loaded.", None
    
    if audio_path is None:
        return "Error: Please upload or record an audio file.", None
    
    try:
        # Load audio
        audio = [process_audio_input(audio_path)]
        
        # Set generation parameters for speech+text mode
        sp_gen_kwargs = DEFAULT_SP_GEN_KWARGS.copy()
        sp_gen_kwargs['text_greedy'] = True
        gen_kwargs = DEFAULT_S2M_GEN_KWARGS.copy()
        gen_kwargs['max_new_tokens'] = 2048
        model.sp_gen_kwargs.update(sp_gen_kwargs)
        
        # Build conversation
        prompt = system_prompt if system_prompt else SPOKEN_S2M_PROMPT
        conversation = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": AUDIO_TEMPLATE},
        ]
        
        # Process input
        text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
        inputs = processor(text=text, audio=audio, return_tensors="pt", return_token_type_ids=False).to(model.device)
        
        # Generate text and audio tokens
        generate_ids, audio_ids = model.generate(**inputs, **gen_kwargs)
        generate_ids = generate_ids[:, inputs.input_ids.size(1):]
        generate_text = processor.decode(generate_ids[0], skip_special_tokens=True)
        
        # Filter valid audio tokens (0-6560)
        token_for_cosyvoice = list(filter(lambda x: 0 <= x < 6561, audio_ids[0].tolist()))
        
        if len(token_for_cosyvoice) == 0:
            return generate_text, None
        
        # Load speaker embedding (default Chinese female voice)
        current_dir = os.path.dirname(os.path.abspath(__file__))
        spk_emb_path = os.path.join(current_dir, "utils/new_spk2info.pt")
        embedding = torch.load(spk_emb_path)["ä¸­æ–‡å¥³"]["embedding"]
        
        # Convert audio tokens to waveform
        logger.info(f"Converting {len(token_for_cosyvoice)} audio tokens to waveform...")
        speech = token2wav(
            cosyvoice_model, 
            token_for_cosyvoice, 
            embedding=embedding, 
            token_hop_len=25 * 30, 
            pre_lookahead_len=3
        )
        
        # Save output audio
        output_uuid = str(uuid.uuid4())[:8]
        output_dir = "outputs"
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, f"output_{output_uuid}.wav")
        
        torchaudio.save(output_path, speech.cpu(), cosyvoice_model.sample_rate)
        logger.info(f"Audio saved to: {output_path}")
        
        return generate_text, output_path
        
    except Exception as e:
        logger.error(f"Error in speech_to_speech: {e}")
        import traceback
        traceback.print_exc()
        return f"Error: {str(e)}", None

def update_prompt_from_preset(preset_key):
    """Update system prompt based on preset selection"""
    if preset_key and preset_key in PRESET_PROMPTS:
        return PRESET_PROMPTS[preset_key][1]
    return ""

def rag_speech_to_text(audio_path, knowledge_text=None):
    """
    RAG-based Speech-to-Text: First transcribe, then search knowledge base, then answer.
    
    Args:
        audio_path: Path to input audio file
        knowledge_text: Knowledge base content (one entry per line)
    
    Returns:
        Tuple of (transcribed_question, retrieved_knowledge, generated_answer)
    """
    global model, processor
    
    if model is None or processor is None:
        return "Error: Models not loaded.", "", ""
    
    if audio_path is None:
        return "Error: Please upload or record an audio file.", "", ""
    
    try:
        # Step 1: Transcribe audio to get the question
        audio = [process_audio_input(audio_path)]
        
        model.sp_gen_kwargs.update({
            'text_greedy': True, 
            'disable_speech': True,
        })
        
        # First pass: transcribe the question
        transcribe_prompt = "Please transcribe the audio content accurately."
        conversation = [
            {"role": "system", "content": transcribe_prompt},
            {"role": "user", "content": AUDIO_TEMPLATE},
        ]
        
        text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
        inputs = processor(text=text, audio=audio, return_tensors="pt", return_token_type_ids=False).to(model.device)
        generate_ids, _ = model.generate(**inputs)
        generate_ids = generate_ids[:, inputs.input_ids.size(1):]
        question = processor.decode(generate_ids[0], skip_special_tokens=True)
        
        logger.info(f"Transcribed question: {question}")
        
        # Step 2: Search knowledge base
        if knowledge_text and knowledge_text.strip():
            # Build knowledge base from input text
            kb = {}
            for i, line in enumerate(knowledge_text.strip().split('\n')):
                line = line.strip()
                if line:
                    kb[f"doc_{i}"] = line
            
            # Search for relevant knowledge
            retrieved_docs = simple_search(question, kb, top_k=3)
            knowledge_content = "\n".join([f"- {doc}" for doc in retrieved_docs]) if retrieved_docs else "No relevant knowledge found."
        else:
            knowledge_content = "No knowledge base provided."
            retrieved_docs = []
        
        logger.info(f"Retrieved knowledge: {knowledge_content}")
        
        # Step 3: Generate answer based on knowledge
        rag_prompt = RAG_SYSTEM_PROMPT_TEMPLATE.format(knowledge_content=knowledge_content)
        
        conversation = [
            {"role": "system", "content": rag_prompt},
            {"role": "user", "content": AUDIO_TEMPLATE},
        ]
        
        text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
        inputs = processor(text=text, audio=audio, return_tensors="pt", return_token_type_ids=False).to(model.device)
        generate_ids, _ = model.generate(**inputs)
        generate_ids = generate_ids[:, inputs.input_ids.size(1):]
        answer = processor.decode(generate_ids[0], skip_special_tokens=True)
        
        return question, knowledge_content, answer
        
    except Exception as e:
        logger.error(f"Error in rag_speech_to_text: {e}")
        import traceback
        traceback.print_exc()
        return f"Error: {str(e)}", "", ""

def rag_speech_to_speech(audio_path, knowledge_text=None):
    """
    RAG-based Speech-to-Speech: Answer with both text and audio based on knowledge base.
    
    Args:
        audio_path: Path to input audio file
        knowledge_text: Knowledge base content (one entry per line)
    
    Returns:
        Tuple of (transcribed_question, retrieved_knowledge, generated_answer, audio_output_path)
    """
    global model, processor, cosyvoice_model
    
    if model is None or processor is None:
        return "Error: Models not loaded.", "", "", None
    
    if cosyvoice_model is None:
        return "Error: TTS model not loaded.", "", "", None
    
    if audio_path is None:
        return "Error: Please upload or record an audio file.", "", "", None
    
    try:
        # Step 1: Transcribe audio to get the question
        audio = [process_audio_input(audio_path)]
        
        model.sp_gen_kwargs.update({
            'text_greedy': True, 
            'disable_speech': True,
        })
        
        transcribe_prompt = "Please transcribe the audio content accurately."
        conversation = [
            {"role": "system", "content": transcribe_prompt},
            {"role": "user", "content": AUDIO_TEMPLATE},
        ]
        
        text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
        inputs = processor(text=text, audio=audio, return_tensors="pt", return_token_type_ids=False).to(model.device)
        generate_ids, _ = model.generate(**inputs)
        generate_ids = generate_ids[:, inputs.input_ids.size(1):]
        question = processor.decode(generate_ids[0], skip_special_tokens=True)
        
        logger.info(f"Transcribed question: {question}")
        
        # Step 2: Search knowledge base
        if knowledge_text and knowledge_text.strip():
            kb = {}
            for i, line in enumerate(knowledge_text.strip().split('\n')):
                line = line.strip()
                if line:
                    kb[f"doc_{i}"] = line
            
            retrieved_docs = simple_search(question, kb, top_k=3)
            knowledge_content = "\n".join([f"- {doc}" for doc in retrieved_docs]) if retrieved_docs else "No relevant knowledge found."
        else:
            knowledge_content = "No knowledge base provided."
            retrieved_docs = []
        
        logger.info(f"Retrieved knowledge: {knowledge_content}")
        
        # Step 3: Generate answer with speech
        rag_prompt = RAG_SYSTEM_PROMPT_TEMPLATE.format(knowledge_content=knowledge_content)
        # Add speech generation instruction
        rag_prompt_with_speech = rag_prompt + "\n\nYou are asked to generate both text and speech tokens at the same time."
        
        sp_gen_kwargs = DEFAULT_SP_GEN_KWARGS.copy()
        sp_gen_kwargs['text_greedy'] = True
        gen_kwargs = DEFAULT_S2M_GEN_KWARGS.copy()
        gen_kwargs['max_new_tokens'] = 2048
        model.sp_gen_kwargs.update(sp_gen_kwargs)
        
        conversation = [
            {"role": "system", "content": rag_prompt_with_speech},
            {"role": "user", "content": AUDIO_TEMPLATE},
        ]
        
        text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
        inputs = processor(text=text, audio=audio, return_tensors="pt", return_token_type_ids=False).to(model.device)
        generate_ids, audio_ids = model.generate(**inputs, **gen_kwargs)
        generate_ids = generate_ids[:, inputs.input_ids.size(1):]
        answer = processor.decode(generate_ids[0], skip_special_tokens=True)
        
        # Convert audio tokens to speech
        token_for_cosyvoice = list(filter(lambda x: 0 <= x < 6561, audio_ids[0].tolist()))
        
        output_path = None
        if len(token_for_cosyvoice) > 0:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            spk_emb_path = os.path.join(current_dir, "utils/new_spk2info.pt")
            embedding = torch.load(spk_emb_path)["ä¸­æ–‡å¥³"]["embedding"]
            
            logger.info(f"Converting {len(token_for_cosyvoice)} audio tokens to waveform...")
            speech = token2wav(
                cosyvoice_model, 
                token_for_cosyvoice, 
                embedding=embedding, 
                token_hop_len=25 * 30, 
                pre_lookahead_len=3
            )
            
            output_uuid = str(uuid.uuid4())[:8]
            output_dir = "outputs"
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir, f"rag_output_{output_uuid}.wav")
            
            torchaudio.save(output_path, speech.cpu(), cosyvoice_model.sample_rate)
            logger.info(f"Audio saved to: {output_path}")
        
        return question, knowledge_content, answer, output_path
        
    except Exception as e:
        logger.error(f"Error in rag_speech_to_speech: {e}")
        import traceback
        traceback.print_exc()
        return f"Error: {str(e)}", "", "", None

def create_gradio_interface():
    """Create and return the Gradio interface"""
    
    with gr.Blocks(
        title="Fun-Audio-Chat Demo"
    ) as demo:
        
        gr.Markdown(
            """
            # ğŸ™ï¸ Fun-Audio-Chat Demo
            
            **Fun-Audio-Chat** æ˜¯ä¸€ä¸ªå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒè‡ªç„¶ã€ä½å»¶è¿Ÿçš„è¯­éŸ³äº¤äº’ã€‚
            
            | åŠŸèƒ½ | è¯´æ˜ |
            |------|------|
            | ğŸ“ **Speech-to-Text** | è¯­éŸ³è½¬æ–‡å­— - ä¸Šä¼ éŸ³é¢‘è·å–æ–‡å­—å›å¤ |
            | ğŸ”Š **Speech-to-Speech** | è¯­éŸ³è½¬è¯­éŸ³ - ä¸Šä¼ éŸ³é¢‘è·å–è¯­éŸ³+æ–‡å­—å›å¤ |
            | ï¿½ **Knowledge Base QA** | çŸ¥è¯†åº“é—®ç­” - åŸºäºçŸ¥è¯†åº“çš„è¯­éŸ³é—®ç­” |
            
            ---
            """
        )
        
        # Model status display
        model_status = "âœ… Models loaded / æ¨¡å‹å·²åŠ è½½" if model is not None else "â³ Loading... / åŠ è½½ä¸­..."
        gr.Markdown(f"**æ¨¡å‹çŠ¶æ€:** {model_status}")
        
        gr.Markdown("---")
        
        # Main interface with tabs
        with gr.Tabs():
            # Speech-to-Text Tab
            with gr.TabItem("ğŸ“ Speech-to-Text (è¯­éŸ³è½¬æ–‡å­—)"):
                gr.Markdown("ä¸Šä¼ æˆ–å½•åˆ¶éŸ³é¢‘ï¼Œè·å–æ–‡å­—å›å¤ã€‚æ”¯æŒå¤šç§ä»»åŠ¡ï¼šå¯¹è¯ã€è½¬å†™ã€ç¿»è¯‘ã€æ€»ç»“ç­‰ã€‚")
                
                with gr.Row():
                    with gr.Column():
                        s2t_audio_input = gr.Audio(
                            label="Input Audio / è¾“å…¥éŸ³é¢‘",
                            type="filepath",
                            sources=["upload", "microphone"]
                        )
                        s2t_preset = gr.Dropdown(
                            label="Preset Prompt / é¢„è®¾æç¤ºè¯",
                            choices=[(v[0], k) for k, v in PRESET_PROMPTS.items()],
                            value="default",
                            interactive=True
                        )
                        s2t_system_prompt = gr.Textbox(
                            label="System Prompt / ç³»ç»Ÿæç¤ºè¯ (å¯è‡ªå®šä¹‰)",
                            placeholder="é€‰æ‹©é¢„è®¾æˆ–è‡ªå®šä¹‰æç¤ºè¯...",
                            value=DEFAULT_S2T_PROMPT,
                            lines=3
                        )
                        s2t_btn = gr.Button("ğŸ¯ Generate / ç”Ÿæˆ", variant="primary")
                    
                    with gr.Column():
                        s2t_output = gr.Textbox(
                            label="Generated Text / ç”Ÿæˆæ–‡æœ¬",
                            lines=10
                        )
                
                # Connect preset dropdown to prompt textbox
                s2t_preset.change(
                    fn=update_prompt_from_preset,
                    inputs=[s2t_preset],
                    outputs=[s2t_system_prompt]
                )
                
                # Example section
                gr.Markdown("**ğŸ“‚ ç¤ºä¾‹éŸ³é¢‘ (ç‚¹å‡»åŠ è½½):**")
                gr.Examples(
                    examples=[
                        ["examples/ck7vv9ag.wav", "default", DEFAULT_S2T_PROMPT],
                    ],
                    inputs=[s2t_audio_input, s2t_preset, s2t_system_prompt],
                    label="ç¤ºä¾‹"
                )
                
                s2t_btn.click(
                    fn=speech_to_text,
                    inputs=[s2t_audio_input, s2t_system_prompt],
                    outputs=s2t_output
                )
            
            # Speech-to-Speech Tab
            with gr.TabItem("ğŸ”Š Speech-to-Speech (è¯­éŸ³è½¬è¯­éŸ³)"):
                gr.Markdown("ä¸Šä¼ æˆ–å½•åˆ¶éŸ³é¢‘ï¼ŒåŒæ—¶è·å–è¯­éŸ³å’Œæ–‡å­—å›å¤ã€‚é»˜è®¤ä½¿ç”¨ä¸­æ–‡å¥³å£° (å°äº‘)ã€‚")
                
                with gr.Row():
                    with gr.Column():
                        s2s_audio_input = gr.Audio(
                            label="Input Audio / è¾“å…¥éŸ³é¢‘",
                            type="filepath",
                            sources=["upload", "microphone"]
                        )
                        s2s_system_prompt = gr.Textbox(
                            label="System Prompt / ç³»ç»Ÿæç¤ºè¯ (å¯é€‰)",
                            placeholder="ç•™ç©ºä½¿ç”¨é»˜è®¤äººè®¾ (å°äº‘: æ¥è‡ªæ­å·çš„æ¸©æŸ”å¥³å­©)",
                            value="",
                            lines=3
                        )
                        s2s_btn = gr.Button("ğŸ¯ Generate / ç”Ÿæˆ", variant="primary")
                    
                    with gr.Column():
                        s2s_text_output = gr.Textbox(
                            label="Generated Text / ç”Ÿæˆæ–‡æœ¬",
                            lines=5
                        )
                        s2s_audio_output = gr.Audio(
                            label="Generated Audio / ç”ŸæˆéŸ³é¢‘",
                            type="filepath"
                        )
                
                # Example section
                gr.Markdown("**ğŸ“‚ ç¤ºä¾‹éŸ³é¢‘ (ç‚¹å‡»åŠ è½½):**")
                gr.Examples(
                    examples=[
                        ["examples/ck7vv9ag.wav", ""],
                    ],
                    inputs=[s2s_audio_input, s2s_system_prompt],
                    label="ç¤ºä¾‹"
                )
                
                s2s_btn.click(
                    fn=speech_to_speech,
                    inputs=[s2s_audio_input, s2s_system_prompt],
                    outputs=[s2s_text_output, s2s_audio_output]
                )
            
            # RAG Knowledge Base Tab
            with gr.TabItem("ğŸ“š Knowledge Base QA (çŸ¥è¯†åº“é—®ç­”)"):
                gr.Markdown(
                    """
                    åŸºäºçŸ¥è¯†åº“çš„è¯­éŸ³é—®ç­”ã€‚ä¸Šä¼ ä½ çš„çŸ¥è¯†åº“å†…å®¹ï¼Œç„¶åç”¨è¯­éŸ³æé—®ï¼Œæ¨¡å‹ä¼šåŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”ã€‚
                    
                    **ä½¿ç”¨æ–¹æ³•:**
                    1. åœ¨å·¦ä¾§æ–‡æœ¬æ¡†è¾“å…¥ä½ çš„çŸ¥è¯†åº“å†…å®¹ï¼ˆæ¯è¡Œä¸€æ¡çŸ¥è¯†ï¼‰
                    2. å½•åˆ¶æˆ–ä¸Šä¼ ä½ çš„é—®é¢˜éŸ³é¢‘
                    3. é€‰æ‹©ä»…æ–‡å­—å›ç­”æˆ–è¯­éŸ³+æ–‡å­—å›ç­”
                    
                    **ç¤ºä¾‹çŸ¥è¯†åº“:**
                    ```
                    Fun-Audio-Chatæ˜¯é˜¿é‡Œäº‘å¼€å‘çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒè¯­éŸ³å¯¹è¯ã€‚
                    Fun-Audio-Chatä½¿ç”¨åŒåˆ†è¾¨ç‡è¯­éŸ³è¡¨ç¤ºæŠ€æœ¯ï¼Œå¸§ç‡ä¸º5Hzï¼Œæ¯”å…¶ä»–æ¨¡å‹æ›´é«˜æ•ˆã€‚
                    Fun-Audio-Chatæ”¯æŒè¯­éŸ³é—®ç­”ã€éŸ³é¢‘ç†è§£ã€è¯­éŸ³å‡½æ•°è°ƒç”¨ç­‰åŠŸèƒ½ã€‚
                    CosyVoiceæ˜¯ç”¨äºè¯­éŸ³åˆæˆçš„æ¨¡å‹ï¼Œå¯ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸ºè‡ªç„¶è¯­éŸ³ã€‚
                    ```
                    """
                )
                
                with gr.Row():
                    with gr.Column():
                        rag_knowledge = gr.Textbox(
                            label="Knowledge Base / çŸ¥è¯†åº“å†…å®¹ (æ¯è¡Œä¸€æ¡)",
                            placeholder="è¾“å…¥ä½ çš„çŸ¥è¯†åº“å†…å®¹ï¼Œæ¯è¡Œä¸€æ¡çŸ¥è¯†...\nä¾‹å¦‚:\nå…¬å¸æˆç«‹äº2020å¹´ï¼Œæ€»éƒ¨ä½äºæ­å·ã€‚\nå…¬å¸ä¸»è¦ä¸šåŠ¡æ˜¯äººå·¥æ™ºèƒ½ç ”å‘ã€‚\nå…¬å¸æœ‰500åå‘˜å·¥ã€‚",
                            lines=10,
                            value="""Fun-Audio-Chatæ˜¯é˜¿é‡Œäº‘é€šä¹‰å®éªŒå®¤å¼€å‘çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒè‡ªç„¶ã€ä½å»¶è¿Ÿçš„è¯­éŸ³äº¤äº’ã€‚
Fun-Audio-Chatä½¿ç”¨åŒåˆ†è¾¨ç‡è¯­éŸ³è¡¨ç¤ºæŠ€æœ¯ï¼ˆ5Hzéª¨å¹²ç½‘ç»œ+25Hzç²¾ç»†å¤´éƒ¨ï¼‰ï¼Œè®¡ç®—æ•ˆç‡æ¯”å…¶ä»–æ¨¡å‹é«˜50%ã€‚
Fun-Audio-Chatæ”¯æŒè¯­éŸ³é—®ç­”ã€éŸ³é¢‘ç†è§£ã€è¯­éŸ³å‡½æ•°è°ƒç”¨ã€è¯­éŸ³æŒ‡ä»¤è·Ÿéšç­‰åŠŸèƒ½ã€‚
Fun-Audio-Chatåœ¨OpenAudioBenchã€VoiceBenchç­‰å¤šä¸ªè¯„æµ‹ä¸­å–å¾—é¢†å…ˆæˆç»©ã€‚
CosyVoiceæ˜¯é˜¿é‡Œäº‘å¼€å‘çš„è¯­éŸ³åˆæˆæ¨¡å‹ï¼Œå¯ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸ºè‡ªç„¶æµç•…çš„è¯­éŸ³ã€‚
å°äº‘æ˜¯Fun-Audio-Chatçš„é»˜è®¤è¯­éŸ³äººè®¾ï¼Œæ˜¯ä¸€ä½æ¥è‡ªæ­å·çš„æ¸©æŸ”å‹å–„çš„å¥³å­©ã€‚"""
                        )
                        rag_audio_input = gr.Audio(
                            label="Question Audio / é—®é¢˜éŸ³é¢‘",
                            type="filepath",
                            sources=["upload", "microphone"]
                        )
                        with gr.Row():
                            rag_text_btn = gr.Button("ğŸ“ æ–‡å­—å›ç­”", variant="primary")
                            rag_speech_btn = gr.Button("ğŸ”Š è¯­éŸ³å›ç­”", variant="secondary")
                    
                    with gr.Column():
                        rag_question = gr.Textbox(
                            label="Transcribed Question / è¯†åˆ«çš„é—®é¢˜",
                            lines=2
                        )
                        rag_retrieved = gr.Textbox(
                            label="Retrieved Knowledge / æ£€ç´¢åˆ°çš„çŸ¥è¯†",
                            lines=4
                        )
                        rag_answer = gr.Textbox(
                            label="Generated Answer / ç”Ÿæˆçš„å›ç­”",
                            lines=5
                        )
                        rag_audio_output = gr.Audio(
                            label="Audio Response / è¯­éŸ³å›ç­”",
                            type="filepath"
                        )
                
                # Example section
                gr.Markdown("**ğŸ“‚ ç¤ºä¾‹ (ç‚¹å‡»åŠ è½½):**")
                gr.Examples(
                    examples=[
                        ["examples/ck7vv9ag.wav", """Fun-Audio-Chatæ˜¯é˜¿é‡Œäº‘é€šä¹‰å®éªŒå®¤å¼€å‘çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚
Fun-Audio-Chatä½¿ç”¨åŒåˆ†è¾¨ç‡è¯­éŸ³è¡¨ç¤ºæŠ€æœ¯ï¼Œå¸§ç‡ä¸º5Hzã€‚
å°äº‘æ˜¯é»˜è®¤è¯­éŸ³äººè®¾ï¼Œæ˜¯ä¸€ä½æ¥è‡ªæ­å·çš„æ¸©æŸ”å¥³å­©ã€‚"""],
                    ],
                    inputs=[rag_audio_input, rag_knowledge],
                    label="ç¤ºä¾‹"
                )
                
                rag_text_btn.click(
                    fn=rag_speech_to_text,
                    inputs=[rag_audio_input, rag_knowledge],
                    outputs=[rag_question, rag_retrieved, rag_answer]
                )
                
                rag_speech_btn.click(
                    fn=rag_speech_to_speech,
                    inputs=[rag_audio_input, rag_knowledge],
                    outputs=[rag_question, rag_retrieved, rag_answer, rag_audio_output]
                )
        
        gr.Markdown(
            """
            ---
            ### ğŸ“Œ ä½¿ç”¨è¯´æ˜
            1. æ¨¡å‹å·²è‡ªåŠ¨åŠ è½½ï¼Œç­‰å¾…åŠ è½½å®Œæˆå³å¯ä½¿ç”¨
            2. é€‰æ‹©åŠŸèƒ½ Tabï¼Œä¸Šä¼ æˆ–å½•åˆ¶éŸ³é¢‘
            3. ç‚¹å‡»ç”ŸæˆæŒ‰é’®è·å–ç»“æœ
            
            ### âš™ï¸ ç³»ç»Ÿè¦æ±‚
            - GPU æ˜¾å­˜: ~24GB
            - æ”¯æŒæ ¼å¼: WAV, MP3, FLAC ç­‰
            - é»˜è®¤è¯­éŸ³: ä¸­æ–‡å¥³å£° (å°äº‘)
            
            ### ğŸ”— ç›¸å…³é“¾æ¥
            - [GitHub](https://github.com/FunAudioLLM/Fun-Audio-Chat) | [HuggingFace](https://huggingface.co/FunAudioLLM/Fun-Audio-Chat-8B) | [Technical Report](https://github.com/FunAudioLLM/Fun-Audio-Chat/blob/main/Fun-Audio-Chat-Technical-Report.pdf)
            """
        )
    
    return demo

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Fun-Audio-Chat Gradio Demo")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=7860, help="Port to run on")
    parser.add_argument("--share", action="store_true", help="Create a public link")
    args = parser.parse_args()
    
    # Auto-load models on startup
    logger.info("Auto-loading models on startup...")
    load_models()
    
    # Create and launch the demo
    demo = create_gradio_interface()
    demo.launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share
    )
