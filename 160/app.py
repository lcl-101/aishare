"""
Gradio WebUI for Higgs Audio Generation
Supports zero-shot voice cloning, single-speaker generation, and multi-speaker dialog
"""

import gradio as gr
import os
import torch
import torchaudio
import tempfile
import soundfile as sf
from pathlib import Path
import re
from typing import List, Optional, Tuple, Dict

from boson_multimodal.serve.serve_engine import HiggsAudioServeEngine, HiggsAudioResponse
from boson_multimodal.data_types import ChatMLSample, Message, AudioContent

# Configuration
MODEL_PATH = "checkpoints/higgs-audio-v2-generation-3B-base"
AUDIO_TOKENIZER_PATH = "checkpoints/higgs-audio-v2-tokenizer"
VOICE_PROMPTS_DIR = "examples/voice_prompts"
TRANSCRIPT_DIR = "examples/transcript"

# Initialize the serve engine
device = "cuda" if torch.cuda.is_available() else "cpu"
serve_engine = None

def initialize_model():
    """Initialize the HiggsAudio model"""
    global serve_engine
    if serve_engine is None:
        serve_engine = HiggsAudioServeEngine(MODEL_PATH, AUDIO_TOKENIZER_PATH, device=device)
    return serve_engine

def get_available_voices():
    """Get list of available voice references"""
    voice_dir = Path(VOICE_PROMPTS_DIR)
    if not voice_dir.exists():
        return []
    
    voices = []
    for wav_file in voice_dir.glob("*.wav"):
        voice_name = wav_file.stem
        voices.append(voice_name)
    
    return sorted(voices)

def get_available_transcripts():
    """Get list of available multi-speaker transcripts"""
    transcript_dir = Path(TRANSCRIPT_DIR) / "multi_speaker"
    if not transcript_dir.exists():
        return []
    
    transcripts = []
    for txt_file in transcript_dir.glob("*.txt"):
        transcripts.append(txt_file.name)
    
    return sorted(transcripts)

def load_transcript_content(transcript_name):
    """Load content from a transcript file"""
    if not transcript_name:
        return ""
    
    transcript_path = Path(TRANSCRIPT_DIR) / "multi_speaker" / transcript_name
    if transcript_path.exists():
        return transcript_path.read_text().strip()
    return ""

def create_system_message(scene_desc="Audio is recorded from a quiet room."):
    """Create system message with scene description"""
    return f"Generate audio following instruction.\n\n<|scene_desc_start|>\n{scene_desc}\n<|scene_desc_end|>"

def generate_single_speaker_audio(
    transcript: str,
    ref_audio: str,
    scene_desc: str,
    temperature: float,
    top_p: float,
    top_k: int,
    max_tokens: int,
    seed: int
):
    """Generate single-speaker audio with optional voice cloning"""
    
    if not transcript.strip():
        return None, "è¯·æä¾›è½¬å½•æ–‡æœ¬"
    
    try:
        # Initialize model if needed
        engine = initialize_model()
        
        # Set seed for reproducibility
        if seed > 0:
            torch.manual_seed(seed)
        
        # Create system message
        system_prompt = create_system_message(scene_desc)
        
        messages = [
            Message(role="system", content=system_prompt)
        ]
        
        # Add reference audio if provided (as conversation history for voice cloning)
        ref_audio_loaded = False
        if ref_audio and ref_audio != "None":
            ref_audio_path = Path(VOICE_PROMPTS_DIR) / f"{ref_audio}.wav"
            ref_text_path = Path(VOICE_PROMPTS_DIR) / f"{ref_audio}.txt"
            
            if ref_audio_path.exists() and ref_text_path.exists():
                # Read the reference text
                with open(ref_text_path, "r", encoding="utf-8") as f:
                    ref_text = f.read().strip()
                
                # Add reference audio as user-assistant conversation history for voice cloning
                # This teaches the model what voice to use
                messages.extend([
                    Message(role="user", content=ref_text),
                    Message(role="assistant", content=AudioContent(audio_url=str(ref_audio_path)))
                ])
                ref_audio_loaded = True
            else:
                missing_files = []
                if not ref_audio_path.exists():
                    missing_files.append(f"{ref_audio}.wav")
                if not ref_text_path.exists():
                    missing_files.append(f"{ref_audio}.txt")
                return None, f"ç¼ºå°‘å‚è€ƒæ–‡ä»¶ï¼š{', '.join(missing_files)}"
        
        # Add the actual user transcript that we want to generate
        messages.append(Message(role="user", content=transcript.strip()))
        
        # Generate audio
        output: HiggsAudioResponse = engine.generate(
            chat_ml_sample=ChatMLSample(messages=messages),
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            stop_strings=["<|end_of_text|>", "<|eot_id|>"],
        )
        
        # Save to temporary file
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
            torchaudio.save(
                tmp_file.name, 
                torch.from_numpy(output.audio)[None, :], 
                output.sampling_rate
            )
            return tmp_file.name, f"éŸ³é¢‘ç”ŸæˆæˆåŠŸï¼{' ï¼ˆä½¿ç”¨å‚è€ƒè¯­éŸ³ï¼š' + ref_audio + 'ï¼‰' if ref_audio_loaded else ''}"
            
    except Exception as e:
        return None, f"ç”ŸæˆéŸ³é¢‘æ—¶å‡ºé”™ï¼š{str(e)}"

def generate_multi_speaker_audio(
    transcript: str,
    speaker1_voice: str,
    speaker2_voice: str,
    scene_desc: str,
    temperature: float,
    top_p: float,
    top_k: int,
    max_tokens: int,
    seed: int,
    use_smart_voice: bool
):
    """Generate multi-speaker dialog audio"""
    
    if not transcript.strip():
        return None, "è¯·æä¾›è½¬å½•æ–‡æœ¬"
    
    try:
        # Initialize model if needed
        engine = initialize_model()
        
        # Set seed for reproducibility
        if seed > 0:
            torch.manual_seed(seed)
        
        # Create system message for multi-speaker
        system_prompt = """You are an AI assistant designed to convert text into speech.
If the user's message includes a [SPEAKER*] tag, do not read out the tag and generate speech for the following text, using the specified voice.
If no speaker tag is present, select a suitable voice on your own."""
        
        if scene_desc.strip():
            system_prompt += f"\n\n<|scene_desc_start|>\n{scene_desc}\n<|scene_desc_end|>"
        
        messages = [
            Message(role="system", content=system_prompt)
        ]
        
        # Add reference audio if provided and not using smart voice
        ref_voices_loaded = []
        if not use_smart_voice:
            ref_voices = []
            if speaker1_voice and speaker1_voice != "None":
                ref_voices.append(speaker1_voice)
            if speaker2_voice and speaker2_voice != "None":
                ref_voices.append(speaker2_voice)
            
            for spk_id, voice in enumerate(ref_voices):
                ref_audio_path = Path(VOICE_PROMPTS_DIR) / f"{voice}.wav"
                ref_text_path = Path(VOICE_PROMPTS_DIR) / f"{voice}.txt"
                
                if ref_audio_path.exists() and ref_text_path.exists():
                    # Read the reference text
                    with open(ref_text_path, "r", encoding="utf-8") as f:
                        ref_text = f.read().strip()
                    
                    # Add reference audio as user-assistant conversation history
                    # Use speaker tags for multi-speaker
                    speaker_text = f"[SPEAKER{spk_id}] {ref_text}" if len(ref_voices) > 1 else ref_text
                    messages.extend([
                        Message(role="user", content=speaker_text),
                        Message(role="assistant", content=AudioContent(audio_url=str(ref_audio_path)))
                    ])
                    ref_voices_loaded.append(voice)
                else:
                    missing_files = []
                    if not ref_audio_path.exists():
                        missing_files.append(f"{voice}.wav")
                    if not ref_text_path.exists():
                        missing_files.append(f"{voice}.txt")
                    return None, f"è¯­éŸ³ '{voice}' ç¼ºå°‘å‚è€ƒæ–‡ä»¶ï¼š{', '.join(missing_files)}"
        
        # Add the actual user transcript that we want to generate
        messages.append(Message(role="user", content=transcript.strip()))
        
        # Generate audio
        output: HiggsAudioResponse = engine.generate(
            chat_ml_sample=ChatMLSample(messages=messages),
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            stop_strings=["<|end_of_text|>", "<|eot_id|>"],
        )
        
        # Save to temporary file
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
            torchaudio.save(
                tmp_file.name, 
                torch.from_numpy(output.audio)[None, :], 
                output.sampling_rate
            )
            status_msg = "å¤šè¯´è¯äººéŸ³é¢‘ç”ŸæˆæˆåŠŸï¼"
            if ref_voices_loaded:
                status_msg += f" ï¼ˆä½¿ç”¨å‚è€ƒè¯­éŸ³ï¼š{', '.join(ref_voices_loaded)}ï¼‰"
            elif use_smart_voice:
                status_msg += " ï¼ˆä½¿ç”¨æ™ºèƒ½è¯­éŸ³é€‰æ‹©ï¼‰"
            return tmp_file.name, status_msg
            
    except Exception as e:
        return None, f"ç”ŸæˆéŸ³é¢‘æ—¶å‡ºé”™ï¼š{str(e)}"

def load_preset_transcript(transcript_name):
    """Load preset transcript content"""
    return load_transcript_content(transcript_name)

def load_reference_audio_preview(voice_name):
    """Load reference audio and text for preview"""
    if not voice_name or voice_name == "None":
        return None, "", gr.update(visible=False)
    
    ref_audio_path = Path(VOICE_PROMPTS_DIR) / f"{voice_name}.wav"
    ref_text_path = Path(VOICE_PROMPTS_DIR) / f"{voice_name}.txt"
    
    audio_file = None
    text_content = ""
    
    if ref_audio_path.exists():
        audio_file = str(ref_audio_path)
    
    if ref_text_path.exists():
        with open(ref_text_path, "r", encoding="utf-8") as f:
            text_content = f.read().strip()
    
    # Show the preview section if we have either audio or text
    show_preview = audio_file is not None or text_content != ""
    
    return audio_file, text_content, gr.update(visible=show_preview)

def load_multi_speaker_reference_preview(speaker1_voice, speaker2_voice, use_smart_voice):
    """Load reference audios for both speakers with preview"""
    if use_smart_voice:
        # Hide all preview sections when using smart voice
        return (
            gr.update(visible=False),  # speaker1_preview
            None, "",                  # speaker1 audio and text
            gr.update(visible=False),  # speaker2_preview  
            None, "",                  # speaker2 audio and text
            ""                         # ref_audio_list for generation
        )
    
    # Process Speaker 1
    speaker1_audio = None
    speaker1_text = ""
    show_speaker1 = False
    
    if speaker1_voice and speaker1_voice != "None":
        ref_audio_path = Path(VOICE_PROMPTS_DIR) / f"{speaker1_voice}.wav"
        ref_text_path = Path(VOICE_PROMPTS_DIR) / f"{speaker1_voice}.txt"
        
        if ref_audio_path.exists():
            speaker1_audio = str(ref_audio_path)
        
        if ref_text_path.exists():
            with open(ref_text_path, "r", encoding="utf-8") as f:
                speaker1_text = f.read().strip()
        
        show_speaker1 = speaker1_audio is not None or speaker1_text != ""
    
    # Process Speaker 2
    speaker2_audio = None
    speaker2_text = ""
    show_speaker2 = False
    
    if speaker2_voice and speaker2_voice != "None":
        ref_audio_path = Path(VOICE_PROMPTS_DIR) / f"{speaker2_voice}.wav"
        ref_text_path = Path(VOICE_PROMPTS_DIR) / f"{speaker2_voice}.txt"
        
        if ref_audio_path.exists():
            speaker2_audio = str(ref_audio_path)
        
        if ref_text_path.exists():
            with open(ref_text_path, "r", encoding="utf-8") as f:
                speaker2_text = f.read().strip()
        
        show_speaker2 = speaker2_audio is not None or speaker2_text != ""
    
    # Create reference audio list for generation function
    ref_voices = []
    if speaker1_voice and speaker1_voice != "None":
        ref_voices.append(speaker1_voice)
    if speaker2_voice and speaker2_voice != "None":
        ref_voices.append(speaker2_voice)
    
    ref_audio_list = ",".join(ref_voices)
    
    return (
        gr.update(visible=show_speaker1),  # speaker1_preview
        speaker1_audio, speaker1_text,     # speaker1 audio and text
        gr.update(visible=show_speaker2),  # speaker2_preview
        speaker2_audio, speaker2_text,     # speaker2 audio and text
        ref_audio_list                     # for generation
    )

def create_gradio_interface():
    """Create the Gradio interface"""
    
    # Get available options
    available_voices = get_available_voices()
    available_transcripts = get_available_transcripts()
    
    with gr.Blocks(title="Higgs Audio Generation WebUI", theme=gr.themes.Soft()) as demo:
        
        gr.Markdown("# ğŸµ Higgs Audio éŸ³é¢‘ç”Ÿæˆ WebUI")
        gr.Markdown("ä½¿ç”¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†å’Œå¤šè¯´è¯äººå¯¹è¯åŠŸèƒ½ç”Ÿæˆé«˜è´¨é‡éŸ³é¢‘ã€‚")
        
        with gr.Tabs():
            
            # Single Speaker Tab
            with gr.TabItem("ğŸ¤ å•è¯´è¯äººç”Ÿæˆ"):
                gr.Markdown("### ç”Ÿæˆå¸¦å¯é€‰è¯­éŸ³å…‹éš†çš„å•è¯´è¯äººéŸ³é¢‘")
                
                with gr.Row():
                    with gr.Column(scale=2):
                        single_transcript = gr.Textbox(
                            label="è½¬å½•æ–‡æœ¬",
                            placeholder="è¾“å…¥æ‚¨æƒ³è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬...",
                            lines=5,
                            value="The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years."
                        )
                        
                        single_ref_audio = gr.Dropdown(
                            label="å‚è€ƒè¯­éŸ³ï¼ˆå¯é€‰ï¼‰",
                            choices=["None"] + available_voices,
                            value="None",
                            info="é€‰æ‹©ç”¨äºå…‹éš†çš„å‚è€ƒè¯­éŸ³ï¼Œæˆ–é€‰æ‹© 'None' è¿›è¡Œæ™ºèƒ½è¯­éŸ³é€‰æ‹©"
                        )
                        
                        # Reference Audio Preview Section
                        with gr.Group(visible=False) as single_ref_preview:
                            gr.Markdown("#### ğŸ§ å‚è€ƒéŸ³é¢‘é¢„è§ˆ")
                            gr.Markdown("*æ­¤å‚è€ƒéŸ³é¢‘å°†ç”¨äºå…‹éš†è¯­éŸ³ç‰¹å¾ï¼Œä½†æ‚¨ä¸Šé¢è¾“å…¥çš„è½¬å½•æ–‡æœ¬å°†æ˜¯å®é™…ç”Ÿæˆçš„å†…å®¹ã€‚*")
                            single_ref_audio_player = gr.Audio(
                                label="å‚è€ƒéŸ³é¢‘", 
                                type="filepath",
                                interactive=False
                            )
                            single_ref_text_display = gr.Textbox(
                                label="å‚è€ƒæ–‡æœ¬ï¼ˆä»…ç”¨äºè¯­éŸ³å…‹éš†ï¼‰",
                                interactive=False,
                                lines=3,
                                info="æ­¤æ–‡æœ¬å¯¹åº”å‚è€ƒéŸ³é¢‘ï¼Œä»…ç”¨äºè¯­éŸ³å­¦ä¹ "
                            )
                        
                        single_scene_desc = gr.Textbox(
                            label="åœºæ™¯æè¿°",
                            value="Audio is recorded from a quiet room.",
                            lines=2,
                            info="æè¿°å£°å­¦ç¯å¢ƒ"
                        )
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### ç”Ÿæˆå‚æ•°")
                        single_temperature = gr.Slider(0.1, 1.0, value=0.3, label="æ¸©åº¦", info="åˆ›é€ æ€§æ°´å¹³")
                        single_top_p = gr.Slider(0.1, 1.0, value=0.95, label="Top P", info="æ ¸é‡‡æ ·")
                        single_top_k = gr.Slider(1, 100, value=50, label="Top K", info="Top-ké‡‡æ ·")
                        single_max_tokens = gr.Slider(128, 2048, value=1024, label="æœ€å¤§ä»¤ç‰Œæ•°", info="æœ€å¤§ç”Ÿæˆé•¿åº¦")
                        single_seed = gr.Number(label="éšæœºç§å­ï¼ˆ0ä¸ºéšæœºï¼‰", value=0, precision=0)
                
                single_generate_btn = gr.Button("ğŸµ ç”ŸæˆéŸ³é¢‘", variant="primary", size="lg")
                
                with gr.Row():
                    single_audio_output = gr.Audio(label="ç”Ÿæˆçš„éŸ³é¢‘", type="filepath")
                    single_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
            
            # Multi-Speaker Tab
            with gr.TabItem("ğŸ‘¥ å¤šè¯´è¯äººç”Ÿæˆ"):
                gr.Markdown("### ç”Ÿæˆå¤šè¯´è¯äººå¯¹è¯éŸ³é¢‘")
                
                with gr.Row():
                    with gr.Column(scale=2):
                        with gr.Row():
                            multi_preset_transcript = gr.Dropdown(
                                label="é¢„è®¾è½¬å½•æ–‡æœ¬",
                                choices=[""] + available_transcripts,
                                value="",
                                info="é€‰æ‹©é¢„è®¾è½¬å½•æ–‡æœ¬æˆ–åœ¨ä¸‹æ–¹ç¼–å†™è‡ªå·±çš„æ–‡æœ¬"
                            )
                            load_preset_btn = gr.Button("ğŸ“ åŠ è½½é¢„è®¾", size="sm")
                        
                        multi_transcript = gr.Textbox(
                            label="å¤šè¯´è¯äººè½¬å½•æ–‡æœ¬",
                            placeholder="ä½¿ç”¨ [SPEAKER0]ã€[SPEAKER1] æ ‡ç­¾è¡¨ç¤ºä¸åŒè¯´è¯äºº...\nç¤ºä¾‹ï¼š\n[SPEAKER0] Hello, how are you?\n[SPEAKER1] I'm doing great, thanks!",
                            lines=8,
                            info="ä½¿ç”¨ [SPEAKER0]ã€[SPEAKER1] ç­‰æ ‡ç­¾è¡¨ç¤ºä¸åŒè¯´è¯äºº"
                        )
                        
                        multi_smart_voice = gr.Checkbox(
                            label="ä½¿ç”¨æ™ºèƒ½è¯­éŸ³é€‰æ‹©",
                            value=False,  # é»˜è®¤å…³é—­ï¼Œè®©ç”¨æˆ·å¯ä»¥é€‰æ‹©å£°éŸ³
                            info="è®©æ¨¡å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è¯­éŸ³"
                        )
                        
                        # Speaker Voice Selection Section
                        with gr.Group():
                            gr.Markdown("#### ğŸ­ è¯´è¯äººè¯­éŸ³é€‰æ‹©")
                            
                            with gr.Row():
                                with gr.Column():
                                    speaker1_voice = gr.Dropdown(
                                        label="è¯´è¯äºº0è¯­éŸ³",
                                        choices=["None"] + available_voices,
                                        value="None",
                                        info="ä¸º SPEAKER0 é€‰æ‹©è¯­éŸ³",
                                        interactive=True  # é»˜è®¤å¯ç”¨
                                    )
                                    
                                    # Speaker 1 Preview
                                    with gr.Group(visible=False) as speaker1_preview:
                                        gr.Markdown("**ğŸ§ è¯´è¯äºº0é¢„è§ˆ**")
                                        speaker1_audio_player = gr.Audio(
                                            label="è¯´è¯äºº0å‚è€ƒéŸ³é¢‘", 
                                            type="filepath",
                                            interactive=False
                                        )
                                        speaker1_text_display = gr.Textbox(
                                            label="è¯´è¯äºº0å‚è€ƒæ–‡æœ¬",
                                            interactive=False,
                                            lines=2
                                        )
                                
                                with gr.Column():
                                    speaker2_voice = gr.Dropdown(
                                        label="è¯´è¯äºº1è¯­éŸ³",
                                        choices=["None"] + available_voices,
                                        value="None", 
                                        info="ä¸º SPEAKER1 é€‰æ‹©è¯­éŸ³",
                                        interactive=True  # é»˜è®¤å¯ç”¨
                                    )
                                    
                                    # Speaker 2 Preview
                                    with gr.Group(visible=False) as speaker2_preview:
                                        gr.Markdown("**ğŸ§ è¯´è¯äºº1é¢„è§ˆ**")
                                        speaker2_audio_player = gr.Audio(
                                            label="è¯´è¯äºº1å‚è€ƒéŸ³é¢‘",
                                            type="filepath", 
                                            interactive=False
                                        )
                                        speaker2_text_display = gr.Textbox(
                                            label="è¯´è¯äºº1å‚è€ƒæ–‡æœ¬",
                                            interactive=False,
                                            lines=2
                                        )
                        
                        # Hidden field to store the combined reference audio list for the generation function
                        multi_ref_audio_combined = gr.Textbox(visible=False, value="")
                        
                        multi_scene_desc = gr.Textbox(
                            label="åœºæ™¯æè¿°",
                            value="Audio is recorded from a quiet room.",
                            lines=2
                        )
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### ç”Ÿæˆå‚æ•°")
                        multi_temperature = gr.Slider(0.1, 1.0, value=0.3, label="æ¸©åº¦")
                        multi_top_p = gr.Slider(0.1, 1.0, value=0.95, label="Top P")
                        multi_top_k = gr.Slider(1, 100, value=50, label="Top K")
                        multi_max_tokens = gr.Slider(128, 2048, value=1024, label="æœ€å¤§ä»¤ç‰Œæ•°")
                        multi_seed = gr.Number(label="éšæœºç§å­ï¼ˆ0ä¸ºéšæœºï¼‰", value=12345, precision=0)
                
                multi_generate_btn = gr.Button("ğŸ­ ç”Ÿæˆå¤šè¯´è¯äººéŸ³é¢‘", variant="primary", size="lg")
                
                with gr.Row():
                    multi_audio_output = gr.Audio(label="ç”Ÿæˆçš„éŸ³é¢‘", type="filepath")
                    multi_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
            
            # Help Tab
            with gr.TabItem("â„¹ï¸ å¸®åŠ©"):
                gr.Markdown("""
                ## å¦‚ä½•ä½¿ç”¨ Higgs Audio éŸ³é¢‘ç”Ÿæˆ WebUI
                
                ### å•è¯´è¯äººç”Ÿæˆ
                1. **è¾“å…¥è½¬å½•æ–‡æœ¬** - æ‚¨æƒ³è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬
                2. **é€‰æ‹©å‚è€ƒè¯­éŸ³**ï¼ˆå¯é€‰ï¼‰- ä»å¯ç”¨è¯­éŸ³ä¸­é€‰æ‹©ç”¨äºè¯­éŸ³å…‹éš†çš„å‚è€ƒè¯­éŸ³
                   - å‚è€ƒéŸ³é¢‘æä¾›è¯­éŸ³ç‰¹å¾
                   - æ‚¨çš„è½¬å½•æ–‡æœ¬å°†ç”¨äºç”Ÿæˆï¼Œè€Œä¸æ˜¯å‚è€ƒæ–‡æœ¬
                3. **è®¾ç½®åœºæ™¯æè¿°** - æè¿°å£°å­¦ç¯å¢ƒ
                4. **è°ƒæ•´å‚æ•°** - æ¸©åº¦æ§åˆ¶åˆ›é€ æ€§ï¼Œå…¶ä»–å‚æ•°å½±å“ç”Ÿæˆè´¨é‡
                5. **ç‚¹å‡»ç”Ÿæˆ** - ç­‰å¾…éŸ³é¢‘ç”Ÿæˆ
                
                ### å¤šè¯´è¯äººç”Ÿæˆ
                1. **æ ¼å¼åŒ–è½¬å½•æ–‡æœ¬** - ä½¿ç”¨ `[SPEAKER0]`ã€`[SPEAKER1]` æ ‡ç­¾è¡¨ç¤ºä¸åŒè¯´è¯äºº
                2. **é€‰æ‹©è¯­éŸ³** - ä½¿ç”¨æ™ºèƒ½è¯­éŸ³é€‰æ‹©æˆ–æŒ‡å®šå‚è€ƒè¯­éŸ³
                3. **è®¾ç½®å‚æ•°** - ä¸å•è¯´è¯äººç”Ÿæˆç±»ä¼¼
                4. **ç”Ÿæˆ** - æ¨¡å‹å°†åˆ›å»ºå¤šè¯´è¯äººå¯¹è¯
                
                ### ä½¿ç”¨æŠ€å·§
                - **æ™ºèƒ½è¯­éŸ³é€‰æ‹©**ï¼šè®©æ¨¡å‹æ ¹æ®å†…å®¹é€‰æ‹©åˆé€‚çš„è¯­éŸ³
                - **è¯­éŸ³å…‹éš†**ï¼šä½¿ç”¨å‚è€ƒè¯­éŸ³è·å¾—ä¸€è‡´çš„è§’è‰²è¯­éŸ³
                - **éšæœºç§å­**ï¼šä½¿ç”¨ç›¸åŒç§å­è·å¾—å¯é‡ç°çš„ç»“æœ
                - **æ¸©åº¦**ï¼šè¾ƒä½å€¼ï¼ˆ0.1-0.3ï¼‰è·å¾—æ›´ä¸€è‡´çš„è¯­éŸ³ï¼Œè¾ƒé«˜å€¼ï¼ˆ0.5-0.8ï¼‰è·å¾—æ›´å¤šå˜åŒ–
                
                ### å¤šè¯´è¯äººæ ¼å¼ç¤ºä¾‹
                ```
                [SPEAKER0] I can't believe you did that without even asking me first!
                [SPEAKER1] Oh, come on! It wasn't a big deal, and I knew you would overreact like this.
                [SPEAKER0] Overreact? You made a decision that affects both of us!
                ```
                """)
        
        # Event handlers
        single_generate_btn.click(
            fn=generate_single_speaker_audio,
            inputs=[
                single_transcript, single_ref_audio, single_scene_desc,
                single_temperature, single_top_p, single_top_k, 
                single_max_tokens, single_seed
            ],
            outputs=[single_audio_output, single_status]
        )
        
        multi_generate_btn.click(
            fn=generate_multi_speaker_audio,
            inputs=[
                multi_transcript, speaker1_voice, speaker2_voice, multi_scene_desc,
                multi_temperature, multi_top_p, multi_top_k,
                multi_max_tokens, multi_seed, multi_smart_voice
            ],
            outputs=[multi_audio_output, multi_status]
        )
        
        load_preset_btn.click(
            fn=load_preset_transcript,
            inputs=[multi_preset_transcript],
            outputs=[multi_transcript]
        )
        
        # Reference audio preview for single speaker
        single_ref_audio.change(
            fn=load_reference_audio_preview,
            inputs=[single_ref_audio],
            outputs=[single_ref_audio_player, single_ref_text_display, single_ref_preview]
        )
        
        # Reference audio preview for multi-speaker
        def update_multi_speaker_previews(speaker1, speaker2, smart_voice):
            return load_multi_speaker_reference_preview(speaker1, speaker2, smart_voice)
        
        # Update previews when speaker voices change
        speaker1_voice.change(
            fn=update_multi_speaker_previews,
            inputs=[speaker1_voice, speaker2_voice, multi_smart_voice],
            outputs=[
                speaker1_preview, speaker1_audio_player, speaker1_text_display,
                speaker2_preview, speaker2_audio_player, speaker2_text_display,
                multi_ref_audio_combined
            ]
        )
        
        speaker2_voice.change(
            fn=update_multi_speaker_previews,
            inputs=[speaker1_voice, speaker2_voice, multi_smart_voice],
            outputs=[
                speaker1_preview, speaker1_audio_player, speaker1_text_display,
                speaker2_preview, speaker2_audio_player, speaker2_text_display,
                multi_ref_audio_combined
            ]
        )
        
        # Update previews and enable/disable dropdowns when smart voice changes
        multi_smart_voice.change(
            fn=update_multi_speaker_previews,
            inputs=[speaker1_voice, speaker2_voice, multi_smart_voice],
            outputs=[
                speaker1_preview, speaker1_audio_player, speaker1_text_display,
                speaker2_preview, speaker2_audio_player, speaker2_text_display,
                multi_ref_audio_combined
            ]
        )
        
        # Enable/disable speaker voice dropdowns based on smart voice checkbox
        multi_smart_voice.change(
            fn=lambda smart: (gr.update(interactive=not smart), gr.update(interactive=not smart)),
            inputs=[multi_smart_voice],
            outputs=[speaker1_voice, speaker2_voice]
        )
    
    return demo

if __name__ == "__main__":
    # Check if model files exist
    if not Path(MODEL_PATH).exists():
        print(f"âŒ Model not found at {MODEL_PATH}")
        print("Please ensure the model checkpoints are in the correct location.")
        exit(1)
    
    if not Path(AUDIO_TOKENIZER_PATH).exists():
        print(f"âŒ Audio tokenizer not found at {AUDIO_TOKENIZER_PATH}")
        print("Please ensure the tokenizer checkpoints are in the correct location.")
        exit(1)
    
    print(f"ğŸš€ Starting Higgs Audio Generation WebUI...")
    print(f"ğŸ“± Device: {device}")
    print(f"ğŸ­ Available voices: {len(get_available_voices())}")
    
    # Create and launch the interface
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        debug=True
    )
