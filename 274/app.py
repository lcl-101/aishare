import os
import math
import time
import random
import tempfile
from pathlib import Path

import gradio as gr
import numpy as np
import PIL.Image
import torch
import librosa
import soundfile as sf

from transformers import AutoTokenizer, UMT5EncoderModel, Wav2Vec2FeatureExtractor
from diffusers.utils import load_image

from longcat_video.pipeline_longcat_video_avatar import LongCatVideoAvatarPipeline
from longcat_video.modules.scheduling_flow_match_euler_discrete import FlowMatchEulerDiscreteScheduler
from longcat_video.modules.autoencoder_kl_wan import AutoencoderKLWan
from longcat_video.modules.avatar.longcat_video_dit_avatar import LongCatVideoAvatarTransformer3DModel
from longcat_video import context_parallel as context_parallel_module
from longcat_video.context_parallel import context_parallel_util

from longcat_video.audio_process.wav2vec2 import Wav2Vec2ModelWrapper
from longcat_video.audio_process.torch_utils import save_video_ffmpeg
from audio_separator.separator import Separator


def init_single_gpu_context_parallel():
    """Initialize context parallel for single GPU mode without distributed training"""
    # Directly set the global variables in context_parallel_util for single GPU mode
    context_parallel_util.cp_size = 1
    context_parallel_util.cp_rank = 0
    context_parallel_util.dp_size = 1
    context_parallel_util.dp_rank = 0
    context_parallel_util.dp_group = None
    context_parallel_util.cp_group = None
    context_parallel_util.dp_ranks = [0]
    context_parallel_util.cp_ranks = [0]
    print("[Single GPU Mode] Context parallel initialized: cp_size=1, cp_rank=0")


# Global variables for model caching
pipe_single = None
pipe_multi = None
vocal_separator = None
audio_output_dir_temp = None
device = None
models_loaded = {"single": False, "multi": False}


def torch_gc():
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()


def generate_random_uid():
    timestamp_part = str(int(time.time()))[-6:]
    random_part = str(random.randint(100000, 999999))
    uid = timestamp_part + random_part
    return uid


def extract_vocal_from_speech(source_path, target_path, vocal_separator, audio_output_dir_temp):
    if source_path is None:
        return None
    outputs = vocal_separator.separate(source_path)
    if len(outputs) <= 0:
        print("Audio separate failed. Using raw audio.")
        return None
        
    default_vocal_path = audio_output_dir_temp / "vocals" / outputs[0]
    default_vocal_path = default_vocal_path.resolve().as_posix()
    cmd = f"mv '{default_vocal_path}' '{target_path}'"
    os.system(cmd)    
    return target_path


def audio_prepare_multi(left_temp_vocal_path, right_temp_vocal_path, generate_duration, 
                        left_raw_speech_path, right_raw_speech_path, sample_rate=16000, audio_type='para'):
    """Prepare multi-person audio embeddings"""
    left_speech_array, right_speech_array = None, None
    left_raw_speech_array, right_raw_speech_array = None, None
    
    if left_temp_vocal_path is not None:
        left_speech_array, sr = librosa.load(left_temp_vocal_path, sr=sample_rate)
        left_raw_speech_array, _ = librosa.load(left_raw_speech_path, sr=sample_rate)
    
    if right_temp_vocal_path is not None:
        right_speech_array, sr = librosa.load(right_temp_vocal_path, sr=sample_rate)
        right_raw_speech_array, _ = librosa.load(right_raw_speech_path, sr=sample_rate)
    
    if left_speech_array is None:
        left_speech_array = np.zeros_like(right_speech_array)
        left_raw_speech_array = np.zeros_like(right_raw_speech_array)
    
    if right_speech_array is None:
        right_speech_array = np.zeros_like(left_speech_array)
        right_raw_speech_array = np.zeros_like(left_raw_speech_array)
    
    if audio_type == 'add':
        # Concatenation mode: person1 speaks first, then person2
        left_speech_array_ext = np.concatenate([left_speech_array, np.zeros_like(right_speech_array)])
        right_speech_array_ext = np.concatenate([np.zeros_like(left_speech_array), right_speech_array])
        merge_raw_speech = np.concatenate([left_raw_speech_array, np.zeros_like(right_raw_speech_array)]) + \
                           np.concatenate([np.zeros_like(left_raw_speech_array), right_raw_speech_array])
    elif audio_type == 'para':
        # Parallel mode: both speak at the same time
        left_speech_array_ext = left_speech_array
        right_speech_array_ext = right_speech_array
        merge_raw_speech = left_raw_speech_array + right_raw_speech_array
    else:
        raise NotImplementedError(f"Unsupported audio_type: {audio_type}")
    
    assert len(left_speech_array_ext) == len(right_speech_array_ext), "The two speech lengths should be equal"
    
    source_duration = len(left_speech_array_ext) / sample_rate
    added_sample_nums = math.ceil((generate_duration - source_duration) * sample_rate)
    if added_sample_nums > 0:
        left_speech_array_ext = np.append(left_speech_array_ext, [0.] * added_sample_nums)
        right_speech_array_ext = np.append(right_speech_array_ext, [0.] * added_sample_nums)
    
    return left_speech_array_ext, right_speech_array_ext, merge_raw_speech, source_duration


def load_models(checkpoint_dir="./checkpoints/LongCat-Video-Avatar", model_type="single"):
    """Load all required models"""
    global pipe_single, pipe_multi, vocal_separator, audio_output_dir_temp, device, models_loaded
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    local_rank = 0
    torch_dtype = torch.bfloat16
    
    # Initialize context parallel for single GPU mode (bypass distributed training)
    init_single_gpu_context_parallel()
    cp_split_hw = context_parallel_util.get_optimal_split(1)
    
    # Load base model components
    base_model_dir = os.path.join(checkpoint_dir, '..', 'LongCat-Video')
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_dir, subfolder="tokenizer", torch_dtype=torch_dtype)
    text_encoder = UMT5EncoderModel.from_pretrained(base_model_dir, subfolder="text_encoder", torch_dtype=torch_dtype)
    vae = AutoencoderKLWan.from_pretrained(base_model_dir, subfolder="vae", torch_dtype=torch_dtype)
    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(base_model_dir, subfolder="scheduler", torch_dtype=torch_dtype)
    
    # Load DiT model based on type
    if model_type == "single":
        dit = LongCatVideoAvatarTransformer3DModel.from_pretrained(checkpoint_dir, subfolder="avatar_single", cp_split_hw=cp_split_hw, torch_dtype=torch_dtype)
    else:
        dit = LongCatVideoAvatarTransformer3DModel.from_pretrained(checkpoint_dir, subfolder="avatar_multi", cp_split_hw=cp_split_hw, torch_dtype=torch_dtype)
    
    # Load audio models
    wav2vec_path = os.path.join(checkpoint_dir, 'chinese-wav2vec2-base')
    audio_encoder = Wav2Vec2ModelWrapper(wav2vec_path).to(local_rank)
    audio_encoder.feature_extractor._freeze_parameters()
    
    wav2vec_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(wav2vec_path, local_files_only=True)
    
    # Setup vocal separator (only once)
    if vocal_separator is None:
        vocal_separator_path = os.path.join(checkpoint_dir, 'vocal_separator/Kim_Vocal_2.onnx')
        audio_output_dir_temp = Path("./audio_temp_file")
        os.makedirs(audio_output_dir_temp, exist_ok=True)
        
        audio_separator_model_path = os.path.dirname(vocal_separator_path)
        audio_separator_model_name = os.path.basename(vocal_separator_path)
        
        vocal_separator = Separator(
            output_dir=audio_output_dir_temp / "vocals",
            output_single_stem="vocals",
            model_file_dir=audio_separator_model_path,
        )
        vocal_separator.load_model(audio_separator_model_name)
    
    # Initialize pipeline
    pipe = LongCatVideoAvatarPipeline(
        tokenizer=tokenizer,
        text_encoder=text_encoder,
        vae=vae,
        scheduler=scheduler,
        dit=dit,
        audio_encoder=audio_encoder,
        wav2vec_feature_extractor=wav2vec_feature_extractor
    )
    pipe.to(local_rank)
    
    if model_type == "single":
        pipe_single = pipe
        models_loaded["single"] = True
    else:
        pipe_multi = pipe
        models_loaded["multi"] = True
    
    return f"{model_type.capitalize()} model loaded successfully!"


def load_single_model(checkpoint_dir):
    return load_models(checkpoint_dir, "single")


def load_multi_model(checkpoint_dir):
    return load_models(checkpoint_dir, "multi")


def generate_avatar_video(
    prompt,
    audio_file,
    image_file,
    stage_1,
    resolution,
    auto_segments,
    num_segments,
    num_inference_steps,
    text_guidance_scale,
    audio_guidance_scale,
    seed,
    ref_img_index,
    mask_frame_range,
    progress=gr.Progress()
):
    """Generate avatar video from audio and optional image"""
    global pipe_single, vocal_separator, audio_output_dir_temp, device
    
    if pipe_single is None:
        return None, "Error: Please load models first!"
    
    if audio_file is None:
        return None, "Error: Please upload an audio file!"
    
    if stage_1 == "ai2v" and image_file is None:
        return None, "Error: AI2V mode requires an image input!"
    
    progress(0.1, desc="Processing audio...")
    
    # Default parameters
    save_fps = 16
    num_frames = 93
    num_cond_frames = 13
    audio_stride = 2
    local_rank = 0
    
    negative_prompt = "Close-up, Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"
    # åŠ å…¥ä¸ç›¸æœºè¿åŠ¨ç›¸å…³çš„è´Ÿé¢æç¤ºï¼Œé¿å…é•œå¤´æ¨è¿›/å˜ç„¦/å¹³ç§»ç­‰
    negative_prompt += ", zoom, zoom-in, zoom-out, camera zoom, camera movement, moving camera, dolly, push in, pull back, pan, tilt, tracking shot, camera motion, handheld, shaky camera, dolly zoom, crane shot, camera dolly"
    
    # Resolution settings
    if resolution == '480p':
        height, width = 480, 832
    elif resolution == '720p':
        height, width = 768, 1280
    
    # Extract vocal
    temp_vocal_path = extract_vocal_from_speech(
        audio_file, 
        f"/tmp/temp_speech_{generate_random_uid()}_vocal.wav", 
        vocal_separator, 
        audio_output_dir_temp
    )
    
    if temp_vocal_path is None or not os.path.exists(temp_vocal_path):
        return None, "Error: No vocal detected in the audio file!"
    
    progress(0.2, desc="Processing audio embedding...")
    
    # Load audio and get duration
    speech_array, sr = librosa.load(temp_vocal_path, sr=16000)
    source_duration = len(speech_array) / sr
    
    # Auto-calculate number of segments based on audio duration
    if auto_segments:
        # First segment duration: num_frames / save_fps
        # Each additional segment adds: (num_frames - num_cond_frames) / save_fps
        first_segment_duration = num_frames / save_fps  # ~5.8125 seconds
        additional_segment_duration = (num_frames - num_cond_frames) / save_fps  # 5 seconds
        
        if source_duration <= first_segment_duration:
            num_segments = 1
        else:
            num_segments = 1 + math.ceil((source_duration - first_segment_duration) / additional_segment_duration)
        
        print(f"[Auto Segments] Audio duration: {source_duration:.2f}s, calculated segments: {num_segments}")
    
    # Audio padding
    generate_duration = num_frames / save_fps + (num_segments - 1) * (num_frames - num_cond_frames) / save_fps
    added_sample_nums = math.ceil((generate_duration - source_duration) * sr)
    if added_sample_nums > 0:
        speech_array = np.append(speech_array, [0.] * added_sample_nums)
    
    # Audio embedding
    full_audio_emb = pipe_single.get_audio_embedding(speech_array, fps=save_fps * audio_stride, device=local_rank, sample_rate=sr)
    if torch.isnan(full_audio_emb).any():
        return None, "Error: Broken audio embedding with nan values!"
    
    # Cleanup temp vocal file
    if os.path.exists(temp_vocal_path):
        os.remove(temp_vocal_path)
    
    # Prepare audio embedding for the first clip
    indices = torch.arange(2 * 2 + 1) - 2
    audio_start_idx = 0
    audio_end_idx = audio_start_idx + audio_stride * num_frames
    
    center_indices = torch.arange(audio_start_idx, audio_end_idx, audio_stride).unsqueeze(1) + indices.unsqueeze(0)
    center_indices = torch.clamp(center_indices, min=0, max=full_audio_emb.shape[0] - 1)
    audio_emb = full_audio_emb[center_indices][None, ...].to(local_rank)
    
    # Set random seed
    generator = torch.Generator(device=local_rank)
    generator.manual_seed(seed)
    
    progress(0.3, desc=f"Generating segment 1/{num_segments}...")
    
    if stage_1 == 'at2v':
        # Audio-to-Video (AT2V)
        output_tuple = pipe_single.generate_at2v(
            prompt=prompt,
            negative_prompt=negative_prompt,
            height=height,
            width=width,
            num_frames=num_frames,
            num_inference_steps=num_inference_steps,
            text_guidance_scale=text_guidance_scale,
            audio_guidance_scale=audio_guidance_scale,
            generator=generator,
            output_type='both',
            resize_mode='default',
            audio_emb=audio_emb
        )
    elif stage_1 == 'ai2v':
        # Audio+Image-to-Video (AI2V)
        image = load_image(image_file)
        output_tuple = pipe_single.generate_ai2v(
            image=image,
            prompt=prompt,
            negative_prompt=negative_prompt,
            resolution=resolution,
            num_frames=num_frames,
            num_inference_steps=num_inference_steps,
            text_guidance_scale=text_guidance_scale,
            audio_guidance_scale=audio_guidance_scale,
            output_type='both',
            resize_mode='default',
            generator=generator,
            audio_emb=audio_emb
        )
    
    output, latent = output_tuple
    output = output[0]
    video = [(output[i] * 255).astype(np.uint8) for i in range(output.shape[0])]
    video = [PIL.Image.fromarray(img) for img in video]
    del output
    torch_gc()
    
    # Get actual video dimensions from the generated frames
    actual_width, actual_height = video[0].size
    print(f"[Video Generation] Actual video size: {actual_width}x{actual_height}")
    
    # Long video generation
    all_generated_frames = video
    ref_latent = latent[:, :, :1].clone()
    current_video = video
    
    for segment_idx in range(1, num_segments):
        progress_val = 0.3 + 0.6 * (segment_idx / num_segments)
        progress(progress_val, desc=f"Generating segment {segment_idx + 1}/{num_segments}...")
        
        # Prepare audio embedding for next clip
        audio_start_idx = audio_start_idx + audio_stride * (num_frames - num_cond_frames)
        audio_end_idx = audio_start_idx + audio_stride * num_frames
        center_indices = torch.arange(audio_start_idx, audio_end_idx, audio_stride).unsqueeze(1) + indices.unsqueeze(0)
        center_indices = torch.clamp(center_indices, min=0, max=full_audio_emb.shape[0] - 1)
        audio_emb = full_audio_emb[center_indices][None, ...].to(local_rank)
        
        output_tuple = pipe_single.generate_avc(
            video=current_video,
            video_latent=latent,
            prompt=prompt,
            negative_prompt=negative_prompt,
            height=actual_height,
            width=actual_width,
            num_frames=num_frames,
            num_cond_frames=num_cond_frames,
            num_inference_steps=num_inference_steps,
            text_guidance_scale=text_guidance_scale,
            audio_guidance_scale=audio_guidance_scale,
            generator=generator,
            output_type='both',
            use_kv_cache=True,
            offload_kv_cache=False,
            enhance_hf=False,
            resize_mode='default',
            audio_emb=audio_emb,
            ref_latent=ref_latent,
            ref_img_index=ref_img_index,
            mask_frame_range=mask_frame_range
        )
        output, latent = output_tuple
        
        output = output[0]
        new_video = [(output[i] * 255).astype(np.uint8) for i in range(output.shape[0])]
        new_video = [PIL.Image.fromarray(img) for img in new_video]
        del output
        
        all_generated_frames.extend(new_video[num_cond_frames:])
        current_video = new_video
        torch_gc()
    
    progress(0.95, desc="Saving video...")
    
    # Save video
    output_dir = tempfile.mkdtemp()
    output_path = os.path.join(output_dir, "avatar_video")
    output_tensor = torch.from_numpy(np.array(all_generated_frames))
    save_video_ffmpeg(output_tensor, output_path, audio_file, fps=save_fps, quality=5)
    
    output_video_path = output_path + ".mp4"
    
    progress(1.0, desc="Done!")
    
    return output_video_path, f"Video generated successfully! Duration: {len(all_generated_frames) / save_fps:.2f}s"


def load_example(example_name):
    """Load example data"""
    import json
    
    example_path = f"assets/avatar/{example_name}.json"
    if not os.path.exists(example_path):
        return None, None, ""
    
    with open(example_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    prompt = data.get('prompt', '')
    image_path = data.get('cond_image', None)
    audio_path = data.get('cond_audio', {}).get('person1', None)
    
    return audio_path, image_path, prompt


def load_multi_example(example_name):
    """Load multi-person example data"""
    import json
    
    example_path = f"assets/avatar/{example_name}.json"
    if not os.path.exists(example_path):
        return None, None, None, "", "", ""
    
    with open(example_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    prompt = data.get('prompt', '')
    image_path = data.get('cond_image', None)
    audio1_path = data.get('cond_audio', {}).get('person1', None)
    audio2_path = data.get('cond_audio', {}).get('person2', None)
    audio_type = data.get('audio_type', 'para')
    
    # Format bbox info
    bbox_info = ""
    if 'bbox' in data:
        bbox1 = data['bbox'].get('person1', None)
        bbox2 = data['bbox'].get('person2', None)
        if bbox1:
            bbox_info += f"Person1: {bbox1}\n"
        if bbox2:
            bbox_info += f"Person2: {bbox2}"
    
    return audio1_path, audio2_path, image_path, prompt, audio_type, bbox_info


def generate_multi_avatar_video(
    prompt,
    audio_file1,
    audio_file2,
    image_file,
    audio_type,
    resolution,
    auto_segments,
    num_segments,
    num_inference_steps,
    text_guidance_scale,
    audio_guidance_scale,
    seed,
    ref_img_index,
    mask_frame_range,
    bbox_person1,
    bbox_person2,
    progress=gr.Progress()
):
    """Generate multi-person avatar video from two audio inputs and image"""
    global pipe_multi, vocal_separator, audio_output_dir_temp, device
    
    if pipe_multi is None:
        return None, "Error: Please load Multi-Person model first!"
    
    if audio_file1 is None and audio_file2 is None:
        return None, "Error: At least one audio file is required!"
    
    if image_file is None:
        return None, "Error: Reference image is required for multi-person mode!"
    
    progress(0.05, desc="Processing audio...")
    
    # Default parameters
    save_fps = 16
    num_frames = 93
    num_cond_frames = 13
    audio_stride = 2
    local_rank = 0
    sr = 16000
    
    negative_prompt = "Close-up, bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"
    # åŠ å…¥ä¸ç›¸æœºè¿åŠ¨ç›¸å…³çš„è´Ÿé¢æç¤ºï¼Œé¿å…é•œå¤´æ¨è¿›/å˜ç„¦/å¹³ç§»ç­‰
    negative_prompt += ", zoom, zoom-in, zoom-out, camera zoom, camera movement, moving camera, dolly, push in, pull back, pan, tilt, tracking shot, camera motion, handheld, shaky camera, dolly zoom, crane shot, camera dolly"
    
    # Extract vocals
    left_temp_vocal_path = None
    right_temp_vocal_path = None
    
    if audio_file1:
        left_temp_vocal_path = extract_vocal_from_speech(
            audio_file1, 
            f"/tmp/temp_speech_{generate_random_uid()}_left_vocal.wav", 
            vocal_separator, 
            audio_output_dir_temp
        )
    
    if audio_file2:
        right_temp_vocal_path = extract_vocal_from_speech(
            audio_file2, 
            f"/tmp/temp_speech_{generate_random_uid()}_right_vocal.wav", 
            vocal_separator, 
            audio_output_dir_temp
        )
    
    if left_temp_vocal_path is None and right_temp_vocal_path is None:
        return None, "Error: No vocal detected in any audio file!"
    
    progress(0.15, desc="Processing audio embeddings...")
    
    # Calculate initial generate duration for auto segments
    first_segment_duration = num_frames / save_fps
    additional_segment_duration = (num_frames - num_cond_frames) / save_fps
    
    # Prepare multi-person audio
    generate_duration = num_frames / save_fps + (num_segments - 1) * (num_frames - num_cond_frames) / save_fps
    
    left_speech_array_ext, right_speech_array_ext, merge_speech, source_duration = audio_prepare_multi(
        left_temp_vocal_path, right_temp_vocal_path, generate_duration,
        audio_file1, audio_file2, sample_rate=sr, audio_type=audio_type
    )
    
    # Auto-calculate segments based on audio duration
    if auto_segments:
        if source_duration <= first_segment_duration:
            num_segments = 1
        else:
            num_segments = 1 + math.ceil((source_duration - first_segment_duration) / additional_segment_duration)
        
        print(f"[Auto Segments] Audio duration: {source_duration:.2f}s, calculated segments: {num_segments}")
        
        # Recalculate generate duration and re-prepare audio
        generate_duration = num_frames / save_fps + (num_segments - 1) * (num_frames - num_cond_frames) / save_fps
        left_speech_array_ext, right_speech_array_ext, merge_speech, _ = audio_prepare_multi(
            left_temp_vocal_path, right_temp_vocal_path, generate_duration,
            audio_file1, audio_file2, sample_rate=sr, audio_type=audio_type
        )
    
    # Save merged audio for final video
    merge_speech_path = f"/tmp/temp_speech_{generate_random_uid()}_merge.wav"
    sf.write(merge_speech_path, merge_speech, sr)
    
    # Get audio embeddings
    left_full_audio_emb = pipe_multi.get_audio_embedding(left_speech_array_ext, fps=save_fps * audio_stride, device=local_rank, sample_rate=sr)
    right_full_audio_emb = pipe_multi.get_audio_embedding(right_speech_array_ext, fps=save_fps * audio_stride, device=local_rank, sample_rate=sr)
    
    if torch.isnan(left_full_audio_emb).any() or torch.isnan(right_full_audio_emb).any():
        return None, "Error: Broken audio embedding with nan values!"
    
    # Cleanup temp vocal files
    if left_temp_vocal_path and os.path.exists(left_temp_vocal_path):
        os.remove(left_temp_vocal_path)
    if right_temp_vocal_path and os.path.exists(right_temp_vocal_path):
        os.remove(right_temp_vocal_path)
    
    # Prepare audio embedding for the first clip
    indices = torch.arange(2 * 2 + 1) - 2
    audio_start_idx = 0
    audio_end_idx = audio_start_idx + audio_stride * num_frames
    
    center_indices = torch.arange(audio_start_idx, audio_end_idx, audio_stride).unsqueeze(1) + indices.unsqueeze(0)
    center_indices = torch.clamp(center_indices, min=0, max=left_full_audio_emb.shape[0] - 1)
    left_audio_emb = left_full_audio_emb[center_indices][None, ...].to(local_rank)
    right_audio_emb = right_full_audio_emb[center_indices][None, ...].to(local_rank)
    audio_embs = torch.cat([left_audio_emb, right_audio_emb])
    
    # Set random seed
    generator = torch.Generator(device=local_rank)
    generator.manual_seed(seed)
    
    progress(0.2, desc="Preparing image and masks...")
    
    # Load image and prepare masks
    image = load_image(image_file)
    src_width, src_height = image.size
    
    # Define human / background mask
    background_mask = torch.zeros([src_height, src_width])
    human_mask1 = torch.zeros([src_height, src_width])
    human_mask2 = torch.zeros([src_height, src_width])
    
    # Parse bbox
    left_person_bbox = None
    right_person_bbox = None
    
    if bbox_person1 and bbox_person1.strip():
        try:
            left_person_bbox = [int(x.strip()) for x in bbox_person1.split(',')]
        except:
            pass
    
    if bbox_person2 and bbox_person2.strip():
        try:
            right_person_bbox = [int(x.strip()) for x in bbox_person2.split(',')]
        except:
            pass
    
    if left_person_bbox is None and right_person_bbox is None:
        # Default: split image in half
        face_scale = 0.1
        left_y_min, left_y_max = int(src_height * face_scale), int(src_height * (1 - face_scale))
        right_y_min, right_y_max = left_y_min, left_y_max
        half_width = src_width // 2
        left_x_min, left_x_max = int(half_width * face_scale), int(half_width * (1 - face_scale))
        right_x_min, right_x_max = int(half_width * face_scale + half_width), int(half_width * (1 - face_scale) + half_width)
    elif left_person_bbox is not None and right_person_bbox is not None:
        left_y_min, left_x_min, left_y_max, left_x_max = left_person_bbox
        right_y_min, right_x_min, right_y_max, right_x_max = right_person_bbox
    else:
        return None, "Error: Both person bboxes must be provided or both must be empty!"
    
    human_mask1[left_y_min:left_y_max, left_x_min:left_x_max] = 1
    human_mask2[right_y_min:right_y_max, right_x_min:right_x_max] = 1
    background_mask += human_mask1
    background_mask += human_mask2
    background_mask = torch.where(background_mask > 0, torch.tensor(0), torch.tensor(1))
    ref_target_masks = torch.stack([human_mask1, human_mask2, background_mask], dim=0).to(local_rank)
    
    progress(0.3, desc=f"Generating segment 1/{num_segments}...")
    
    # Generate first segment
    output_tuple = pipe_multi.generate_ai2v(
        image=image,
        prompt=prompt,
        negative_prompt=negative_prompt,
        resolution=resolution,
        num_frames=num_frames,
        num_inference_steps=num_inference_steps,
        text_guidance_scale=text_guidance_scale,
        audio_guidance_scale=audio_guidance_scale,
        output_type='both',
        resize_mode='default',
        generator=generator,
        audio_emb=audio_embs,
        ref_target_masks=ref_target_masks
    )
    
    output, latent = output_tuple
    output = output[0]
    video = [(output[i] * 255).astype(np.uint8) for i in range(output.shape[0])]
    video = [PIL.Image.fromarray(img) for img in video]
    del output
    torch_gc()
    
    # Get actual video dimensions
    actual_width, actual_height = video[0].size
    print(f"[Video Generation] Actual video size: {actual_width}x{actual_height}")
    
    # Long video generation
    all_generated_frames = video
    ref_latent = latent[:, :, :1].clone()
    current_video = video
    
    for segment_idx in range(1, num_segments):
        progress_val = 0.3 + 0.6 * (segment_idx / num_segments)
        progress(progress_val, desc=f"Generating segment {segment_idx + 1}/{num_segments}...")
        
        # Prepare audio embedding for next clip
        audio_start_idx = audio_start_idx + audio_stride * (num_frames - num_cond_frames)
        audio_end_idx = audio_start_idx + audio_stride * num_frames
        center_indices = torch.arange(audio_start_idx, audio_end_idx, audio_stride).unsqueeze(1) + indices.unsqueeze(0)
        center_indices = torch.clamp(center_indices, min=0, max=left_full_audio_emb.shape[0] - 1)
        left_audio_emb = left_full_audio_emb[center_indices][None, ...].to(local_rank)
        right_audio_emb = right_full_audio_emb[center_indices][None, ...].to(local_rank)
        audio_embs = torch.cat([left_audio_emb, right_audio_emb])
        
        output_tuple = pipe_multi.generate_avc(
            video=current_video,
            video_latent=latent,
            prompt=prompt,
            negative_prompt=negative_prompt,
            height=actual_height,
            width=actual_width,
            num_frames=num_frames,
            num_cond_frames=num_cond_frames,
            num_inference_steps=num_inference_steps,
            text_guidance_scale=text_guidance_scale,
            audio_guidance_scale=audio_guidance_scale,
            generator=generator,
            output_type='both',
            use_kv_cache=True,
            offload_kv_cache=False,
            enhance_hf=True,
            resize_mode='default',
            audio_emb=audio_embs,
            ref_latent=ref_latent,
            ref_img_index=ref_img_index,
            mask_frame_range=mask_frame_range,
            ref_target_masks=ref_target_masks
        )
        output, latent = output_tuple
        
        output = output[0]
        new_video = [(output[i] * 255).astype(np.uint8) for i in range(output.shape[0])]
        new_video = [PIL.Image.fromarray(img) for img in new_video]
        del output
        
        all_generated_frames.extend(new_video[num_cond_frames:])
        current_video = new_video
        torch_gc()
    
    progress(0.95, desc="Saving video...")
    
    # Save video
    output_dir = tempfile.mkdtemp()
    output_path = os.path.join(output_dir, "multi_avatar_video")
    output_tensor = torch.from_numpy(np.array(all_generated_frames))
    save_video_ffmpeg(output_tensor, output_path, merge_speech_path, fps=save_fps, quality=5)
    
    # Cleanup
    if os.path.exists(merge_speech_path):
        os.remove(merge_speech_path)
    
    output_video_path = output_path + ".mp4"
    
    progress(1.0, desc="Done!")
    
    return output_video_path, f"Video generated successfully! Duration: {len(all_generated_frames) / save_fps:.2f}s"


# Create Gradio interface
def create_ui():
    with gr.Blocks(title="LongCat Avatar è§†é¢‘ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ­ LongCat Avatar è§†é¢‘ç”Ÿæˆå™¨
        
        åŸºäºéŸ³é¢‘è¾“å…¥ç”Ÿæˆè¯´è¯äººè§†é¢‘ï¼Œæ”¯æŒå•äººå’Œå¤šäººæ¨¡å¼ã€‚
        """)
        
        with gr.Tabs():
            # ==================== å•äººæ¨¡å¼ Tab ====================
            with gr.TabItem("ğŸ‘¤ å•äººæ¨¡å¼"):
                gr.Markdown("""
                ### å•äººéŸ³é¢‘è½¬è§†é¢‘
                - **AI2V (éŸ³é¢‘+å›¾ç‰‡è½¬è§†é¢‘)**: åŸºäºéŸ³é¢‘ã€å›¾ç‰‡å’Œæ–‡æœ¬æç¤ºç”Ÿæˆè§†é¢‘
                - **AT2V (éŸ³é¢‘è½¬è§†é¢‘)**: ä»…åŸºäºéŸ³é¢‘å’Œæ–‡æœ¬æç¤ºç”Ÿæˆè§†é¢‘
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ è¾“å…¥")
                        
                        # ç¤ºä¾‹é€‰æ‹©
                        single_example_dropdown = gr.Dropdown(
                            choices=["single_example_1"],
                            label="åŠ è½½ç¤ºä¾‹",
                            info="é€‰æ‹©ä¸€ä¸ªç¤ºä¾‹åŠ è½½"
                        )
                        single_load_example_btn = gr.Button("ğŸ“‚ åŠ è½½ç¤ºä¾‹")
                        
                        single_prompt = gr.Textbox(
                            label="æç¤ºè¯",
                            placeholder="æè¿°åœºæ™¯å’Œäººç‰©...",
                            lines=4,
                            value="A western man stands on stage under dramatic lighting, holding a microphone close to their mouth. Wearing a vibrant red jacket with gold embroidery, the singer is speaking while smoke swirls around them, creating a dynamic and atmospheric scene."
                        )
                        
                        single_audio_input = gr.Audio(
                            label="éŸ³é¢‘è¾“å…¥",
                            type="filepath",
                            sources=["upload", "microphone"]
                        )
                        
                        single_image_input = gr.Image(
                            label="å‚è€ƒå›¾ç‰‡ (AI2V æ¨¡å¼å¿…éœ€)",
                            type="filepath"
                        )
                        
                        gr.Markdown("### âš™ï¸ ç”Ÿæˆè®¾ç½®")
                        
                        single_stage_1 = gr.Radio(
                            choices=["ai2v", "at2v"],
                            value="ai2v",
                            label="ç”Ÿæˆæ¨¡å¼",
                            info="AI2V: éŸ³é¢‘+å›¾ç‰‡è½¬è§†é¢‘ | AT2V: éŸ³é¢‘è½¬è§†é¢‘"
                        )
                        
                        single_resolution = gr.Radio(
                            choices=["480p", "720p"],
                            value="480p",
                            label="åˆ†è¾¨ç‡"
                        )
                        
                        single_auto_segments = gr.Checkbox(
                            label="æ ¹æ®éŸ³é¢‘æ—¶é•¿è‡ªåŠ¨è®¡ç®—åˆ†æ®µæ•°",
                            value=True,
                            info="æ ¹æ®éŸ³é¢‘é•¿åº¦è‡ªåŠ¨ç¡®å®šè§†é¢‘åˆ†æ®µæ•°é‡"
                        )
                        
                        single_num_segments = gr.Slider(
                            minimum=1,
                            maximum=30,
                            value=1,
                            step=1,
                            label="åˆ†æ®µæ•°é‡ (æ‰‹åŠ¨)",
                            info="ä»…åœ¨å…³é—­è‡ªåŠ¨è®¡ç®—æ—¶ä½¿ç”¨ï¼Œæ¯æ®µçº¦ 5 ç§’"
                        )
                        
                        with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                            single_num_inference_steps = gr.Slider(
                                minimum=4,
                                maximum=100,
                                value=50,
                                step=1,
                                label="æ¨ç†æ­¥æ•°"
                            )
                            
                            single_text_guidance_scale = gr.Slider(
                                minimum=1.0,
                                maximum=10.0,
                                value=4.0,
                                step=0.5,
                                label="æ–‡æœ¬å¼•å¯¼å¼ºåº¦"
                            )
                            
                            single_audio_guidance_scale = gr.Slider(
                                minimum=1.0,
                                maximum=10.0,
                                value=4.0,
                                step=0.5,
                                label="éŸ³é¢‘å¼•å¯¼å¼ºåº¦"
                            )
                            
                            single_seed = gr.Number(
                                value=42,
                                label="éšæœºç§å­",
                                precision=0
                            )
                            
                            single_ref_img_index = gr.Slider(
                                minimum=1,
                                maximum=20,
                                value=10,
                                step=1,
                                label="å‚è€ƒå›¾åƒç´¢å¼•"
                            )
                            
                            single_mask_frame_range = gr.Slider(
                                minimum=1,
                                maximum=10,
                                value=6,
                                step=1,
                                label="é®ç½©å¸§èŒƒå›´"
                            )
                        
                        single_generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ¥ è¾“å‡º")
                        single_output_video = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘")
                        single_output_status = gr.Textbox(label="ç”ŸæˆçŠ¶æ€", interactive=False)
                        
                        gr.Markdown("### ğŸ“š ç¤ºä¾‹æ–‡ä»¶")
                        gr.Markdown("""
                        **å¯ç”¨ç¤ºä¾‹ï¼š**
                        - `single_example_1`: èˆå°ä¸Šç¯å…‰ä¸‹çš„è¥¿æ–¹ç”·å­
                        
                        **ç¤ºä¾‹æ–‡ä»¶ä½ç½®ï¼š**
                        - å›¾ç‰‡: `assets/avatar/single/man.png`
                        - éŸ³é¢‘: `assets/avatar/single/man.mp3`
                        """)
            
            # ==================== å¤šäººæ¨¡å¼ Tab ====================
            with gr.TabItem("ğŸ‘¥ å¤šäººæ¨¡å¼"):
                gr.Markdown("""
                ### å¤šäººéŸ³é¢‘è½¬è§†é¢‘
                ç”Ÿæˆä¸¤ä¸ªäººå„è‡ªè¯´è¯çš„è§†é¢‘ï¼Œæ¯äººä½¿ç”¨ç‹¬ç«‹çš„éŸ³é¢‘è½¨é“ã€‚
                
                **éŸ³é¢‘æ¨¡å¼ï¼š**
                - **para (å¹¶è¡Œ)**: ä¸¤äººåŒæ—¶è¯´è¯ï¼ˆéŸ³é¢‘æ—¶é•¿éœ€ç›¸åŒï¼‰
                - **add (ä¸²è”)**: äººç‰©1å…ˆè¯´ï¼Œç„¶åäººç‰©2è¯´
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ è¾“å…¥")
                        
                        # ç¤ºä¾‹é€‰æ‹©
                        multi_example_dropdown = gr.Dropdown(
                            choices=["multi_example_1", "multi_example_2"],
                            label="åŠ è½½ç¤ºä¾‹",
                            info="é€‰æ‹©ä¸€ä¸ªç¤ºä¾‹åŠ è½½"
                        )
                        multi_load_example_btn = gr.Button("ğŸ“‚ åŠ è½½ç¤ºä¾‹")
                        
                        multi_prompt = gr.Textbox(
                            label="æç¤ºè¯",
                            placeholder="æè¿°ä¸¤äººåœºæ™¯...",
                            lines=4,
                            value="Static camera, In a professional recording studio, two people stand facing each other, both wearing large headphones. They are speaking clearly into a large condenser microphone suspended between them."
                        )
                        
                        with gr.Row():
                            multi_audio_input1 = gr.Audio(
                                label="äººç‰©1 éŸ³é¢‘ (å·¦)",
                                type="filepath",
                                sources=["upload", "microphone"]
                            )
                            multi_audio_input2 = gr.Audio(
                                label="äººç‰©2 éŸ³é¢‘ (å³)",
                                type="filepath",
                                sources=["upload", "microphone"]
                            )
                        
                        multi_image_input = gr.Image(
                            label="å‚è€ƒå›¾ç‰‡ (å¿…éœ€)",
                            type="filepath"
                        )
                        
                        gr.Markdown("### âš™ï¸ ç”Ÿæˆè®¾ç½®")
                        
                        multi_audio_type = gr.Radio(
                            choices=["para", "add"],
                            value="para",
                            label="éŸ³é¢‘æ¨¡å¼",
                            info="para: åŒæ—¶è¯´è¯ | add: äººç‰©1å…ˆè¯´ï¼Œç„¶åäººç‰©2"
                        )
                        
                        multi_resolution = gr.Radio(
                            choices=["480p", "720p"],
                            value="480p",
                            label="åˆ†è¾¨ç‡"
                        )
                        
                        multi_auto_segments = gr.Checkbox(
                            label="æ ¹æ®éŸ³é¢‘æ—¶é•¿è‡ªåŠ¨è®¡ç®—åˆ†æ®µæ•°",
                            value=True,
                            info="æ ¹æ®éŸ³é¢‘é•¿åº¦è‡ªåŠ¨ç¡®å®šè§†é¢‘åˆ†æ®µæ•°é‡"
                        )
                        
                        multi_num_segments = gr.Slider(
                            minimum=1,
                            maximum=30,
                            value=1,
                            step=1,
                            label="åˆ†æ®µæ•°é‡ (æ‰‹åŠ¨)",
                            info="ä»…åœ¨å…³é—­è‡ªåŠ¨è®¡ç®—æ—¶ä½¿ç”¨"
                        )
                        
                        with gr.Accordion("äººç‰©è¾¹ç•Œæ¡† (å¯é€‰)", open=False):
                            gr.Markdown("""
                            **æ ¼å¼:** `y_min, x_min, y_max, x_max`
                            
                            ç•™ç©ºåˆ™è‡ªåŠ¨æ£€æµ‹ï¼ˆå°†å›¾ç‰‡å¯¹åŠåˆ†ï¼‰ã€‚
                            """)
                            multi_bbox_person1 = gr.Textbox(
                                label="äººç‰©1 è¾¹ç•Œæ¡†",
                                placeholder="ä¾‹å¦‚: 100, 80, 800, 640",
                                info="äººç‰©1ï¼ˆå·¦ä¾§ï¼‰çš„è¾¹ç•Œæ¡†"
                            )
                            multi_bbox_person2 = gr.Textbox(
                                label="äººç‰©2 è¾¹ç•Œæ¡†", 
                                placeholder="ä¾‹å¦‚: 50, 720, 820, 1300",
                                info="äººç‰©2ï¼ˆå³ä¾§ï¼‰çš„è¾¹ç•Œæ¡†"
                            )
                        
                        with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                            multi_num_inference_steps = gr.Slider(
                                minimum=4,
                                maximum=100,
                                value=50,
                                step=1,
                                label="æ¨ç†æ­¥æ•°"
                            )
                            
                            multi_text_guidance_scale = gr.Slider(
                                minimum=1.0,
                                maximum=10.0,
                                value=4.0,
                                step=0.5,
                                label="æ–‡æœ¬å¼•å¯¼å¼ºåº¦"
                            )
                            
                            multi_audio_guidance_scale = gr.Slider(
                                minimum=1.0,
                                maximum=10.0,
                                value=4.0,
                                step=0.5,
                                label="éŸ³é¢‘å¼•å¯¼å¼ºåº¦"
                            )
                            
                            multi_seed = gr.Number(
                                value=42,
                                label="éšæœºç§å­",
                                precision=0
                            )
                            
                            multi_ref_img_index = gr.Slider(
                                minimum=1,
                                maximum=20,
                                value=10,
                                step=1,
                                label="å‚è€ƒå›¾åƒç´¢å¼•"
                            )
                            
                            multi_mask_frame_range = gr.Slider(
                                minimum=1,
                                maximum=10,
                                value=6,
                                step=1,
                                label="é®ç½©å¸§èŒƒå›´"
                            )
                        
                        multi_generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ¥ è¾“å‡º")
                        multi_output_video = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘")
                        multi_output_status = gr.Textbox(label="ç”ŸæˆçŠ¶æ€", interactive=False)
                        
                        gr.Markdown("### ğŸ“š ç¤ºä¾‹æ–‡ä»¶")
                        gr.Markdown("""
                        **å¯ç”¨ç¤ºä¾‹ï¼š**
                        - `multi_example_1`: å½•éŸ³æ£šä¸­çš„ä¸¤äºº (å¹¶è¡Œæ¨¡å¼)
                        - `multi_example_2`: å’–å•¡é¦†ä¸­çš„ä¸¤äººå¯¹è¯ (ä¸²è”æ¨¡å¼ï¼Œå¸¦è¾¹ç•Œæ¡†)
                        
                        **ç¤ºä¾‹æ–‡ä»¶ä½ç½®ï¼š**
                        - å›¾ç‰‡: `assets/avatar/multi/sing.png`, `assets/avatar/multi/introduce.png`
                        - éŸ³é¢‘: `assets/avatar/multi/sing_man.WAV`, `assets/avatar/multi/sing_woman.WAV` ç­‰
                        """)
        
        # ==================== äº‹ä»¶å¤„ç† ====================
        # å•äººæ¨¡å¼äº‹ä»¶
        single_load_example_btn.click(
            fn=load_example,
            inputs=[single_example_dropdown],
            outputs=[single_audio_input, single_image_input, single_prompt]
        )
        
        single_generate_btn.click(
            fn=generate_avatar_video,
            inputs=[
                single_prompt,
                single_audio_input,
                single_image_input,
                single_stage_1,
                single_resolution,
                single_auto_segments,
                single_num_segments,
                single_num_inference_steps,
                single_text_guidance_scale,
                single_audio_guidance_scale,
                single_seed,
                single_ref_img_index,
                single_mask_frame_range
            ],
            outputs=[single_output_video, single_output_status]
        )
        
        # å¤šäººæ¨¡å¼äº‹ä»¶
        multi_load_example_btn.click(
            fn=load_multi_example,
            inputs=[multi_example_dropdown],
            outputs=[multi_audio_input1, multi_audio_input2, multi_image_input, multi_prompt, multi_audio_type, multi_bbox_person1]
        )
        
        multi_generate_btn.click(
            fn=generate_multi_avatar_video,
            inputs=[
                multi_prompt,
                multi_audio_input1,
                multi_audio_input2,
                multi_image_input,
                multi_audio_type,
                multi_resolution,
                multi_auto_segments,
                multi_num_segments,
                multi_num_inference_steps,
                multi_text_guidance_scale,
                multi_audio_guidance_scale,
                multi_seed,
                multi_ref_img_index,
                multi_mask_frame_range,
                multi_bbox_person1,
                multi_bbox_person2
            ],
            outputs=[multi_output_video, multi_output_status]
        )
    
    return demo


def load_all_models(checkpoint_dir="./checkpoints/LongCat-Video-Avatar"):
    """å¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ¨¡å‹"""
    print("=" * 50)
    print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
    print("=" * 50)
    
    print("\n[1/2] æ­£åœ¨åŠ è½½å•äººæ¨¡å¼æ¨¡å‹...")
    load_models(checkpoint_dir, "single")
    print("âœ“ å•äººæ¨¡å¼æ¨¡å‹åŠ è½½å®Œæˆ")
    
    print("\n[2/2] æ­£åœ¨åŠ è½½å¤šäººæ¨¡å¼æ¨¡å‹...")
    load_models(checkpoint_dir, "multi")
    print("âœ“ å¤šäººæ¨¡å¼æ¨¡å‹åŠ è½½å®Œæˆ")
    
    print("\n" + "=" * 50)
    print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("=" * 50)


if __name__ == "__main__":
    # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
    load_all_models()
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_ui()
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
