#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CosyVoice3 Gradio Web Application
åŸºäº Fun-CosyVoice3-0.5B çš„è¯­éŸ³åˆæˆ Web åº”ç”¨

åŠŸèƒ½:
1. Zero-Shot è¯­éŸ³å…‹éš† (3ç§’æé€Ÿå¤åˆ»)
2. è·¨è¯­ç§/ç»†ç²’åº¦æ§åˆ¶
3. æŒ‡ä»¤æ§åˆ¶è¯­éŸ³åˆæˆ

ä½¿ç”¨æ–¹æ³•:
    python app.py
    python app.py --port 7860 --share
"""

import os
import sys
import argparse
import random

import gradio as gr
import numpy as np
import torch
import torchaudio

# æ·»åŠ  Matcha-TTS åˆ°è·¯å¾„
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(ROOT_DIR, 'third_party/Matcha-TTS'))

# åˆ›å»º pretrained_models è½¯é“¾æ¥æŒ‡å‘ checkpointsï¼ˆttsfrd éœ€è¦ï¼‰
pretrained_models_path = os.path.join(ROOT_DIR, 'pretrained_models')
checkpoints_path = os.path.join(ROOT_DIR, 'checkpoints')
if not os.path.exists(pretrained_models_path) and os.path.exists(checkpoints_path):
    os.symlink(checkpoints_path, pretrained_models_path)
    print(f"å·²åˆ›å»ºè½¯é“¾æ¥: pretrained_models -> checkpoints")

from cosyvoice.cli.cosyvoice import AutoModel
from cosyvoice.utils.file_utils import logging
from cosyvoice.utils.common import set_all_random_seed

# é»˜è®¤æ¨¡å‹è·¯å¾„
DEFAULT_MODEL_DIR = "checkpoints/Fun-CosyVoice3-0.5B-2512"

# å…¨å±€å˜é‡
cosyvoice = None
sample_rate = 22050


def generate_seed():
    """ç”Ÿæˆéšæœºç§å­"""
    return random.randint(1, 100000000)


def load_model(model_dir: str):
    """åŠ è½½æ¨¡å‹"""
    global cosyvoice, sample_rate
    
    if not os.path.exists(model_dir):
        return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_dir}", gr.update()
    
    try:
        logging.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_dir}")
        cosyvoice = AutoModel(model_dir=model_dir)
        sample_rate = cosyvoice.sample_rate
        
        spks = cosyvoice.list_available_spks()
        spk_choices = spks if spks else ["æ— é¢„è®­ç»ƒéŸ³è‰²"]
        
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\né‡‡æ ·ç‡: {sample_rate}Hz\né¢„è®­ç»ƒéŸ³è‰²: {', '.join(spks) if spks else 'æ— '}", \
               gr.update(choices=spk_choices, value=spk_choices[0] if spk_choices else None)
    except Exception as e:
        logging.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", gr.update()


def inference_zero_shot(tts_text, prompt_text, prompt_audio, seed, speed):
    """
    Zero-Shot è¯­éŸ³å…‹éš†
    CosyVoice3 æ ¼å¼: prompt_text éœ€è¦åŠ ä¸Š system prompt
    """
    if cosyvoice is None:
        gr.Warning("è¯·å…ˆåŠ è½½æ¨¡å‹ï¼")
        return None
    
    if not tts_text.strip():
        gr.Warning("è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬ï¼")
        return None
    
    if prompt_audio is None:
        gr.Warning("è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘ï¼")
        return None
    
    if not prompt_text.strip():
        gr.Warning("è¯·è¾“å…¥å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬ï¼")
        return None
    
    # CosyVoice3 éœ€è¦æ·»åŠ  system prompt
    full_prompt_text = f"You are a helpful assistant.<|endofprompt|>{prompt_text}"
    
    logging.info(f"Zero-Shot æ¨ç†: tts_text={tts_text[:30]}...")
    set_all_random_seed(seed)
    
    try:
        # æ”¶é›†æ‰€æœ‰è¾“å‡º
        audio_segments = []
        for result in cosyvoice.inference_zero_shot(
            tts_text, full_prompt_text, prompt_audio, stream=False
        ):
            audio_segments.append(result['tts_speech'])
        
        if audio_segments:
            # åˆå¹¶éŸ³é¢‘
            full_audio = torch.cat(audio_segments, dim=1)
            return (sample_rate, full_audio.numpy().flatten())
        return None
    except Exception as e:
        logging.error(f"æ¨ç†å¤±è´¥: {e}")
        gr.Error(f"æ¨ç†å¤±è´¥: {str(e)}")
        return None


def inference_cross_lingual(tts_text, prompt_audio, seed, speed):
    """
    è·¨è¯­ç§/ç»†ç²’åº¦æ§åˆ¶
    æ”¯æŒæ§åˆ¶æ ‡ç­¾: [breath], [laughter] ç­‰
    """
    if cosyvoice is None:
        gr.Warning("è¯·å…ˆåŠ è½½æ¨¡å‹ï¼")
        return None
    
    if not tts_text.strip():
        gr.Warning("è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬ï¼")
        return None
    
    if prompt_audio is None:
        gr.Warning("è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘ï¼")
        return None
    
    # CosyVoice3 æ ¼å¼
    if not tts_text.startswith("You are"):
        tts_text = f"You are a helpful assistant.<|endofprompt|>{tts_text}"
    
    logging.info(f"è·¨è¯­ç§æ¨ç†: tts_text={tts_text[:50]}...")
    set_all_random_seed(seed)
    
    try:
        audio_segments = []
        for result in cosyvoice.inference_cross_lingual(
            tts_text, prompt_audio, stream=False
        ):
            audio_segments.append(result['tts_speech'])
        
        if audio_segments:
            full_audio = torch.cat(audio_segments, dim=1)
            return (sample_rate, full_audio.numpy().flatten())
        return None
    except Exception as e:
        logging.error(f"æ¨ç†å¤±è´¥: {e}")
        gr.Error(f"æ¨ç†å¤±è´¥: {str(e)}")
        return None


def inference_instruct(tts_text, instruct_text, prompt_audio, seed, speed):
    """
    æŒ‡ä»¤æ§åˆ¶åˆæˆ
    ä¾‹å¦‚: ç”¨å››å·è¯è¯´ã€ç”¨å¹¿ä¸œè¯è¡¨è¾¾ã€ç”¨å¿«é€Ÿè¯­é€Ÿè¯´ç­‰
    """
    if cosyvoice is None:
        gr.Warning("è¯·å…ˆåŠ è½½æ¨¡å‹ï¼")
        return None
    
    if not tts_text.strip():
        gr.Warning("è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬ï¼")
        return None
    
    if not instruct_text.strip():
        gr.Warning("è¯·è¾“å…¥æŒ‡ä»¤æ–‡æœ¬ï¼")
        return None
    
    if prompt_audio is None:
        gr.Warning("è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘ï¼")
        return None
    
    # ç¡®ä¿æŒ‡ä»¤æ–‡æœ¬æ ¼å¼æ­£ç¡®
    if not instruct_text.startswith("You are"):
        instruct_text = f"You are a helpful assistant. {instruct_text}"
    if not instruct_text.endswith("<|endofprompt|>"):
        instruct_text = f"{instruct_text}<|endofprompt|>"
    
    logging.info(f"æŒ‡ä»¤æ§åˆ¶æ¨ç†: tts_text={tts_text[:30]}..., instruct={instruct_text[:50]}...")
    set_all_random_seed(seed)
    
    try:
        audio_segments = []
        for result in cosyvoice.inference_instruct2(
            tts_text, instruct_text, prompt_audio, stream=False
        ):
            audio_segments.append(result['tts_speech'])
        
        if audio_segments:
            full_audio = torch.cat(audio_segments, dim=1)
            return (sample_rate, full_audio.numpy().flatten())
        return None
    except Exception as e:
        logging.error(f"æ¨ç†å¤±è´¥: {e}")
        gr.Error(f"æ¨ç†å¤±è´¥: {str(e)}")
        return None


def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(title="CosyVoice3 è¯­éŸ³åˆæˆ", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸ¤ CosyVoice3 è¯­éŸ³åˆæˆç³»ç»Ÿ
            
            åŸºäºé˜¿é‡Œé€šä¹‰å®éªŒå®¤ **Fun-CosyVoice3-0.5B** æ¨¡å‹
            
            ğŸ“– [GitHub](https://github.com/FunAudioLLM/CosyVoice) | 
            [CosyVoice3 ä»‹ç»](https://funaudiollm.github.io/cosyvoice3/)
            """     
        )
        
        # åŠŸèƒ½é€‰é¡¹å¡
        with gr.Tabs():
            # Tab 1: Zero-Shot
            with gr.TabItem("ğŸ­ 3ç§’æé€Ÿå¤åˆ»"):
                gr.Markdown(
                    """
                    ä¸Šä¼  **3-10ç§’** å‚è€ƒéŸ³é¢‘ï¼Œè¾“å…¥å¯¹åº”æ–‡æœ¬ï¼Œå³å¯å…‹éš†éŸ³è‰²åˆæˆæ–°è¯­éŸ³ã€‚
                    """
                )
                with gr.Row():
                    with gr.Column():
                        zs_prompt_audio = gr.Audio(
                            label="ğŸ“ å‚è€ƒéŸ³é¢‘ï¼ˆ3-30ç§’ï¼‰",
                            sources=["upload", "microphone"],
                            type="filepath"
                        )
                        zs_prompt_text = gr.Textbox(
                            label="å‚è€ƒéŸ³é¢‘æ–‡æœ¬",
                            placeholder="è¯·è¾“å…¥å‚è€ƒéŸ³é¢‘ä¸­è¯´çš„å†…å®¹...",
                            lines=2,
                            value="å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚"
                        )
                    with gr.Column():
                        zs_tts_text = gr.Textbox(
                            label="åˆæˆæ–‡æœ¬",
                            placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                            lines=4,
                            value="æ”¶åˆ°å¥½å‹ä»è¿œæ–¹å¯„æ¥çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œé‚£ä»½æ„å¤–çš„æƒŠå–œä¸æ·±æ·±çš„ç¥ç¦è®©æˆ‘å¿ƒä¸­å……æ»¡äº†ç”œèœœçš„å¿«ä¹ï¼Œç¬‘å®¹å¦‚èŠ±å„¿èˆ¬ç»½æ”¾ã€‚"
                        )
                        with gr.Row():
                            zs_seed = gr.Number(label="éšæœºç§å­", value=42, precision=0)
                            zs_speed = gr.Slider(0.5, 2.0, value=1.0, step=0.1, label="è¯­é€Ÿ")
                
                zs_btn = gr.Button("ğŸµ ç”Ÿæˆè¯­éŸ³", variant="primary", size="lg")
                zs_output = gr.Audio(label="åˆæˆç»“æœ", type="numpy")
                
                # ç¤ºä¾‹
                gr.Examples(
                    examples=[
                        ["å…«ç™¾æ ‡å…µå¥”åŒ—å¡ï¼ŒåŒ—å¡ç‚®å…µå¹¶æ’è·‘ï¼Œç‚®å…µæ€•æŠŠæ ‡å…µç¢°ï¼Œæ ‡å…µæ€•ç¢°ç‚®å…µç‚®ã€‚", 
                         "å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚",
                         "./asset/zero_shot_prompt.wav"],
                    ],
                    inputs=[zs_tts_text, zs_prompt_text, zs_prompt_audio],
                    label="ç¤ºä¾‹"
                )
                
                zs_btn.click(
                    fn=inference_zero_shot,
                    inputs=[zs_tts_text, zs_prompt_text, zs_prompt_audio, zs_seed, zs_speed],
                    outputs=[zs_output]
                )
            
            # Tab 2: è·¨è¯­ç§å¤åˆ»
            with gr.TabItem("ğŸŒ è·¨è¯­ç§å¤åˆ»"):
                gr.Markdown(
                    """
                    ç”¨ä¸åŒè¯­è¨€åˆæˆè¯­éŸ³ï¼Œä¿æŒå‚è€ƒéŸ³é¢‘çš„éŸ³è‰²ç‰¹å¾ã€‚
                    
                    **CosyVoice3 æ”¯æŒå¤šè¯­è¨€æ··åˆ**ï¼Œç›´æ¥è¾“å…¥ç›®æ ‡è¯­è¨€æ–‡æœ¬å³å¯ï¼Œæ— éœ€è¯­è¨€æ ‡ç­¾ã€‚
                    """
                )
                with gr.Row():
                    with gr.Column():
                        xl_prompt_audio = gr.Audio(
                            label="ğŸ“ å‚è€ƒéŸ³é¢‘ï¼ˆæä¾›éŸ³è‰²ï¼‰",
                            sources=["upload", "microphone"],
                            type="filepath",
                            value="./asset/cross_lingual_prompt.wav"
                        )
                    with gr.Column():
                        xl_tts_text = gr.Textbox(
                            label="åˆæˆæ–‡æœ¬ï¼ˆç›´æ¥è¾“å…¥ç›®æ ‡è¯­è¨€ï¼‰",
                            placeholder="ç›´æ¥è¾“å…¥è‹±æ–‡ã€æ—¥æ–‡ã€ä¸­æ–‡ç­‰æ–‡æœ¬...",
                            lines=4,
                            value="And then later on, fully acquiring that company. So keeping management in line, interest in line with the asset that's coming into the family."
                        )
                        with gr.Row():
                            xl_seed = gr.Number(label="éšæœºç§å­", value=42, precision=0)
                            xl_speed = gr.Slider(0.5, 2.0, value=1.0, step=0.1, label="è¯­é€Ÿ")
                
                xl_btn = gr.Button("ğŸµ ç”Ÿæˆè¯­éŸ³", variant="primary", size="lg")
                xl_output = gr.Audio(label="åˆæˆç»“æœ", type="numpy")
                
                # è·¨è¯­ç§ç¤ºä¾‹
                gr.Examples(
                    examples=[
                        ["Hello, I am a text to speech model. How can I help you today?", "./asset/cross_lingual_prompt.wav"],
                        ["ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿ", "./asset/cross_lingual_prompt.wav"],
                        ["This is a mixed language test. è¿™æ˜¯ä¸­è‹±æ··åˆæµ‹è¯•ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚", "./asset/cross_lingual_prompt.wav"],
                    ],
                    inputs=[xl_tts_text, xl_prompt_audio],
                    label="ğŸ“Œ ç‚¹å‡»ç¤ºä¾‹å¿«é€Ÿå¡«å……ï¼ˆè‹±æ–‡/æ—¥æ–‡/ä¸­è‹±æ··åˆï¼‰"
                )
                
                xl_btn.click(
                    fn=inference_cross_lingual,
                    inputs=[xl_tts_text, xl_prompt_audio, xl_seed, xl_speed],
                    outputs=[xl_output]
                )
            
            # Tab 3: ç»†ç²’åº¦æ§åˆ¶
            with gr.TabItem("ğŸ›ï¸ ç»†ç²’åº¦æ§åˆ¶"):
                gr.Markdown(
                    """
                    ä½¿ç”¨æ§åˆ¶æ ‡ç­¾ç²¾ç»†æ§åˆ¶è¯­éŸ³åˆæˆæ•ˆæœã€‚
                    
                    **æ”¯æŒçš„æ ‡ç­¾**: `[breath]` å‘¼å¸å£°, `[laughter]` ç¬‘å£° ç­‰
                    """
                )
                with gr.Row():
                    with gr.Column():
                        cl_prompt_audio = gr.Audio(
                            label="ğŸ“ å‚è€ƒéŸ³é¢‘",
                            sources=["upload", "microphone"],
                            type="filepath",
                            value="./asset/zero_shot_prompt.wav"
                        )
                    with gr.Column():
                        cl_tts_text = gr.Textbox(
                            label="åˆæˆæ–‡æœ¬ï¼ˆå¯åŒ…å«æ§åˆ¶æ ‡ç­¾ï¼‰",
                            placeholder="ä¾‹å¦‚: [breath]å› ä¸ºä»–ä»¬é‚£ä¸€è¾ˆäºº[breath]åœ¨ä¹¡é‡Œé¢ä½...",
                            lines=4,
                            value="[breath]å› ä¸ºä»–ä»¬é‚£ä¸€è¾ˆäºº[breath]åœ¨ä¹¡é‡Œé¢ä½çš„è¦ä¹ æƒ¯ä¸€ç‚¹ï¼Œ[breath]é‚»å±…éƒ½å¾ˆæ´»ç»œï¼Œ[breath]å—¯ï¼Œéƒ½å¾ˆç†Ÿæ‚‰ã€‚[breath]"
                        )
                        with gr.Row():
                            cl_seed = gr.Number(label="éšæœºç§å­", value=42, precision=0)
                            cl_speed = gr.Slider(0.5, 2.0, value=1.0, step=0.1, label="è¯­é€Ÿ")
                
                cl_btn = gr.Button("ğŸµ ç”Ÿæˆè¯­éŸ³", variant="primary", size="lg")
                cl_output = gr.Audio(label="åˆæˆç»“æœ", type="numpy")
                
                # ç»†ç²’åº¦æ§åˆ¶ç¤ºä¾‹
                gr.Examples(
                    examples=[
                        ["[breath]å› ä¸ºä»–ä»¬é‚£ä¸€è¾ˆäºº[breath]åœ¨ä¹¡é‡Œé¢ä½çš„è¦ä¹ æƒ¯ä¸€ç‚¹ï¼Œ[breath]é‚»å±…éƒ½å¾ˆæ´»ç»œï¼Œ[breath]å—¯ï¼Œéƒ½å¾ˆç†Ÿæ‚‰ã€‚[breath]", "./asset/zero_shot_prompt.wav"],
                        ["åœ¨ä»–è®²è¿°é‚£ä¸ªè’è¯æ•…äº‹çš„è¿‡ç¨‹ä¸­ï¼Œä»–çªç„¶[laughter]åœä¸‹æ¥ï¼Œå› ä¸ºä»–è‡ªå·±ä¹Ÿè¢«é€—ç¬‘äº†[laughter]ã€‚", "./asset/zero_shot_prompt.wav"],
                    ],
                    inputs=[cl_tts_text, cl_prompt_audio],
                    label="ğŸ“Œ ç‚¹å‡»ç¤ºä¾‹å¿«é€Ÿå¡«å……"
                )
                
                cl_btn.click(
                    fn=inference_cross_lingual,
                    inputs=[cl_tts_text, cl_prompt_audio, cl_seed, cl_speed],
                    outputs=[cl_output]
                )
            
            # Tab 4: æŒ‡ä»¤æ§åˆ¶
            with gr.TabItem("ğŸ“ æŒ‡ä»¤æ§åˆ¶"):
                gr.Markdown(
                    """
                    ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ§åˆ¶è¯­éŸ³é£æ ¼ã€æ–¹è¨€ç­‰ã€‚
                    """
                )
                with gr.Row():
                    with gr.Column():
                        inst_prompt_audio = gr.Audio(
                            label="ğŸ“ å‚è€ƒéŸ³é¢‘",
                            sources=["upload", "microphone"],
                            type="filepath",
                            value="./asset/zero_shot_prompt.wav"
                        )
                        inst_instruct = gr.Textbox(
                            label="æŒ‡ä»¤",
                            placeholder="ä¾‹å¦‚: è¯·ç”¨å¹¿ä¸œè¯è¡¨è¾¾",
                            lines=2,
                            value="è¯·ç”¨å¹¿ä¸œè¯è¡¨è¾¾"
                        )
                    with gr.Column():
                        inst_tts_text = gr.Textbox(
                            label="åˆæˆæ–‡æœ¬",
                            placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                            lines=4,
                            value="å¥½å°‘å’¯ï¼Œä¸€èˆ¬ç³»æ”¾å—°å•²å›½åº†å•Šï¼Œä¸­ç§‹å—°å•²å¯èƒ½ä¼šå’¯ã€‚"
                        )
                        with gr.Row():
                            inst_seed = gr.Number(label="éšæœºç§å­", value=42, precision=0)
                            inst_speed = gr.Slider(0.5, 2.0, value=1.0, step=0.1, label="è¯­é€Ÿ")
                
                inst_btn = gr.Button("ğŸµ ç”Ÿæˆè¯­éŸ³", variant="primary", size="lg")
                inst_output = gr.Audio(label="åˆæˆç»“æœ", type="numpy")
                
                # æŒ‡ä»¤ç¤ºä¾‹
                gr.Examples(
                    examples=[
                        ["å¥½å°‘å’¯ï¼Œä¸€èˆ¬ç³»æ”¾å—°å•²å›½åº†å•Šï¼Œä¸­ç§‹å—°å•²å¯èƒ½ä¼šå’¯ã€‚", "è¯·ç”¨å¹¿ä¸œè¯è¡¨è¾¾", "./asset/zero_shot_prompt.wav"],
                        ["æ”¶åˆ°å¥½å‹ä»è¿œæ–¹å¯„æ¥çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œé‚£ä»½æ„å¤–çš„æƒŠå–œè®©æˆ‘å¿ƒä¸­å……æ»¡äº†å¿«ä¹ã€‚", "è¯·ç”¨å››å·è¯è¯´è¿™å¥è¯", "./asset/zero_shot_prompt.wav"],
                        ["æ”¶åˆ°å¥½å‹ä»è¿œæ–¹å¯„æ¥çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œé‚£ä»½æ„å¤–çš„æƒŠå–œä¸æ·±æ·±çš„ç¥ç¦è®©æˆ‘å¿ƒä¸­å……æ»¡äº†ç”œèœœçš„å¿«ä¹ï¼Œç¬‘å®¹å¦‚èŠ±å„¿èˆ¬ç»½æ”¾ã€‚", "è¯·ç”¨å°½å¯èƒ½å¿«çš„è¯­é€Ÿè¯´", "./asset/zero_shot_prompt.wav"],
                        ["æ”¶åˆ°å¥½å‹ä»è¿œæ–¹å¯„æ¥çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œé‚£ä»½æ„å¤–çš„æƒŠå–œä¸æ·±æ·±çš„ç¥ç¦è®©æˆ‘å¿ƒä¸­å……æ»¡äº†ç”œèœœçš„å¿«ä¹ã€‚", "è¯·ç”¨æ¸©æŸ”çš„è¯­æ°”è¯´", "./asset/zero_shot_prompt.wav"],
                    ],
                    inputs=[inst_tts_text, inst_instruct, inst_prompt_audio],
                    label="ğŸ“Œ ç‚¹å‡»ç¤ºä¾‹å¿«é€Ÿå¡«å……"
                )
                
                inst_btn.click(
                    fn=inference_instruct,
                    inputs=[inst_tts_text, inst_instruct, inst_prompt_audio, inst_seed, inst_speed],
                    outputs=[inst_output]
                )
        
        gr.Markdown(
            """
            ---
            ### ğŸ“ ç¤ºä¾‹éŸ³é¢‘æ–‡ä»¶
            - `./asset/zero_shot_prompt.wav` - å‚è€ƒéŸ³é¢‘ç¤ºä¾‹
            - `./asset/cross_lingual_prompt.wav` - è·¨è¯­ç§å‚è€ƒéŸ³é¢‘
            """
        )
    
    return demo


def main():
    parser = argparse.ArgumentParser(description="CosyVoice3 Web UI")
    parser.add_argument("--port", type=int, default=7860, help="æœåŠ¡ç«¯å£")
    parser.add_argument("--model_dir", type=str, default=DEFAULT_MODEL_DIR, help="æ¨¡å‹è·¯å¾„")
    args = parser.parse_args()
    
    # å¯åŠ¨æ—¶ç›´æ¥åŠ è½½æ¨¡å‹
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {args.model_dir}")
    status, _ = load_model(args.model_dir)
    print(status)
    
    if cosyvoice is None:
        print("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„ï¼")
        return
    
    # å¯åŠ¨ UI
    demo = create_ui()
    demo.queue(max_size=10)
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=False
    )


if __name__ == "__main__":
    main()
