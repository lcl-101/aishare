#!/usr/bin/env python3
"""
FLUX WebUI - A Gradio-based web interface for FLUX image generation
ä½¿ç”¨æœ¬åœ°ä¸‹è½½çš„ FLUX æ¨¡å‹è¿›è¡Œå›¾åƒç”Ÿæˆ
"""

import torch
import gradio as gr
from diffusers import FluxPipeline
import os
from PIL import Image
import time

class FluxWebUI:
    def __init__(self):
        self.pipe = None
        self.current_model = None
        self.model_paths = {
            "FLUX.1-dev": "checkpoints/FLUX.1-dev",
            "FLUX.1-Krea-dev": "checkpoints/FLUX.1-Krea-dev"
        }
        
    def load_model(self, model_name):
        """åŠ è½½æŒ‡å®šçš„æ¨¡å‹"""
        if self.current_model == model_name and self.pipe is not None:
            return f"æ¨¡å‹ {model_name} å·²ç»åŠ è½½"
            
        try:
            model_path = self.model_paths[model_name]
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name} ä»è·¯å¾„: {model_path}")
            
            # é‡Šæ”¾ä¹‹å‰çš„æ¨¡å‹å†…å­˜
            if self.pipe is not None:
                del self.pipe
                torch.cuda.empty_cache()
            
            # æ£€æŸ¥GPUå¯ç”¨æ€§
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ä½¿ç”¨è®¾å¤‡: {device}")
            if device == "cuda":
                print(f"GPU åç§°: {torch.cuda.get_device_name()}")
                print(f"GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            
            # åŠ è½½æ–°æ¨¡å‹å¹¶ä¼˜åŒ–GPUä½¿ç”¨
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # å°è¯•ä¸åŒçš„åŠ è½½ç­–ç•¥
            try:
                # æ–¹æ³•1: ç›´æ¥åŠ è½½
                self.pipe = FluxPipeline.from_pretrained(
                    model_path, 
                    torch_dtype=torch.bfloat16,
                    local_files_only=True,
                )
                self.pipe = self.pipe.to(device)
                print("âœ… ä½¿ç”¨ç›´æ¥åŠ è½½æ–¹æ³•")
            except Exception as e1:
                print(f"ç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•: {e1}")
                try:
                    # æ–¹æ³•2: ä½¿ç”¨balancedç­–ç•¥
                    self.pipe = FluxPipeline.from_pretrained(
                        model_path, 
                        torch_dtype=torch.bfloat16,
                        local_files_only=True,
                        device_map="balanced",
                    )
                    print("âœ… ä½¿ç”¨balancedç­–ç•¥")
                except Exception as e2:
                    print(f"balancedç­–ç•¥å¤±è´¥: {e2}")
                    # æ–¹æ³•3: CPUæ¨¡å¼
                    self.pipe = FluxPipeline.from_pretrained(
                        model_path, 
                        torch_dtype=torch.bfloat16,
                        local_files_only=True,
                    )
                    device = "cpu"
                    print("âš ï¸ å›é€€åˆ°CPUæ¨¡å¼")
            
            # ä¸ä½¿ç”¨CPU offloadï¼Œå› ä¸ºæ‚¨æœ‰è¶³å¤Ÿçš„æ˜¾å­˜
            # self.pipe.enable_model_cpu_offload()  # æ³¨é‡Šæ‰è¿™è¡Œ
            
            # å¯ç”¨å…¶ä»–GPUä¼˜åŒ–
            if device == "cuda":
                # å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
                self.pipe.enable_attention_slicing()
                # å¯ç”¨VAEåˆ‡ç‰‡ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
                self.pipe.enable_vae_slicing()
                # å¦‚æœæ”¯æŒï¼Œå¯ç”¨flash attention
                try:
                    self.pipe.enable_flash_attention()
                    print("âœ… Flash Attention å·²å¯ç”¨")
                except:
                    print("âš ï¸ Flash Attention ä¸å¯ç”¨")
                
                # æ¸…ç†GPUå†…å­˜
                torch.cuda.empty_cache()
                
            self.current_model = model_name
            return f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name} åˆ° {device.upper()}"
            
        except Exception as e:
            return f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}"
    
    def generate_image(self, prompt, model_name, height, width, guidance_scale, num_inference_steps):
        """ç”Ÿæˆå›¾åƒ"""
        if self.pipe is None or self.current_model != model_name:
            load_result = self.load_model(model_name)
            if "âŒ" in load_result:
                return None, load_result
    
    def generate_comparison_images(self, prompt, height, width, guidance_scale, num_inference_steps):
        """åŒæ—¶ä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹ç”Ÿæˆå›¾åƒè¿›è¡Œå¯¹æ¯”"""
        results = {}
        total_start_time = time.time()
        
        # è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        available_models = [name for name, path in self.model_paths.items() if os.path.exists(path)]
        
        if len(available_models) < 2:
            return None, None, "âŒ éœ€è¦è‡³å°‘ä¸¤ä¸ªå¯ç”¨æ¨¡å‹æ‰èƒ½è¿›è¡Œå¯¹æ¯”"
        
        print(f"ğŸ”„ å¼€å§‹å¯¹æ¯”æ¨¡å¼ç”Ÿæˆ...")
        print(f"æç¤ºè¯: {prompt}")
        print(f"å°ºå¯¸: {width}x{height}")
        print(f"å¼•å¯¼æ¯”ä¾‹: {guidance_scale}")
        print(f"æ¨ç†æ­¥æ•°: {num_inference_steps}")
        
        status_messages = []
        
        for i, model_name in enumerate(available_models):
            try:
                print(f"\nğŸ¨ æ­£åœ¨ä½¿ç”¨æ¨¡å‹ {i+1}/2: {model_name}")
                
                # åŠ è½½æ¨¡å‹
                if self.current_model != model_name:
                    load_result = self.load_model(model_name)
                    if "âŒ" in load_result:
                        status_messages.append(f"âŒ {model_name}: åŠ è½½å¤±è´¥")
                        continue
                
                # æ£€æŸ¥GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    memory_allocated = torch.cuda.memory_allocated() / 1024**3
                    print(f"GPU å†…å­˜ä½¿ç”¨: {memory_allocated:.1f}GB")
                
                start_time = time.time()
                
                # ç”Ÿæˆå›¾åƒ
                with torch.amp.autocast('cuda', enabled=True, dtype=torch.bfloat16):
                    result = self.pipe(
                        prompt,
                        height=height,
                        width=width,
                        guidance_scale=guidance_scale,
                        num_inference_steps=num_inference_steps,
                        generator=torch.Generator().manual_seed(42),  # å›ºå®šç§å­ç¡®ä¿å¯é‡å¤
                    )
                
                generation_time = time.time() - start_time
                image = result.images[0]
                
                # ä¿å­˜å›¾åƒ
                timestamp = int(time.time())
                filename = f"flux_compare_{model_name.replace('.', '_')}_{timestamp}.png"
                filepath = f"/workspace/flux/{filename}"
                image.save(filepath)
                
                results[model_name] = {
                    'image': image,
                    'time': generation_time,
                    'filename': filename
                }
                
                status_messages.append(f"âœ… {model_name}: {generation_time:.2f}ç§’ -> {filename}")
                print(f"âœ… {model_name} ç”Ÿæˆå®Œæˆ: {generation_time:.2f}ç§’")
                
                # æ¸…ç†GPUå†…å­˜
                torch.cuda.empty_cache()
                
            except Exception as e:
                error_msg = f"âŒ {model_name} ç”Ÿæˆå¤±è´¥: {str(e)}"
                status_messages.append(error_msg)
                print(error_msg)
        
        total_time = time.time() - total_start_time
        
        # å‡†å¤‡è¿”å›ç»“æœ
        model_names = list(results.keys())
        if len(model_names) >= 2:
            image1 = results[model_names[0]]['image']
            image2 = results[model_names[1]]['image']
            
            final_status = f"ğŸ‰ å¯¹æ¯”ç”Ÿæˆå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’\n\n"
            final_status += "\n".join(status_messages)
            
            # æ˜¾ç¤ºæœ€ç»ˆGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                final_memory = torch.cuda.memory_allocated() / 1024**3
                final_status += f"\n\nğŸ’¾ æœ€ç»ˆGPUå†…å­˜ä½¿ç”¨: {final_memory:.1f}GB"
            
            return image1, image2, final_status
        elif len(model_names) == 1:
            model_name = model_names[0]
            return results[model_name]['image'], None, f"âš ï¸ åªæœ‰ä¸€ä¸ªæ¨¡å‹ç”ŸæˆæˆåŠŸ: {model_name}\n" + "\n".join(status_messages)
        else:
            return None, None, "âŒ æ‰€æœ‰æ¨¡å‹éƒ½ç”Ÿæˆå¤±è´¥\n" + "\n".join(status_messages)

    def generate_single_image(self, prompt, model_name, height, width, guidance_scale, num_inference_steps):
        """ä½¿ç”¨å•ä¸ªæ¨¡å‹ç”Ÿæˆå›¾åƒ"""
        if self.pipe is None or self.current_model != model_name:
            load_result = self.load_model(model_name)
            if "âŒ" in load_result:
                return None, load_result
        
        try:
            print(f"æ­£åœ¨ç”Ÿæˆå›¾åƒ...")
            print(f"æç¤ºè¯: {prompt}")
            print(f"å°ºå¯¸: {width}x{height}")
            print(f"å¼•å¯¼æ¯”ä¾‹: {guidance_scale}")
            print(f"æ¨ç†æ­¥æ•°: {num_inference_steps}")
            
            # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                torch.cuda.empty_cache()  # æ¸…ç†å†…å­˜
                memory_allocated = torch.cuda.memory_allocated() / 1024**3
                memory_reserved = torch.cuda.memory_reserved() / 1024**3
                print(f"GPU å†…å­˜ä½¿ç”¨: {memory_allocated:.1f}GB å·²åˆ†é…, {memory_reserved:.1f}GB å·²ä¿ç•™")
            
            start_time = time.time()
            
            # ç”Ÿæˆå›¾åƒï¼Œç¡®ä¿ä½¿ç”¨GPU
            with torch.amp.autocast('cuda', enabled=True, dtype=torch.bfloat16):
                result = self.pipe(
                    prompt,
                    height=height,
                    width=width,
                    guidance_scale=guidance_scale,
                    num_inference_steps=num_inference_steps,
                    generator=torch.Generator().manual_seed(42),  # å›ºå®šç§å­ä»¥ä¾¿å¤ç°
                )
            
            image = result.images[0]
            generation_time = time.time() - start_time
            
            # æ˜¾ç¤ºæœ€ç»ˆGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                final_memory_allocated = torch.cuda.memory_allocated() / 1024**3
                final_memory_reserved = torch.cuda.memory_reserved() / 1024**3
                print(f"ç”ŸæˆåGPUå†…å­˜: {final_memory_allocated:.1f}GB å·²åˆ†é…, {final_memory_reserved:.1f}GB å·²ä¿ç•™")
            
            # ä¿å­˜å›¾åƒ
            timestamp = int(time.time())
            filename = f"flux_generated_{timestamp}.png"
            filepath = f"/workspace/flux/{filename}"
            image.save(filepath)
            
            status = f"âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼\næ—¶é—´: {generation_time:.2f}ç§’\nä¿å­˜è·¯å¾„: {filename}\nGPUå†…å­˜ä½¿ç”¨: {final_memory_allocated:.1f}GB"
            return image, status
            
        except Exception as e:
            error_msg = f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥: {str(e)}"
            print(error_msg)
            return None, error_msg
        
        try:
            print(f"æ­£åœ¨ç”Ÿæˆå›¾åƒ...")
            print(f"æç¤ºè¯: {prompt}")
            print(f"å°ºå¯¸: {width}x{height}")
            print(f"å¼•å¯¼æ¯”ä¾‹: {guidance_scale}")
            print(f"æ¨ç†æ­¥æ•°: {num_inference_steps}")
            
            # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                torch.cuda.empty_cache()  # æ¸…ç†å†…å­˜
                memory_allocated = torch.cuda.memory_allocated() / 1024**3
                memory_reserved = torch.cuda.memory_reserved() / 1024**3
                print(f"GPU å†…å­˜ä½¿ç”¨: {memory_allocated:.1f}GB å·²åˆ†é…, {memory_reserved:.1f}GB å·²ä¿ç•™")
            
            start_time = time.time()
            
            # ç”Ÿæˆå›¾åƒï¼Œç¡®ä¿ä½¿ç”¨GPU
            with torch.amp.autocast('cuda', enabled=True, dtype=torch.bfloat16):
                result = self.pipe(
                    prompt,
                    height=height,
                    width=width,
                    guidance_scale=guidance_scale,
                    num_inference_steps=num_inference_steps,
                    generator=torch.Generator().manual_seed(42),  # å›ºå®šç§å­ä»¥ä¾¿å¤ç°
                )
            
            image = result.images[0]
            generation_time = time.time() - start_time
            
            # æ˜¾ç¤ºæœ€ç»ˆGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                final_memory_allocated = torch.cuda.memory_allocated() / 1024**3
                final_memory_reserved = torch.cuda.memory_reserved() / 1024**3
                print(f"ç”ŸæˆåGPUå†…å­˜: {final_memory_allocated:.1f}GB å·²åˆ†é…, {final_memory_reserved:.1f}GB å·²ä¿ç•™")
            
            # ä¿å­˜å›¾åƒ
            timestamp = int(time.time())
            filename = f"flux_generated_{timestamp}.png"
            filepath = f"/workspace/flux/{filename}"
            image.save(filepath)
            
            status = f"âœ… å›¾åƒç”ŸæˆæˆåŠŸï¼\næ—¶é—´: {generation_time:.2f}ç§’\nä¿å­˜è·¯å¾„: {filename}\nGPUå†…å­˜ä½¿ç”¨: {final_memory_allocated:.1f}GB"
            return image, status
            
        except Exception as e:
            error_msg = f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥: {str(e)}"
            print(error_msg)
            return None, error_msg

def get_gpu_info():
    """è·å–GPUä¿¡æ¯"""
    if not torch.cuda.is_available():
        return "âŒ CUDA ä¸å¯ç”¨"
    
    gpu_name = torch.cuda.get_device_name()
    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
    memory_allocated = torch.cuda.memory_allocated() / 1024**3
    memory_free = memory_total - memory_allocated
    
    return f"""
ğŸ”§ **GPUçŠ¶æ€:**
- è®¾å¤‡: {gpu_name}  
- æ€»å†…å­˜: {memory_total:.1f} GB
- å·²ä½¿ç”¨: {memory_allocated:.1f} GB  
- å¯ç”¨: {memory_free:.1f} GB
"""

def create_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    flux_ui = FluxWebUI()
    
    with gr.Blocks(title="FLUX Image Generator", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¨ FLUX Image Generator WebUI
        
        åŸºäº FLUX æ¨¡å‹çš„å›¾åƒç”Ÿæˆå·¥å…·ï¼Œä½¿ç”¨æœ¬åœ°é¢„ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶ã€‚
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # ç”Ÿæˆæ¨¡å¼é€‰æ‹©
                generation_mode = gr.Radio(
                    choices=["å•æ¨¡å‹ç”Ÿæˆ", "åŒæ¨¡å‹å¯¹æ¯”"],
                    value="å•æ¨¡å‹ç”Ÿæˆ",
                    label="ç”Ÿæˆæ¨¡å¼",
                    info="é€‰æ‹©å•ä¸ªæ¨¡å‹ç”Ÿæˆæˆ–ä¸¤ä¸ªæ¨¡å‹å¯¹æ¯”ç”Ÿæˆ"
                )
                
                # æ¨¡å‹é€‰æ‹©ï¼ˆåªåœ¨å•æ¨¡å‹æ¨¡å¼ä¸‹æ˜¾ç¤ºï¼‰
                model_dropdown = gr.Dropdown(
                    choices=list(flux_ui.model_paths.keys()),
                    value="FLUX.1-Krea-dev",
                    label="é€‰æ‹©æ¨¡å‹",
                    info="é€‰æ‹©è¦ä½¿ç”¨çš„ FLUX æ¨¡å‹",
                    visible=True
                )
                
                # æç¤ºè¯è¾“å…¥
                prompt_input = gr.Textbox(
                    label="æç¤ºè¯ (Prompt)",
                    placeholder="A frog holding a sign that says hello world",
                    value="A frog holding a sign that says hello world",
                    lines=3
                )
                
                # ç”Ÿæˆå‚æ•°
                with gr.Group():
                    gr.Markdown("### ç”Ÿæˆå‚æ•°")
                    
                    with gr.Row():
                        width_slider = gr.Slider(
                            minimum=512, maximum=2048, step=64, value=1024,
                            label="å®½åº¦"
                        )
                        height_slider = gr.Slider(
                            minimum=512, maximum=2048, step=64, value=1024,
                            label="é«˜åº¦"
                        )
                    
                    guidance_scale_slider = gr.Slider(
                        minimum=1.0, maximum=20.0, step=0.5, value=4.5,
                        label="å¼•å¯¼æ¯”ä¾‹ (Guidance Scale)",
                        info="æ§åˆ¶ç”Ÿæˆå›¾åƒä¸æç¤ºè¯çš„ä¸€è‡´æ€§"
                    )
                    
                    steps_slider = gr.Slider(
                        minimum=1, maximum=100, step=1, value=20,
                        label="æ¨ç†æ­¥æ•° (Inference Steps)",
                        info="æ›´å¤šæ­¥æ•°é€šå¸¸äº§ç”Ÿæ›´å¥½çš„è´¨é‡ï¼Œä½†éœ€è¦æ›´é•¿æ—¶é—´"
                    )
                
                # ç”ŸæˆæŒ‰é’®
                with gr.Row():
                    generate_single_btn = gr.Button(
                        "ğŸ¨ å•æ¨¡å‹ç”Ÿæˆ", 
                        variant="primary",
                        size="lg",
                        visible=True
                    )
                    generate_compare_btn = gr.Button(
                        "ğŸ”„ åŒæ¨¡å‹å¯¹æ¯”", 
                        variant="secondary",
                        size="lg",
                        visible=False
                    )
            
            with gr.Column(scale=1):
                # å•æ¨¡å‹è¾“å‡º
                with gr.Group(visible=True) as single_output:
                    gr.Markdown("### ç”Ÿæˆç»“æœ")
                    output_image = gr.Image(
                        label="ç”Ÿæˆçš„å›¾åƒ",
                        type="pil",
                        height=600
                    )
                
                # å¯¹æ¯”æ¨¡å¼è¾“å‡º
                with gr.Group(visible=False) as compare_output:
                    gr.Markdown("### æ¨¡å‹å¯¹æ¯”ç»“æœ")
                    with gr.Row():
                        output_image1 = gr.Image(
                            label="FLUX.1-dev",
                            type="pil",
                            height=400
                        )
                        output_image2 = gr.Image(
                            label="FLUX.1-Krea-dev",
                            type="pil",
                            height=400
                        )
                
                # çŠ¶æ€ä¿¡æ¯
                status_text = gr.Textbox(
                    label="çŠ¶æ€ä¿¡æ¯",
                    lines=5,
                    interactive=False
                )
                
                # GPUçŠ¶æ€æ˜¾ç¤º
                gpu_info_text = gr.Markdown(
                    value=get_gpu_info(),
                    label="GPUçŠ¶æ€"
                )
                
                # åˆ·æ–°GPUçŠ¶æ€æŒ‰é’®
                refresh_gpu_btn = gr.Button(
                    "ğŸ”„ åˆ·æ–°GPUçŠ¶æ€",
                    size="sm"
                )
        
        # ç¤ºä¾‹æç¤ºè¯
        gr.Markdown("### ğŸ’¡ ç¤ºä¾‹æç¤ºè¯")
        example_prompts = [
            "A majestic dragon flying over a medieval castle at sunset",
            "A futuristic city with flying cars and neon lights",
            "A peaceful zen garden with cherry blossoms",
            "A cute robot playing with a kitten in a cozy room",
            "An astronaut riding a horse on the moon"
        ]
        
        gr.Examples(
            examples=[[prompt] for prompt in example_prompts],
            inputs=[prompt_input],
            label="ç‚¹å‡»ä½¿ç”¨ç¤ºä¾‹æç¤ºè¯"
        )
        
        # æ¨¡å¼åˆ‡æ¢åŠŸèƒ½
        def toggle_generation_mode(mode):
            if mode == "å•æ¨¡å‹ç”Ÿæˆ":
                return (
                    gr.update(visible=True),   # model_dropdown
                    gr.update(visible=True),   # generate_single_btn
                    gr.update(visible=False),  # generate_compare_btn
                    gr.update(visible=True),   # single_output
                    gr.update(visible=False),  # compare_output
                )
            else:  # åŒæ¨¡å‹å¯¹æ¯”
                return (
                    gr.update(visible=False),  # model_dropdown
                    gr.update(visible=False),  # generate_single_btn
                    gr.update(visible=True),   # generate_compare_btn
                    gr.update(visible=False),  # single_output
                    gr.update(visible=True),   # compare_output
                )
        
        generation_mode.change(
            fn=toggle_generation_mode,
            inputs=[generation_mode],
            outputs=[model_dropdown, generate_single_btn, generate_compare_btn, single_output, compare_output]
        )
        
        # ç»‘å®šå•æ¨¡å‹ç”Ÿæˆäº‹ä»¶
        generate_single_btn.click(
            fn=flux_ui.generate_single_image,
            inputs=[
                prompt_input,
                model_dropdown,
                height_slider,
                width_slider,
                guidance_scale_slider,
                steps_slider
            ],
            outputs=[output_image, status_text]
        )
        
        # ç»‘å®šåŒæ¨¡å‹å¯¹æ¯”ç”Ÿæˆäº‹ä»¶
        generate_compare_btn.click(
            fn=flux_ui.generate_comparison_images,
            inputs=[
                prompt_input,
                height_slider,
                width_slider,
                guidance_scale_slider,
                steps_slider
            ],
            outputs=[output_image1, output_image2, status_text]
        )
        
        # ç»‘å®šGPUçŠ¶æ€åˆ·æ–°äº‹ä»¶
        refresh_gpu_btn.click(
            fn=get_gpu_info,
            outputs=[gpu_info_text]
        )
    
    return demo

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ FLUX WebUI...")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_paths = {
        "FLUX.1-dev": "checkpoints/FLUX.1-dev",
        "FLUX.1-Krea-dev": "checkpoints/FLUX.1-Krea-dev"
    }
    
    available_models = []
    for model_name, model_path in model_paths.items():
        if os.path.exists(model_path):
            available_models.append(model_name)
            print(f"âœ… å‘ç°æ¨¡å‹: {model_name}")
        else:
            print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_name} (è·¯å¾„: {model_path})")
    
    if not available_models:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶ï¼")
        return
    
    print(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"ğŸ” å¯ç”¨æ¨¡å‹: {', '.join(available_models)}")
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_interface()
    
    # å¯åŠ¨æœåŠ¡å™¨
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,       # ç«¯å£
        share=False,            # ä¸åˆ›å»ºå…¬å…±é“¾æ¥
        debug=True,             # è°ƒè¯•æ¨¡å¼
        show_error=True         # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
    )

if __name__ == "__main__":
    main()
