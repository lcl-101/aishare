"""
NeMo Streaming ASR Gradio Web Application
åŸºäº NVIDIA Nemotron Speech Streaming æ¨¡å‹çš„è¯­éŸ³è¯†åˆ« Web åº”ç”¨
"""

import os
import tempfile
import logging
import time
from pathlib import Path
from typing import Optional, Tuple

import gradio as gr
import numpy as np
from omegaconf import OmegaConf

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
pipeline = None
asr_model = None

# æœ¬åœ°æ¨¡å‹è·¯å¾„
MODEL_PATH = Path(__file__).parent / "checkpoints" / "nemotron-speech-streaming-en-0.6b" / "nemotron-speech-streaming-en-0.6b.nemo"

# é»˜è®¤é…ç½®ï¼ˆå†…åµŒåœ¨ä»£ç ä¸­ï¼Œä¸éœ€è¦å•ç‹¬é…ç½®æ–‡ä»¶ï¼‰
DEFAULT_CONFIG = {
    # ASR é…ç½®
    "asr": {
        "model_name": str(MODEL_PATH),  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ–‡ä»¶
        "device": "cuda",
        "device_id": 0,
        "compute_dtype": "bfloat16",
        "use_amp": True,
        "decoding": {
            "strategy": "greedy_batch",
            "preserve_alignments": False,
            "fused_batch_size": -1,
            "greedy": {
                "use_cuda_graph_decoder": False,
                "enable_per_stream_biasing": False,
                "max_symbols": 10,
                "ngram_lm_model": None,
                "ngram_lm_alpha": 0.0,
                "boosting_tree": {
                    "model_path": None,
                    "key_phrases_file": None,
                    "key_phrases_list": None,
                    "key_phrase_items_list": None,
                    "source_lang": "en",
                },
                "boosting_tree_alpha": 0.0,
            },
        },
    },
    # ITN é…ç½®
    "itn": {
        "input_case": "lower_cased",
        "whitelist": None,
        "overwrite_cache": False,
        "max_number_of_permutations_per_split": 729,
        "left_padding_size": 4,
        "batch_size": 32,
        "n_jobs": 16,
    },
    # NMT é…ç½®
    "nmt": {
        "model_name": "utter-project/EuroLLM-1.7B-Instruct",
        "source_language": "English",
        "target_language": "Russian",
        "waitk": -1,
        "device": "cuda",
        "device_id": 1,
        "batch_size": 16,
        "llm_params": {
            "dtype": "auto",
            "seed": 42,
        },
        "sampling_params": {
            "max_tokens": 100,
            "temperature": 0.0,
            "top_p": 0.9,
            "seed": 42,
        },
    },
    # ç½®ä¿¡åº¦é…ç½®
    "confidence": {
        "exclude_blank": True,
        "aggregation": "mean",
        "method_cfg": {
            "name": "entropy",
            "entropy_type": "tsallis",
            "alpha": 0.5,
            "entropy_norm": "exp",
        },
    },
    # ç«¯ç‚¹æ£€æµ‹é…ç½®
    "endpointing": {
        "stop_history_eou": 800,
        "residue_tokens_at_end": 2,
    },
    # æµå¼é…ç½®
    "streaming": {
        "sample_rate": 16000,
        "batch_size": 64,
        "word_boundary_tolerance": 4,
        "att_context_size": [70, 13],
        "use_cache": True,
        "use_feat_cache": True,
        "chunk_size_in_secs": None,
        "request_type": "frame",
        "num_slots": 256,
    },
    # Pipeline è®¾ç½®
    "matmul_precision": "high",
    "log_level": 20,
    "pipeline_type": "cache_aware",
    "asr_decoding_type": "rnnt",
    # è¿è¡Œæ—¶å‚æ•°
    "audio_file": None,
    "output_filename": None,
    "output_dir": None,
    "enable_pnc": False,
    "enable_itn": False,
    "enable_nmt": False,
    "asr_output_granularity": "segment",
    "cache_dir": None,
    "lang": None,
    "return_tail_result": False,
    "calculate_wer": True,
    "calculate_bleu": True,
    # æŒ‡æ ‡é…ç½®
    "metrics": {
        "asr": {
            "gt_text_attr_name": "text",
            "clean_groundtruth_text": False,
            "langid": "en",
            "use_cer": False,
            "ignore_capitalization": True,
            "ignore_punctuation": True,
            "strip_punc_space": False,
        },
        "nmt": {
            "gt_text_attr_name": "answer",
            "ignore_capitalization": False,
            "ignore_punctuation": False,
            "strip_punc_space": False,
        },
    },
}


def get_config(
    enable_pnc: bool = False,
    enable_itn: bool = False,
    att_context_size: list = None
) -> OmegaConf:
    """è·å–é…ç½®å¯¹è±¡"""
    cfg = OmegaConf.create(DEFAULT_CONFIG)
    
    # æ›´æ–°è¿è¡Œæ—¶é…ç½®
    cfg.enable_pnc = enable_pnc
    cfg.enable_itn = enable_itn
    
    if att_context_size:
        cfg.streaming.att_context_size = att_context_size
    
    return cfg


def load_pipeline():
    """åŠ è½½ ASR Pipeline"""
    global pipeline
    
    if pipeline is not None:
        return pipeline
    
    try:
        from nemo.collections.asr.inference.factory.pipeline_builder import PipelineBuilder
        
        logger.info("Loading config...")
        cfg = get_config()
        
        logger.info("Building ASR pipeline...")
        pipeline = PipelineBuilder.build_pipeline(cfg)
        logger.info("Pipeline loaded successfully!")
        
        return pipeline
    except Exception as e:
        logger.error(f"Failed to load pipeline: {e}")
        raise


def load_simple_model():
    """åŠ è½½ç®€å•çš„ ASR æ¨¡å‹ï¼ˆä¸ä½¿ç”¨ pipelineï¼‰"""
    global asr_model
    
    if asr_model is not None:
        return asr_model
    
    try:
        import nemo.collections.asr as nemo_asr
        
        logger.info(f"Loading ASR model from {MODEL_PATH}...")
        # ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ–‡ä»¶
        asr_model = nemo_asr.models.ASRModel.restore_from(str(MODEL_PATH))
        logger.info("Model loaded successfully!")
        
        return asr_model
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        raise


def transcribe_audio_file(
    audio_path: str,
    use_pipeline: bool = True,
    enable_pnc: bool = False,
    enable_itn: bool = False,
    att_context_size: str = "[70, 13]"
) -> Tuple[str, str]:
    """
    è½¬å½•éŸ³é¢‘æ–‡ä»¶
    
    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        use_pipeline: æ˜¯å¦ä½¿ç”¨ pipeline æ¨¡å¼
        enable_pnc: æ˜¯å¦å¯ç”¨æ ‡ç‚¹å’Œå¤§å°å†™
        enable_itn: æ˜¯å¦å¯ç”¨é€†æ–‡æœ¬è§„èŒƒåŒ–
        att_context_size: æ³¨æ„åŠ›ä¸Šä¸‹æ–‡å¤§å°é…ç½®
        
    Returns:
        (è½¬å½•æ–‡æœ¬, æ¨ç†æ—¶é—´ä¿¡æ¯)
    """
    if audio_path is None:
        return "è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", ""
    
    try:
        start_time = time.time()
        
        if use_pipeline:
            # ä½¿ç”¨ Pipeline æ¨¡å¼
            from nemo.collections.asr.inference.factory.pipeline_builder import PipelineBuilder
            
            # è§£æ att_context_size
            context_size = None
            try:
                parsed = eval(att_context_size)
                if isinstance(parsed, (list, tuple)) and len(parsed) == 2:
                    context_size = list(parsed)
            except:
                pass
            
            # è·å–é…ç½®
            cfg = get_config(
                enable_pnc=enable_pnc,
                enable_itn=enable_itn,
                att_context_size=context_size
            )
            
            # æ„å»º pipeline å¹¶è¿è¡Œ
            pipe = PipelineBuilder.build_pipeline(cfg)
            output = pipe.run([audio_path])
            
            inference_time = time.time() - start_time
            
            # æå–æ–‡æœ¬ - Pipeline è¿”å›æ ¼å¼: {file_index: {'text': ..., 'segments': [...], ...}}
            if output is None:
                return "æœªæ£€æµ‹åˆ°è¯­éŸ³å†…å®¹", f"â±ï¸ æ¨ç†æ—¶é—´: {inference_time:.2f} ç§’"
            
            results = []
            
            if isinstance(output, dict):
                # Pipeline è¿”å› {0: {'text': '...', 'segments': [...], ...}, 1: {...}, ...}
                for file_idx, file_result in output.items():
                    if isinstance(file_result, dict) and 'text' in file_result:
                        text = file_result['text']
                        if text and isinstance(text, str) and text.strip():
                            results.append(text.strip())
                    elif isinstance(file_result, str) and file_result.strip():
                        results.append(file_result.strip())
            elif isinstance(output, str) and output.strip():
                results.append(output.strip())
            elif hasattr(output, 'text'):
                results.append(str(output.text).strip())
            
            transcription = "\n".join(results) if results else "æœªæ£€æµ‹åˆ°è¯­éŸ³å†…å®¹"
            
            # è®¡ç®—éŸ³é¢‘æ—¶é•¿ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                import librosa
                duration = librosa.get_duration(path=audio_path)
                rtf = inference_time / duration  # Real-Time Factor
                time_info = f"â±ï¸ æ¨ç†æ—¶é—´: {inference_time:.2f} ç§’ | éŸ³é¢‘æ—¶é•¿: {duration:.2f} ç§’ | RTF: {rtf:.3f}x"
            except:
                time_info = f"â±ï¸ æ¨ç†æ—¶é—´: {inference_time:.2f} ç§’"
            
            return transcription, time_info
        
        else:
            # ä½¿ç”¨ç®€å•æ¨¡å‹æ¨¡å¼
            model = load_simple_model()
            transcription = model.transcribe([audio_path])
            
            inference_time = time.time() - start_time
            
            if isinstance(transcription, list) and len(transcription) > 0:
                if isinstance(transcription[0], str):
                    result = transcription[0]
                elif hasattr(transcription[0], 'text'):
                    result = transcription[0].text
                else:
                    result = str(transcription)
            else:
                result = str(transcription)
            
            try:
                import librosa
                duration = librosa.get_duration(path=audio_path)
                rtf = inference_time / duration
                time_info = f"â±ï¸ æ¨ç†æ—¶é—´: {inference_time:.2f} ç§’ | éŸ³é¢‘æ—¶é•¿: {duration:.2f} ç§’ | RTF: {rtf:.3f}x"
            except:
                time_info = f"â±ï¸ æ¨ç†æ—¶é—´: {inference_time:.2f} ç§’"
            
            return result, time_info
            
    except Exception as e:
        logger.error(f"Transcription error: {e}")
        return f"è½¬å½•é”™è¯¯: {str(e)}", ""


def transcribe_microphone(
    audio: Optional[Tuple[int, np.ndarray]],
    use_pipeline: bool = True,
    enable_pnc: bool = False,
    enable_itn: bool = False,
    att_context_size: str = "[70, 13]"
) -> Tuple[str, str]:
    """
    è½¬å½•éº¦å…‹é£å½•éŸ³
    
    Args:
        audio: éº¦å…‹é£å½•éŸ³æ•°æ® (sample_rate, audio_data)
        use_pipeline: æ˜¯å¦ä½¿ç”¨ pipeline æ¨¡å¼
        enable_pnc: æ˜¯å¦å¯ç”¨æ ‡ç‚¹å’Œå¤§å°å†™
        enable_itn: æ˜¯å¦å¯ç”¨é€†æ–‡æœ¬è§„èŒƒåŒ–
        att_context_size: æ³¨æ„åŠ›ä¸Šä¸‹æ–‡å¤§å°é…ç½®
        
    Returns:
        (è½¬å½•æ–‡æœ¬, æ¨ç†æ—¶é—´ä¿¡æ¯)
    """
    if audio is None:
        return "è¯·å½•åˆ¶éŸ³é¢‘", ""
    
    try:
        import scipy.io.wavfile as wav
        
        sample_rate, audio_data = audio
        
        # ç¡®ä¿éŸ³é¢‘æ˜¯å•å£°é“
        if len(audio_data.shape) > 1:
            audio_data = audio_data.mean(axis=1)
        
        # ç¡®ä¿æ˜¯ 16kHz
        if sample_rate != 16000:
            from scipy import signal
            num_samples = int(len(audio_data) * 16000 / sample_rate)
            audio_data = signal.resample(audio_data, num_samples)
            sample_rate = 16000
        
        # å½’ä¸€åŒ–éŸ³é¢‘
        if audio_data.dtype != np.float32:
            audio_data = audio_data.astype(np.float32)
        if np.abs(audio_data).max() > 1.0:
            audio_data = audio_data / np.abs(audio_data).max()
        
        # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
            temp_path = f.name
            wav.write(temp_path, 16000, (audio_data * 32767).astype(np.int16))
        
        # è½¬å½•
        result, time_info = transcribe_audio_file(
            temp_path, 
            use_pipeline, 
            enable_pnc, 
            enable_itn,
            att_context_size
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_path)
        
        return result, time_info
        
    except Exception as e:
        logger.error(f"Microphone transcription error: {e}")
        return f"è½¬å½•é”™è¯¯: {str(e)}", ""


def get_latency_info(att_context_size: str) -> str:
    """è·å–å»¶è¿Ÿä¿¡æ¯"""
    latency_map = {
        "[70, 0]": "Chunk size = 1 (1 Ã— 80ms = 0.08s) - æœ€ä½å»¶è¿Ÿ",
        "[70, 1]": "Chunk size = 2 (2 Ã— 80ms = 0.16s)",
        "[70, 6]": "Chunk size = 7 (7 Ã— 80ms = 0.56s)",
        "[70, 13]": "Chunk size = 14 (14 Ã— 80ms = 1.12s) - æœ€é«˜ç²¾åº¦",
    }
    return latency_map.get(att_context_size, "è‡ªå®šä¹‰é…ç½®")


# è‡ªå®šä¹‰ CSS
CUSTOM_CSS = """
.gradio-container {
    max-width: 1200px !important;
}
.title {
    text-align: center;
    margin-bottom: 20px;
}
"""

# åˆ›å»º Gradio ç•Œé¢
def create_app():
    """åˆ›å»º Gradio åº”ç”¨"""
    
    with gr.Blocks(title="NeMo Streaming ASR") as demo:
        
        gr.HTML("""
        <div style="text-align: center; margin-bottom: 10px;">
            <a href="https://www.youtube.com/@rongyikanshijie-ai" target="_blank" style="text-decoration: none; color: #ff0000;">
                ğŸ“º <strong>AI æŠ€æœ¯åˆ†äº«é¢‘é“</strong> - æ¬¢è¿è®¢é˜…æˆ‘çš„ YouTube é¢‘é“ï¼
            </a>
        </div>
        <div class="title">
            <h1>ğŸ™ï¸ NVIDIA NeMo Streaming ASR</h1>
            <p>åŸºäº Nemotron Speech Streaming 0.6B æ¨¡å‹çš„å®æ—¶è¯­éŸ³è¯†åˆ«</p>
        </div>
        """)
        
        with gr.Tabs():
            # æ–‡ä»¶ä¸Šä¼ æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“ æ–‡ä»¶ä¸Šä¼ ", id="file_upload"):
                with gr.Row():
                    with gr.Column(scale=1):
                        file_input = gr.Audio(
                            label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶",
                            type="filepath",
                            sources=["upload"],
                        )
                        
                        with gr.Accordion("âš™ï¸ é«˜çº§è®¾ç½®", open=False):
                            file_use_pipeline = gr.Checkbox(
                                label="ä½¿ç”¨ Pipeline æ¨¡å¼",
                                value=True,
                                info="Pipeline æ¨¡å¼æ”¯æŒæ›´å¤šåŠŸèƒ½ï¼ˆPnCã€ITN ç­‰ï¼‰"
                            )
                            file_enable_pnc = gr.Checkbox(
                                label="å¯ç”¨æ ‡ç‚¹å’Œå¤§å°å†™ (PnC)",
                                value=False,
                                info="è‡ªåŠ¨æ·»åŠ æ ‡ç‚¹ç¬¦å·å’Œæ­£ç¡®çš„å¤§å°å†™"
                            )
                            file_enable_itn = gr.Checkbox(
                                label="å¯ç”¨é€†æ–‡æœ¬è§„èŒƒåŒ– (ITN)",
                                value=False,
                                info="å°†å£è¯­æ•°å­—è½¬æ¢ä¸ºé˜¿æ‹‰ä¼¯æ•°å­—ç­‰"
                            )
                            file_context_size = gr.Dropdown(
                                label="å»¶è¿Ÿé…ç½® (att_context_size)",
                                choices=["[70, 0]", "[70, 1]", "[70, 6]", "[70, 13]"],
                                value="[70, 13]",
                                info="è¾ƒå¤§çš„å³ä¸Šä¸‹æ–‡æä¾›æ›´é«˜ç²¾åº¦ä½†å¢åŠ å»¶è¿Ÿ"
                            )
                            file_latency_info = gr.Textbox(
                                label="å»¶è¿Ÿä¿¡æ¯",
                                value="Chunk size = 14 (14 Ã— 80ms = 1.12s) - æœ€é«˜ç²¾åº¦",
                                interactive=False
                            )
                        
                        file_submit_btn = gr.Button("ğŸš€ å¼€å§‹è½¬å½•", variant="primary")
                    
                    with gr.Column(scale=1):
                        file_output = gr.Textbox(
                            label="è½¬å½•ç»“æœ",
                            lines=10,
                            placeholder="è½¬å½•ç»“æœå°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."
                        )
                        file_time_info = gr.Textbox(
                            label="æ¨ç†ç»Ÿè®¡",
                            interactive=False,
                            placeholder="æ¨ç†æ—¶é—´å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."
                        )
                
                # äº‹ä»¶ç»‘å®š
                file_context_size.change(
                    fn=get_latency_info,
                    inputs=[file_context_size],
                    outputs=[file_latency_info]
                )
                
                file_submit_btn.click(
                    fn=transcribe_audio_file,
                    inputs=[
                        file_input, 
                        file_use_pipeline, 
                        file_enable_pnc, 
                        file_enable_itn,
                        file_context_size
                    ],
                    outputs=[file_output, file_time_info]
                )
            
            # éº¦å…‹é£å½•éŸ³æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ¤ éº¦å…‹é£å½•éŸ³", id="microphone"):
                with gr.Row():
                    with gr.Column(scale=1):
                        mic_input = gr.Audio(
                            label="å½•åˆ¶éŸ³é¢‘",
                            type="numpy",
                            sources=["microphone"],
                        )
                        
                        with gr.Accordion("âš™ï¸ é«˜çº§è®¾ç½®", open=False):
                            mic_use_pipeline = gr.Checkbox(
                                label="ä½¿ç”¨ Pipeline æ¨¡å¼",
                                value=True,
                                info="Pipeline æ¨¡å¼æ”¯æŒæ›´å¤šåŠŸèƒ½ï¼ˆPnCã€ITN ç­‰ï¼‰"
                            )
                            mic_enable_pnc = gr.Checkbox(
                                label="å¯ç”¨æ ‡ç‚¹å’Œå¤§å°å†™ (PnC)",
                                value=False,
                                info="è‡ªåŠ¨æ·»åŠ æ ‡ç‚¹ç¬¦å·å’Œæ­£ç¡®çš„å¤§å°å†™"
                            )
                            mic_enable_itn = gr.Checkbox(
                                label="å¯ç”¨é€†æ–‡æœ¬è§„èŒƒåŒ– (ITN)",
                                value=False,
                                info="å°†å£è¯­æ•°å­—è½¬æ¢ä¸ºé˜¿æ‹‰ä¼¯æ•°å­—ç­‰"
                            )
                            mic_context_size = gr.Dropdown(
                                label="å»¶è¿Ÿé…ç½® (att_context_size)",
                                choices=["[70, 0]", "[70, 1]", "[70, 6]", "[70, 13]"],
                                value="[70, 13]",
                                info="è¾ƒå¤§çš„å³ä¸Šä¸‹æ–‡æä¾›æ›´é«˜ç²¾åº¦ä½†å¢åŠ å»¶è¿Ÿ"
                            )
                            mic_latency_info = gr.Textbox(
                                label="å»¶è¿Ÿä¿¡æ¯",
                                value="Chunk size = 14 (14 Ã— 80ms = 1.12s) - æœ€é«˜ç²¾åº¦",
                                interactive=False
                            )
                        
                        mic_submit_btn = gr.Button("ğŸš€ å¼€å§‹è½¬å½•", variant="primary")
                    
                    with gr.Column(scale=1):
                        mic_output = gr.Textbox(
                            label="è½¬å½•ç»“æœ",
                            lines=10,
                            placeholder="è½¬å½•ç»“æœå°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."
                        )
                        mic_time_info = gr.Textbox(
                            label="æ¨ç†ç»Ÿè®¡",
                            interactive=False,
                            placeholder="æ¨ç†æ—¶é—´å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."
                        )
                
                # äº‹ä»¶ç»‘å®š
                mic_context_size.change(
                    fn=get_latency_info,
                    inputs=[mic_context_size],
                    outputs=[mic_latency_info]
                )
                
                mic_submit_btn.click(
                    fn=transcribe_microphone,
                    inputs=[
                        mic_input, 
                        mic_use_pipeline, 
                        mic_enable_pnc, 
                        mic_enable_itn,
                        mic_context_size
                    ],
                    outputs=[mic_output, mic_time_info]
                )
        
        # ä½¿ç”¨è¯´æ˜
        gr.Markdown("""
### ğŸ“– ä½¿ç”¨è¯´æ˜

- **æ”¯æŒçš„éŸ³é¢‘æ ¼å¼:** WAV, MP3, FLAC, OGG ç­‰å¸¸è§æ ¼å¼
- **é‡‡æ ·ç‡è¦æ±‚:** 16kHzï¼ˆå…¶ä»–é‡‡æ ·ç‡ä¼šè‡ªåŠ¨è½¬æ¢ï¼‰
- **æœ€å°éŸ³é¢‘é•¿åº¦:** è‡³å°‘ 80ms
- **è¾“å‡º:** è‹±æ–‡æ–‡æœ¬è½¬å½•ï¼Œæ”¯æŒæ ‡ç‚¹å’Œå¤§å°å†™

#### ğŸ”§ å»¶è¿Ÿé…ç½®è¯´æ˜

| é…ç½® | Chunk å¤§å° | å»¶è¿Ÿ | è¯´æ˜ |
|:---:|:---:|:---:|:---:|
| [70, 0] | 1 å¸§ | 0.08s | æœ€ä½å»¶è¿Ÿ |
| [70, 1] | 2 å¸§ | 0.16s | ä½å»¶è¿Ÿ |
| [70, 6] | 7 å¸§ | 0.56s | å¹³è¡¡æ¨¡å¼ |
| [70, 13] | 14 å¸§ | 1.12s | æœ€é«˜ç²¾åº¦ |
        """)
        
        gr.HTML("""
        <div style="text-align: center; margin-top: 20px; color: #666;">
            <p>Powered by <a href="https://developer.nvidia.com/nemo" target="_blank">NVIDIA NeMo</a> | 
            Model: <a href="https://huggingface.co/nvidia/nemotron-speech-streaming-en-0.6b" target="_blank">Nemotron Speech Streaming 0.6B</a></p>
        </div>
        """)
    
    return demo


if __name__ == "__main__":
    # é¢„åŠ è½½æ¨¡å‹
    logger.info("é¢„åŠ è½½ ASR Pipeline...")
    try:
        load_pipeline()
        logger.info("æ¨¡å‹é¢„åŠ è½½å®Œæˆï¼")
    except Exception as e:
        logger.warning(f"é¢„åŠ è½½å¤±è´¥ï¼Œå°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åŠ è½½: {e}")
    
    # åˆ›å»ºå¹¶å¯åŠ¨åº”ç”¨
    demo = create_app()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        theme=gr.themes.Soft(),
        css=CUSTOM_CSS
    )
