#!/usr/bin/env python3
"""
Vivid-VR Gradio WebUI
åŸºäºæ­£å¸¸å·¥ä½œçš„ inference.py åˆ›å»ºçš„Webç•Œé¢
"""

import os
import gc
import sys
import cv2
import math
import torch
import torch.nn.functional as F
import numpy as np
import gradio as gr
import tempfile
import shutil
from tqdm import tqdm
from PIL import Image
from transformers import T5EncoderModel
from diffusers import CogVideoXDPMScheduler, AutoencoderKLCogVideoX, CogVideoXVividVRTransformer3DModel, CogVideoXVividVRControlNetModel, CogVideoXVividVRControlNetPipeline

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), './VRDiT')))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), './')))

from VRDiT.cogvlm2 import CogVLM2_Captioner
from VRDiT.colorfix import adaptive_instance_normalization
from VRDiT.utils import VALID_IMAGE_EXTENSIONS, VALID_VIDEO_EXTENSIONS, free_memory, load_video, export_to_video, prepare_validation_prompts, prepare_tiling_infos_generator


class VividVRModel:
    def __init__(self):
        self.pipe = None
        self.captioner_model = None
        self.text_fixer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.initialized = False
        
    def initialize_models(self, progress=gr.Progress()):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ç»„ä»¶"""
        if self.initialized:
            return "âœ… æ¨¡å‹å·²ç»åŠ è½½å®Œæˆ"
            
        try:
            progress(0.1, desc="å¼€å§‹åŠ è½½æ¨¡å‹...")
            
            # é…ç½®è·¯å¾„
            ckpt_dir = './ckpts'
            cogvideox_ckpt_path = './ckpts/CogVideoX1.5-5B'
            cogvlm2_ckpt_path = './ckpts/cogvlm2-llama3-caption'
            vividvr_ckpt_path = os.path.join(ckpt_dir, 'Vivid-VR', 'ckpts', 'Vivid-VR')
            
            progress(0.15, desc="åŠ è½½ CogVLM2 å­—å¹•ç”Ÿæˆå™¨...")
            self.captioner_model = CogVLM2_Captioner(model_path=cogvlm2_ckpt_path)
            
            progress(0.25, desc="åŠ è½½ T5 æ–‡æœ¬ç¼–ç å™¨...")
            text_encoder = T5EncoderModel.from_pretrained(
                cogvideox_ckpt_path,
                subfolder="text_encoder",
                torch_dtype=torch.bfloat16
            )
            text_encoder.requires_grad_(False)
            text_encoder.to(dtype=torch.bfloat16)
            
            progress(0.35, desc="åŠ è½½ Transformer...")
            transformer = CogVideoXVividVRTransformer3DModel.from_pretrained(
                cogvideox_ckpt_path,
                subfolder="transformer",
                torch_dtype=torch.bfloat16
            )
            transformer.requires_grad_(False)
            transformer.to(dtype=torch.bfloat16)
            
            progress(0.45, desc="åŠ è½½ VAE...")
            vae = AutoencoderKLCogVideoX.from_pretrained(
                cogvideox_ckpt_path,
                subfolder="vae"
            )
            vae.requires_grad_(False)
            vae.to(dtype=torch.bfloat16)
            vae.enable_slicing()
            vae.enable_tiling()
            
            progress(0.55, desc="åŠ è½½ ControlNet...")
            controlnet = CogVideoXVividVRControlNetModel.from_transformer(
                transformer=transformer,
                num_layers=6,
            )
            controlnet.requires_grad_(False)
            controlnet.to(dtype=torch.bfloat16)
            
            progress(0.65, desc="åŠ è½½è°ƒåº¦å™¨...")
            scheduler = CogVideoXDPMScheduler.from_pretrained(
                cogvideox_ckpt_path,
                subfolder="scheduler"
            )
            
            progress(0.75, desc="åŠ è½½ Vivid-VR æƒé‡...")
            transformer.connectors.load_state_dict(torch.load(os.path.join(vividvr_ckpt_path, "connectors.pt"), map_location='cpu'))
            transformer.control_feat_proj.load_state_dict(torch.load(os.path.join(vividvr_ckpt_path, "control_feat_proj.pt"), map_location='cpu'))
            transformer.control_patch_embed.load_state_dict(torch.load(os.path.join(vividvr_ckpt_path, "control_patch_embed.pt"), map_location='cpu'))
            
            load_model = CogVideoXVividVRControlNetModel.from_pretrained(vividvr_ckpt_path, subfolder="controlnet")
            controlnet.register_to_config(**load_model.config)
            controlnet.load_state_dict(load_model.state_dict())
            del load_model
            
            free_memory()
            
            progress(0.85, desc="åˆ›å»ºæ¨ç†ç®¡é“...")
            self.pipe = CogVideoXVividVRControlNetPipeline.from_pretrained(
                pretrained_model_name_or_path=cogvideox_ckpt_path,
                scheduler=scheduler,
                vae=vae,
                transformer=transformer,
                controlnet=controlnet,
                text_encoder=text_encoder,
                torch_dtype=torch.bfloat16,
            )
            
            progress(0.95, desc="å®Œæˆåˆå§‹åŒ–...")
            self.initialized = True
            free_memory()
            
            progress(1.0, desc="æ¨¡å‹åŠ è½½å®Œæˆï¼")
            return "âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯ä»¥å¼€å§‹è§†é¢‘å¢å¼ºï¼"
            
        except Exception as e:
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

    def process_video(
        self, 
        input_video, 
        num_inference_steps=50, 
        guidance_scale=6, 
        tile_size=128, 
        upscale=0.0,
        use_dynamic_cfg=False,
        seed=42,
        progress=gr.Progress()
    ):
        """å¤„ç†è§†é¢‘çš„ä¸»è¦å‡½æ•°"""
        if not self.initialized:
            return None, "âŒ è¯·å…ˆç‚¹å‡»'åˆå§‹åŒ–æ¨¡å‹'æŒ‰é’®åŠ è½½æ¨¡å‹"
            
        if input_video is None:
            return None, "âŒ è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶"
            
        try:
            progress(0.05, desc="å¼€å§‹å¤„ç†è§†é¢‘...")
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = tempfile.mkdtemp()
            input_path = os.path.join(temp_dir, "input_video.mp4")
            output_path = os.path.join(temp_dir, "output_video.mp4")
            
            # å¤åˆ¶è¾“å…¥è§†é¢‘åˆ°ä¸´æ—¶ç›®å½•
            shutil.copy2(input_video, input_path)
            
            progress(0.1, desc="åŠ è½½å’Œé¢„å¤„ç†è§†é¢‘...")
            # è·å–è§†é¢‘ä¿¡æ¯
            cap = cv2.VideoCapture(input_path)
            fps = cap.get(cv2.CAP_PROP_FPS) if cap.isOpened() else 24
            cap.release()
            
            # åŠ è½½è§†é¢‘
            control_video = load_video(input_path)
            
            # åº”ç”¨ç¼©æ”¾
            if upscale == 0.:
                scale_factor = 1024. / min(control_video.size()[2], control_video.size()[3])
                control_video = F.interpolate(control_video, scale_factor=scale_factor, mode='bicubic').clip(0, 1)
                height, width = control_video.size()[2], control_video.size()[3]
            elif upscale != 1.0:
                control_video = F.interpolate(control_video, scale_factor=upscale, mode='bicubic').clip(0, 1)
                height, width = control_video.size()[2], control_video.size()[3]
            else:
                height, width = control_video.size()[2], control_video.size()[3]
                
            progress(0.15, desc=f"è§†é¢‘å°ºå¯¸: {height}x{width}")
            
            # å¸§æ•°å¡«å……
            num_padding_frames = 0
            if (control_video.size(0) - 1) % 8 != 0:
                num_padding_frames = 8 - (control_video.size(0) - 1) % 8
                control_video = torch.cat([control_video, control_video[-1:].repeat(num_padding_frames, 1, 1, 1)], dim=0)
            
            # å‡†å¤‡æ¨ç†å‚æ•° - åœ¨ç”Ÿæˆå­—å¹•ä¹‹å‰å®šä¹‰ gen_height å’Œ gen_width
            vae_scale_factor_spatial = 2 ** (len(self.pipe.vae.config.block_out_channels) - 1)
            gen_height = 8 * math.ceil(height / 8) if height < tile_size * vae_scale_factor_spatial else height
            gen_width = 8 * math.ceil(width / 8) if width < tile_size * vae_scale_factor_spatial else width
            
            progress(0.25, desc="ç”Ÿæˆè§†é¢‘æè¿°...")
            # ç”Ÿæˆå­—å¹• - æŒ‰ç…§ inference.py çš„æ–¹å¼
            video_for_caption = F.interpolate(control_video, size=(gen_height, gen_width), mode='bicubic')
            prompt_list, negative_prompt_list = prepare_validation_prompts(
                video_for_caption=video_for_caption,
                video_fps=fps,
                captioner_model=self.captioner_model,
                tile_size=tile_size * vae_scale_factor_spatial,
                tile_stride=(tile_size // 2) * vae_scale_factor_spatial,
                device=self.device
            )
            
            progress(0.35, desc=f"ç”Ÿæˆçš„æè¿°: {prompt_list[0][:50] if prompt_list else 'N/A'}...")
            
            progress(0.4, desc="å¼€å§‹è§†é¢‘å¢å¼ºæ¨ç†...")
            # è¿è¡Œæ¨ç†
            self.pipe.enable_model_cpu_offload(device=self.device)
            
            result = self.pipe(
                control_video=control_video,
                prompt=prompt_list,
                negative_prompt=negative_prompt_list,
                guidance_scale=guidance_scale,
                use_dynamic_cfg=use_dynamic_cfg,
                height=gen_height,
                width=gen_width,
                num_inference_steps=num_inference_steps,
                enable_spatial_tiling=True,
                tile_size=tile_size,
                tile_stride=tile_size // 2,
                generator=torch.Generator(device=self.device).manual_seed(seed),
                output_type="np"
            )
            
            video = result.frames[0]
            progress(0.85, desc="åå¤„ç†è§†é¢‘...")
            
            # ç§»é™¤å¡«å……å¸§
            if num_padding_frames > 0:
                control_video = control_video[:-num_padding_frames]  # ç§»é™¤å¡«å……å¸§
                video = video[:-num_padding_frames]
                
            # ç§»é™¤å‰å‡ å¸§ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if video.shape[0] % 4 == 0:
                video = video[3:]
                
            progress(0.9, desc="ä¿å­˜è¾“å‡ºè§†é¢‘...")
            # ä¿å­˜è§†é¢‘ - ç›´æ¥ä¼ é€’ numpy æ•°ç»„ï¼Œå‡½æ•°ä¼šè‡ªåŠ¨å¤„ç†æ ¼å¼è½¬æ¢
            export_to_video(video, output_path, fps=fps)
            
            progress(1.0, desc="å¤„ç†å®Œæˆï¼")
            
            # æ¸…ç†å†…å­˜
            free_memory()
            
            return output_path, f"âœ… è§†é¢‘å¢å¼ºå®Œæˆï¼\\nåŸå§‹å°ºå¯¸: {control_video.shape}\\nè¾“å‡ºå°ºå¯¸: {video.shape}\\nç”Ÿæˆæè¿°: {prompt_list[0] if prompt_list else 'N/A'}"
            
        except Exception as e:
            return None, f"âŒ å¤„ç†å¤±è´¥: {str(e)}"

# å…¨å±€æ¨¡å‹å®ä¾‹
model = VividVRModel()

def update_video_preview(video_file):
    """æ›´æ–°è§†é¢‘é¢„è§ˆ"""
    if video_file is None:
        return None
    return video_file

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(title="Vivid-VR è§†é¢‘å¢å¼º", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¬ Vivid-VR è§†é¢‘å¢å¼ºç³»ç»Ÿ
        
        ä¸Šä¼ è§†é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨Vivid-VRæ¨¡å‹è¿›è¡Œæ™ºèƒ½å¢å¼ºï¼Œæå‡è§†é¢‘è´¨é‡å’Œç»†èŠ‚ã€‚
        
        **ä½¿ç”¨æ­¥éª¤ï¼š**
        1. é¦–å…ˆç‚¹å‡»"åˆå§‹åŒ–æ¨¡å‹"æŒ‰é’®åŠ è½½æ‰€æœ‰æ¨¡å‹ç»„ä»¶
        2. ä¸Šä¼ è¦å¢å¼ºçš„è§†é¢‘æ–‡ä»¶
        3. è°ƒæ•´æ¨ç†å‚æ•°ï¼ˆå¯é€‰ï¼‰
        4. ç‚¹å‡»"å¼€å§‹å¢å¼º"æŒ‰é’®å¤„ç†è§†é¢‘
        """)
        
        # æ¨¡å‹åˆå§‹åŒ–åŒºåŸŸ
        with gr.Row():
            with gr.Column():
                init_btn = gr.Button("ğŸš€ åˆå§‹åŒ–æ¨¡å‹", variant="primary", scale=2)
                init_status = gr.Textbox(
                    label="åˆå§‹åŒ–çŠ¶æ€", 
                    value="â³ ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¼€å§‹åŠ è½½æ¨¡å‹...",
                    interactive=False
                )
        
        # ä¸»è¦å¤„ç†åŒºåŸŸ
        with gr.Row():
            # å·¦ä¾§ï¼šè¾“å…¥å’Œå‚æ•°
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ è¾“å…¥è§†é¢‘")
                input_video = gr.File(
                    label="ä¸Šä¼ è§†é¢‘æ–‡ä»¶",
                    file_types=['video'],
                    type="filepath"
                )
                
                # æ·»åŠ è¾“å…¥è§†é¢‘é¢„è§ˆ
                input_video_preview = gr.Video(
                    label="è¾“å…¥è§†é¢‘é¢„è§ˆ",
                    height=300
                )
                
                gr.Markdown("### âš™ï¸ æ¨ç†å‚æ•°")
                with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                    num_inference_steps = gr.Slider(
                        minimum=1, maximum=100, value=50, step=1,
                        label="æ¨ç†æ­¥æ•° (è¶Šé«˜è´¨é‡è¶Šå¥½ï¼Œä½†è€—æ—¶æ›´é•¿)"
                    )
                    guidance_scale = gr.Slider(
                        minimum=1, maximum=20, value=6, step=1,
                        label="å¼•å¯¼å¼ºåº¦"
                    )
                    tile_size = gr.Slider(
                        minimum=64, maximum=256, value=128, step=32,
                        label="ç“¦ç‰‡å¤§å° (å½±å“å†…å­˜ä½¿ç”¨)"
                    )
                    upscale = gr.Slider(
                        minimum=0.0, maximum=2.0, value=0.0, step=0.1,
                        label="ç¼©æ”¾å€æ•° (0ä¸ºè‡ªåŠ¨ç¼©æ”¾åˆ°1024)"
                    )
                    use_dynamic_cfg = gr.Checkbox(
                        label="ä½¿ç”¨åŠ¨æ€CFG", value=False
                    )
                    seed = gr.Number(
                        label="éšæœºç§å­", value=42, precision=0
                    )
                
                process_btn = gr.Button("ğŸ¯ å¼€å§‹å¢å¼º", variant="primary", size="lg")
            
            # å³ä¾§ï¼šè¾“å‡ºå’Œé¢„è§ˆ
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“º è¾“å‡ºç»“æœ")
                output_video = gr.Video(
                    label="å¢å¼ºåçš„è§†é¢‘",
                    height=400
                )
                
                output_info = gr.Textbox(
                    label="å¤„ç†ä¿¡æ¯",
                    lines=6,
                    value="ç­‰å¾…å¤„ç†...",
                    interactive=False
                )
        
        # ç¤ºä¾‹å’Œå¸®åŠ©
        with gr.Accordion("ğŸ’¡ ä½¿ç”¨æç¤º", open=False):
            gr.Markdown("""
            **å‚æ•°è¯´æ˜ï¼š**
            - **æ¨ç†æ­¥æ•°**: æ§åˆ¶ç”Ÿæˆè´¨é‡ï¼Œæ¨èå€¼ï¼šå¿«é€Ÿæµ‹è¯•ç”¨10-20ï¼Œé«˜è´¨é‡ç”¨50-100
            - **å¼•å¯¼å¼ºåº¦**: æ§åˆ¶å¯¹åŸè§†é¢‘çš„ä¿çœŸåº¦ï¼Œé€šå¸¸6-8æ•ˆæœè¾ƒå¥½
            - **ç“¦ç‰‡å¤§å°**: å½±å“æ˜¾å­˜ä½¿ç”¨ï¼Œè¾ƒå°çš„å€¼èŠ‚çœæ˜¾å­˜ä½†å¯èƒ½å½±å“è´¨é‡
            - **ç¼©æ”¾å€æ•°**: 0è¡¨ç¤ºè‡ªåŠ¨ç¼©æ”¾çŸ­è¾¹åˆ°1024åƒç´ ï¼Œå…¶ä»–å€¼ä¸ºç›´æ¥ç¼©æ”¾å€æ•°
            
            **æ”¯æŒæ ¼å¼**: MP4, AVI, MOV, MKVç­‰å¸¸è§è§†é¢‘æ ¼å¼
            
            **ç¡¬ä»¶è¦æ±‚**: æ¨èä½¿ç”¨16GB+æ˜¾å­˜çš„GPUï¼Œè¾ƒå¤§è§†é¢‘å¯èƒ½éœ€è¦è°ƒå°ç“¦ç‰‡å¤§å°
            """)
        
        # äº‹ä»¶ç»‘å®š
        # è§†é¢‘ä¸Šä¼ é¢„è§ˆ
        input_video.change(
            fn=update_video_preview,
            inputs=input_video,
            outputs=input_video_preview
        )
        
        init_btn.click(
            fn=model.initialize_models,
            outputs=init_status,
            show_progress=True
        )
        
        process_btn.click(
            fn=model.process_video,
            inputs=[
                input_video, num_inference_steps, guidance_scale, 
                tile_size, upscale, use_dynamic_cfg, seed
            ],
            outputs=[output_video, output_info],
            show_progress=True
        )
    
    return demo

if __name__ == "__main__":
    # åˆ›å»ºç•Œé¢
    demo = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
