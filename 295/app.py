#!/usr/bin/env python3
"""
LTX-2 Gradio Web Application
æ”¯æŒ3ç§æ ¸å¿ƒè§†é¢‘ç”ŸæˆåŠŸèƒ½ï¼š
1. æ–‡æœ¬ç”Ÿæˆè§†é¢‘
2. å›¾ç‰‡ç”Ÿæˆè§†é¢‘
3. é¦–å°¾å¸§æ’å€¼
"""

import os
import logging
import gradio as gr
import torch
import gc

from ltx_pipelines.ti2vid_two_stages import TI2VidTwoStagesPipeline
from ltx_pipelines.keyframe_interpolation import KeyframeInterpolationPipeline
from ltx_pipelines.utils.media_io import encode_video
from ltx_pipelines.utils.constants import AUDIO_SAMPLE_RATE
from ltx_core.loader import LoraPathStrengthAndSDOps, LTXV_LORA_COMFY_RENAMING_MAP
from ltx_core.model.video_vae import TilingConfig, get_video_chunks_number

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== æ¨¡å‹è·¯å¾„é…ç½® ====================
CHECKPOINT_DIR = "checkpoints"
LTX2_MODEL_PATH = os.path.join(CHECKPOINT_DIR, "LTX-2/ltx-2-19b-dev-fp8.safetensors")
SPATIAL_UPSAMPLER_PATH = os.path.join(CHECKPOINT_DIR, "LTX-2/ltx-2-spatial-upscaler-x2-1.0.safetensors")
DISTILLED_LORA_PATH = os.path.join(CHECKPOINT_DIR, "LTX-2/ltx-2-19b-distilled-lora-384.safetensors")
GEMMA_ROOT = os.path.join(CHECKPOINT_DIR, "gemma-3-12b-it-qat-q4_0-unquantized")

# å…¨å±€ç®¡é“å¯¹è±¡
pipelines = {}


def get_device():
    """è·å–å¯ç”¨è®¾å¤‡"""
    return "cuda" if torch.cuda.is_available() else "cpu"


def cleanup_memory():
    """æ¸…ç†GPUæ˜¾å­˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


def cleanup_all_pipelines():
    """æ¸…ç†æ‰€æœ‰ç®¡é“"""
    global pipelines
    logger.info("æ­£åœ¨æ¸…ç†æ‰€æœ‰ç®¡é“...")
    for name in list(pipelines.keys()):
        pipeline = pipelines.pop(name)
        del pipeline
    
    for _ in range(3):
        gc.collect()
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    gc.collect()
    logger.info("æ‰€æœ‰ç®¡é“å·²æ¸…ç†")


def get_pipeline(pipeline_name):
    """è·å–æˆ–åˆå§‹åŒ–ç®¡é“"""
    global pipelines
    
    # å¦‚æœå·²åŠ è½½ï¼Œç›´æ¥è¿”å›
    if pipeline_name in pipelines:
        logger.info(f"ä½¿ç”¨å·²åŠ è½½çš„ {pipeline_name} ç®¡é“")
        return pipelines[pipeline_name]
    
    # æ¸…ç†å…¶ä»–ç®¡é“
    if len(pipelines) > 0:
        logger.info(f"åˆ‡æ¢ç®¡é“ï¼Œæ¸…ç†å·²åŠ è½½: {list(pipelines.keys())}")
        cleanup_all_pipelines()
    
    logger.info(f"æ­£åœ¨åˆå§‹åŒ– {pipeline_name} ç®¡é“...")
    
    # åˆ›å»º LoRA é…ç½®
    distilled_lora = [
        LoraPathStrengthAndSDOps(
            path=DISTILLED_LORA_PATH,
            strength=1.0,
            sd_ops=LTXV_LORA_COMFY_RENAMING_MAP
        )
    ]
    
    if pipeline_name == "ti2vid_two_stages":
        pipeline = TI2VidTwoStagesPipeline(
            checkpoint_path=LTX2_MODEL_PATH,
            distilled_lora=distilled_lora,
            spatial_upsampler_path=SPATIAL_UPSAMPLER_PATH,
            gemma_root=GEMMA_ROOT,
            loras=[],
            device=get_device(),
            fp8transformer=True,
        )
    elif pipeline_name == "keyframe_interpolation":
        pipeline = KeyframeInterpolationPipeline(
            checkpoint_path=LTX2_MODEL_PATH,
            distilled_lora=distilled_lora,
            spatial_upsampler_path=SPATIAL_UPSAMPLER_PATH,
            gemma_root=GEMMA_ROOT,
            loras=[],
            device=get_device(),
            fp8transformer=True,
        )
    else:
        raise ValueError(f"æœªçŸ¥çš„ç®¡é“: {pipeline_name}")
    
    pipelines[pipeline_name] = pipeline
    logger.info(f"{pipeline_name} ç®¡é“åˆå§‹åŒ–æˆåŠŸ")
    return pipeline


# ==================== ç”Ÿæˆå‡½æ•° ====================

def generate_text_to_video(prompt, negative_prompt, seed, height, width, num_frames,
                           frame_rate, num_inference_steps, cfg_guidance_scale,
                           enhance_prompt, progress=gr.Progress()):
    """æ–‡æœ¬ç”Ÿæˆè§†é¢‘"""
    try:
        progress(0, desc="æ­£åœ¨åˆå§‹åŒ–ç®¡é“...")
        pipeline = get_pipeline("ti2vid_two_stages")
        
        # ä½¿ç”¨ tiling_config æ”¯æŒå¤§åˆ†è¾¨ç‡è§†é¢‘è§£ç 
        tiling_config = TilingConfig.default()
        video_chunks_number = get_video_chunks_number(int(num_frames), tiling_config)
        
        progress(0.1, desc="æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
        video, audio = pipeline(
            prompt=prompt,
            negative_prompt=negative_prompt,
            seed=int(seed),
            height=int(height),
            width=int(width),
            num_frames=int(num_frames),
            frame_rate=float(frame_rate),
            num_inference_steps=int(num_inference_steps),
            cfg_guidance_scale=float(cfg_guidance_scale),
            images=[],  # æ— å›¾ç‰‡è¾“å…¥
            enhance_prompt=enhance_prompt,
            tiling_config=tiling_config,
        )
        
        progress(0.9, desc="æ­£åœ¨ä¿å­˜è§†é¢‘...")
        output_path = f"/tmp/ltx2_t2v_{int(seed)}.mp4"
        with torch.inference_mode():
            encode_video(
                video=video,
                fps=float(frame_rate),
                audio=audio,
                audio_sample_rate=AUDIO_SAMPLE_RATE,
                output_path=output_path,
                video_chunks_number=video_chunks_number,
            )
        
        progress(1.0, desc="å®Œæˆ!")
        cleanup_memory()
        return output_path
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_image_to_video(input_image, image_strength, prompt, negative_prompt, seed, 
                            height, width, num_frames, frame_rate, num_inference_steps, 
                            cfg_guidance_scale, enhance_prompt, progress=gr.Progress()):
    """å›¾ç‰‡ç”Ÿæˆè§†é¢‘"""
    try:
        if input_image is None:
            raise ValueError("è¯·ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ï¼")
        
        progress(0, desc="æ­£åœ¨åˆå§‹åŒ–ç®¡é“...")
        pipeline = get_pipeline("ti2vid_two_stages")
        
        # ä½¿ç”¨ tiling_config æ”¯æŒå¤§åˆ†è¾¨ç‡è§†é¢‘è§£ç 
        tiling_config = TilingConfig.default()
        video_chunks_number = get_video_chunks_number(int(num_frames), tiling_config)
        
        # æ„å»ºå›¾ç‰‡æ¡ä»¶ï¼š(å›¾ç‰‡è·¯å¾„, å¸§ç´¢å¼•, å¼ºåº¦)
        images = [(input_image, 0, float(image_strength))]
        
        progress(0.1, desc="æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
        video, audio = pipeline(
            prompt=prompt,
            negative_prompt=negative_prompt,
            seed=int(seed),
            height=int(height),
            width=int(width),
            num_frames=int(num_frames),
            frame_rate=float(frame_rate),
            num_inference_steps=int(num_inference_steps),
            cfg_guidance_scale=float(cfg_guidance_scale),
            images=images,
            enhance_prompt=enhance_prompt,
            tiling_config=tiling_config,
        )
        
        progress(0.9, desc="æ­£åœ¨ä¿å­˜è§†é¢‘...")
        output_path = f"/tmp/ltx2_i2v_{int(seed)}.mp4"
        with torch.inference_mode():
            encode_video(
                video=video,
                fps=float(frame_rate),
                audio=audio,
                audio_sample_rate=AUDIO_SAMPLE_RATE,
                output_path=output_path,
                video_chunks_number=video_chunks_number,
            )
        
        progress(1.0, desc="å®Œæˆ!")
        cleanup_memory()
        return output_path
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_keyframe_interpolation(start_image, end_image, start_strength, end_strength,
                                    prompt, negative_prompt, seed, height, width, num_frames,
                                    frame_rate, num_inference_steps, cfg_guidance_scale,
                                    enhance_prompt, progress=gr.Progress()):
    """é¦–å°¾å¸§æ’å€¼"""
    try:
        if start_image is None or end_image is None:
            raise ValueError("è¯·ä¸Šä¼ èµ·å§‹å¸§å’Œç»“æŸå¸§å›¾ç‰‡ï¼")
        
        progress(0, desc="æ­£åœ¨åˆå§‹åŒ–ç®¡é“...")
        pipeline = get_pipeline("keyframe_interpolation")
        
        # ä½¿ç”¨ tiling_config æ”¯æŒå¤§åˆ†è¾¨ç‡è§†é¢‘è§£ç 
        tiling_config = TilingConfig.default()
        video_chunks_number = get_video_chunks_number(int(num_frames), tiling_config)
        
        # æ„å»ºå›¾ç‰‡åˆ—è¡¨: (è·¯å¾„, å¸§ç´¢å¼•, å¼ºåº¦)
        images = [
            (start_image, 0, float(start_strength)),
            (end_image, int(num_frames) - 1, float(end_strength)),
        ]
        
        progress(0.1, desc="æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
        video, audio = pipeline(
            prompt=prompt,
            negative_prompt=negative_prompt,
            seed=int(seed),
            height=int(height),
            width=int(width),
            num_frames=int(num_frames),
            frame_rate=float(frame_rate),
            num_inference_steps=int(num_inference_steps),
            cfg_guidance_scale=float(cfg_guidance_scale),
            images=images,
            enhance_prompt=enhance_prompt,
            tiling_config=tiling_config,
        )
        
        progress(0.9, desc="æ­£åœ¨ä¿å­˜è§†é¢‘...")
        output_path = f"/tmp/ltx2_keyframe_{int(seed)}.mp4"
        with torch.inference_mode():
            encode_video(
                video=video,
                fps=float(frame_rate),
                audio=audio,
                audio_sample_rate=AUDIO_SAMPLE_RATE,
                output_path=output_path,
                video_chunks_number=video_chunks_number,
            )
        
        progress(1.0, desc="å®Œæˆ!")
        cleanup_memory()
        return output_path
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


# ==================== Gradio ç•Œé¢ ====================

def create_demo():
    """åˆ›å»ºGradioç•Œé¢"""
    with gr.Blocks(title="LTX-2 è§†é¢‘ç”Ÿæˆ", theme=gr.themes.Soft()) as demo:
        # é¡¶éƒ¨ä¿¡æ¯
        gr.Markdown("""
        # ğŸ¬ LTX-2 è§†é¢‘ç”Ÿæˆå¹³å°
        ### ğŸ“º [AI æŠ€æœ¯åˆ†äº«é¢‘é“](https://www.youtube.com/@rongyikanshijie-ai)
        æ¬¢è¿è®¢é˜…æˆ‘çš„YouTubeé¢‘é“ï¼Œè·å–æ›´å¤šAIæŠ€æœ¯æ•™ç¨‹å’Œåˆ†äº«ï¼
        
        ---
        **æ”¯æŒåŠŸèƒ½**: æ–‡æœ¬ç”Ÿæˆè§†é¢‘ | å›¾ç‰‡ç”Ÿæˆè§†é¢‘ | é¦–å°¾å¸§æ’å€¼
        """)
        
        with gr.Tabs():
            # ==================== Tab 1: æ–‡æœ¬ç”Ÿæˆè§†é¢‘ ====================
            with gr.Tab("ğŸ“ æ–‡æœ¬ç”Ÿæˆè§†é¢‘"):
                gr.Markdown("""
                ### ä»æ–‡å­—æè¿°ç”Ÿæˆè§†é¢‘å’ŒåŒæ­¥éŸ³é¢‘
                è¾“å…¥è¯¦ç»†çš„åœºæ™¯æè¿°ï¼ŒAI ä¼šç”Ÿæˆå¯¹åº”çš„è§†é¢‘å†…å®¹ã€‚
                """)
                
                with gr.Row():
                    with gr.Column(scale=2):
                        t2v_prompt = gr.Textbox(
                            label="æç¤ºè¯ (Prompt)",
                            placeholder="æè¿°ä½ æƒ³è¦ç”Ÿæˆçš„è§†é¢‘å†…å®¹...",
                            lines=4,
                            value="A serene lake surrounded by mountains at sunset, with reflections on the water and birds flying across the sky"
                        )
                        t2v_negative_prompt = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯",
                            placeholder="ä¸æƒ³å‡ºç°çš„å†…å®¹...",
                            lines=2,
                            value="blurry, low quality, distorted, watermark"
                        )
                        t2v_enhance_prompt = gr.Checkbox(label="âœ¨ å¢å¼ºæç¤ºè¯ (AIä¼˜åŒ–)", value=False)
                        
                    with gr.Column(scale=1):
                        t2v_seed = gr.Number(label="éšæœºç§å­", value=42, precision=0)
                        t2v_cfg_scale = gr.Slider(label="CFGå¼•å¯¼å¼ºåº¦", minimum=1.0, maximum=20.0, value=7.5, step=0.5)
                        t2v_steps = gr.Slider(label="æ¨ç†æ­¥æ•°", minimum=10, maximum=50, value=40, step=1)
                
                with gr.Row():
                    t2v_width = gr.Slider(label="è¾“å‡ºå®½åº¦", minimum=512, maximum=1024, value=768, step=64,
                                         info="æœ€ç»ˆè§†é¢‘å®½åº¦(Stage1ç”Ÿæˆä¸€åŠ,Stage2ä¸Šé‡‡æ ·)")
                    t2v_height = gr.Slider(label="è¾“å‡ºé«˜åº¦", minimum=512, maximum=1024, value=768, step=64,
                                          info="æœ€ç»ˆè§†é¢‘é«˜åº¦(Stage1ç”Ÿæˆä¸€åŠ,Stage2ä¸Šé‡‡æ ·)")
                    t2v_num_frames = gr.Slider(label="å¸§æ•° (1+8k)", minimum=17, maximum=257, value=65, step=8, 
                                               info="å¸§æ•°å¿…é¡»æ˜¯1+8kæ ¼å¼: 17,25,33...257")
                    t2v_fps = gr.Slider(label="å¸§ç‡ (FPS)", minimum=8, maximum=30, value=24, step=1)
                
                t2v_generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
                
                with gr.Row():
                    with gr.Column(scale=2):
                        t2v_output = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘", height=400)
                    with gr.Column(scale=1):
                        gr.Markdown("""
                        ### ğŸ“Œ æç¤º
                        - ç‚¹å‡»å³ä¸Šè§’ä¸‹è½½æŒ‰é’®è·å–åŸå§‹åˆ†è¾¨ç‡è§†é¢‘
                        - é¢„è§ˆå¯èƒ½æ˜¾å¾—æ¨¡ç³Šï¼Œå®é™…è§†é¢‘æ˜¯é«˜æ¸…çš„
                        - å»ºè®®åˆ†è¾¨ç‡ï¼š768Ã—768 æˆ– 1024Ã—1024
                        """)
                
                t2v_generate_btn.click(
                    fn=generate_text_to_video,
                    inputs=[t2v_prompt, t2v_negative_prompt, t2v_seed, t2v_height, t2v_width,
                           t2v_num_frames, t2v_fps, t2v_steps, t2v_cfg_scale, t2v_enhance_prompt],
                    outputs=t2v_output
                )
            
            # ==================== Tab 2: å›¾ç‰‡ç”Ÿæˆè§†é¢‘ ====================
            with gr.Tab("ğŸ–¼ï¸ å›¾ç‰‡ç”Ÿæˆè§†é¢‘"):
                gr.Markdown("""
                ### ä»ä¸€å¼ å›¾ç‰‡ç”Ÿæˆè§†é¢‘
                ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ä½œä¸ºè§†é¢‘çš„ç¬¬ä¸€å¸§ï¼ŒAI ä¼šåŸºäºå›¾ç‰‡å†…å®¹ç”ŸæˆåŠ¨æ€è§†é¢‘ã€‚
                """)
                
                with gr.Row():
                    with gr.Column(scale=1):
                        i2v_image = gr.Image(
                            label="ğŸ“· ä¸Šä¼ å›¾ç‰‡ (ä½œä¸ºç¬¬ä¸€å¸§)",
                            type="filepath",
                            height=300,
                        )
                        i2v_strength = gr.Slider(
                            label="å›¾ç‰‡å¼ºåº¦", 
                            minimum=0.1, maximum=1.0, value=1.0, step=0.1,
                            info="æ§åˆ¶å›¾ç‰‡å¯¹ç”Ÿæˆç»“æœçš„å½±å“ç¨‹åº¦"
                        )
                    
                    with gr.Column(scale=1):
                        i2v_prompt = gr.Textbox(
                            label="æç¤ºè¯ (æè¿°åŠ¨ä½œ/å˜åŒ–)",
                            placeholder="æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹åº”è¯¥å¦‚ä½•è¿åŠ¨...",
                            lines=4,
                            value="A beautiful anime elf girl with long flowing silver-white hair and sparkling purple eyes. She looks at the camera, smiles warmly, and says \"Hello LTX 2\" with clear, gentle feminine voice. Her hair gently sways in a soft magical breeze. She blinks slowly and tilts her head. Soft wind chimes ring gently in the background, accompanied by ambient forest sounds. Dreamy purple gradient background with floating sparkles. Soft ethereal lighting, smooth high-quality anime animation, cinematic composition. Camera slowly pushes in."
                        )
                        i2v_negative_prompt = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯",
                            lines=2,
                            value="blurry, low quality, distorted, jerky motion, static, frozen"
                        )
                        i2v_enhance_prompt = gr.Checkbox(label="âœ¨ å¢å¼ºæç¤ºè¯", value=False)
                        i2v_seed = gr.Number(label="éšæœºç§å­", value=123, precision=0)
                
                with gr.Row():
                    i2v_cfg_scale = gr.Slider(label="CFGå¼•å¯¼å¼ºåº¦", minimum=1.0, maximum=20.0, value=7.5, step=0.5)
                    i2v_steps = gr.Slider(label="æ¨ç†æ­¥æ•°", minimum=10, maximum=50, value=40, step=1)
                
                with gr.Row():
                    i2v_width = gr.Slider(label="è¾“å‡ºå®½åº¦", minimum=512, maximum=1024, value=768, step=64,
                                         info="æœ€ç»ˆè§†é¢‘å®½åº¦")
                    i2v_height = gr.Slider(label="è¾“å‡ºé«˜åº¦", minimum=512, maximum=1024, value=768, step=64,
                                          info="æœ€ç»ˆè§†é¢‘é«˜åº¦")
                    i2v_num_frames = gr.Slider(label="å¸§æ•° (1+8k)", minimum=17, maximum=257, value=65, step=8)
                    i2v_fps = gr.Slider(label="å¸§ç‡ (FPS)", minimum=8, maximum=30, value=24, step=1)
                
                i2v_generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆè§†é¢‘", variant="primary", size="lg")
                
                with gr.Row():
                    with gr.Column(scale=2):
                        i2v_output = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘", height=400)
                    with gr.Column(scale=1):
                        gr.Markdown("""
                        ### ğŸ“Œ æç¤º
                        - ç‚¹å‡»å³ä¸Šè§’ä¸‹è½½æŒ‰é’®è·å–é«˜æ¸…è§†é¢‘
                        - å›¾ç‰‡ä¼šä½œä¸ºè§†é¢‘ç¬¬ä¸€å¸§
                        - æç¤ºè¯æè¿°åŠ¨ä½œå’Œå˜åŒ–
                        """)
                
                i2v_generate_btn.click(
                    fn=generate_image_to_video,
                    inputs=[i2v_image, i2v_strength, i2v_prompt, i2v_negative_prompt, i2v_seed,
                           i2v_height, i2v_width, i2v_num_frames, i2v_fps, i2v_steps,
                           i2v_cfg_scale, i2v_enhance_prompt],
                    outputs=i2v_output
                )
            
            # ==================== Tab 3: é¦–å°¾å¸§æ’å€¼ ====================
            with gr.Tab("ğŸï¸ é¦–å°¾å¸§æ’å€¼"):
                gr.Markdown("""
                ### åœ¨ä¸¤å¼ å›¾ç‰‡ä¹‹é—´ç”Ÿæˆè¿‡æ¸¡åŠ¨ç”»
                ä¸Šä¼ èµ·å§‹å¸§å’Œç»“æŸå¸§å›¾ç‰‡ï¼ŒAI ä¼šç”Ÿæˆå®ƒä»¬ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡è§†é¢‘ã€‚
                """)
                
                with gr.Row():
                    with gr.Column():
                        kf_start_image = gr.Image(
                            label="ğŸ–¼ï¸ èµ·å§‹å¸§å›¾ç‰‡",
                            type="filepath",
                            height=256,
                        )
                        kf_start_strength = gr.Slider(
                            label="èµ·å§‹å¸§å¼ºåº¦", 
                            minimum=0.1, maximum=1.0, value=1.0, step=0.1
                        )
                    with gr.Column():
                        kf_end_image = gr.Image(
                            label="ğŸ–¼ï¸ ç»“æŸå¸§å›¾ç‰‡",
                            type="filepath",
                            height=256,
                        )
                        kf_end_strength = gr.Slider(
                            label="ç»“æŸå¸§å¼ºåº¦", 
                            minimum=0.1, maximum=1.0, value=1.0, step=0.1
                        )
                
                with gr.Row():
                    with gr.Column():
                        kf_prompt = gr.Textbox(
                            label="æç¤ºè¯ (æè¿°è¿‡æ¸¡æ•ˆæœ)",
                            placeholder="æè¿°ä¸¤å¼ å›¾ç‰‡ä¹‹é—´çš„è¿‡æ¸¡æ–¹å¼...",
                            lines=3,
                            value="Smooth cinematic transition with natural lighting changes"
                        )
                        kf_negative_prompt = gr.Textbox(
                            label="è´Ÿé¢æç¤ºè¯",
                            lines=2,
                            value="blurry, low quality, jerky motion, flickering"
                        )
                        kf_enhance_prompt = gr.Checkbox(label="âœ¨ å¢å¼ºæç¤ºè¯", value=False)
                        
                    with gr.Column():
                        kf_seed = gr.Number(label="éšæœºç§å­", value=789, precision=0)
                        kf_cfg_scale = gr.Slider(label="CFGå¼•å¯¼å¼ºåº¦", minimum=1.0, maximum=20.0, value=7.5, step=0.5)
                        kf_steps = gr.Slider(label="æ¨ç†æ­¥æ•°", minimum=10, maximum=50, value=40, step=1)
                
                with gr.Row():
                    kf_width = gr.Slider(label="è¾“å‡ºå®½åº¦", minimum=512, maximum=1024, value=768, step=64,
                                        info="æœ€ç»ˆè§†é¢‘å®½åº¦")
                    kf_height = gr.Slider(label="è¾“å‡ºé«˜åº¦", minimum=512, maximum=1024, value=768, step=64,
                                         info="æœ€ç»ˆè§†é¢‘é«˜åº¦")
                    kf_num_frames = gr.Slider(label="å¸§æ•° (1+8k)", minimum=17, maximum=257, value=65, step=8)
                    kf_fps = gr.Slider(label="å¸§ç‡ (FPS)", minimum=8, maximum=30, value=24, step=1)
                
                kf_generate_btn = gr.Button("ğŸ¬ ç”Ÿæˆæ’å€¼è§†é¢‘", variant="primary", size="lg")
                
                with gr.Row():
                    with gr.Column(scale=2):
                        kf_output = gr.Video(label="ç”Ÿæˆçš„è§†é¢‘", height=400)
                    with gr.Column(scale=1):
                        gr.Markdown("""
                        ### ğŸ“Œ æç¤º
                        - ç‚¹å‡»å³ä¸Šè§’ä¸‹è½½æŒ‰é’®è·å–é«˜æ¸…è§†é¢‘
                        - èµ·å§‹å¸§â†’ç»“æŸå¸§çš„å¹³æ»‘è¿‡æ¸¡
                        - æç¤ºè¯æè¿°è¿‡æ¸¡æ•ˆæœ
                        """)
                
                kf_generate_btn.click(
                    fn=generate_keyframe_interpolation,
                    inputs=[kf_start_image, kf_end_image, kf_start_strength, kf_end_strength,
                           kf_prompt, kf_negative_prompt, kf_seed, kf_height, kf_width,
                           kf_num_frames, kf_fps, kf_steps, kf_cfg_scale, kf_enhance_prompt],
                    outputs=kf_output
                )
        
        # åº•éƒ¨è¯´æ˜
        gr.Markdown("""
        ---
        ### ğŸ’¡ æç¤ºè¯ç¼–å†™å»ºè®®
        - ä»ä¸»è¦åŠ¨ä½œå¼€å§‹ï¼Œç”¨ä¸€å¥è¯æè¿°
        - æ·»åŠ å…·ä½“çš„åŠ¨ä½œå’Œæ‰‹åŠ¿ç»†èŠ‚
        - ç²¾ç¡®æè¿°è§’è‰²/ç‰©ä½“å¤–è§‚
        - åŒ…å«èƒŒæ™¯å’Œç¯å¢ƒç»†èŠ‚
        - æŒ‡å®šç›¸æœºè§’åº¦å’Œè¿åŠ¨
        - æè¿°å…‰ç…§å’Œè‰²å½©
        - ä¿æŒåœ¨200å­—ä»¥å†…
        
        ### ğŸ“Š æŠ€æœ¯è§„æ ¼
        - **å¸§æ•°**: å¿…é¡»æ˜¯ 1+8k æ ¼å¼ (17, 25, 33, 41, 49, 57, 65, 73, 81, 89, 97, 105, 113, 121, 129, ..., 257)
        - **åˆ†è¾¨ç‡**: å®½é«˜å¿…é¡»æ˜¯ 32 çš„å€æ•°
        - **æœ€å¤§æ—¶é•¿**: 257å¸§ â‰ˆ 10.7ç§’ @24fps
        
        ### ğŸ“š æ›´å¤šä¿¡æ¯
        - [LTX-2 å®˜æ–¹æ–‡æ¡£](https://github.com/Lightricks/LTX-2)
        - [HuggingFace æ¨¡å‹](https://huggingface.co/Lightricks/LTX-2)
        - [æç¤ºè¯ç¼–å†™æŒ‡å—](https://ltx.video/blog/how-to-prompt-for-ltx-2)
        """)
    
    return demo


if __name__ == "__main__":
    demo = create_demo()
    demo.queue()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
